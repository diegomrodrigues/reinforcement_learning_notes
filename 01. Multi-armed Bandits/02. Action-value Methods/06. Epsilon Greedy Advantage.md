## Action-Value Methods e a Converg√™ncia em M√©todos Œµ-Gananciosos

### Introdu√ß√£o

A modelagem de processos de tomada de decis√£o sequenciais, em particular atrav√©s de *Reinforcement Learning*, apresenta um desafio fundamental: como equilibrar a explora√ß√£o do ambiente com a explota√ß√£o do conhecimento j√° adquirido [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). M√©todos baseados em valores de a√ß√£o (*action-value methods*) s√£o uma abordagem comum nesse cen√°rio, visando estimar o valor esperado de cada a√ß√£o em um determinado estado. O problema do *k-armed bandit* serve como um modelo simplificado para explorar esses conceitos, onde um agente deve repetidamente escolher entre $k$ a√ß√µes, cada uma fornecendo uma recompensa estoc√°stica [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1), [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Este cap√≠tulo ir√° se aprofundar em um aspecto particular do m√©todo Œµ-ganancioso (*Œµ-greedy method*): sua garantia de converg√™ncia dos valores de a√ß√£o devido √† amostragem infinita de todas as a√ß√µes.

### Conceitos Fundamentais

**Action-value Methods:** Estes m√©todos estimam o valor de uma a√ß√£o $a$ no tempo $t$, denotado por $Q_t(a)$, atrav√©s da m√©dia das recompensas recebidas quando a a√ß√£o $a$ foi selecionada at√© o tempo $t$. Formalmente, [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3)
$$ Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}} $$

Onde $R_i$ √© a recompensa obtida no tempo $i$, $A_i$ √© a a√ß√£o selecionada no tempo $i$, e $\mathbb{1}_{A_i=a}$ √© uma fun√ß√£o indicadora que √© 1 se $A_i = a$ e 0 caso contr√°rio. Caso o denominador seja zero, $Q_t(a)$ √© definido como um valor default como zero [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3).

> üí° **Exemplo Num√©rico:** Considere um problema de 3-armed bandit (k=3) onde as a√ß√µes s√£o indexadas por 1, 2 e 3. Suponha que nas primeiras itera√ß√µes, as seguintes a√ß√µes e recompensas foram observadas:
>
> | Tempo (t) | A√ß√£o (A\_t) | Recompensa (R\_t) |
> |-----------|-------------|-------------------|
> | 1         | 1           | 0               |
> | 2         | 2           | 1               |
> | 3         | 1           | 0               |
> | 4         | 3           | -1              |
> | 5         | 2           | 2               |
>
> Vamos calcular os valores de a√ß√£o $Q_t(a)$ para cada a√ß√£o no tempo t=6.
>
> - Para a a√ß√£o 1: $Q_6(1) = \frac{0 + 0}{2} = 0$
> - Para a a√ß√£o 2: $Q_6(2) = \frac{1 + 2}{2} = 1.5$
> - Para a a√ß√£o 3: $Q_6(3) = \frac{-1}{1} = -1$
>
> O valor de $Q_6(1)$ √© a m√©dia das recompensas obtidas quando a a√ß√£o 1 foi selecionada (tempos 1 e 3), $Q_6(2)$ √© a m√©dia das recompensas obtidas quando a a√ß√£o 2 foi selecionada (tempos 2 e 5), e $Q_6(3)$ √© a m√©dia das recompensas obtidas quando a a√ß√£o 3 foi selecionada (tempo 4).

```mermaid
graph LR
    A[Tempo "t"] --> B("A√ß√£o \"a\" selecionada");
    B --> C{"Recompensa \"R\" obtida"};
    C --> D{"Atualizar Q_t(a)"};
    D --> E("Q_t(a) = M√©dia(Recompensas)");
     style E fill:#f9f,stroke:#333,stroke-width:2px
```

**Greedy Action Selection:** √â a escolha da a√ß√£o que maximiza o valor estimado. Ou seja,
$A_t = \text{argmax}_a \, Q_t(a)$ [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). Este m√©todo explora o que j√° foi aprendido, mas n√£o garante a identifica√ß√£o da a√ß√£o √≥tima.

> üí° **Exemplo Num√©rico:** Usando os valores de a√ß√£o calculados no exemplo anterior para t=6, $Q_6(1) = 0$, $Q_6(2) = 1.5$, e $Q_6(3) = -1$. A a√ß√£o gananciosa seria a a√ß√£o 2, j√° que $1.5$ √© o maior valor. Portanto, $A_6 = 2$.

```mermaid
graph LR
    A["Valores Q_t(a)"] --> B("argmax Q_t(a)");
    B --> C["A√ß√£o A_t (Greedy)"];
     style C fill:#ccf,stroke:#333,stroke-width:2px
```

**Œµ-Greedy Action Selection:** Este m√©todo melhora o anterior adicionando um componente de explora√ß√£o: com probabilidade $1-\epsilon$, a a√ß√£o gananciosa √© selecionada; e com probabilidade $\epsilon$, uma a√ß√£o √© selecionada aleatoriamente [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). Matematicamente,
$$ A_t = \begin{cases}
\text{argmax}_a \, Q_t(a) & \text{com probabilidade } 1-\epsilon \\
\text{a√ß√£o aleat√≥ria} & \text{com probabilidade } \epsilon
\end{cases} $$

A introdu√ß√£o de $\epsilon$ garante que todas as a√ß√µes sejam amostradas, mesmo aquelas que n√£o parecem ser as melhores, e √© esse ponto que suporta a converg√™ncia dos valores de a√ß√£o.

> üí° **Exemplo Num√©rico:** Suponha que estamos utilizando o m√©todo Œµ-ganancioso com $\epsilon = 0.1$. Usando os valores de a√ß√£o do exemplo anterior ($Q_6(1) = 0$, $Q_6(2) = 1.5$, $Q_6(3) = -1$), a a√ß√£o gananciosa √© a a√ß√£o 2.
>
> - Com probabilidade $1 - \epsilon = 0.9$, escolhemos a a√ß√£o gananciosa, que √© a a√ß√£o 2.
> - Com probabilidade $\epsilon = 0.1$, escolhemos uma a√ß√£o aleatoriamente entre 1, 2 e 3. Cada uma tem probabilidade $\frac{0.1}{3} \approx 0.033$.
>
>  Portanto, a a√ß√£o 2 ser√° selecionada com maior probabilidade (0.9 + 0.033), mas as a√ß√µes 1 e 3 tamb√©m ser√£o selecionadas ocasionalmente.
```mermaid
graph LR
    A["Valores Q_t(a), Œµ"] --> B{"Rand < (1-Œµ)?"};
    B -- "Sim" --> C("A_t = argmax(Q_t(a))");
    B -- "N√£o" --> D("A_t = A√ß√£o aleat√≥ria");
    C --> E["A√ß√£o A_t selecionada"];
    D --> E
     style E fill:#ccf,stroke:#333,stroke-width:2px
```

**Lema 1.1: Probabilidade de Sele√ß√£o de A√ß√£o com Œµ-Greedy**
*Declara√ß√£o:* Em um ambiente *k-armed bandit*, utilizando sele√ß√£o de a√ß√£o Œµ-gananciosa, a probabilidade de selecionar qualquer a√ß√£o $a$ no tempo $t$ √© dada por:
$$P(A_t = a) = \begin{cases}
\frac{\epsilon}{k} + (1-\epsilon)  & \text{se } a = \text{argmax}_{a'} \, Q_t(a') \\
\frac{\epsilon}{k} & \text{caso contr√°rio}
\end{cases}$$
*Prova:* A probabilidade de selecionar uma a√ß√£o aleatoriamente √© $\frac{\epsilon}{k}$ para todas as a√ß√µes. Se a a√ß√£o $a$ √© a a√ß√£o gananciosa (i.e., $\text{argmax}_{a'} \, Q_t(a')$), ela tamb√©m √© selecionada com probabilidade $1-\epsilon$. Somando as probabilidades, chegamos no resultado. $\blacksquare$

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior (k=3, $\epsilon=0.1$), a probabilidade de selecionar a a√ß√£o 2 (que √© a a√ß√£o gananciosa) √©:
>
> $P(A_t = 2) = \frac{0.1}{3} + (1 - 0.1) = \frac{0.1}{3} + 0.9 \approx 0.033 + 0.9 = 0.933$
>
> A probabilidade de selecionar qualquer uma das outras a√ß√µes (1 ou 3) √©:
>
> $P(A_t = 1) = P(A_t = 3) = \frac{0.1}{3} \approx 0.033$
>
> Observe que a soma das probabilidades √© $0.933 + 0.033 + 0.033 \approx 1.0$.

**Lemma 1: Amostragem Infinita em Œµ-Greedy**
*Declara√ß√£o:* Em um ambiente *k-armed bandit*, com sele√ß√£o de a√ß√£o Œµ-gananciosa ($\epsilon > 0$), todas as a√ß√µes ser√£o selecionadas um n√∫mero infinito de vezes √† medida que o n√∫mero de passos $t$ tende ao infinito.
*Prova:* Seja $N_t(a)$ o n√∫mero de vezes que uma a√ß√£o $a$ foi selecionada at√© o tempo $t$. Com probabilidade $\epsilon$, uma a√ß√£o √© selecionada aleatoriamente com probabilidade $\frac{1}{k}$. Portanto, a probabilidade de selecionar qualquer a√ß√£o em um dado tempo $t$ √© pelo menos $\frac{\epsilon}{k}$. Como $\epsilon$ e $k$ s√£o constantes, ent√£o essa probabilidade √© sempre positiva. Assim, o valor esperado do n√∫mero de sele√ß√µes $N_t(a)$ aumenta indefinidamente, e no limite $t \to \infty$ temos $N_t(a) \to \infty$. $\blacksquare$

```mermaid
graph LR
    A["Œµ-greedy"] --> B{"P(A_t = a) >= Œµ/k"};
    B --> C{"P(amostragem) > 0"};
    C --> D{"N_t(a) ‚Üí ‚àû quando t ‚Üí ‚àû"};
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

**Lema 1.2: Limite Inferior da Amostragem de A√ß√µes**
*Declara√ß√£o:* Em um ambiente *k-armed bandit*, com sele√ß√£o de a√ß√£o Œµ-gananciosa ($\epsilon > 0$), o n√∫mero esperado de vezes que cada a√ß√£o $a$ √© selecionada no tempo $t$, denotado por $\mathbb{E}[N_t(a)]$, satisfaz $\mathbb{E}[N_t(a)] \geq \frac{\epsilon}{k}t$.
*Prova:* Seja $I_t(a)$ uma vari√°vel indicadora que √© 1 se a a√ß√£o $a$ foi selecionada no tempo $t$, e 0 caso contr√°rio. Ent√£o, $N_t(a) = \sum_{i=1}^t I_i(a)$.  Pela defini√ß√£o do m√©todo Œµ-ganancioso, a probabilidade de selecionar a a√ß√£o $a$ em qualquer tempo $t$ √© pelo menos $\frac{\epsilon}{k}$ (pelo Lema 1.1).  Portanto, $\mathbb{E}[I_t(a)] \geq \frac{\epsilon}{k}$. Pela linearidade da esperan√ßa, $\mathbb{E}[N_t(a)] = \mathbb{E}[\sum_{i=1}^t I_i(a)] = \sum_{i=1}^t \mathbb{E}[I_i(a)] \geq \sum_{i=1}^t \frac{\epsilon}{k} = \frac{\epsilon}{k}t$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos um k-armed bandit com k=5 e estamos usando $\epsilon=0.2$. De acordo com o Lema 1.2, o n√∫mero esperado de vezes que cada a√ß√£o √© selecionada no tempo t √© dado por:
>
> $\mathbb{E}[N_t(a)] \geq \frac{\epsilon}{k}t = \frac{0.2}{5}t = 0.04t$
>
> Isso significa que ap√≥s 100 passos (t=100), esperamos que cada a√ß√£o seja selecionada pelo menos 4 vezes:
>
> $\mathbb{E}[N_{100}(a)] \geq 0.04 \times 100 = 4$
>
> Ap√≥s 1000 passos (t=1000), o valor esperado sobe para 40:
>
> $\mathbb{E}[N_{1000}(a)] \geq 0.04 \times 1000 = 40$
>
> Note que este √© um limite inferior, o n√∫mero real de sele√ß√µes pode ser maior.

```mermaid
graph LR
    A["Œµ-greedy, k, t"] --> B{"P(selecionar a√ß√£o a) >= Œµ/k"};
    B --> C{"E[I_t(a)] >= Œµ/k"};
    C --> D{"E[N_t(a)] = Œ£ E[I_i(a)]"};
    D --> E{"E[N_t(a)] >= (Œµ/k) * t"};
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

**Corol√°rio 1: Converg√™ncia da Frequ√™ncia de Amostragem**
*Declara√ß√£o:* Em um ambiente *k-armed bandit*, com sele√ß√£o de a√ß√£o Œµ-gananciosa, a frequ√™ncia com que cada a√ß√£o $a$ √© selecionada, dada por $\frac{N_t(a)}{t}$, converge para um valor maior ou igual a $\frac{\epsilon}{k}$ quando $t$ tende ao infinito.
*Prova:*  Pelo Lema 1.2, temos que $\mathbb{E}[N_t(a)] \geq \frac{\epsilon}{k}t$. Dividindo por $t$ e tomando o limite, temos $\lim_{t \to \infty} \frac{\mathbb{E}[N_t(a)]}{t} \geq \frac{\epsilon}{k}$. Como a m√©dia amostral converge para a m√©dia verdadeira, a frequ√™ncia de amostragem $\frac{N_t(a)}{t}$ converge para um valor maior ou igual a $\frac{\epsilon}{k}$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior com k=5 e $\epsilon = 0.2$, o corol√°rio 1 afirma que a frequ√™ncia de amostragem $\frac{N_t(a)}{t}$ converge para um valor maior ou igual a $\frac{0.2}{5}=0.04$. Isso significa que, √† medida que o n√∫mero de itera√ß√µes tende ao infinito, cada a√ß√£o ser√° selecionada pelo menos 4% das vezes.
```mermaid
graph LR
    A["Lema 1.2: E[N_t(a)] >= (Œµ/k)t"] --> B{"Dividir por t"};
    B --> C{"lim (t‚Üí‚àû) E[N_t(a)]/t >= Œµ/k"};
    C --> D{"Frequ√™ncia N_t(a)/t converge >= Œµ/k"};
     style D fill:#ccf,stroke:#333,stroke-width:2px
```

**Converg√™ncia dos valores de a√ß√£o:** O *lemma 1* garante que, para qualquer a√ß√£o $a$, o n√∫mero de sele√ß√µes $N_t(a)$ tende ao infinito quando $t$ tende ao infinito. Isso √© essencial para a converg√™ncia dos valores estimados de a√ß√£o $Q_t(a)$. Pela Lei dos Grandes N√∫meros, √† medida que o n√∫mero de amostras de uma vari√°vel aleat√≥ria aumenta, a m√©dia amostral converge para a m√©dia verdadeira [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3).  No caso dos action-value methods, a m√©dia amostral √© dada por $Q_t(a)$, que portanto converge para o valor verdadeiro esperado da a√ß√£o $a$, denotado por $q_*(a)$, ou seja, $Q_t(a) \to q_*(a)$ quando $t \to \infty$. Este resultado s√≥ √© garantido quando todas as a√ß√µes s√£o amostradas infinitas vezes, que √© o caso do m√©todo Œµ-ganancioso.

**Teorema 1: Converg√™ncia dos Valores de A√ß√£o**
*Declara√ß√£o:* Em um ambiente *k-armed bandit*, com sele√ß√£o de a√ß√£o Œµ-gananciosa ($\epsilon > 0$), o valor estimado de cada a√ß√£o $Q_t(a)$ converge para o valor verdadeiro $q_*(a)$ quando $t$ tende ao infinito, i.e., $\lim_{t \to \infty} Q_t(a) = q_*(a)$ para toda a√ß√£o $a$.
*Prova:*  Como demonstrado no Lema 1, todas as a√ß√µes s√£o amostradas um n√∫mero infinito de vezes quando $t \rightarrow \infty$.  Pela Lei Forte dos Grandes N√∫meros, a m√©dia amostral de uma sequ√™ncia de vari√°veis aleat√≥rias converge quase certamente para o valor esperado dessa vari√°vel aleat√≥ria, dada que a m√©dia amostral seja bem definida.  No contexto do m√©todo *action-value*, o valor estimado $Q_t(a)$ √© a m√©dia amostral das recompensas obtidas quando a a√ß√£o $a$ foi selecionada.  Como a a√ß√£o $a$ √© amostrada um n√∫mero infinito de vezes, $Q_t(a)$ converge para o valor esperado da recompensa obtida quando $a$ √© selecionada, o qual √© precisamente $q_*(a)$. $\blacksquare$
```mermaid
graph LR
    A["Lema 1: Amostragem Infinita"] --> B{"Lei Forte dos Grandes N√∫meros"};
    B --> C{"Q_t(a) √© m√©dia amostral"};
    C --> D{"Q_t(a) ‚Üí q_*(a)"};
     style D fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**  Vamos simular um cen√°rio com 3-armed bandit, onde as recompensas para as a√ß√µes s√£o geradas de uma distribui√ß√£o normal com m√©dias $q_*(1)=1$, $q_*(2)=2$, $q_*(3)=0$ e desvio padr√£o 1. Vamos simular 1000 passos usando $\epsilon = 0.1$ e observar a evolu√ß√£o dos valores estimados $Q_t(a)$ para cada a√ß√£o.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> k = 3
> epsilon = 0.1
> num_steps = 1000
> q_star = [1, 2, 0]
>
> Q = np.zeros(k)
> N = np.zeros(k)
> rewards = []
>
> for t in range(num_steps):
>     if np.random.rand() < epsilon:
>        action = np.random.choice(k)
>     else:
>        action = np.argmax(Q)
>
>     reward = np.random.normal(q_star[action], 1)
>     N[action] += 1
>     Q[action] = Q[action] + (reward - Q[action]) / N[action]
>     rewards.append(reward)
>
> print("Valores estimados Q:", Q)
>
> plt.figure(figsize=(10,5))
> for i in range(k):
>   plt.plot( [Q[i]]*num_steps, label = f"Q({i+1})")
> plt.xlabel('Time Step')
> plt.ylabel('Estimated Action Value')
> plt.title('Convergence of Action Values')
> plt.legend()
> plt.show()
> ```
> Este c√≥digo simula o aprendizado e mostra como os valores Q para cada a√ß√£o convergem para os valores verdadeiros.  Observa-se que $Q(1)$ converge para 1, $Q(2)$ converge para 2, e $Q(3)$ converge para 0. O gr√°fico mostrar√° os valores estimados de $Q$ ao longo do tempo, ilustrando a converg√™ncia.

### Conclus√£o

O m√©todo Œµ-ganancioso, apesar de sua simplicidade, oferece uma abordagem eficaz para equilibrar explora√ß√£o e explota√ß√£o no problema do *k-armed bandit*. A garantia de amostragem infinita de todas as a√ß√µes (Lemma 1) √© o fundamento para a converg√™ncia dos valores de a√ß√£o estimados para seus valores verdadeiros. Esta caracter√≠stica √© crucial para permitir que o agente aprenda a a√ß√£o √≥tima a longo prazo. √â importante notar, no entanto, que essa converg√™ncia √© assint√≥tica, ou seja, garantida apenas no limite quando o n√∫mero de intera√ß√µes com o ambiente tende ao infinito [4](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-4). Em cen√°rios reais, a taxa de converg√™ncia pode ser lenta, e outras t√©cnicas mais sofisticadas podem ser necess√°rias para obter um bom desempenho em um n√∫mero finito de intera√ß√µes.

### Refer√™ncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior." *(Trecho de Multi-armed Bandits)*
[^2]: "The particular nonassociative, evaluative feedback problem that we explore is a simple version of the k-armed bandit problem." *(Trecho de Multi-armed Bandits)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: ... where 1predicate denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define Qt(a) as some default value, such as 0. As the denominator goes to infinity, by the law of large numbers, Qt(a) converges to q*(a)." *(Trecho de Multi-armed Bandits)*
[^4]: "An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the Qt(a) converge to their respective q*(a). This of course implies that the probability of selecting the optimal action converges to greater than 1 ‚Äì Œµ, that is, to near certainty. These are just asymptotic guarantees, however, and say little about the practical effectiveness of the methods." *(Trecho de Multi-armed Bandits)*
