## M√©todos de Valor de A√ß√£o: Sele√ß√£o Gananciosa
### Introdu√ß√£o
No contexto do aprendizado por refor√ßo, particularmente no problema do *k-armed bandit*, o aprendizado se concentra em m√©todos que avaliam as a√ß√µes tomadas em vez de instruir as a√ß√µes corretas [^1]. Essa abordagem avaliativa introduz a necessidade de **explora√ß√£o ativa** para descobrir os melhores comportamentos. Enquanto o feedback instrutivo indica a a√ß√£o correta independentemente da a√ß√£o tomada, o feedback avaliativo depende inteiramente da a√ß√£o selecionada [^1]. O presente cap√≠tulo foca no aspecto avaliativo do aprendizado por refor√ßo em um cen√°rio simplificado, que n√£o envolve aprender a agir em m√∫ltiplas situa√ß√µes, um cen√°rio conhecido como *nonassociative* [^1]. Este cen√°rio permite analisar claramente como o feedback avaliativo difere e pode ser combinado com o feedback instrutivo [^1]. Dentro deste contexto, exploramos o problema do *k-armed bandit* e suas varia√ß√µes, que s√£o √∫teis para introduzir m√©todos b√°sicos de aprendizagem que ser√£o expandidos em cap√≠tulos futuros [^1].

### Conceitos Fundamentais
O problema do *k-armed bandit* apresenta um cen√°rio em que um agente escolhe repetidamente entre *k* op√ß√µes ou a√ß√µes diferentes. Ap√≥s cada escolha, o agente recebe uma recompensa num√©rica, que √© selecionada a partir de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o selecionada [^1]. O objetivo √© maximizar a recompensa total esperada ao longo de um per√≠odo de tempo [^2]. Este problema √© denominado analogamente a uma m√°quina ca√ßa-n√≠queis (ou "bandido de um bra√ßo"), mas com *k* alavancas [^2]. Cada sele√ß√£o de a√ß√£o pode ser vista como o acionamento de uma alavanca, e as recompensas s√£o os pagamentos obtidos [^2]. O agente deve, atrav√©s de sele√ß√µes repetidas, concentrar-se nas melhores alavancas para maximizar seus ganhos [^2].

```mermaid
graph LR
    A("Agente") --> B("A√ß√µes (k alavancas)");
    B --> C("Recompensa (Distribui√ß√£o de Probabilidade)");
    C --> A;
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

No problema do *k-armed bandit*, cada a√ß√£o tem um valor esperado ou recompensa m√©dia, que denotamos como **q\*(a)**. Este valor √© definido como a recompensa esperada, dado que a a√ß√£o *a* foi selecionada:

$$ q_*(a) = E[R_t | A_t = a] $$ [^2]

onde $A_t$ √© a a√ß√£o selecionada no passo de tempo *t*, e $R_t$ √© a recompensa correspondente [^2]. Se o valor de cada a√ß√£o fosse conhecido, o problema se tornaria trivial, pois o agente sempre selecionaria a a√ß√£o com maior valor. No entanto, geralmente n√£o conhecemos os valores das a√ß√µes com certeza [^2]. Em vez disso, mantemos *estimativas* dos valores das a√ß√µes, denotadas como $Q_t(a)$, que esperamos que se aproximem dos valores reais, $q_*(a)$ [^2].

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de 3-armed bandit (k=3), com as seguintes recompensas esperadas para cada a√ß√£o: $q_*(1) = 2$, $q_*(2) = 4$, e $q_*(3) = 1$. Inicialmente, o agente n√£o conhece esses valores. Portanto, suas estimativas $Q_t(a)$ ser√£o diferentes dos valores verdadeiros $q_*(a)$.

As a√ß√µes cujo valor estimado √© o maior em qualquer dado passo de tempo s√£o chamadas de **a√ß√µes gananciosas** [^2]. A sele√ß√£o de uma dessas a√ß√µes √© chamada **explora√ß√£o**, pois o agente utiliza o conhecimento atual sobre os valores das a√ß√µes [^2]. Em contraste, selecionar uma a√ß√£o n√£o gananciosa √© chamado de **explora√ß√£o**, pois permite melhorar a estimativa do valor da a√ß√£o n√£o gananciosa [^2]. A explora√ß√£o maximiza a recompensa esperada a curto prazo, enquanto a explora√ß√£o pode gerar uma maior recompensa total a longo prazo, pois permite descobrir a√ß√µes com maior potencial [^2].

```mermaid
graph LR
    A("Estado atual: Estimativas Q_t(a)") --> B{"A√ß√£o com maior Q_t(a)"};
    B -- "Explora√ß√£o" --> C("Recompensa imediata maximizada");
    A --> D{"A√ß√µes com menor Q_t(a)"};
    D -- "Explora√ß√£o" --> E("Melhoria de Q_t(a)");
    E --> F("Recompensa potencial a longo prazo");
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
```

A escolha entre explora√ß√£o e explora√ß√£o √© um problema central em aprendizado por refor√ßo, pois n√£o √© poss√≠vel explorar e explorar simultaneamente [^2]. Uma forma simples de abordar essa quest√£o √© atrav√©s do m√©todo de **sele√ß√£o de a√ß√£o gananciosa**. Esse m√©todo consiste em sempre selecionar a a√ß√£o com a maior estimativa de valor no momento [^3]. Formalmente, o m√©todo de sele√ß√£o de a√ß√£o gananciosa √© descrito como:

$$ A_t = \text{argmax}_a \, Q_t(a) $$ [^3]

onde $A_t$ √© a a√ß√£o selecionada no passo de tempo *t*, e $\text{argmax}_a$ denota a a√ß√£o *a* para a qual a express√£o que segue √© maximizada. Se houver v√°rias a√ß√µes gananciosas, a sele√ß√£o √© feita de forma arbitr√°ria [^3]. Embora o m√©todo de sele√ß√£o gananciosa seja simples, ele possui limita√ß√µes. O m√©todo sempre explora o conhecimento atual, e n√£o dedica tempo para explorar a√ß√µes aparentemente inferiores para descobrir se s√£o melhores. Uma alternativa simples √© o m√©todo $\epsilon$-ganancioso, que explora a maior parte do tempo, mas em uma pequena porcentagem de vezes (denotada $\epsilon$) seleciona uma a√ß√£o aleatoriamente [^3].

> üí° **Exemplo Num√©rico:**  Continuando o exemplo anterior, vamos supor que inicialmente, o agente tem as seguintes estimativas de valor: $Q_0(1) = 1$, $Q_0(2) = 2$, e $Q_0(3) = 0.5$. Usando a sele√ß√£o gananciosa, a a√ß√£o $A_0 = 2$ seria escolhida pois $Q_0(2)$ √© a maior estimativa inicial. No passo seguinte, ap√≥s receber uma recompensa $R_1$ da a√ß√£o 2, o agente atualizar√° $Q_1(2)$ e ent√£o selecionar√° a a√ß√£o que maximiza $Q_1(a)$.

#### Lemma 1
A **sele√ß√£o de a√ß√£o gananciosa** n√£o garante a identifica√ß√£o da a√ß√£o √≥tima em cen√°rios onde os valores das a√ß√µes s√£o inicialmente desconhecidos ou incertos, pois este m√©todo prioriza a explora√ß√£o sem dedicar tempo √† explora√ß√£o de a√ß√µes aparentemente sub√≥timas, limitando o aprendizado e a descoberta de alternativas melhores no longo prazo.

*Prova*:
Pela defini√ß√£o do m√©todo de sele√ß√£o de a√ß√£o gananciosa, ele sempre seleciona a a√ß√£o com a maior estimativa de valor no momento atual. Se a estimativa inicial de valor para uma a√ß√£o √≥tima for baixa (e este for um cen√°rio poss√≠vel), esta a√ß√£o n√£o ser√° selecionada. O algoritmo n√£o explorar√° novas a√ß√µes e convergir√° a um valor sub√≥timo, se este valor estiver pr√≥ximo ao valor √≥timo, mas a a√ß√£o √≥tima n√£o for explorada. Se, em contrapartida, esta a√ß√£o tiver um valor relativamente alto no in√≠cio, e logo tiver uma recompensa pior, o algoritmo pode n√£o voltar a essa a√ß√£o. Portanto, a sele√ß√£o de a√ß√£o gananciosa pode levar a convergencia a um valor sub√≥timo. $\blacksquare$

> üí° **Exemplo Num√©rico (Lema 1):** Vamos supor que os valores verdadeiros das a√ß√µes s√£o $q_*(1) = 1$, $q_*(2) = 5$, e $q_*(3) = 2$. Suponha que inicialmente o agente tem estimativas $Q_0(1) = 2$, $Q_0(2) = 1$ e $Q_0(3) = 0.5$. A a√ß√£o gananciosa seria a a√ß√£o 1, pois possui a maior estimativa inicial, mesmo que a a√ß√£o 2 seja a √≥tima. O m√©todo de sele√ß√£o gananciosa continuaria a escolher a a√ß√£o 1, a menos que uma recompensa muito menor alterasse significativamente a estimativa, o que pode n√£o acontecer no curto prazo, fazendo com que o agente n√£o explore outras a√ß√µes e n√£o encontre a a√ß√£o √≥tima.

**Proposi√ß√£o 1** Uma condi√ß√£o necess√°ria para a sele√ß√£o de a√ß√£o gananciosa convergir para a a√ß√£o √≥tima √© que a estimativa inicial $Q_0(a)$ para a a√ß√£o √≥tima seja maior que a estimativa inicial de todas as a√ß√µes sub√≥timas.

```mermaid
graph LR
    subgraph "Condi√ß√£o para Converg√™ncia da A√ß√£o Gananciosa"
        A("Estimativa Inicial: Q_0(a)")
        B{"A√ß√£o √ìtima: a*"}
        C{"A√ß√µes Sub√≥timas: a_i"}
        A --> B
        A --> C
        B -- "Q_0(a*) > Q_0(a_i)" --> D("Converg√™ncia para a*")
        C -- "Q_0(a*) <= Q_0(a_i)" --> E("Converg√™ncia Sub√≥tima")
    end
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#fcc,stroke:#333,stroke-width:2px
```

*Prova*:
Seja $a^*$ a a√ß√£o √≥tima e $a_i$ uma a√ß√£o sub√≥tima qualquer. O m√©todo de sele√ß√£o gananciosa seleciona a a√ß√£o que maximiza $Q_t(a)$ em cada passo *t*. Se $Q_0(a^*) < Q_0(a_i)$ para alguma a√ß√£o sub√≥tima $a_i$, ent√£o $a^*$ n√£o ser√° selecionada no in√≠cio. Como a a√ß√£o gananciosa nunca explora outras a√ß√µes, se $a^*$ n√£o for selecionada inicialmente, ela nunca poder√° ser explorada, e o m√©todo convergir√° para uma a√ß√£o sub√≥tima. Portanto, uma condi√ß√£o necess√°ria para convergir para $a^*$ √© que $Q_0(a^*) \geq Q_0(a_i)$ para todas as a√ß√µes sub√≥timas. $\blacksquare$

> üí° **Exemplo Num√©rico (Proposi√ß√£o 1):** Para as a√ß√µes com valores $q_*(1) = 3$, $q_*(2) = 6$, e $q_*(3) = 2$. A a√ß√£o √≥tima √© a a√ß√£o 2. Se $Q_0(1) = 4$, $Q_0(2) = 1$ e $Q_0(3) = 0.5$, a a√ß√£o 1 seria escolhida inicialmente e, seguindo o m√©todo ganancioso, a a√ß√£o 2 dificilmente seria explorada, pois $Q_0(1)$ √© a maior estimativa inicial. Este exemplo ilustra a Proposi√ß√£o 1, mostrando que para a converg√™ncia √† a√ß√£o √≥tima, a estimativa inicial da a√ß√£o √≥tima ($Q_0(2)$) deve ser maior ou igual a todas as demais.

**Lema 1.1** Se a estimativa inicial $Q_0(a)$ for a mesma para todas as a√ß√µes, ent√£o a sele√ß√£o de a√ß√£o gananciosa, sem a adi√ß√£o de mecanismos de explora√ß√£o, nunca aprender√° a escolher a a√ß√£o √≥tima em um problema do tipo k-armed bandit com recompensas estoc√°sticas.

*Prova*:
Se $Q_0(a) = c$ para todas as a√ß√µes $a$, onde $c$ √© uma constante, ent√£o a sele√ß√£o de a√ß√£o inicial ser√° arbitr√°ria. Se uma a√ß√£o sub√≥tima for selecionada, o m√©todo de sele√ß√£o de a√ß√£o gananciosa explorar√° essa a√ß√£o continuamente. Em um cen√°rio com recompensas estoc√°sticas, haver√° varia√ß√µes nas recompensas. Se uma a√ß√£o sub√≥tima apresentar uma sequ√™ncia de recompensas aleatoriamente elevadas, a estimativa de valor $Q_t(a)$ dessa a√ß√£o se tornar√° maior do que as outras a√ß√µes e a sele√ß√£o de a√ß√£o gananciosa a escolher√° novamente. Portanto, em um problema estoc√°stico, a sele√ß√£o gananciosa n√£o convergir√° para a a√ß√£o √≥tima com um valor inicial igual para todas as a√ß√µes. $\blacksquare$

> üí° **Exemplo Num√©rico (Lema 1.1):** Suponha que temos tr√™s a√ß√µes com $q_*(1) = 2$, $q_*(2) = 4$ e $q_*(3) = 1$, e iniciamos com estimativas iguais $Q_0(1) = Q_0(2) = Q_0(3) = 0$. A primeira a√ß√£o √© escolhida aleatoriamente, digamos a a√ß√£o 1. Devido √† estocasticidade das recompensas, as recompensas podem ser um pouco diferentes a cada itera√ß√£o. Por exemplo, nas primeiras itera√ß√µes, a a√ß√£o 1 pode retornar 2.1, 1.8, 2.3, e a a√ß√£o 2 pode retornar 3.5, 4.1, 3.8. As estimativas de $Q_t(a)$ seriam atualizadas com essas recompensas, e $Q_t(1)$ e $Q_t(2)$ poderiam ter valores semelhantes devido √† estocasticidade, mesmo que $q_*(2)$ seja maior, n√£o garantindo a converg√™ncia para a a√ß√£o √≥tima. Portanto, a sele√ß√£o gananciosa n√£o convergiria para a a√ß√£o √≥tima (a√ß√£o 2) com valores iniciais iguais.

**Lema 1.2** Em um ambiente determin√≠stico, com recompensas fixas, a sele√ß√£o de a√ß√£o gananciosa, partindo de estimativas iniciais iguais, convergiria para a a√ß√£o que tiver o maior valor no in√≠cio, o que pode n√£o ser a a√ß√£o √≥tima, se a estimativa inicial dessa a√ß√£o j√° for maior que as demais.

*Prova*:
Se $Q_0(a) = c$ para todas as a√ß√µes e a recompensa de cada a√ß√£o √© fixa, ao selecionar qualquer a√ß√£o a, a estimativa $Q_t(a)$ se atualizaria com a recompensa recebida. Caso uma a√ß√£o inicial $a_1$ for selecionada, a estimativa dessa a√ß√£o, $Q_1(a_1)$, ser√° a recompensa obtida. Se esta recompensa for a maior entre todas as a√ß√µes, a a√ß√£o $a_1$ sempre ser√° selecionada. Caso contr√°rio, se a a√ß√£o $a_1$ tiver uma recompensa sub√≥tima, a a√ß√£o $a_1$ seria explorada. Se, em um dado momento, uma a√ß√£o $a_2$ tiver uma recompensa maior que as demais, a a√ß√£o $a_2$ passar√° a ser selecionada, e o algoritmo convergir√° para esta a√ß√£o. Portanto, em um ambiente determin√≠stico com valores iniciais iguais, o m√©todo de sele√ß√£o gananciosa convergir√° para a primeira a√ß√£o de maior valor, que pode n√£o ser a a√ß√£o √≥tima. $\blacksquare$

```mermaid
sequenceDiagram
    participant Agente
    participant Ambiente
    Agente->>Ambiente: Escolhe a√ß√£o a_1
    Ambiente-->>Agente: Recompensa fixa R(a_1)
    Agente->>Agente: Atualiza Q_1(a_1) com R(a_1)
    loop Para outras a√ß√µes a_i
        Agente->>Ambiente: Escolhe a√ß√£o a_i
        Ambiente-->>Agente: Recompensa fixa R(a_i)
        Agente->>Agente: Atualiza Q_1(a_i) com R(a_i)
    end
    Agente->>Agente: Seleciona a√ß√£o com maior R(a)
    Agente-->>Ambiente: Seleciona sempre a mesma a√ß√£o
```

> üí° **Exemplo Num√©rico (Lema 1.2):** Suponha que temos tr√™s a√ß√µes com recompensas fixas $q_*(1) = 2$, $q_*(2) = 4$ e $q_*(3) = 1$, e estimativas iniciais $Q_0(1) = Q_0(2) = Q_0(3) = 0$. Se a a√ß√£o 1 for selecionada primeiro, a estimativa $Q_1(1)$ se torna 2. Em seguida, se a a√ß√£o 2 for selecionada, $Q_1(2)$ se torna 4, e agora a a√ß√£o 2 √© selecionada para sempre. Se a a√ß√£o 3 for selecionada inicialmente, $Q_1(3)$ se tornar√° 1, e n√£o ser√° escolhida novamente. O m√©todo convergir√° para a primeira a√ß√£o que produzir a maior recompensa, que neste caso √© 2.

**Teorema 1** O m√©todo de sele√ß√£o de a√ß√£o gananciosa converge para um comportamento √≥timo apenas quando as estimativas iniciais favorecem a a√ß√£o √≥tima, ou, em casos determin√≠sticos, a a√ß√£o √≥tima √© explorada na primeira itera√ß√£o ou tem uma recompensa inicial maior que as demais.

*Prova:*
Do Lema 1, sabemos que a sele√ß√£o de a√ß√£o gananciosa n√£o garante a identifica√ß√£o da a√ß√£o √≥tima. A Proposi√ß√£o 1 nos diz que uma condi√ß√£o necess√°ria √© que a estimativa inicial da a√ß√£o √≥tima seja maior que a das a√ß√µes sub√≥timas. O Lema 1.1 estabelece que em um cen√°rio estoc√°stico, com estimativas iniciais iguais, o m√©todo nunca aprender√° a a√ß√£o √≥tima. O Lema 1.2 estabelece que, mesmo em cen√°rios determin√≠sticos com estimativas iniciais iguais, o m√©todo pode convergir para uma a√ß√£o sub√≥tima. Combinando esses resultados, a converg√™ncia para um comportamento √≥timo ocorre apenas em circunst√¢ncias favor√°veis, seja com uma vantagem inicial da a√ß√£o √≥tima ou, no caso de valores iniciais iguais, quando a a√ß√£o √≥tima √© descoberta no in√≠cio. $\blacksquare$

> üí° **Exemplo Num√©rico (Teorema 1):** O Teorema 1 resume todos os casos abordados anteriormente. Se os valores iniciais $Q_0(a)$ forem favor√°veis √† a√ß√£o √≥tima, ou seja, $Q_0(a_{otimo}) > Q_0(a_{subotimal})$, ou se o ambiente for determin√≠stico e a a√ß√£o √≥tima tiver a primeira recompensa explorada ou uma recompensa inicial maior que as demais, ent√£o a sele√ß√£o gananciosa pode convergir para um comportamento √≥timo. Por exemplo, se $q_*(1) = 2$, $q_*(2) = 4$ e $q_*(3) = 1$ e as estimativas iniciais forem $Q_0(1) = 1$, $Q_0(2) = 5$ e $Q_0(3) = 0.5$, ent√£o o m√©todo ganancioso selecionar√° a a√ß√£o √≥tima 2 (pois a estimativa inicial √© a maior) e continuar√° explorando a mesma, levando a um comportamento √≥timo.

### Conclus√£o
O m√©todo de sele√ß√£o de a√ß√£o gananciosa √© uma estrat√©gia fundamental em aprendizado por refor√ßo que busca maximizar a recompensa imediata selecionando a a√ß√£o com o maior valor estimado atual. Embora essa abordagem seja simples e intuitiva, ela possui limita√ß√µes, como a potencial falta de explora√ß√£o de a√ß√µes que poderiam ser melhores, levando a resultados sub√≥timos a longo prazo. O m√©todo de sele√ß√£o de a√ß√£o gananciosa √© um componente chave na constru√ß√£o de m√©todos mais avan√ßados que procuram equilibrar a explora√ß√£o e a explora√ß√£o no problema do *k-armed bandit*. Uma alternativa a essa limita√ß√£o √© o m√©todo $\epsilon$-ganancioso, que balanceia explora√ß√£o com explota√ß√£o.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken." *(Trecho de <Multi-armed Bandits>)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected: $q_*(a) = E[R_t | A_t=a]$. If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time stept as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_*(a)$. If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action‚Äôs value. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run." *(Trecho de <Multi-armed Bandits>)*
[^3]: "The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as $A_t = \text{argmax}_a \, Q_t(a)$, where argmaxa denotes the action a for which the expression that follows is maximized (with ties broken arbitrarily). Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better." *(Trecho de <Multi-armed Bandits>)*
