## Estimativa de Valores de AÃ§Ã£o por MÃ©dia de Amostras

### IntroduÃ§Ã£o

No contexto do aprendizado por reforÃ§o, especificamente no problema de **k-armed bandits**, o objetivo central Ã© maximizar a recompensa total esperada ao longo do tempo [^1]. Para atingir esse objetivo, Ã© crucial estimar o valor das aÃ§Ãµes disponÃ­veis, ou seja, a recompensa mÃ©dia esperada ao selecionar cada aÃ§Ã£o. Uma abordagem natural e intuitiva para essa estimativa Ã© calcular a mÃ©dia das recompensas obtidas cada vez que uma aÃ§Ã£o especÃ­fica Ã© escolhida. Este mÃ©todo, conhecido como **sample-average method**, serve como base para o desenvolvimento de diversas estratÃ©gias de aprendizado por reforÃ§o [^3]. Este capÃ­tulo abordarÃ¡ em detalhe como este mÃ©todo funciona e como ele se relaciona com as decisÃµes de aÃ§Ã£o.

### Conceitos Fundamentais

A essÃªncia do **sample-average method** reside na ideia de que o valor verdadeiro de uma aÃ§Ã£o, denotado por $q_*(a)$, Ã© a recompensa mÃ©dia esperada quando essa aÃ§Ã£o Ã© selecionada. No entanto, em um cenÃ¡rio de aprendizado, o valor verdadeiro Ã© desconhecido, e o agente deve estimÃ¡-lo com base nas recompensas recebidas [^2]. O texto define $q_*(a)$ como:
$$q_*(a) = \mathbb{E}[R_t | A_t = a]$$
onde $R_t$ Ã© a recompensa no instante *t* e $A_t$ Ã© a aÃ§Ã£o selecionada nesse mesmo instante [^2]. O **sample-average method** estima $q_*(a)$ atravÃ©s da mÃ©dia das recompensas observadas quando a aÃ§Ã£o *a* foi selecionada. O valor estimado de uma aÃ§Ã£o *a* no instante *t* Ã© denotado por $Q_t(a)$, que Ã© calculado da seguinte forma [^3]:
```mermaid
graph LR
    A[/"AÃ§Ã£o 'a' selecionada"/] --> B("Recompensa R_i obtida");
    B --> C{/"Contagem de seleÃ§Ãµes de 'a'"/};
    C --> D("Soma das recompensas de 'a'");
    D --> E("Q_t(a) = Soma / Contagem");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

$$Q_t(a) = \frac{\text{soma das recompensas quando a foi selecionada antes de t}}{\text{nÃºmero de vezes que a foi selecionada antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}$$

onde $\mathbb{1}_{\text{predicate}}$ Ã© uma funÃ§Ã£o indicadora que retorna 1 se o predicado for verdadeiro e 0 caso contrÃ¡rio. Se o denominador for zero, $Q_t(a)$ Ã© definido como um valor padrÃ£o, por exemplo, 0 [^3]. Ã€ medida que o nÃºmero de seleÃ§Ãµes da aÃ§Ã£o *a* tende ao infinito, a **lei dos grandes nÃºmeros** garante que $Q_t(a)$ converge para $q_*(a)$ [^3].

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos um problema de 2-armed bandit (k=2), com aÃ§Ãµes $a_1$ e $a_2$.  Inicialmente, $Q_1(a_1) = 0$ e $Q_1(a_2) = 0$. ApÃ³s as primeiras 5 interaÃ§Ãµes, as seguintes aÃ§Ãµes e recompensas sÃ£o obtidas:
>
> | IteraÃ§Ã£o (t) | AÃ§Ã£o ($A_t$) | Recompensa ($R_t$) |
> |---|---|---|
> | 1 | $a_1$ | 1 |
> | 2 | $a_2$ | 0 |
> | 3 | $a_1$ | 2 |
> | 4 | $a_1$ | 1 |
> | 5 | $a_2$ | 1 |
>
> Vamos calcular $Q_t(a)$ para cada aÃ§Ã£o:
>
> - **Para $a_1$:**
>   - $Q_1(a_1) = 0$ (valor inicial)
>   - $Q_2(a_1) = (1) / 1 = 1$
>   - $Q_3(a_1) = (1+2) / 2 = 1.5$
>   - $Q_4(a_1) = (1+2+1) / 3 = 1.33$
>   - $Q_5(a_1) = 1.33$ (pois $a_1$ nÃ£o foi selecionada na interaÃ§Ã£o 5)
>
> - **Para $a_2$:**
>   - $Q_1(a_2) = 0$ (valor inicial)
>   - $Q_2(a_2) = 0 / 1 = 0$
>   - $Q_3(a_2) = 0$ (pois $a_2$ nÃ£o foi selecionada na interaÃ§Ã£o 3)
>   - $Q_4(a_2) = 0$ (pois $a_2$ nÃ£o foi selecionada na interaÃ§Ã£o 4)
>   - $Q_5(a_2) = (0 + 1) / 2 = 0.5$
>
>  Observamos que as estimativas $Q_t(a)$ se ajustam com cada nova recompensa.

**Lema 1.1:** _A atualizaÃ§Ã£o incremental da mÃ©dia amostral pode ser expressa de forma recursiva._
_Prova:_
Seja $N_t(a) = \sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}$ o nÃºmero de vezes que a aÃ§Ã£o $a$ foi selecionada antes do instante $t$. EntÃ£o, $Q_t(a)$ pode ser reescrito como
$$Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}.$$
Quando a aÃ§Ã£o $a$ Ã© selecionada no instante $t$, ou seja, $A_t = a$, o novo valor estimado $Q_{t+1}(a)$ pode ser calculado de forma incremental:
$$Q_{t+1}(a) = \frac{1}{N_t(a) + 1} \left( \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a} + R_t \right) = \frac{1}{N_t(a) + 1} \left( N_t(a) Q_t(a) + R_t \right).$$
Rearranjando, obtemos a atualizaÃ§Ã£o recursiva:
```mermaid
graph LR
    A["Q_t(a)"] -->| "N_t(a)" | B("N_t(a) * Q_t(a)");
    C["R_t"] --> D("B + R_t");
    D --> E("N_t(a) + 1");
    E --> F("Q_{t+1}(a) = D / E");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px

```
$$Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a) + 1} \left( R_t - Q_t(a) \right).$$
Se $A_t \neq a$, entÃ£o $Q_{t+1}(a) = Q_t(a)$.  $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico (AtualizaÃ§Ã£o Incremental):** Usando os mesmos dados do exemplo anterior, vamos calcular $Q_t(a)$ usando a atualizaÃ§Ã£o incremental.
>
> - **Para $a_1$:**
>   - $Q_1(a_1) = 0$
>   - $N_1(a_1) = 0$, $Q_2(a_1) = 0 + \frac{1}{0+1}(1-0) = 1$
>   - $N_2(a_1) = 1$, $Q_3(a_1) = 1 + \frac{1}{1+1}(2-1) = 1 + 0.5 = 1.5$
>   - $N_3(a_1) = 2$, $Q_4(a_1) = 1.5 + \frac{1}{2+1}(1-1.5) = 1.5 - 0.166 = 1.33$
>   - $Q_5(a_1) = 1.33$ (pois $a_1$ nÃ£o foi selecionada)
>
> - **Para $a_2$:**
>   - $Q_1(a_2) = 0$
>   - $N_1(a_2) = 0$, $Q_2(a_2) = 0 + \frac{1}{0+1}(0-0) = 0$
>   - $Q_3(a_2) = 0$ (pois $a_2$ nÃ£o foi selecionada)
>   - $Q_4(a_2) = 0$ (pois $a_2$ nÃ£o foi selecionada)
>  - $N_4(a_2) = 1$, $Q_5(a_2) = 0 + \frac{1}{1+1}(1-0) = 0 + 0.5 = 0.5$
>
> Os resultados sÃ£o idÃªnticos ao cÃ¡lculo direto, demonstrando a equivalÃªncia da atualizaÃ§Ã£o incremental.
>
> A vantagem da atualizaÃ§Ã£o incremental Ã© que nÃ£o precisamos guardar todas as recompensas anteriores, apenas o valor estimado e o nÃºmero de vezes que a aÃ§Ã£o foi selecionada, economizando memÃ³ria e cÃ¡lculos.

Este lema demonstra que o valor estimado $Q_t(a)$ pode ser atualizado incrementalmente, sem a necessidade de recalcular a soma total das recompensas a cada passo. Esta forma recursiva Ã© fundamental para a implementaÃ§Ã£o eficiente de algoritmos de aprendizado por reforÃ§o, especialmente em ambientes onde o nÃºmero de iteraÃ§Ãµes Ã© grande.

Ã‰ importante ressaltar que o **sample-average method** Ã© apenas uma das formas de estimar os valores de aÃ§Ã£o e, embora seja simples, nÃ£o Ã© necessariamente a melhor em todos os contextos. O mÃ©todo Ã© especialmente adequado para **problemas estacionÃ¡rios**, onde as probabilidades de recompensa nÃ£o mudam ao longo do tempo [^8].

Para a tomada de decisÃµes, o agente pode utilizar o **greedy action selection**, ou seja, escolher a aÃ§Ã£o com maior valor estimado a cada instante [^3]:
$$A_t = \text{argmax}_a \, Q_t(a)$$
onde $\text{argmax}_a$ denota a aÃ§Ã£o *a* que maximiza a expressÃ£o seguinte [^3]. Esta regra garante que o agente explore o conhecimento atual para maximizar a recompensa imediata. Se houver mais de uma aÃ§Ã£o com o maior valor estimado, a escolha pode ser feita arbitrariamente, por exemplo, aleatoriamente. Contudo, ao selecionar sempre as aÃ§Ãµes greedy, o agente pode nÃ£o estar explorando outras aÃ§Ãµes com potencial para melhor recompensa a longo prazo [^3]. Para mitigar essa limitaÃ§Ã£o, o agente pode introduzir exploraÃ§Ã£o por meio de aÃ§Ãµes nÃ£o-greedy.

> ğŸ’¡ **Exemplo NumÃ©rico (SeleÃ§Ã£o Greedy):** Usando os valores estimados no exemplo anterior na interaÃ§Ã£o 5, $Q_5(a_1) = 1.33$ e $Q_5(a_2) = 0.5$.  A seleÃ§Ã£o greedy escolheria a aÃ§Ã£o $a_1$ pois $Q_5(a_1) > Q_5(a_2)$.

**Lema 1:**
_A lei dos grandes nÃºmeros garante que $Q_t(a)$ converge para $q_*(a)$, dada a definiÃ§Ã£o do **sample-average method**_.
_Prova:_
A lei dos grandes nÃºmeros afirma que, para uma sequÃªncia de variÃ¡veis aleatÃ³rias independentes e identicamente distribuÃ­das com uma mÃ©dia $\mu$, a mÃ©dia amostral converge para $\mu$ Ã  medida que o nÃºmero de amostras tende para o infinito. No contexto do sample-average method, a recompensa $R_i$ Ã© uma variÃ¡vel aleatÃ³ria com mÃ©dia $q_*(a)$ dado que a aÃ§Ã£o $a$ foi escolhida, e sÃ£o independentes. Portanto, a mÃ©dia amostral $Q_t(a)$ converge para $q_*(a)$ quando t tende ao infinito. $\blacksquare$

**CorolÃ¡rio 1:**
_O sample-average method Ã© um estimador consistente do valor de uma aÃ§Ã£o._

Este corolÃ¡rio Ã© uma consequÃªncia direta do Lemma 1. Como $Q_t(a)$ converge para $q_*(a)$ com o aumento do nÃºmero de amostras, o mÃ©todo de mÃ©dias amostrais Ã© um estimador consistente para o valor da aÃ§Ã£o.

**Teorema 1:** _Se as recompensas sÃ£o limitadas, isto Ã©, existe um $M > 0$ tal que $|R_t| \leq M$ para todo $t$, e se todas as aÃ§Ãµes sÃ£o selecionadas infinitamente, entÃ£o, com probabilidade 1, $Q_t(a) \to q_*(a)$ quando $t \to \infty$, para toda aÃ§Ã£o $a$._
_Prova:_
Pelo Lema 1, sabemos que $Q_t(a)$ converge para $q_*(a)$ se o nÃºmero de amostras para a aÃ§Ã£o $a$ tende para o infinito. Se as recompensas sÃ£o limitadas e todas as aÃ§Ãµes sÃ£o selecionadas infinitamente, o nÃºmero de amostras para cada aÃ§Ã£o tenderÃ¡ para o infinito Ã  medida que $t$ tende para o infinito. A lei forte dos grandes nÃºmeros garante que, nesse caso, a convergÃªncia de $Q_t(a)$ para $q_*(a)$ ocorre com probabilidade 1, completando a prova. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico (ConvergÃªncia):**  Suponha que a aÃ§Ã£o $a_1$ tenha um valor verdadeiro de $q_*(a_1)= 2.0$ e gera recompensas aleatÃ³rias com mÃ©dia 2.0 (por exemplo, usando uma distribuiÃ§Ã£o normal com mÃ©dia 2.0 e desvio padrÃ£o 1.0). Ao longo de muitas interaÃ§Ãµes, a estimativa $Q_t(a_1)$ obtida pelo sample-average method irÃ¡ gradualmente convergir para 2.0. Podemos simular isso para visualizar a convergÃªncia.
>
>```python
>import numpy as np
>import matplotlib.pyplot as plt
>
>np.random.seed(42)
>
>def simulate_sample_average(true_value, num_iterations):
>    rewards = np.random.normal(loc=true_value, scale=1.0, size=num_iterations)
>    estimated_values = np.cumsum(rewards) / np.arange(1, num_iterations + 1)
>    return estimated_values
>
>true_value_a1 = 2.0
>iterations = 1000
>estimated_values_a1 = simulate_sample_average(true_value_a1, iterations)
>
>plt.plot(range(1, iterations + 1), estimated_values_a1)
>plt.axhline(y=true_value_a1, color='r', linestyle='--', label='Valor Verdadeiro')
>plt.xlabel('IteraÃ§Ãµes')
>plt.ylabel('Valor Estimado $Q_t(a_1)$')
>plt.title('ConvergÃªncia de $Q_t(a_1)$ para $q_*(a_1)$')
>plt.legend()
>plt.grid(True)
>plt.show()
>```
> Este cÃ³digo simula a evoluÃ§Ã£o de  $Q_t(a_1)$ ao longo de 1000 iteraÃ§Ãµes, mostrando a convergÃªncia para o valor verdadeiro. O grÃ¡fico demonstra que a mÃ©dia amostral $Q_t(a_1)$ aproxima-se gradualmente de $q_*(a_1)$ confirmando a convergÃªncia.

```mermaid
graph LR
    A["NÃºmero de Amostras (t)"] --> B("MÃ©dia Amostral Q_t(a)");
    B --> C{/"Lei dos Grandes NÃºmeros"/};
    C --> D["ConvergÃªncia de Q_t(a) para q_*(a)"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2 stroke-width:2px;

```

O Teorema 1 formaliza a propriedade de convergÃªncia do mÃ©todo de mÃ©dia amostral sob condiÃ§Ãµes mais gerais, incluindo a limitaÃ§Ã£o das recompensas, uma condiÃ§Ã£o geralmente satisfeita em muitos problemas prÃ¡ticos.

### ConclusÃ£o

O **sample-average method** oferece uma abordagem simples e fundamental para estimar o valor das aÃ§Ãµes no problema de **k-armed bandits**. Ele utiliza a mÃ©dia das recompensas observadas para calcular o valor estimado das aÃ§Ãµes e converge para o valor verdadeiro quando o nÃºmero de amostras Ã© grande o suficiente. Apesar de sua simplicidade, este mÃ©todo Ã© uma base importante para abordagens mais complexas de aprendizado por reforÃ§o e oferece uma intuiÃ§Ã£o clara sobre como os valores de aÃ§Ã£o podem ser estimados a partir da experiÃªncia. A limitaÃ§Ã£o principal do mÃ©todo Ã© sua dependÃªncia de dados passados, o que o torna inadequado para ambientes nÃ£o estacionÃ¡rios onde as recompensas mudam com o tempo. Em tais cenÃ¡rios, mÃ©todos mais adaptativos se fazem necessÃ¡rios.

### ReferÃªncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de <Chapter 2>)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted qâˆ—(a), is the expected reward given that a is selected: q*(a) = E[Rt | At=a]." *(Trecho de <Chapter 2>)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: Qt(a) = (sum of rewards when a taken prior to t) / (number of times a taken prior to t) = (Î£(t-1)_(i=1) Ri1_(A_i=a)) / (Î£(t-1)_(i=1) 1_(A_i=a)), where 1_(predicate) denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define Qt(a) as some default value, such as 0. As the denominator goes to infinity, by the law of large numbers, Qt(a) converges to qâˆ—(a). We call this the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards. Of course this is just one way to estimate action values, and not necessarily the best one. Nevertheless, for now let us stay with this simple estimation method and turn to the question of how the estimates might be used to select actions." *(Trecho de <Chapter 2>)*
[^8]: "The averaging methods discussed so far are appropriate for stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time." *(Trecho de <Chapter 2>)*
