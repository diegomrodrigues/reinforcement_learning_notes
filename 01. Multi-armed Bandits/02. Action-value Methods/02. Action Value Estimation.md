## Estimativa de Valores de A√ß√£o por M√©dia de Amostras

### Introdu√ß√£o

No contexto do aprendizado por refor√ßo, especificamente no problema de **k-armed bandits**, o objetivo central √© maximizar a recompensa total esperada ao longo do tempo [^1]. Para atingir esse objetivo, √© crucial estimar o valor das a√ß√µes dispon√≠veis, ou seja, a recompensa m√©dia esperada ao selecionar cada a√ß√£o. Uma abordagem natural e intuitiva para essa estimativa √© calcular a m√©dia das recompensas obtidas cada vez que uma a√ß√£o espec√≠fica √© escolhida. Este m√©todo, conhecido como **sample-average method**, serve como base para o desenvolvimento de diversas estrat√©gias de aprendizado por refor√ßo [^3]. Este cap√≠tulo abordar√° em detalhe como este m√©todo funciona e como ele se relaciona com as decis√µes de a√ß√£o.

### Conceitos Fundamentais

A ess√™ncia do **sample-average method** reside na ideia de que o valor verdadeiro de uma a√ß√£o, denotado por $q_*(a)$, √© a recompensa m√©dia esperada quando essa a√ß√£o √© selecionada. No entanto, em um cen√°rio de aprendizado, o valor verdadeiro √© desconhecido, e o agente deve estim√°-lo com base nas recompensas recebidas [^2]. O texto define $q_*(a)$ como:
$$q_*(a) = \mathbb{E}[R_t | A_t = a]$$
onde $R_t$ √© a recompensa no instante *t* e $A_t$ √© a a√ß√£o selecionada nesse mesmo instante [^2]. O **sample-average method** estima $q_*(a)$ atrav√©s da m√©dia das recompensas observadas quando a a√ß√£o *a* foi selecionada. O valor estimado de uma a√ß√£o *a* no instante *t* √© denotado por $Q_t(a)$, que √© calculado da seguinte forma [^3]:
```mermaid
graph LR
    A[/"A√ß√£o 'a' selecionada"/] --> B("Recompensa R_i obtida");
    B --> C{/"Contagem de sele√ß√µes de 'a'"/};
    C --> D("Soma das recompensas de 'a'");
    D --> E("Q_t(a) = Soma / Contagem");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

$$Q_t(a) = \frac{\text{soma das recompensas quando a foi selecionada antes de t}}{\text{n√∫mero de vezes que a foi selecionada antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}$$

onde $\mathbb{1}_{\text{predicate}}$ √© uma fun√ß√£o indicadora que retorna 1 se o predicado for verdadeiro e 0 caso contr√°rio. Se o denominador for zero, $Q_t(a)$ √© definido como um valor padr√£o, por exemplo, 0 [^3]. √Ä medida que o n√∫mero de sele√ß√µes da a√ß√£o *a* tende ao infinito, a **lei dos grandes n√∫meros** garante que $Q_t(a)$ converge para $q_*(a)$ [^3].

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de 2-armed bandit (k=2), com a√ß√µes $a_1$ e $a_2$.  Inicialmente, $Q_1(a_1) = 0$ e $Q_1(a_2) = 0$. Ap√≥s as primeiras 5 intera√ß√µes, as seguintes a√ß√µes e recompensas s√£o obtidas:
>
> | Itera√ß√£o (t) | A√ß√£o ($A_t$) | Recompensa ($R_t$) |
> |---|---|---|
> | 1 | $a_1$ | 1 |
> | 2 | $a_2$ | 0 |
> | 3 | $a_1$ | 2 |
> | 4 | $a_1$ | 1 |
> | 5 | $a_2$ | 1 |
>
> Vamos calcular $Q_t(a)$ para cada a√ß√£o:
>
> - **Para $a_1$:**
>   - $Q_1(a_1) = 0$ (valor inicial)
>   - $Q_2(a_1) = (1) / 1 = 1$
>   - $Q_3(a_1) = (1+2) / 2 = 1.5$
>   - $Q_4(a_1) = (1+2+1) / 3 = 1.33$
>   - $Q_5(a_1) = 1.33$ (pois $a_1$ n√£o foi selecionada na intera√ß√£o 5)
>
> - **Para $a_2$:**
>   - $Q_1(a_2) = 0$ (valor inicial)
>   - $Q_2(a_2) = 0 / 1 = 0$
>   - $Q_3(a_2) = 0$ (pois $a_2$ n√£o foi selecionada na intera√ß√£o 3)
>   - $Q_4(a_2) = 0$ (pois $a_2$ n√£o foi selecionada na intera√ß√£o 4)
>   - $Q_5(a_2) = (0 + 1) / 2 = 0.5$
>
>  Observamos que as estimativas $Q_t(a)$ se ajustam com cada nova recompensa.

**Lema 1.1:** _A atualiza√ß√£o incremental da m√©dia amostral pode ser expressa de forma recursiva._
_Prova:_
Seja $N_t(a) = \sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}$ o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada antes do instante $t$. Ent√£o, $Q_t(a)$ pode ser reescrito como
$$Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}.$$
Quando a a√ß√£o $a$ √© selecionada no instante $t$, ou seja, $A_t = a$, o novo valor estimado $Q_{t+1}(a)$ pode ser calculado de forma incremental:
$$Q_{t+1}(a) = \frac{1}{N_t(a) + 1} \left( \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a} + R_t \right) = \frac{1}{N_t(a) + 1} \left( N_t(a) Q_t(a) + R_t \right).$$
Rearranjando, obtemos a atualiza√ß√£o recursiva:
```mermaid
graph LR
    A["Q_t(a)"] -->| "N_t(a)" | B("N_t(a) * Q_t(a)");
    C["R_t"] --> D("B + R_t");
    D --> E("N_t(a) + 1");
    E --> F("Q_{t+1}(a) = D / E");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px

```
$$Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a) + 1} \left( R_t - Q_t(a) \right).$$
Se $A_t \neq a$, ent√£o $Q_{t+1}(a) = Q_t(a)$.  $\blacksquare$

> üí° **Exemplo Num√©rico (Atualiza√ß√£o Incremental):** Usando os mesmos dados do exemplo anterior, vamos calcular $Q_t(a)$ usando a atualiza√ß√£o incremental.
>
> - **Para $a_1$:**
>   - $Q_1(a_1) = 0$
>   - $N_1(a_1) = 0$, $Q_2(a_1) = 0 + \frac{1}{0+1}(1-0) = 1$
>   - $N_2(a_1) = 1$, $Q_3(a_1) = 1 + \frac{1}{1+1}(2-1) = 1 + 0.5 = 1.5$
>   - $N_3(a_1) = 2$, $Q_4(a_1) = 1.5 + \frac{1}{2+1}(1-1.5) = 1.5 - 0.166 = 1.33$
>   - $Q_5(a_1) = 1.33$ (pois $a_1$ n√£o foi selecionada)
>
> - **Para $a_2$:**
>   - $Q_1(a_2) = 0$
>   - $N_1(a_2) = 0$, $Q_2(a_2) = 0 + \frac{1}{0+1}(0-0) = 0$
>   - $Q_3(a_2) = 0$ (pois $a_2$ n√£o foi selecionada)
>   - $Q_4(a_2) = 0$ (pois $a_2$ n√£o foi selecionada)
>  - $N_4(a_2) = 1$, $Q_5(a_2) = 0 + \frac{1}{1+1}(1-0) = 0 + 0.5 = 0.5$
>
> Os resultados s√£o id√™nticos ao c√°lculo direto, demonstrando a equival√™ncia da atualiza√ß√£o incremental.
>
> A vantagem da atualiza√ß√£o incremental √© que n√£o precisamos guardar todas as recompensas anteriores, apenas o valor estimado e o n√∫mero de vezes que a a√ß√£o foi selecionada, economizando mem√≥ria e c√°lculos.

Este lema demonstra que o valor estimado $Q_t(a)$ pode ser atualizado incrementalmente, sem a necessidade de recalcular a soma total das recompensas a cada passo. Esta forma recursiva √© fundamental para a implementa√ß√£o eficiente de algoritmos de aprendizado por refor√ßo, especialmente em ambientes onde o n√∫mero de itera√ß√µes √© grande.

√â importante ressaltar que o **sample-average method** √© apenas uma das formas de estimar os valores de a√ß√£o e, embora seja simples, n√£o √© necessariamente a melhor em todos os contextos. O m√©todo √© especialmente adequado para **problemas estacion√°rios**, onde as probabilidades de recompensa n√£o mudam ao longo do tempo [^8].

Para a tomada de decis√µes, o agente pode utilizar o **greedy action selection**, ou seja, escolher a a√ß√£o com maior valor estimado a cada instante [^3]:
$$A_t = \text{argmax}_a \, Q_t(a)$$
onde $\text{argmax}_a$ denota a a√ß√£o *a* que maximiza a express√£o seguinte [^3]. Esta regra garante que o agente explore o conhecimento atual para maximizar a recompensa imediata. Se houver mais de uma a√ß√£o com o maior valor estimado, a escolha pode ser feita arbitrariamente, por exemplo, aleatoriamente. Contudo, ao selecionar sempre as a√ß√µes greedy, o agente pode n√£o estar explorando outras a√ß√µes com potencial para melhor recompensa a longo prazo [^3]. Para mitigar essa limita√ß√£o, o agente pode introduzir explora√ß√£o por meio de a√ß√µes n√£o-greedy.

> üí° **Exemplo Num√©rico (Sele√ß√£o Greedy):** Usando os valores estimados no exemplo anterior na intera√ß√£o 5, $Q_5(a_1) = 1.33$ e $Q_5(a_2) = 0.5$.  A sele√ß√£o greedy escolheria a a√ß√£o $a_1$ pois $Q_5(a_1) > Q_5(a_2)$.

**Lema 1:**
_A lei dos grandes n√∫meros garante que $Q_t(a)$ converge para $q_*(a)$, dada a defini√ß√£o do **sample-average method**_.
_Prova:_
A lei dos grandes n√∫meros afirma que, para uma sequ√™ncia de vari√°veis aleat√≥rias independentes e identicamente distribu√≠das com uma m√©dia $\mu$, a m√©dia amostral converge para $\mu$ √† medida que o n√∫mero de amostras tende para o infinito. No contexto do sample-average method, a recompensa $R_i$ √© uma vari√°vel aleat√≥ria com m√©dia $q_*(a)$ dado que a a√ß√£o $a$ foi escolhida, e s√£o independentes. Portanto, a m√©dia amostral $Q_t(a)$ converge para $q_*(a)$ quando t tende ao infinito. $\blacksquare$

**Corol√°rio 1:**
_O sample-average method √© um estimador consistente do valor de uma a√ß√£o._

Este corol√°rio √© uma consequ√™ncia direta do Lemma 1. Como $Q_t(a)$ converge para $q_*(a)$ com o aumento do n√∫mero de amostras, o m√©todo de m√©dias amostrais √© um estimador consistente para o valor da a√ß√£o.

**Teorema 1:** _Se as recompensas s√£o limitadas, isto √©, existe um $M > 0$ tal que $|R_t| \leq M$ para todo $t$, e se todas as a√ß√µes s√£o selecionadas infinitamente, ent√£o, com probabilidade 1, $Q_t(a) \to q_*(a)$ quando $t \to \infty$, para toda a√ß√£o $a$._
_Prova:_
Pelo Lema 1, sabemos que $Q_t(a)$ converge para $q_*(a)$ se o n√∫mero de amostras para a a√ß√£o $a$ tende para o infinito. Se as recompensas s√£o limitadas e todas as a√ß√µes s√£o selecionadas infinitamente, o n√∫mero de amostras para cada a√ß√£o tender√° para o infinito √† medida que $t$ tende para o infinito. A lei forte dos grandes n√∫meros garante que, nesse caso, a converg√™ncia de $Q_t(a)$ para $q_*(a)$ ocorre com probabilidade 1, completando a prova. $\blacksquare$

> üí° **Exemplo Num√©rico (Converg√™ncia):**  Suponha que a a√ß√£o $a_1$ tenha um valor verdadeiro de $q_*(a_1)= 2.0$ e gera recompensas aleat√≥rias com m√©dia 2.0 (por exemplo, usando uma distribui√ß√£o normal com m√©dia 2.0 e desvio padr√£o 1.0). Ao longo de muitas intera√ß√µes, a estimativa $Q_t(a_1)$ obtida pelo sample-average method ir√° gradualmente convergir para 2.0. Podemos simular isso para visualizar a converg√™ncia.
>
>```python
>import numpy as np
>import matplotlib.pyplot as plt
>
>np.random.seed(42)
>
>def simulate_sample_average(true_value, num_iterations):
>    rewards = np.random.normal(loc=true_value, scale=1.0, size=num_iterations)
>    estimated_values = np.cumsum(rewards) / np.arange(1, num_iterations + 1)
>    return estimated_values
>
>true_value_a1 = 2.0
>iterations = 1000
>estimated_values_a1 = simulate_sample_average(true_value_a1, iterations)
>
>plt.plot(range(1, iterations + 1), estimated_values_a1)
>plt.axhline(y=true_value_a1, color='r', linestyle='--', label='Valor Verdadeiro')
>plt.xlabel('Itera√ß√µes')
>plt.ylabel('Valor Estimado $Q_t(a_1)$')
>plt.title('Converg√™ncia de $Q_t(a_1)$ para $q_*(a_1)$')
>plt.legend()
>plt.grid(True)
>plt.show()
>```
> Este c√≥digo simula a evolu√ß√£o de  $Q_t(a_1)$ ao longo de 1000 itera√ß√µes, mostrando a converg√™ncia para o valor verdadeiro. O gr√°fico demonstra que a m√©dia amostral $Q_t(a_1)$ aproxima-se gradualmente de $q_*(a_1)$ confirmando a converg√™ncia.

```mermaid
graph LR
    A["N√∫mero de Amostras (t)"] --> B("M√©dia Amostral Q_t(a)");
    B --> C{/"Lei dos Grandes N√∫meros"/};
    C --> D["Converg√™ncia de Q_t(a) para q_*(a)"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2 stroke-width:2px;

```

O Teorema 1 formaliza a propriedade de converg√™ncia do m√©todo de m√©dia amostral sob condi√ß√µes mais gerais, incluindo a limita√ß√£o das recompensas, uma condi√ß√£o geralmente satisfeita em muitos problemas pr√°ticos.

### Conclus√£o

O **sample-average method** oferece uma abordagem simples e fundamental para estimar o valor das a√ß√µes no problema de **k-armed bandits**. Ele utiliza a m√©dia das recompensas observadas para calcular o valor estimado das a√ß√µes e converge para o valor verdadeiro quando o n√∫mero de amostras √© grande o suficiente. Apesar de sua simplicidade, este m√©todo √© uma base importante para abordagens mais complexas de aprendizado por refor√ßo e oferece uma intui√ß√£o clara sobre como os valores de a√ß√£o podem ser estimados a partir da experi√™ncia. A limita√ß√£o principal do m√©todo √© sua depend√™ncia de dados passados, o que o torna inadequado para ambientes n√£o estacion√°rios onde as recompensas mudam com o tempo. Em tais cen√°rios, m√©todos mais adaptativos se fazem necess√°rios.

### Refer√™ncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de <Chapter 2>)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected: q*(a) = E[Rt | At=a]." *(Trecho de <Chapter 2>)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: Qt(a) = (sum of rewards when a taken prior to t) / (number of times a taken prior to t) = (Œ£(t-1)_(i=1) Ri1_(A_i=a)) / (Œ£(t-1)_(i=1) 1_(A_i=a)), where 1_(predicate) denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define Qt(a) as some default value, such as 0. As the denominator goes to infinity, by the law of large numbers, Qt(a) converges to q‚àó(a). We call this the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards. Of course this is just one way to estimate action values, and not necessarily the best one. Nevertheless, for now let us stay with this simple estimation method and turn to the question of how the estimates might be used to select actions." *(Trecho de <Chapter 2>)*
[^8]: "The averaging methods discussed so far are appropriate for stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time." *(Trecho de <Chapter 2>)*
