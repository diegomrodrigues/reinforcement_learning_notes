## M√©todos de Valor de A√ß√£o para o Problema Multi-Armed Bandit

### Introdu√ß√£o
No contexto do aprendizado por refor√ßo (reinforcement learning), uma das caracter√≠sticas distintivas √© o uso de informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de simplesmente instruir sobre as a√ß√µes corretas [1]. Essa abordagem gera uma necessidade de explora√ß√£o ativa, ou seja, uma busca expl√≠cita por um comportamento √≥timo. O feedback puramente avaliativo indica qu√£o boa foi uma a√ß√£o tomada, mas n√£o necessariamente se foi a melhor ou a pior poss√≠vel. Em contraste, o feedback puramente instrutivo indica a a√ß√£o correta a ser tomada, independentemente da a√ß√£o que foi efetivamente realizada. Em um ambiente de aprendizado por refor√ßo simplificado, onde n√£o h√° necessidade de aprender a agir em mais de uma situa√ß√£o, o foco recai sobre a avalia√ß√£o, sendo que esse cen√°rio √© bem explorado atrav√©s do problema do *k-armed bandit* [1]. Este cap√≠tulo abordar√° os m√©todos de valor de a√ß√£o, cruciais para resolver o problema do *k-armed bandit*, atrav√©s da estimativa do valor das a√ß√µes para tomada de decis√µes, que ser√£o os pilares para as pr√≥ximas se√ß√µes do livro.

### Conceitos Fundamentais

Um problema de *k-armed bandit* envolve escolher repetidamente entre *k* diferentes op√ß√µes, ou a√ß√µes. Ap√≥s cada escolha, o agente recebe uma recompensa num√©rica proveniente de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o selecionada. O objetivo √© maximizar a recompensa total esperada ao longo de um per√≠odo de tempo [1].

Cada a√ß√£o *a* possui um valor esperado ou m√©dio, denotado por $q_*(a)$, que representa a recompensa m√©dia obtida ao selecionar essa a√ß√£o. Formalmente, $q_*(a) = E[R_t|A_t = a]$, onde $A_t$ √© a a√ß√£o selecionada no instante *t* e $R_t$ √© a recompensa correspondente [1]. A dificuldade √© que esses valores $q_*(a)$ s√£o desconhecidos e precisam ser estimados. Denota-se a estimativa do valor da a√ß√£o *a* no instante *t* por $Q_t(a)$. O objetivo √© que $Q_t(a)$ se aproxime de $q_*(a)$ ao longo do tempo.

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de 3-armed bandit (k=3). As recompensas reais para cada a√ß√£o s√£o: $q_*(1) = 2$, $q_*(2) = 4$, e $q_*(3) = 1$. Inicialmente, n√£o sabemos esses valores, e nossas estimativas s√£o $Q_1(1) = 0$, $Q_1(2) = 0$, e $Q_1(3) = 0$. A cada passo, selecionamos uma a√ß√£o, recebemos uma recompensa (que √© uma amostra da distribui√ß√£o com a m√©dia $q_*(a)$) e atualizamos nossa estimativa $Q_t(a)$.

**Explora√ß√£o vs. Explota√ß√£o:** Uma vez que as estimativas dos valores das a√ß√µes s√£o mantidas, a cada instante existe pelo menos uma a√ß√£o com maior valor estimado, que chamamos de **a√ß√µes gananciosas** (*greedy actions*).  Ao selecionar uma dessas a√ß√µes, o agente est√° *explorando* seu conhecimento atual sobre os valores das a√ß√µes. Caso o agente selecione a√ß√µes n√£o gananciosas, o mesmo estar√° *explorando*, o que possibilita o melhoramento das estimativas do valor das a√ß√µes n√£o gananciosas. A explora√ß√£o maximiza a recompensa esperada no momento, enquanto a explora√ß√£o pode levar a maiores recompensas totais a longo prazo. O equil√≠brio entre esses dois objetivos √© conhecido como o "conflito" entre explora√ß√£o e explota√ß√£o [1]. A decis√£o de quando explorar e quando explotar √© complexa e depende das incertezas das estimativas e do n√∫mero de passos restantes.

Os **m√©todos de valor de a√ß√£o** s√£o usados para estimar os valores das a√ß√µes e usar essas estimativas para tomar decis√µes de sele√ß√£o de a√ß√µes. Uma forma natural de estimar o valor de uma a√ß√£o √© atrav√©s da m√©dia das recompensas obtidas ao selecionar essa a√ß√£o, o que se conhece por m√©todo de m√©dias amostrais (*sample-average method*):
$$Q_t(a) = \frac{\text{soma das recompensas quando a foi selecionada at√© t}}{\text{n√∫mero de vezes que a foi selecionada at√© t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}},$$
onde $\mathbb{1}_{predicate}$ √© uma fun√ß√£o indicadora que retorna 1 se o *predicate* for verdadeiro e 0 caso contr√°rio. Se o denominador for zero, $Q_t(a)$ √© definido como um valor padr√£o, como 0. Pela lei dos grandes n√∫meros, quando o denominador tende ao infinito, $Q_t(a)$ converge para $q_*(a)$ [1].

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, ap√≥s a primeira rodada, suponha que escolhemos a a√ß√£o 2 (o bra√ßo do meio) e recebemos uma recompensa de 3. As estimativas das a√ß√µes se tornam:
>
> $Q_2(1) = 0$ (n√£o foi selecionada)
> $Q_2(2) = \frac{3}{1} = 3$
> $Q_2(3) = 0$ (n√£o foi selecionada)
>
> Agora, suponha que na segunda rodada escolhemos a a√ß√£o 2 novamente, e recebemos uma recompensa de 5. Ent√£o as estimativas s√£o atualizadas:
>
> $Q_3(1) = 0$
> $Q_3(2) = \frac{3 + 5}{2} = 4$
> $Q_3(3) = 0$
>
> Se na terceira rodada, escolhemos a a√ß√£o 1 e recebemos uma recompensa de 2, as estimativas se tornam:
>
> $Q_4(1) = \frac{2}{1} = 2$
> $Q_4(2) = 4$
> $Q_4(3) = 0$
>
> Observe que $Q_t(2)$ j√° est√° igual ao valor esperado $q_*(2)$.

**Lema 1** O m√©todo de m√©dias amostrais pode ser computado de forma incremental, o que √© computacionalmente eficiente e mais adequado para problemas em que o n√∫mero de amostras por a√ß√£o √© grande.

*Prova:* Seja $N_t(a)$ o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o instante *t*. Ent√£o, podemos reescrever a f√≥rmula de atualiza√ß√£o de $Q_t(a)$ como:
$Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}$.
Agora, consideremos $Q_{t-1}(a) = \frac{1}{N_{t-1}(a)} \sum_{i=1}^{t-2} R_i \mathbb{1}_{A_i=a}$. Se a a√ß√£o *a* foi selecionada no instante *t-1*, ent√£o $N_t(a) = N_{t-1}(a) + 1$ e a soma de recompensas aumentou em $R_{t-1}$. Se n√£o, $N_t(a) = N_{t-1}(a)$ e a soma de recompensas n√£o se altera. Portanto,
$$Q_t(a) =  \begin{cases} Q_{t-1}(a) + \frac{1}{N_t(a)}[R_{t-1} - Q_{t-1}(a)] & \text{se } A_{t-1} = a \\ Q_{t-1}(a) & \text{se } A_{t-1} \neq a \end{cases} $$
Esta forma incremental evita recalcular a soma inteira das recompensas a cada passo, tornando o m√©todo computacionalmente mais eficiente. Note que, se $N_t(a) = 0$, podemos assumir $Q_t(a) = 0$, para algum valor padr√£o. $\square$
```mermaid
graph LR
    A("Estado t-1: Q_t-1(a)") -->|A√ß√£o a Selecionada| B("Atualiza√ß√£o: N_t(a) = N_t-1(a) + 1");
    A -->|A√ß√£o a N√£o Selecionada| C("Nenhuma Atualiza√ß√£o: Q_t(a) = Q_t-1(a)");
    B --> D("Calcular Q_t(a) = Q_t-1(a) + (1/N_t(a)) * (R_t-1 - Q_t-1(a))");
    D --> E("Estado t: Q_t(a)");
    C --> E
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#aaf,stroke:#333,stroke-width:2px
    style E fill:#afa,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, com as atualiza√ß√µes incrementais. Sabemos que $Q_3(2) = 4$ e que na terceira rodada escolhemos a a√ß√£o 1 e recebemos $R_3=2$. Temos tamb√©m: $N_3(1) = 0$, $N_3(2) = 2$ e $N_3(3) = 0$. Para calcular $Q_4$ incrementalmente:
>
> $Q_4(1) = Q_3(1) + \frac{1}{N_4(1)}(R_3 - Q_3(1)) = 0 + \frac{1}{1}(2-0) = 2$
> $Q_4(2) = Q_3(2) = 4$
> $Q_4(3) = Q_3(3) = 0$
>
> Assim, chegamos ao mesmo resultado da forma n√£o incremental.

O m√©todo de sele√ß√£o de a√ß√µes mais simples √© selecionar aquela com o maior valor estimado, ou seja, uma a√ß√£o gananciosa:

$$A_t = \underset{a}{\text{argmax}} \, Q_t(a)$$

onde $\text{argmax}_a$ denota a a√ß√£o *a* que maximiza a express√£o que segue. Caso haja empate, a sele√ß√£o √© feita de forma arbitr√°ria, talvez aleatoriamente. Essa abordagem explora o conhecimento atual, mas n√£o explora outras a√ß√µes potencialmente melhores. Um m√©todo alternativo √© comportar-se de forma gananciosa na maioria das vezes, mas ocasionalmente, com uma pequena probabilidade $\epsilon$, selecionar uma a√ß√£o aleatoriamente, chamada de m√©todo **$\epsilon$-ganancioso** [1]. M√©todos $\epsilon$-gananciosos garantem que, no limite, todas as a√ß√µes ser√£o amostradas infinitas vezes, assegurando que $Q_t(a)$ converja para $q_*(a)$, e a probabilidade de selecionar a a√ß√£o √≥tima converge para valores pr√≥ximos de 1 - $\epsilon$.

> üí° **Exemplo Num√©rico:** Continuando com o exemplo, ap√≥s 4 passos, temos $Q_4(1) = 2$, $Q_4(2) = 4$, e $Q_4(3) = 0$. Se usarmos o m√©todo ganancioso, escolher√≠amos a a√ß√£o 2 (porque 4 √© o maior valor). Se usarmos o m√©todo $\epsilon$-ganancioso com $\epsilon = 0.1$, escolher√≠amos a a√ß√£o 2 com probabilidade 0.9, e uma das tr√™s a√ß√µes aleatoriamente (incluindo a 2) com probabilidade 0.1.
```mermaid
sequenceDiagram
    participant Agente
    participant Ambiente
    Agente->>Ambiente: Escolhe A√ß√£o (At)
    Ambiente-->>Agente: Recompensa (Rt)
    Agente->>Agente: Calcula Q_t(a) usando m√©dia amostral
    alt M√©todo Ganancioso
        Agente->>Agente: A_t = argmax Q_t(a)
    else M√©todo Œµ-Ganancioso
        Agente->>Agente: Com probabilidade Œµ, escolhe a√ß√£o aleat√≥ria
        Agente->>Agente: Com probabilidade 1-Œµ, A_t = argmax Q_t(a)
    end
    loop Pr√≥ximo passo
        Agente->>Ambiente: Escolhe A√ß√£o (At)
        Ambiente-->>Agente: Recompensa (Rt)
        Agente->>Agente: Calcula Q_t(a)
    end
```
**Proposi√ß√£o 1** O m√©todo $\epsilon$-ganancioso garante que todas as a√ß√µes ser√£o amostradas infinitas vezes se o n√∫mero de passos tende ao infinito.

*Prova:* A probabilidade de selecionar qualquer a√ß√£o *a* usando a estrat√©gia $\epsilon$-gananciosa √© de pelo menos $\frac{\epsilon}{k}$, onde *k* √© o n√∫mero total de a√ß√µes.  Como $\epsilon > 0$, essa probabilidade √© sempre positiva.  Se executarmos um n√∫mero infinito de passos, haver√° uma quantidade infinita de tentativas com probabilidade pelo menos $\frac{\epsilon}{k}$ de selecionar qualquer a√ß√£o *a*.  Assim, cada a√ß√£o ser√° selecionada um n√∫mero infinito de vezes.  Note que n√£o √© necessario que $\epsilon$ seja constante, podendo decrescer com o tempo, por exemplo. $\square$

**Teorema 1** Sob certas condi√ß√µes, para um problema k-armed bandit com recompensas estacion√°rias e usando um m√©todo $\epsilon$-ganancioso com m√©dias amostrais, a estimativa $Q_t(a)$ converge para o valor verdadeiro $q_*(a)$ √† medida que o tempo $t$ tende ao infinito.

*Prova:* Pela lei dos grandes n√∫meros, a m√©dia amostral de um conjunto de vari√°veis aleat√≥rias independentes e identicamente distribu√≠das converge para o valor esperado dessas vari√°veis √† medida que o n√∫mero de amostras tende ao infinito. No caso do m√©todo de m√©dias amostrais, $Q_t(a)$ √© a m√©dia das recompensas obtidas ao selecionar a a√ß√£o *a*. Como o problema do k-armed bandit pressup√µe uma distribui√ß√£o de probabilidade estacion√°ria para cada a√ß√£o *a*, as recompensas recebidas para uma determinada a√ß√£o s√£o vari√°veis aleat√≥rias independentes e identicamente distribu√≠das. O m√©todo $\epsilon$-ganancioso garante que, no limite, todas as a√ß√µes ser√£o selecionadas infinitas vezes (Proposi√ß√£o 1). Portanto, com um n√∫mero infinito de amostras, $Q_t(a)$ converge para $q_*(a)$ para todas as a√ß√µes $a$.
$\square$
```mermaid
graph LR
    A["Lei dos Grandes N√∫meros: M√©dia Amostral -> Valor Esperado"]
    B["Problema k-Armed Bandit: Recompensas Estacion√°rias"]
    C["M√©todo Œµ-Ganancioso: Todas a√ß√µes amostradas infinitas vezes"]
    D["Estimativa Q_t(a)"]
    E["Valor Verdadeiro q*(a)"]
    A --> D
    B --> D
    C --> D
    D --> E
    style A fill:#f9f,stroke:#333,stroke-width:2px
     style B fill:#ccf,stroke:#333,stroke-width:2px
      style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#aaf,stroke:#333,stroke-width:2px
        style E fill:#afa,stroke:#333,stroke-width:2px
    linkStyle 0,1,2 stroke-width:2px
    linkStyle 3 stroke-width:2px,stroke:green
```

**Corol√°rio 1** Em um problema k-armed bandit, se para cada a√ß√£o *a*, as recompensas forem limitadas no intervalo [0, M], e se usarmos o m√©todo $\epsilon$-ganancioso, ent√£o o erro $|Q_t(a) - q_*(a)|$ ser√° limitado superiormente por uma fun√ß√£o que tende a zero quando $t \to \infty$.
*Prova:* O teorema 1 garante a converg√™ncia de $Q_t(a)$ para $q_*(a)$. Como as recompensas est√£o limitadas no intervalo [0, M], podemos usar a desigualdade de Hoeffding para caracterizar a converg√™ncia.
Seja $X_1, X_2, \ldots, X_n$ vari√°veis aleat√≥rias independentes tais que $a \leq X_i \leq b$ para todo $i$.
Seja $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$. Ent√£o, para todo $\epsilon > 0$:
$$P(|\bar{X} - E[\bar{X}]| \geq \epsilon) \leq 2 \exp\left(-\frac{2n\epsilon^2}{(b-a)^2}\right)$$
No nosso caso, as recompensas est√£o limitadas no intervalo [0,M]. Portanto, $b-a = M$. Seja $N_t(a)$ o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o instante *t*. Ent√£o:
$$P(|Q_t(a) - q_*(a)| \geq \epsilon) \leq 2 \exp\left(-\frac{2N_t(a)\epsilon^2}{M^2}\right)$$
Como o m√©todo $\epsilon$-ganancioso garante que $N_t(a)$ cresce at√© o infinito quando $t$ tamb√©m tende a infinito, o termo $2 \exp\left(-\frac{2N_t(a)\epsilon^2}{M^2}\right)$ tende para 0. Isso demonstra que a probabilidade de $|Q_t(a) - q_*(a)|$ ser maior que um $\epsilon$ arbitr√°rio tende a 0 quando $t$ tende a infinito. Portanto, o erro $|Q_t(a) - q_*(a)|$ √© limitado superiormente por uma fun√ß√£o que tende a zero quando $t \to \infty$. $\square$

### Conclus√£o

Este cap√≠tulo estabeleceu os fundamentos dos m√©todos de valor de a√ß√£o, apresentando o problema do *k-armed bandit* e o dilema entre explora√ß√£o e explota√ß√£o. A introdu√ß√£o do m√©todo de m√©dias amostrais e a estrat√©gia $\epsilon$-gananciosa s√£o cruciais para as pr√≥ximas se√ß√µes, que aprofundar√£o as t√©cnicas de aprendizado por refor√ßo, construindo a base necess√°ria para lidar com problemas mais complexos. Ao longo do livro, esses m√©todos ser√£o estendidos e combinados com outras t√©cnicas para resolver os desafios do aprendizado por refor√ßo em sua forma mais geral.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken. In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback differs from, and yet can be combined with, instructive feedback. The particular nonassociative, evaluative feedback problem that we explore is a simple version of the k-armed bandit problem. We use this problem to introduce a number of basic learning methods which we extend in later chapters to apply to the full rein- forcement learning problem. At the end of this chapter, we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when the best action depends on the situation. Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps. This is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or ‚Äúone-armed bandit,‚Äù except that it has k levers instead of one. Each action selection is like a play of one of the slot machine‚Äôs levers, and the rewards are the payoffs for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers. Another analogy is that of a doctor choosing between experimental treatments for a series of seriously ill patients. Each action is the selection of a treatment, and each reward is the survival or well-being of the patient. Today the term ‚Äúbandit problem" is sometimes used for a generalization of the problem described above, but in this book we use it to refer just to this simple case. In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected:
$q_*(a) = E[R_t | A_t=a]$. If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time stept as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_*(a)$. If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action‚Äôs value. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. For example, suppose a greedy action‚Äôs value is known with certainty, while several other actions are estimated to be nearly as good but with substantial uncertainty. The uncertainty is such that at least one of these other actions probably is actually better than the greedy action, but you don‚Äôt know which one. If you have many time steps ahead on which to make action selections, then it may be better to explore the nongreedy actions and discover which of them are better than the greedy action. Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the ‚Äúconflict‚Äù between exploration and exploitation. In any specific case, whether it is better to explore or exploit depends in a complex way on the precise values of the estimates, uncertainties, and the number of remaining steps. There are many sophisticated methods for balancing exploration and exploitation for particular mathematical formulations of the k-armed bandit and related problems."
[^2]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received:
$Q_t(a) = \frac{\text{sum of rewards when a taken prior to t}}{\text{number of times a taken prior to t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}$
where $\mathbb{1}_{predicate}$ denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as 0. As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_*(a)$. We call this the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards. Of course this is just one way to estimate action values, and not necessarily the best one. Nevertheless, for now let us stay with this simple estimation method and turn to the question of how the estimates might be used to select actions. The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as
$A_t = \underset{a}{\text{argmax}} \, Q_t(a)$,
where argmaxa denotes the action a for which the expression that follows is maximized (with ties broken arbitrarily). Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability …õ, instead select randomly"
