## M√©todos Œµ-Gananciosos para o Problema Multi-Armed Bandit

### Introdu√ß√£o

O aprendizado por refor√ßo (reinforcement learning) se distingue de outros tipos de aprendizado por utilizar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de instru√≠-las fornecendo a√ß√µes corretas [1]. Essa caracter√≠stica cria a necessidade de explora√ß√£o ativa, ou seja, uma busca expl√≠cita por um bom comportamento. O feedback puramente avaliativo indica o qu√£o boa foi a a√ß√£o tomada, mas n√£o se foi a melhor ou a pior a√ß√£o poss√≠vel [1]. O problema do *k-armed bandit* √© usado para explorar esse aspecto avaliativo do aprendizado por refor√ßo em um cen√°rio simplificado, onde o aprendizado n√£o envolve agir em diversas situa√ß√µes, um cen√°rio *n√£o associativo* [1]. Este cap√≠tulo foca em m√©todos que equilibram a explora√ß√£o e a explota√ß√£o de forma elementar [3].

### Conceitos Fundamentais

Em um problema *k-armed bandit*, o agente repetidamente escolhe entre *k* op√ß√µes (a√ß√µes), recebendo uma recompensa num√©rica de acordo com a a√ß√£o selecionada [1]. O objetivo √© maximizar a recompensa total esperada ao longo de um per√≠odo de tempo [2]. Cada a√ß√£o *a* possui um valor $q_*(a)$, representando a recompensa esperada quando essa a√ß√£o √© selecionada [2]:

$$
q_*(a) = E[R_t | A_t = a]
$$

onde $A_t$ √© a a√ß√£o selecionada no instante $t$ e $R_t$ √© a recompensa correspondente [2]. O valor real das a√ß√µes √© desconhecido, mas podemos estim√°-los. Denotamos a estimativa do valor da a√ß√£o *a* no instante *t* como $Q_t(a)$. O objetivo √© que $Q_t(a)$ se aproxime de $q_*(a)$ [2].

As **a√ß√µes gananciosas** s√£o aquelas com a maior estimativa de valor. **Explorar** significa escolher uma a√ß√£o n√£o gananciosa com o objetivo de melhorar as estimativas dos valores das a√ß√µes [2]. Enquanto a explota√ß√£o maximiza a recompensa no passo atual, a explora√ß√£o pode gerar uma recompensa total maior no longo prazo [2]. Existe um conflito entre explora√ß√£o e explota√ß√£o, pois n√£o √© poss√≠vel fazer ambas simultaneamente em uma √∫nica sele√ß√£o de a√ß√£o [2].

**M√©todos de valor de a√ß√£o** s√£o aqueles que estimam os valores das a√ß√µes e usam essas estimativas para tomar decis√µes de sele√ß√£o [3]. O valor verdadeiro de uma a√ß√£o √© a recompensa m√©dia quando essa a√ß√£o √© selecionada. Uma forma natural de estimar isso √© calcular a m√©dia das recompensas recebidas:
$$
Q_t(a) = \frac{\text{soma das recompensas quando } a \text{ foi escolhida antes de } t}{\text{n√∫mero de vezes que } a \text{ foi escolhida antes de } t} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$
onde $\mathbb{1}_{\text{predicate}}$ √© uma vari√°vel aleat√≥ria que vale 1 se a predi√ß√£o √© verdadeira e 0 caso contr√°rio [3]. Se o denominador for zero, define-se $Q_t(a)$ como um valor padr√£o, como 0. Essa √© a forma de **m√©dia amostral** para estimar valores de a√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Considere um problema de 3-armed bandit. Inicialmente, assumimos que $Q_1(a)$ = 0 para todas as a√ß√µes.
>
> - Na itera√ß√£o $t=1$, escolhemos a a√ß√£o 1 e recebemos uma recompensa $R_1$ = 5.
> - Na itera√ß√£o $t=2$, escolhemos a a√ß√£o 2 e recebemos uma recompensa $R_2$ = 2.
> - Na itera√ß√£o $t=3$, escolhemos a a√ß√£o 1 novamente e recebemos $R_3$ = 6.
>
> Agora, vamos calcular os valores de a√ß√£o estimados $Q_t(a)$ para cada a√ß√£o at√© o instante $t=4$:
>
> - Para a a√ß√£o 1:
>   - $Q_1(1)$ = 0 (valor padr√£o)
>   - $Q_2(1)$ = 5 / 1 = 5
>   - $Q_3(1)$ = (5 + 6) / 2 = 5.5
> - Para a a√ß√£o 2:
>   - $Q_1(2)$ = 0
>   - $Q_2(2)$ = 2 / 1 = 2
>   - $Q_3(2)$ = 2 / 1 = 2
> - Para a a√ß√£o 3:
>   - $Q_1(3)$ = 0
>   - $Q_2(3)$ = 0
>   - $Q_3(3)$ = 0
>
> Assim, no instante $t=4$, teremos $Q_4(1)$ = 5.5, $Q_4(2)$ = 2, e $Q_4(3)$ = 0, considerando que nenhuma nova a√ß√£o foi selecionada ap√≥s t=3. Esses valores ser√£o usados para tomar decis√µes sobre qual a√ß√£o selecionar. Este exemplo ilustra como os valores de a√ß√£o s√£o atualizados com base nas recompensas recebidas e no n√∫mero de vezes que cada a√ß√£o √© escolhida.

**Proposi√ß√£o 1:** *A m√©dia amostral, como definida acima, √© um estimador n√£o-viesado para o valor real da a√ß√£o, $q_*(a)$, sob a condi√ß√£o de que as recompensas obtidas para cada a√ß√£o s√£o independentes e identicamente distribu√≠das (i.i.d.).*

*Prova:* Seja $N_t(a) = \sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}$ o n√∫mero de vezes que a a√ß√£o *a* foi selecionada antes de *t*. Se $N_t(a) > 0$, ent√£o
$E[Q_t(a)] = E\left[ \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{N_t(a)} \right] = \frac{1}{E[N_t(a)]} E\left[\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}\right]$. Como as recompensas s√£o i.i.d, o valor esperado da soma √© a soma dos valores esperados. $E[Q_t(a)] = \frac{1}{E[N_t(a)]} \sum_{i=1}^{t-1} E[R_i \mathbb{1}_{A_i=a}] $. Como $R_i$ √© condicionado a $A_i = a$, $E[R_i \mathbb{1}_{A_i=a}] = E[R_i|A_i=a] P(A_i=a) = q_*(a) P(A_i=a)$.  $E[Q_t(a)] = \frac{1}{E[N_t(a)]} \sum_{i=1}^{t-1} q_*(a) P(A_i=a) = q_*(a) \frac{1}{E[N_t(a)]} \sum_{i=1}^{t-1} P(A_i=a) = q_*(a) \frac{E[N_t(a)]}{E[N_t(a)]} = q_*(a)$. Assim, $E[Q_t(a)] = q_*(a)$, mostrando que $Q_t(a)$ √© um estimador n√£o-viesado para $q_*(a)$. $\blacksquare$
```mermaid
graph LR
    A["$E[Q_t(a)]$"] --> B["$E[ (\sum R_i * \mathbb{1}_{A_i=a}) / N_t(a) ]$"]
    B --> C["$(1 / E[N_t(a)]) * E[ \sum R_i * \mathbb{1}_{A_i=a} ]$"]
    C --> D["$(1 / E[N_t(a)]) * \sum E[ R_i * \mathbb{1}_{A_i=a} ]$"]
    D --> E["$(1 / E[N_t(a)]) * \sum E[R_i|A_i=a] * P(A_i=a)$"]
    E --> F["$(1 / E[N_t(a)]) * \sum q_*(a) * P(A_i=a)$"]
    F --> G["$q_*(a) * (1 / E[N_t(a)]) * \sum P(A_i=a)$"]
     G --> H["$q_*(a) * (E[N_t(a)] / E[N_t(a)])$"]
    H --> I["$q_*(a)$"]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style I fill:#ccf,stroke:#333,stroke-width:2px
```

### M√©todos Œµ-Gananciosos

Os m√©todos **Œµ-gananciosos** s√£o uma forma simples de equilibrar a explora√ß√£o e a explota√ß√£o [3]. A ideia √© selecionar a a√ß√£o com maior valor estimado (a√ß√£o gananciosa) na maioria das vezes, mas, com uma pequena probabilidade Œµ, selecionar uma a√ß√£o aleat√≥ria entre todas as a√ß√µes [3]. A probabilidade de selecionar uma a√ß√£o gananciosa √©, portanto, 1 - Œµ. Formalmente, a a√ß√£o $A_t$ selecionada √©:

$$
A_t =
\begin{cases}
\underset{a}{\text{argmax}} \, Q_t(a), & \text{com probabilidade } 1-\epsilon \\
\text{a√ß√£o aleat√≥ria}, & \text{com probabilidade } \epsilon
\end{cases}
$$

A a√ß√£o √© escolhida dentre as a√ß√µes que maximizam a fun√ß√£o Q com probabilidade $1 - \epsilon$, sendo os empates quebrados aleatoriamente. Caso contr√°rio, uma a√ß√£o aleat√≥ria √© escolhida [3]. Os m√©todos Œµ-gananciosos garantem que, com o tempo, todas as a√ß√µes ser√£o amostradas infinitas vezes e, por isso, todas as estimativas convergir√£o para seus valores verdadeiros, de forma que a probabilidade de selecionar a a√ß√£o √≥tima se aproximar√° de 1 - Œµ [4].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um problema de 4-armed bandit, onde as estimativas de valor $Q_t(a)$ no tempo $t$ s√£o:
>
> -   $Q_t(1)$ = 10
> -   $Q_t(2)$ = 12
> -   $Q_t(3)$ = 8
> -   $Q_t(4)$ = 11
>
> Usando um m√©todo Œµ-ganancioso com Œµ = 0.2, temos o seguinte processo de sele√ß√£o de a√ß√£o:
>
> 1.  Com probabilidade 1 - Œµ = 0.8, escolhemos a a√ß√£o gananciosa, que √© a a√ß√£o 2, pois tem o maior valor estimado (12).
> 2.  Com probabilidade Œµ = 0.2, escolhemos uma a√ß√£o aleat√≥ria entre as 4 a√ß√µes. Cada a√ß√£o tem uma probabilidade de 0.2/4 = 0.05 de ser selecionada.
>
>   Podemos visualizar esse processo usando um diagrama:
>
>   ```mermaid
>   graph LR
>       A["In√≠cio"] -->| "1-Œµ (0.8)" | B("Seleciona A√ß√£o Gananciosa (argmax Qt(a))")
>       A -->| "Œµ (0.2)" | C("Seleciona A√ß√£o Aleat√≥ria")
>       C -->| "1/k (0.25)" | D("A√ß√£o 1")
>       C -->| "1/k (0.25)" | E("A√ß√£o 2")
>        C -->| "1/k (0.25)" | F("A√ß√£o 3")
>        C -->| "1/k (0.25)" | G("A√ß√£o 4")
>       style A fill:#f9f,stroke:#333,stroke-width:2px
>       style B fill:#ccf,stroke:#333,stroke-width:2px
>       style C fill:#ccf,stroke:#333,stroke-width:2px
>   ```
>
> Portanto, no instante *t*, a probabilidade de escolher cada a√ß√£o √©:
>
>   -   A√ß√£o 1: 0.05 (s√≥ pela explora√ß√£o)
>   -   A√ß√£o 2: 0.8 + 0.05 = 0.85 (explora√ß√£o e explota√ß√£o)
>   -   A√ß√£o 3: 0.05 (s√≥ pela explora√ß√£o)
>   -   A√ß√£o 4: 0.05 (s√≥ pela explora√ß√£o)
>
> Esse exemplo ilustra como o m√©todo Œµ-ganancioso balanceia a escolha da a√ß√£o com maior valor estimado com a chance de explorar outras a√ß√µes para eventualmente encontrar melhores recompensas.

**Lemma 1:** *Em um m√©todo Œµ-ganancioso com duas a√ß√µes e Œµ = 0.5, a probabilidade de a a√ß√£o gananciosa ser selecionada √© 0.75.*

*Prova:* Seja $A_g$ a a√ß√£o gananciosa e $A_n$ a outra a√ß√£o. A probabilidade de selecionar $A_g$ √© a probabilidade de selecion√°-la de forma gananciosa, que √© 1 - Œµ, mais a probabilidade de selecion√°-la aleatoriamente, que √© Œµ/2 (pois h√° duas a√ß√µes e cada uma tem igual probabilidade de ser escolhida aleatoriamente). Portanto, P( $A_g$ ) = (1 - Œµ) + (Œµ/2) = 1 - Œµ/2. Com Œµ = 0.5, P( $A_g$ ) = 1 - 0.5/2 = 0.75. $\blacksquare$
```mermaid
graph LR
    A["P($A_g$)"] --> B["(1 - Œµ) + (Œµ/2)"]
    B --> C["1 - Œµ/2"]
    C --> D["1 - 0.5/2 = 0.75"]
     style A fill:#f9f,stroke:#333,stroke-width:2px
     style D fill:#ccf,stroke:#333,stroke-width:2px
```

**Lema 1.1:** *Em um m√©todo Œµ-ganancioso com k a√ß√µes, a probabilidade de uma a√ß√£o gananciosa ser selecionada √© dada por (1 - Œµ) + (Œµ/k), assumindo que h√° pelo menos uma a√ß√£o gananciosa.*

*Prova:* A probabilidade de selecionar uma a√ß√£o gananciosa √© a probabilidade de selecion√°-la de forma gananciosa, que √© (1 - Œµ), mais a probabilidade de selecion√°-la aleatoriamente, que √© Œµ/k (j√° que h√° k a√ß√µes e cada uma tem igual probabilidade de ser escolhida aleatoriamente). Portanto, P(A√ß√£o Gananciosa) = (1 - Œµ) + (Œµ/k). $\blacksquare$
```mermaid
graph LR
    A["P(A√ß√£o Gananciosa)"] --> B["(1 - Œµ) + (Œµ/k)"]
     style A fill:#f9f,stroke:#333,stroke-width:2px
     style B fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o exemplo de 4 a√ß√µes do exemplo anterior, com Œµ = 0.2. De acordo com o Lema 1.1, a probabilidade de selecionar a a√ß√£o gananciosa √©:
>
> $P(\text{A√ß√£o Gananciosa}) = (1 - \epsilon) + \frac{\epsilon}{k} = (1 - 0.2) + \frac{0.2}{4} = 0.8 + 0.05 = 0.85$.
>
> Isso confirma o resultado obtido no exemplo num√©rico anterior.

**Observa√ß√£o 1:** *O Lema 1 √© um caso especial do Lema 1.1 onde k=2.*

**Teorema 1:** *Se todas as recompensas s√£o limitadas, os valores de a√ß√£o estimados ($Q_t(a)$) em um m√©todo Œµ-ganancioso convergem para os seus valores verdadeiros ($q_*(a)$) com probabilidade 1 (quase certamente), conforme $t \rightarrow \infty$.*

*Prova:* Pelo Lema de Borel-Cantelli, se a soma das probabilidades de um evento ser infinito for finita, ent√£o a probabilidade de esse evento ocorrer infinitamente muitas vezes √© 0. Como cada a√ß√£o √© selecionada infinitas vezes no limite e os valores de recompensa s√£o limitados, a m√©dia amostral para cada a√ß√£o converge para o valor real da a√ß√£o quase certamente. $\blacksquare$
```mermaid
graph LR
    A["Recompensas limitadas"] --> B["Todas as a√ß√µes s√£o amostradas infinitas vezes (t->‚àû)"]
    B --> C["$Q_t(a)$ convergem para $q_*(a)$ quase certamente"]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar o Teorema 1, vamos simular um cen√°rio de 2-armed bandit por algumas itera√ß√µes (t) com recompensas limitadas (entre 0 e 10), executando um experimento e observando a converg√™ncia dos valores estimados.
>
> Suponha que os valores verdadeiros das a√ß√µes s√£o *$q_*(1) = 7$* e *$q_*(2) = 3$*. Usaremos um m√©todo Œµ-ganancioso com Œµ = 0.1.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def bandit_experiment(q_star, epsilon, steps):
>     k = len(q_star)
>     Q = np.zeros(k) # Initialize action values
>     N = np.zeros(k) # Initialize action counts
>     rewards_history = []
>
>     for t in range(steps):
>         if np.random.rand() < epsilon: # Explore
>             action = np.random.choice(k)
>         else: # Exploit
>             action = np.argmax(Q)
>
>         reward = np.random.normal(q_star[action], 1) # Sample reward (mean is q*, std =1)
>         N[action] += 1
>         Q[action] = Q[action] + (reward - Q[action]) / N[action] # Update action values
>         rewards_history.append(reward)
>
>     return Q, rewards_history
>
>
> # Experiment parameters
> q_star = [7, 3] # True action values
> epsilon = 0.1
> steps = 1000
>
> Q_final, rewards = bandit_experiment(q_star, epsilon, steps)
>
> print(f"Estimated action values after {steps} steps: Q = {Q_final}")
>
> # Visualization of Estimated Action Values
> plt.figure(figsize=(10, 5))
> plt.plot(np.cumsum(rewards) / np.arange(1, steps+1), label = "Average Reward", color='blue')
> plt.xlabel('Step')
> plt.ylabel('Average Reward')
> plt.title('Convergence of average reward')
> plt.legend()
> plt.grid(True)
> plt.show()
>
> # This will also print the estimated values:
> # Estimated action values after 1000 steps: Q = [6.986, 3.043]
> ```
>
> Ap√≥s 1000 passos, os valores estimados *Q* ‚âà [6.986, 3.043], que s√£o bastante pr√≥ximos dos valores verdadeiros *$q_*(1) = 7$* e *$q_*(2) = 3$*. O gr√°fico mostra a converg√™ncia da recompensa m√©dia ao longo do tempo, indicando que o m√©todo est√° aprendendo. O m√©todo Œµ-ganancioso garante que todas as a√ß√µes ser√£o selecionadas ao longo do tempo, resultando na converg√™ncia das estimativas para os seus valores reais.

### Conclus√£o

Os m√©todos Œµ-gananciosos s√£o uma abordagem simples e eficaz para equilibrar a explora√ß√£o e a explota√ß√£o em problemas do tipo *k-armed bandit*. Eles garantem que todas as a√ß√µes sejam amostradas, levando a uma converg√™ncia eventual das estimativas de valor, ao mesmo tempo que aproveitam a informa√ß√£o j√° obtida para maximizar a recompensa esperada. A simplicidade e a efic√°cia dos m√©todos Œµ-gananciosos os tornam uma ferramenta fundamental no aprendizado por refor√ßo.

### Refer√™ncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected: q*(a) = E[Rt | At=a]. If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time stept as Qt(a). We would like Qt(a) to be close to q*(a). If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action's value. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. For example, suppose a greedy action's value is known with certainty, while several other actions are estimated to be nearly as good but with substantial uncertainty. The uncertainty is such that at least one of these other actions probably is actually better than the greedy action, but you don't know which one. If you have many time steps ahead on which to make action selections, then it may be better to explore the nongreedy actions and discover which of them are better than the greedy action. Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the ‚Äúconflict‚Äù between exploration and exploitation." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: Qt(a) = sum of rewards when a taken prior to t/number of times a taken prior to t = (Œ£{t-1}_{i=1} Ri1{A_i=a})/(Œ£{t-1}_{i=1} 1{A_i=a}) where 1{predicate} denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define Qt(a) as some default value, such as 0. As the denominator goes to infinity, by the law of large numbers, Qt(a) converges to q‚àó(a). We call this the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards. Of course this is just one way to estimate action values, and not necessarily the best one. Nevertheless, for now let us stay with this simple estimation method and turn to the question of how the estimates might be used to select actions. The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as At = argmax(a) Qt(a), where argmax(a) denotes the action a for which the expression that follows is maximized (with ties broken arbitrarily). Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability …õ, instead select randomly from among all the actions with equal probability, independently of the action-value estimates. We call methods using this near-greedy action selection rule …õ-greedy methods." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^4]: "An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the Qt(a) converge to their respective q*(a). This of course implies that the probability of selecting the optimal action converges to greater than 1 ‚Äì …õ, that is, to near certainty. These are just asymptotic guarantees, however, and say little about the practical effectiveness of the methods." *(Trecho de Chapter 2: Multi-armed Bandits)*
