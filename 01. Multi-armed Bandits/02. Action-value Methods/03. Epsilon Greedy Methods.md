## M√©todos de Valor da A√ß√£o e a Abordagem $\epsilon$-Greedy

### Introdu√ß√£o

No contexto dos *k*-armed bandits, o aprendizado por refor√ßo busca o equil√≠brio entre **explora√ß√£o** e **explota√ß√£o** para maximizar a recompensa total esperada ao longo do tempo [^2]. Como vimos anteriormente, um desafio fundamental √© estimar os valores das a√ß√µes, $q_*(a)$, e utilizar essas estimativas, $Q_t(a)$, para tomar decis√µes informadas [^2]. Esta se√ß√£o explora em detalhes os **m√©todos de valor da a√ß√£o**, com √™nfase na abordagem $\epsilon$-greedy, que fornece um mecanismo simples e eficaz para equilibrar a explora√ß√£o de a√ß√µes menos conhecidas e a explota√ß√£o das a√ß√µes atualmente consideradas √≥timas [^2].

### Conceitos Fundamentais

**M√©todos de Valor da A√ß√£o:** Os m√©todos de valor da a√ß√£o [^2] baseiam-se na estimativa do valor de cada a√ß√£o e na utiliza√ß√£o dessas estimativas para selecionar a a√ß√£o a ser tomada. A estimativa do valor de uma a√ß√£o, $Q_t(a)$, representa a recompensa m√©dia esperada ao selecionar a a√ß√£o *a* no instante *t* [^2]. Uma abordagem natural para estimar esses valores √© a utiliza√ß√£o de m√©dias amostrais:

$$
Q_t(a) = \frac{\text{soma das recompensas quando *a* foi tomada antes de *t*}} {\text{n√∫mero de vezes que *a* foi tomada antes de *t*}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}} {\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$

onde $\mathbb{1}_{\text{predicate}}$ √© uma vari√°vel aleat√≥ria que vale 1 se *predicate* √© verdadeiro e 0 caso contr√°rio [^3]. Esta √© a abordagem de **m√©dia amostral** (sample-average method) [^3], onde cada estimativa √© a m√©dia das recompensas relevantes observadas.

> üí° **Exemplo Num√©rico:**
>
> Imagine um bandit de 3 bra√ßos. Ap√≥s 5 tentativas, o bra√ßo 1 foi puxado 2 vezes, com recompensas de 1 e 0. O bra√ßo 2 foi puxado 1 vez, com recompensa de 0. O bra√ßo 3 foi puxado 2 vezes, com recompensas de 0.5 e 1.
>
> As estimativas de valor seriam:
>
> *   $Q_5(1) = (1 + 0) / 2 = 0.5$
> *   $Q_5(2) = 0 / 1 = 0$
> *   $Q_5(3) = (0.5 + 1) / 2 = 0.75$

Para garantir que cada a√ß√£o seja selecionada pelo menos uma vez, especialmente no in√≠cio do aprendizado, √© comum inicializar as estimativas de valor $Q_t(a)$ com valores otimistas. Esse conceito √© conhecido como **inicializa√ß√£o otimista** (optimistic initial values).

> üí° **Exemplo Num√©rico:**
>
> Se inicializarmos todas as a√ß√µes com um valor de $Q_0(a) = 5$, mesmo que as recompensas reais sejam muito menores (e.g., entre 0 e 1), o agente ser√° incentivado a explorar todas as a√ß√µes no in√≠cio para verificar se elas realmente valem tanto quanto a estimativa inicial.

**A Estrat√©gia Greedy:** Uma estrat√©gia de sele√ß√£o de a√ß√£o simples √© a estrat√©gia *greedy*, onde a a√ß√£o com o maior valor estimado √© sempre selecionada [^2, 3]:

$$
A_t = \text{argmax}_a Q_t(a)
$$

onde $\text{argmax}_a$ denota a a√ß√£o *a* que maximiza a express√£o subsequente (com desempates resolvidos arbitrariamente) [^3]. A estrat√©gia *greedy* explora o conhecimento atual para maximizar a recompensa imediata, sem dedicar tempo para amostrar a√ß√µes aparentemente inferiores [^3].

> üí° **Exemplo Num√©rico:**
>
> Usando as estimativas do exemplo anterior ($Q_5(1) = 0.5$, $Q_5(2) = 0$, $Q_5(3) = 0.75$), a estrat√©gia *greedy* selecionaria a a√ß√£o 3 ($A_6 = 3$), pois √© a que tem o maior valor estimado.

**A Abordagem $\epsilon$-Greedy:** Uma alternativa para a abordagem *greedy* pura √© o m√©todo $\epsilon$-greedy, que introduz um grau de explora√ß√£o [^3]. Na maioria das vezes (com probabilidade $1-\epsilon$), a a√ß√£o *greedy* √© selecionada. No entanto, com uma pequena probabilidade $\epsilon$, uma a√ß√£o aleat√≥ria √© selecionada, independentemente das estimativas de valor [^3]. Isso garante que todas as a√ß√µes sejam amostradas infinitamente ao longo do tempo, permitindo a converg√™ncia assint√≥tica para os verdadeiros valores das a√ß√µes, $q_*(a)$ [^4].

Formalmente, a estrat√©gia $\epsilon$-greedy pode ser expressa como:

$$
A_t =
\begin{cases}
\text{argmax}_a Q_t(a) & \text{com probabilidade } 1 - \epsilon \\
\text{a√ß√£o aleat√≥ria} & \text{com probabilidade } \epsilon
\end{cases}
$$

> üí° **Exemplo Num√©rico:**
>
> Usando o mesmo exemplo anterior, mas com $\epsilon = 0.1$, a a√ß√£o 3 seria selecionada com probabilidade $0.9$, e uma das tr√™s a√ß√µes (1, 2 ou 3) seria selecionada aleatoriamente com probabilidade $0.1$. Cada a√ß√£o teria, portanto, uma probabilidade de $\frac{0.1}{3}$ de ser selecionada aleatoriamente.  A a√ß√£o 3, al√©m da probabilidade aleat√≥ria, tem tamb√©m a probabilidade *greedy*, totalizando $0.9 + \frac{0.1}{3} \approx 0.933$.

**Lema 1:** A probabilidade de selecionar a a√ß√£o √≥tima em qualquer passo *t* √© sempre maior ou igual a $\epsilon / k$, onde *k* √© o n√∫mero de a√ß√µes.

*Prova:* Mesmo quando a a√ß√£o *greedy* n√£o √© a a√ß√£o √≥tima, ainda existe uma probabilidade de $\epsilon$ de selecionar uma a√ß√£o aleat√≥ria. Como h√° *k* a√ß√µes, a probabilidade de selecionar a a√ß√£o √≥tima aleatoriamente √© $\epsilon / k$. Portanto, a probabilidade total de selecionar a a√ß√£o √≥tima √© pelo menos $\epsilon / k$.

**Prova de Lema 1:**
I. Seja $a^*$ a a√ß√£o √≥tima.
II. Pela defini√ß√£o da pol√≠tica $\epsilon$-greedy, a a√ß√£o $a^*$ pode ser selecionada de duas maneiras:
    *   Com probabilidade $1-\epsilon$, a a√ß√£o *greedy* (a que tem maior valor estimado) √© selecionada. Se $a^*$ √© a a√ß√£o *greedy*, ent√£o ela √© selecionada com probabilidade $1 - \epsilon$.
    *   Com probabilidade $\epsilon$, uma a√ß√£o √© selecionada aleatoriamente. Como h√° *k* a√ß√µes, a probabilidade de selecionar qualquer a√ß√£o espec√≠fica, incluindo $a^*$, √© $\epsilon / k$.
III. Portanto, a probabilidade total de selecionar a a√ß√£o √≥tima $a^*$ √© a soma das probabilidades de selecionar $a^*$ nas duas maneiras descritas no passo II. Mesmo que a a√ß√£o *greedy* n√£o seja a a√ß√£o √≥tima, ainda h√° uma probabilidade de $\epsilon / k$ de selecionar $a^*$.
IV.  Assim, a probabilidade total de selecionar a a√ß√£o √≥tima √© sempre maior ou igual a $\epsilon / k$. ‚ñ†

**Vantagens e Desvantagens:** Uma vantagem importante dos m√©todos $\epsilon$-greedy √© que, √† medida que o n√∫mero de passos aumenta, todas as a√ß√µes s√£o amostradas um n√∫mero infinito de vezes, garantindo que todas as estimativas $Q_t(a)$ convergem para seus respectivos $q_*(a)$ [^4]. Isso implica que a probabilidade de selecionar a a√ß√£o √≥tima converge para um valor maior que $1 - \epsilon$, ou seja, para uma proximidade da certeza [^4]. No entanto, essas s√£o apenas garantias assint√≥ticas e dizem pouco sobre a efic√°cia pr√°tica dos m√©todos [^4].

**Trade-off Explora√ß√£o-Explota√ß√£o:** A abordagem $\epsilon$-greedy exemplifica o trade-off fundamental entre explora√ß√£o e explota√ß√£o [^2]. A explota√ß√£o visa maximizar a recompensa imediata com base no conhecimento atual, enquanto a explora√ß√£o visa melhorar o conhecimento sobre as a√ß√µes para maximizar a recompensa total a longo prazo [^2]. O valor de $\epsilon$ controla esse trade-off; um valor maior de $\epsilon$ resulta em mais explora√ß√£o e vice-versa [^5].

**Corol√°rio 1.1:** Diminuir $\epsilon$ ao longo do tempo pode melhorar o desempenho a longo prazo, concentrando-se gradualmente mais na explota√ß√£o √† medida que o aprendizado avan√ßa.

*Justificativa:* Ao diminuir $\epsilon$, a probabilidade de explorar a√ß√µes n√£o √≥timas diminui com o tempo, permitindo que o agente se concentre cada vez mais na explota√ß√£o das a√ß√µes que considera √≥timas. Isso pode levar a um aumento na recompensa total esperada ao longo do tempo.

**Prova do Corol√°rio 1.1:**
I. Seja $\epsilon_t$ o valor de $\epsilon$ no tempo *t*. Se $\epsilon_t$ diminui com o tempo (ou seja, $\epsilon_{t+1} < \epsilon_t$), ent√£o a probabilidade de explorar a√ß√µes n√£o √≥timas diminui com o tempo.
II. √Ä medida que o agente explora menos, ele explota mais as a√ß√µes que ele acredita serem as melhores.
III. Se o agente j√° explorou o suficiente para ter uma boa estimativa dos valores das a√ß√µes, ent√£o a explota√ß√£o ser√° mais ben√©fica do que a explora√ß√£o.
IV. Portanto, diminuir $\epsilon$ ao longo do tempo pode levar a um aumento na recompensa total esperada a longo prazo. ‚ñ†

**Exemplo:** Considere um cen√°rio com duas a√ß√µes e $\epsilon = 0.5$ [^4]. Nesse caso, a probabilidade de selecionar a a√ß√£o *greedy* √© $0.5$, e a probabilidade de selecionar uma a√ß√£o aleat√≥ria (incluindo a *greedy*) √© $0.5$. Se houver apenas uma a√ß√£o *greedy*, a probabilidade total de selecion√°-la √© $0.5 + (0.5 * 0.5) = 0.75$.

> üí° **Exemplo Num√©rico (Simula√ß√£o):**
>
> Vamos simular 1000 passos de um *k*-armed bandit com k=5. As recompensas de cada bra√ßo seguem uma distribui√ß√£o normal com m√©dias [0.1, 0.2, 0.5, 0.3, 0.4] e desvio padr√£o de 1. Vamos comparar o desempenho de $\epsilon = 0.1$ versus $\epsilon = 0.4$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Configura√ß√£o do problema
> k = 5  # N√∫mero de bra√ßos
> num_steps = 1000
> true_rewards = [0.1, 0.2, 0.5, 0.3, 0.4] # M√©dias das recompensas para cada bra√ßo
> std_dev = 1.0
>
> # Inicializa√ß√£o
> epsilon_values = [0.1, 0.4]
> Q = np.zeros(k)  # Estimativas de valor inicializadas com 0
> N = np.zeros(k)  # Contador de quantas vezes cada bra√ßo foi puxado
> total_rewards = np.zeros((len(epsilon_values), num_steps))
>
> # Fun√ß√£o para selecionar uma a√ß√£o usando epsilon-greedy
> def select_action(epsilon, Q):
>     if np.random.rand() < epsilon:
>         return np.random.choice(k)  # A√ß√£o aleat√≥ria
>     else:
>         return np.argmax(Q)  # A√ß√£o greedy
>
> # Simula√ß√£o
> np.random.seed(42) # Definindo a semente para reprodu√ß√£o
> for i, epsilon in enumerate(epsilon_values):
>     Q = np.zeros(k)  # Reinicializa as estimativas de valor
>     N = np.zeros(k)  # Reinicializa o contador
>     cumulative_reward = 0
>     for t in range(num_steps):
>         # Seleciona a a√ß√£o
>         action = select_action(epsilon, Q)
>
>         # Recebe a recompensa (amostrada de uma distribui√ß√£o normal)
>         reward = np.random.normal(true_rewards[action], std_dev)
>
>         # Atualiza as estimativas de valor
>         N[action] += 1
>         Q[action] = Q[action] + (1/N[action]) * (reward - Q[action])
>
>         cumulative_reward += reward
>         total_rewards[i, t] = cumulative_reward
>
> # Plotagem dos resultados
> plt.figure(figsize=(10, 6))
> plt.plot(total_rewards[0], label=f'Epsilon = {epsilon_values[0]}')
> plt.plot(total_rewards[1], label=f'Epsilon = {epsilon_values[1]}')
> plt.xlabel('Passos')
> plt.ylabel('Recompensa Cumulativa')
> plt.title('Compara√ß√£o de Epsilon-Greedy com Diferentes Valores de Epsilon')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> A partir do gr√°fico gerado, podemos observar que, no in√≠cio, $\epsilon = 0.4$ (maior explora√ß√£o) pode apresentar um crescimento mais lento, mas ao final da simula√ß√£o, pode alcan√ßar um desempenho melhor por ter explorado mais o espa√ßo de a√ß√µes. O valor ideal de $\epsilon$ depende do problema.

Abaixo uma imagem ilustrando um exemplo de problema *k*-armed bandit.

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

**Import√¢ncia da Vari√¢ncia da Recompensa:** A vantagem dos m√©todos $\epsilon$-greedy sobre os m√©todos *greedy* depende da tarefa [^5]. Por exemplo, se a vari√¢ncia da recompensa for alta, √© necess√°ria mais explora√ß√£o para encontrar a a√ß√£o √≥tima, e os m√©todos $\epsilon$-greedy ter√£o um desempenho relativamente melhor [^5]. Por outro lado, se a vari√¢ncia da recompensa for zero, o m√©todo *greedy* pode ter um desempenho melhor, pois aprenderia o verdadeiro valor de cada a√ß√£o ap√≥s apenas uma tentativa [^5].

> üí° **Exemplo Num√©rico:**
>
> Considere um cen√°rio com duas a√ß√µes. A A√ß√£o 1 sempre retorna uma recompensa de 1. A A√ß√£o 2 retorna uma recompensa de 0 com probabilidade 0.9 e uma recompensa de 10 com probabilidade 0.1. A A√ß√£o 1 tem vari√¢ncia 0, enquanto a A√ß√£o 2 tem uma vari√¢ncia alta.  Nesse caso, um algoritmo *greedy* pode ficar "preso" na A√ß√£o 1, pois inicialmente ela parecer√° melhor, enquanto um $\epsilon$-greedy tem uma chance de descobrir a A√ß√£o 2 e seu potencial de recompensa maior.

A imagem a seguir mostra o pseudoc√≥digo de um algoritmo simples usando a estrat√©gia $\epsilon$-greedy.

![Pseudoc√≥digo de um algoritmo de bandit simples com estrat√©gia Œµ-greedy para explora√ß√£o e explota√ß√£o.](./../images/image4.png)

Abaixo, uma imagem que compara o desempenho de diferentes m√©todos $\epsilon$-greedy no problema do *k*-armed bandit.

![Average performance of Œµ-greedy action-value methods on a 10-armed testbed, demonstrating the exploration-exploitation trade-off.](./../images/image6.png)

**Teorema 2:** Para um problema *k*-armed bandit com recompensas limitadas no intervalo [0, 1], o arrependimento esperado (expected regret) do algoritmo $\epsilon$-greedy com $\epsilon$ fixo √© $O(\epsilon n + \frac{k}{\epsilon})$, onde *n* √© o n√∫mero de passos.

*Prova (Esbo√ßo):* O arrependimento esperado √© a diferen√ßa entre a recompensa esperada da a√ß√£o √≥tima e a recompensa esperada das a√ß√µes realmente tomadas. Com probabilidade $1 - \epsilon$, a a√ß√£o *greedy* √© tomada, o que pode n√£o ser a a√ß√£o √≥tima, contribuindo para o arrependimento. Com probabilidade $\epsilon$, uma a√ß√£o aleat√≥ria √© tomada, tamb√©m contribuindo para o arrependimento. Analisando essas contribui√ß√µes ao longo de *n* passos, obt√©m-se o resultado. O primeiro termo, $O(\epsilon n)$, representa o arrependimento devido √† explora√ß√£o, enquanto o segundo termo, $O(\frac{k}{\epsilon})$, representa o arrependimento inicial devido √† necessidade de identificar as boas a√ß√µes.

**Prova do Teorema 2 (Esbo√ßo Detalhado):**

I.  **Defini√ß√£o de Arrependimento:** O arrependimento no passo *t* √© a diferen√ßa entre a recompensa m√°xima poss√≠vel (obtida ao escolher sempre a a√ß√£o √≥tima) e a recompensa realmente obtida:  $r_t = q_*(a^*) - R_t$, onde $q_*(a^*)$ √© o valor da a√ß√£o √≥tima e $R_t$ √© a recompensa recebida no passo *t*. O arrependimento esperado total √© $E[\sum_{t=1}^{n} r_t]$.

II. **Arrependimento Devido √† Explora√ß√£o:** Com probabilidade $\epsilon$, uma a√ß√£o aleat√≥ria √© selecionada. Seja $\Delta_a = q_*(a^*) - q_*(a)$ a diferen√ßa de valor entre a a√ß√£o √≥tima $a^*$ e outra a√ß√£o *a*. O arrependimento esperado ao selecionar a a√ß√£o *a* √© $\epsilon \Delta_a$.  Somando sobre todas as a√ß√µes n√£o √≥timas, o arrependimento esperado em um √∫nico passo devido √† explora√ß√£o √© $\epsilon \sum_{a \ne a^*} \Delta_a$. Como as recompensas s√£o limitadas em [0, 1], $\Delta_a \le 1$. Portanto, o arrependimento esperado em um √∫nico passo √© limitado por $\epsilon (k-1) < \epsilon k$. Ao longo de *n* passos, o arrependimento total devido √† explora√ß√£o √© limitado por $O(\epsilon n)$.

III. **Arrependimento Devido √† Converg√™ncia Lenta:** No in√≠cio, o agente n√£o conhece os verdadeiros valores das a√ß√µes e pode selecionar a√ß√µes sub√≥timas mesmo quando age de forma *greedy*. Seja $\tau_a$ o n√∫mero de vezes que uma a√ß√£o sub√≥tima *a* precisa ser selecionada para que sua estimativa de valor $Q_t(a)$ seja suficientemente precisa para que a a√ß√£o √≥tima seja preferida com alta probabilidade.  O arrependimento acumulado durante essa fase de aprendizado √© proporcional a $\tau_a \Delta_a$. Uma an√°lise rigorosa mostra que $\tau_a$ √© tipicamente $O(\frac{1}{\epsilon})$ para garantir que a explora√ß√£o aleat√≥ria encontre boas a√ß√µes com probabilidade suficientemente alta.  Somando sobre todas as a√ß√µes sub√≥timas, obtemos um arrependimento de $O(\sum_{a \ne a^*} \frac{\Delta_a}{\epsilon})$, que √© limitado por $O(\frac{k}{\epsilon})$ dado que $\Delta_a \le 1$.

IV. **Arrependimento Total:** Somando as duas fontes de arrependimento, obtemos que o arrependimento esperado total √© $O(\epsilon n + \frac{k}{\epsilon})$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um problema com k=10 e n=1000.
>
> *   Se $\epsilon = 0.1$, o arrependimento esperado seria $O(0.1 * 1000 + 10 / 0.1) = O(100 + 100) = O(200)$.
> *   Se $\epsilon = 0.01$, o arrependimento esperado seria $O(0.01 * 1000 + 10 / 0.01) = O(10 + 1000) = O(1010)$.
> *   Se $\epsilon = 0.5$, o arrependimento esperado seria $O(0.5 * 1000 + 10 / 0.5) = O(500 + 20) = O(520)$.
>
> Observe que existe um valor √≥timo de $\epsilon$ que minimiza o arrependimento. Valores muito pequenos de $\epsilon$ levam a pouca explora√ß√£o e um alto arrependimento no longo prazo. Valores muito grandes de $\epsilon$ levam a muita explora√ß√£o e um alto arrependimento no curto prazo.

### Conclus√£o

Os m√©todos $\epsilon$-greedy representam uma abordagem simples e eficaz para equilibrar explora√ß√£o e explota√ß√£o no problema do *k*-armed bandit [^3]. Ao introduzir uma pequena probabilidade de selecionar a√ß√µes aleat√≥rias, esses m√©todos garantem a converg√™ncia assint√≥tica para os verdadeiros valores das a√ß√µes, mesmo que a vari√¢ncia da recompensa seja alta ou a tarefa seja n√£o-estacion√°ria [^3, 5]. A escolha apropriada de $\epsilon$ depende da tarefa espec√≠fica e do trade-off desejado entre explora√ß√£o e explota√ß√£o. Em se√ß√µes subsequentes, exploraremos outras t√©cnicas para equilibrar a explora√ß√£o e a explota√ß√£o, bem como adapta√ß√µes para lidar com ambientes n√£o estacion√°rios e espa√ßos de estados maiores.

### Refer√™ncias

[^2]: Cap√≠tulo 2: Multi-armed Bandits, Se√ß√£o 2.1 (A *k*-armed Bandit Problem)
[^3]: Cap√≠tulo 2: Multi-armed Bandits, Se√ß√£o 2.2 (Action-value Methods)
[^4]: Cap√≠tulo 2: Multi-armed Bandits, Se√ß√£o 2.3 (The 10-armed Testbed)
[^5]: Cap√≠tulo 2: Multi-armed Bandits, Se√ß√£o 2.3 (The 10-armed Testbed)
<!-- END -->