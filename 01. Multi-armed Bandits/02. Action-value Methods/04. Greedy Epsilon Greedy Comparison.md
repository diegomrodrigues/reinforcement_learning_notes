## An√°lise Comparativa de M√©todos *Greedy* e $\epsilon$*-Greedy* em *k*-Armed Bandit

### Introdu√ß√£o

No contexto de *k*-armed bandit problems, a escolha entre **explora√ß√£o** e **explota√ß√£o** √© fundamental. Os m√©todos *greedy* e $\epsilon$*-greedy* representam abordagens distintas para lidar com esse dilema. O m√©todo *greedy* explora a a√ß√£o com o maior valor estimado no momento, enquanto o m√©todo $\epsilon$*-greedy* introduz uma probabilidade $\epsilon$ de selecionar uma a√ß√£o aleat√≥ria, incentivando a explora√ß√£o [^2]. Para avaliar a efic√°cia relativa dessas abordagens, recorremos a uma an√°lise num√©rica em um conjunto de problemas de teste, como ser√° detalhado nas se√ß√µes seguintes.

Para formalizar a escolha de a√ß√µes, podemos definir a pol√≠tica de cada m√©todo. Seja $Q_t(a)$ a estimativa do valor da a√ß√£o $a$ no instante $t$.

**Defini√ß√£o 1 (Pol√≠tica *Greedy*)** A pol√≠tica *greedy* seleciona a a√ß√£o com maior valor estimado:

$A_t = \underset{a}{\mathrm{argmax}} \, Q_t(a)$.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um problema *k*-armed bandit com $k=5$ e as seguintes estimativas de valor para cada a√ß√£o no instante $t$:
>
> $Q_t(1) = 0.2, Q_t(2) = 0.5, Q_t(3) = 0.1, Q_t(4) = 0.8, Q_t(5) = 0.3$
>
> A pol√≠tica *greedy* selecionaria a a√ß√£o $A_t = 4$, pois $Q_t(4) = 0.8$ √© o maior valor estimado.

**Defini√ß√£o 2 (Pol√≠tica $\epsilon$*-Greedy*)** A pol√≠tica $\epsilon$*-greedy* seleciona a a√ß√£o com maior valor estimado com probabilidade $1-\epsilon$ e uma a√ß√£o aleat√≥ria com probabilidade $\epsilon$:

$A_t = \begin{cases}
\underset{a}{\mathrm{argmax}} \, Q_t(a) & \text{com probabilidade } 1 - \epsilon \\
\text{A√ß√£o aleat√≥ria} & \text{com probabilidade } \epsilon
\end{cases}$.

> üí° **Exemplo Num√©rico:**
>
> Usando as mesmas estimativas de valor do exemplo anterior e definindo $\epsilon = 0.1$, temos:
>
> Com probabilidade $1 - \epsilon = 0.9$, selecionamos a a√ß√£o $A_t = 4$ (a a√ß√£o *greedy*).
> Com probabilidade $\epsilon = 0.1$, selecionamos uma a√ß√£o aleat√≥ria de $\{1, 2, 3, 4, 5\}$. Se selecionarmos aleatoriamente a a√ß√£o 2, ent√£o $A_t = 2$.
>
> Assim, a pol√≠tica $\epsilon$*-greedy* tem uma chance de explorar outras a√ß√µes al√©m da a√ß√£o com maior valor estimado.

### Avalia√ß√£o Num√©rica em *k*-Armed Bandit Testbeds

Para comparar numericamente a efic√°cia dos m√©todos *greedy* e $\epsilon$*-greedy*, utiliza-se um conjunto de problemas de teste *k*-armed bandit gerados aleatoriamente [^2]. No experimento descrito, foi utilizado um conjunto de 2000 problemas *k*-armed bandit, com $k = 10$ [^3].

**Estrutura do Testbed:**

1.  **Gera√ß√£o Aleat√≥ria:** Para cada problema de *bandit*, os valores das a√ß√µes, $q_*(a)$, para $a = 1, \ldots, 10$, foram selecionados de acordo com uma distribui√ß√£o normal (Gaussiana) com m√©dia 0 e vari√¢ncia 1 [^4].

2.  **Recompensa Estoc√°stica:** Quando um m√©todo de aprendizado seleciona uma a√ß√£o $A_t$ no instante de tempo $t$, a recompensa real $R_t$ √© selecionada a partir de uma distribui√ß√£o normal com m√©dia $q_*(A_t)$ e vari√¢ncia 1 [^4]. As distribui√ß√µes das recompensas s√£o ilustradas em cinza na Figura 2.1 [^4].

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

3.  **M√©tricas de Desempenho:** Para cada m√©todo de aprendizado, √© medida sua performance e comportamento ao longo de 1000 *time steps* quando aplicado a um dos problemas *bandit* [^4]. Essa aplica√ß√£o por 1000 *time steps* constitui uma *run*. O processo √© repetido por 2000 *runs* independentes, cada uma com um problema *bandit* diferente, obtendo-se assim medidas do comportamento m√©dio do algoritmo de aprendizado [^4].

**Resultados Experimentais:**

A Figura 2.2 compara um m√©todo *greedy* com dois m√©todos $\epsilon$*-greedy* (com $\epsilon = 0.01$ e $\epsilon = 0.1$) no *testbed* de 10 bra√ßos [^5]. Todos os m√©todos formaram suas estimativas de valor de a√ß√£o usando a t√©cnica de *sample-average* com uma estimativa inicial de 0 [^5].

![Average performance of Œµ-greedy action-value methods on a 10-armed testbed, demonstrating the exploration-exploitation trade-off.](./../images/image6.png)

*   **Recompensa M√©dia:** O gr√°fico superior da Figura 2.2 mostra o aumento na recompensa esperada com a experi√™ncia. O m√©todo *greedy* melhorou ligeiramente mais r√°pido que os outros m√©todos no come√ßo, mas ent√£o estabilizou em um n√≠vel mais baixo. Ele alcan√ßou uma recompensa por passo de apenas 1, comparado com o melhor poss√≠vel de cerca de 1.54 neste *testbed*. O m√©todo *greedy* teve um desempenho significativamente pior a longo prazo porque muitas vezes ficou preso a executar a√ß√µes sub√≥timas [^5].

*   **A√ß√£o √ìtima:** O gr√°fico inferior da Figura 2.2 mostra a porcentagem de vezes que os v√°rios m√©todos selecionaram a a√ß√£o √≥tima [^5]. O m√©todo *greedy* encontrou a a√ß√£o √≥tima em apenas aproximadamente um ter√ßo das tarefas. Nos outros dois ter√ßos, suas amostras iniciais da a√ß√£o √≥tima foram decepcionantes e ele nunca retornou a ela. Os m√©todos $\epsilon$*-greedy* eventualmente tiveram um desempenho melhor porque continuaram a explorar e a melhorar suas chances de reconhecer a a√ß√£o √≥tima. O m√©todo com $\epsilon = 0.1$ explorou mais e geralmente encontrou a a√ß√£o √≥tima mais cedo, mas nunca selecionou essa a√ß√£o mais de 91% das vezes. O m√©todo com $\epsilon = 0.01$ melhorou mais lentamente, mas acabaria por ter um desempenho melhor do que o m√©todo com $\epsilon = 0.1$ em ambas as medidas de desempenho mostradas na figura. Tamb√©m √© poss√≠vel reduzir $\epsilon$ ao longo do tempo para tentar obter o melhor de ambos os valores altos e baixos [^6].

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar como as recompensas s√£o atualizadas com o m√©todo *sample-average*, suponha que a a√ß√£o 2 foi selecionada 3 vezes, resultando nas seguintes recompensas: $R_1 = 0.3, R_2 = 0.7, R_3 = 0.5$. A estimativa do valor da a√ß√£o 2 seria atualizada como:
>
> $Q_3(2) = \frac{0.3 + 0.7 + 0.5}{3} = \frac{1.5}{3} = 0.5$.
>
> Se a pr√≥xima recompensa para a a√ß√£o 2 for $R_4 = 0.9$, a nova estimativa seria:
>
> $Q_4(2) = \frac{0.3 + 0.7 + 0.5 + 0.9}{4} = \frac{2.4}{4} = 0.6$.

Para complementar a discuss√£o sobre a escolha de $\epsilon$, podemos formalizar uma estrat√©gia de decaimento de $\epsilon$ ao longo do tempo.

**Defini√ß√£o 3 (Decaimento de $\epsilon$)** Uma estrat√©gia comum √© reduzir $\epsilon$ linearmente ou exponencialmente ao longo do tempo:

*   **Decaimento Linear:** $\epsilon_t = \max(\epsilon_{\text{min}}, \epsilon_0 - \gamma t)$, onde $\epsilon_0$ √© o valor inicial de $\epsilon$, $\gamma$ √© a taxa de decaimento, e $\epsilon_{\text{min}}$ √© o valor m√≠nimo de $\epsilon$.

*   **Decaimento Exponencial:** $\epsilon_t = \epsilon_0 \cdot \alpha^t$, onde $\epsilon_0$ √© o valor inicial de $\epsilon$ e $\alpha$ √© a taxa de decaimento (0 < $\alpha$ < 1).

Essas estrat√©gias permitem que o agente explore mais no in√≠cio do aprendizado e, em seguida, gradualmente explore menos √† medida que converge para uma solu√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Considere um decaimento linear com $\epsilon_0 = 0.1$, $\gamma = 0.0001$, e $\epsilon_{\text{min}} = 0.01$. Ent√£o, ap√≥s 100 time steps:
>
> $\epsilon_{100} = \max(0.01, 0.1 - 0.0001 \cdot 100) = \max(0.01, 0.1 - 0.01) = \max(0.01, 0.09) = 0.09$.
>
> Ap√≥s 500 time steps:
>
> $\epsilon_{500} = \max(0.01, 0.1 - 0.0001 \cdot 500) = \max(0.01, 0.1 - 0.05) = \max(0.01, 0.05) = 0.05$.
>
> Ap√≥s 1000 time steps:
>
> $\epsilon_{1000} = \max(0.01, 0.1 - 0.0001 \cdot 1000) = \max(0.01, 0.1 - 0.1) = \max(0.01, 0.0) = 0.01$.
>
> Este exemplo mostra como $\epsilon$ diminui linearmente ao longo do tempo, at√© atingir o valor m√≠nimo de 0.01.

**Observa√ß√µes:**

*   A vantagem dos m√©todos $\epsilon$*-greedy* sobre os m√©todos *greedy* depende da tarefa. Por exemplo, se a vari√¢ncia da recompensa fosse maior, digamos 10 em vez de 1, seria necess√°ria mais explora√ß√£o para encontrar a a√ß√£o √≥tima, e os m√©todos $\epsilon$*-greedy* se sairiam ainda melhor em rela√ß√£o ao m√©todo *greedy* [^6].

*   Por outro lado, se as vari√¢ncias da recompensa fossem zero, ent√£o o m√©todo *greedy* conheceria o verdadeiro valor de cada a√ß√£o depois de tentar uma vez. Nesse caso, o m√©todo *greedy* poderia realmente ter o melhor desempenho porque em breve encontraria a a√ß√£o √≥tima e ent√£o nunca mais exploraria [^6].

*   Mesmo no caso determin√≠stico, h√° uma grande vantagem em explorar se enfraquecermos algumas das outras suposi√ß√µes. Por exemplo, suponha que a tarefa de *bandit* n√£o fosse estacion√°ria, isto √©, os verdadeiros valores das a√ß√µes mudassem com o tempo. Nesse caso, a explora√ß√£o √© necess√°ria mesmo no caso determin√≠stico para garantir que uma das a√ß√µes n√£o-gananciosas n√£o tenha mudado para se tornar melhor do que a gananciosa [^6].

**Teorema 1** Em um ambiente n√£o-estacion√°rio, uma pol√≠tica $\epsilon$-greedy com decaimento de $\epsilon$ (conforme Defini√ß√£o 3) pode convergir para uma pol√≠tica √≥tima se a taxa de mudan√ßa do ambiente for suficientemente lenta em rela√ß√£o √† taxa de decaimento de $\epsilon$.

*Prova (Esbo√ßo)*: A prova envolve mostrar que, √† medida que $\epsilon$ diminui, o agente se concentra cada vez mais na explora√ß√£o das a√ß√µes que parecem ser as melhores no momento. Se o ambiente mudar lentamente, as estimativas de valor das a√ß√µes permanecer√£o relativamente precisas, e o agente poder√° convergir para a a√ß√£o √≥tima. A taxa de decaimento de $\epsilon$ deve ser ajustada para garantir que o agente explore o suficiente para acompanhar as mudan√ßas no ambiente, mas n√£o explore tanto que n√£o consiga convergir para uma pol√≠tica est√°vel.

Aqui est√° uma prova mais formal do Teorema 1:

**Prova do Teorema 1:**

Para provar que uma pol√≠tica $\epsilon$-greedy com decaimento de $\epsilon$ converge para uma pol√≠tica √≥tima em um ambiente n√£o-estacion√°rio com mudan√ßas lentas, precisamos mostrar que, com o tempo, a probabilidade de selecionar uma a√ß√£o sub√≥tima diminui para zero.

I. **Defini√ß√µes:**
    *   Seja $A^*$ a a√ß√£o √≥tima no tempo $t$, e $Q_t(A^*)$ seu valor verdadeiro.
    *   Seja $A_t$ a a√ß√£o selecionada no tempo $t$ pela pol√≠tica $\epsilon$-greedy.
    *   Seja $\Delta_t(a) = Q_t(A^*) - Q_t(a)$ a diferen√ßa de valor entre a a√ß√£o √≥tima e uma a√ß√£o $a$ no tempo $t$. Uma a√ß√£o $a$ √© sub√≥tima se $\Delta_t(a) > 0$.
    *   Seja $\epsilon_t$ a probabilidade de explora√ß√£o no tempo $t$, que decai com o tempo.

II. **Condi√ß√£o para Converg√™ncia:**
    Para que a pol√≠tica convirja para a a√ß√£o √≥tima, precisamos que a probabilidade de selecionar uma a√ß√£o sub√≥tima tenda a zero quando $t$ tende ao infinito:
    $$
    \lim_{t \to \infty} P(A_t \neq A^*) = 0
    $$

III. **Probabilidade de Selecionar uma A√ß√£o Sub√≥tima:**
     A pol√≠tica $\epsilon$-greedy seleciona uma a√ß√£o sub√≥tima de duas maneiras: (1) explorando aleatoriamente com probabilidade $\epsilon_t$ ou (2) explorando a a√ß√£o com a maior estimativa, mas que √© sub√≥tima. Portanto:
    $$
    P(A_t \neq A^*) = \epsilon_t + (1 - \epsilon_t) P(\underset{a}{\mathrm{argmax}} \, Q_t(a) \neq A^*)
    $$

IV. **An√°lise do Termo $(1 - \epsilon_t) P(\underset{a}{\mathrm{argmax}} \, Q_t(a) \neq A^*)$:**
    Para que este termo tenda a zero, a probabilidade de que a a√ß√£o com maior estimativa seja sub√≥tima deve diminuir com o tempo. Isso acontece se as estimativas $Q_t(a)$ convergem para os verdadeiros valores das a√ß√µes, $q_*(a)$. No entanto, o ambiente √© n√£o-estacion√°rio, ent√£o os verdadeiros valores $q_*(a)$ mudam com o tempo.

V. **Condi√ß√£o de Mudan√ßa Lenta:**
    Assumimos que a taxa de mudan√ßa do ambiente √© suficientemente lenta, o que significa que, para qualquer a√ß√£o $a$, a mudan√ßa no valor verdadeiro da a√ß√£o em um √∫nico passo de tempo √© limitada:
    $$
    |Q_{t+1}(a) - Q_t(a)| < \delta
    $$
    onde $\delta$ √© uma constante pequena.

VI. **Decaimento de $\epsilon$:**
     Se $\epsilon_t$ decai com o tempo, ent√£o a explora√ß√£o diminui gradualmente. Isso permite que as estimativas $Q_t(a)$ se aproximem dos verdadeiros valores $Q_t(a)$, mesmo que estes mudem lentamente. A taxa de decaimento de $\epsilon_t$ deve ser ajustada para garantir que a explora√ß√£o seja suficiente para rastrear as mudan√ßas no ambiente, mas n√£o t√£o alta que impe√ßa a converg√™ncia. Por exemplo, para um decaimento linear, ter√≠amos $\epsilon_t = \max(\epsilon_{\text{min}}, \epsilon_0 - \gamma t)$.

VII. **Converg√™ncia:**
      √Ä medida que $t \to \infty$, $\epsilon_t \to 0$, e se as estimativas de valor $Q_t(a)$ permanecem razoavelmente precisas devido √† condi√ß√£o de mudan√ßa lenta do ambiente, ent√£o
      $P(\underset{a}{\mathrm{argmax}} \, Q_t(a) \neq A^*) \to 0$. Portanto,
    $$
    \lim_{t \to \infty} P(A_t \neq A^*) = 0
    $$

VIII. **Conclus√£o:**
       Portanto, sob a condi√ß√£o de que o ambiente seja n√£o-estacion√°rio com mudan√ßas lentas e que $\epsilon$ decaia com o tempo, a pol√≠tica $\epsilon$-greedy converge para a pol√≠tica √≥tima. ‚ñ†

### Conclus√£o

A an√°lise num√©rica demonstra que os m√©todos $\epsilon$*-greedy* geralmente superam os m√©todos *greedy* em *k*-armed bandit problems, pois equilibram a explora√ß√£o e a explota√ß√£o de forma mais eficaz [^5]. A capacidade de explorar outras op√ß√µes evita que o algoritmo fique preso a escolhas sub√≥timas, levando a um melhor desempenho a longo prazo [^6]. O valor de $\epsilon$ influencia a taxa de explora√ß√£o, com valores maiores favorecendo a explora√ß√£o e valores menores, a explota√ß√£o. Ajustar $\epsilon$ √© crucial para otimizar o desempenho em diferentes cen√°rios [^6].
$\blacksquare$
<!-- END -->