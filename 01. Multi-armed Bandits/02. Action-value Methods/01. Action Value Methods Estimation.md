## Estimativa de Valores de AÃ§Ã£o via MÃ©todos de Amostragem
### IntroduÃ§Ã£o
No contexto do aprendizado por reforÃ§o em ambientes *k-armed bandit*, a estimativa precisa dos valores de aÃ§Ã£o Ã© crucial para a tomada de decisÃµes otimizadas. Este capÃ­tulo explora em profundidade os **mÃ©todos de valor de aÃ§Ã£o**, focando na tÃ©cnica fundamental de **amostragem da mÃ©dia** para estimar o valor de cada aÃ§Ã£o [^27]. O objetivo Ã© analisar como a estimativa do valor de aÃ§Ã£o, denotada por $Q_t(a)$, evolui com o tempo e converge para o valor real da aÃ§Ã£o, $q_*(a)$, sob condiÃ§Ãµes de estacionariedade.

### Conceitos Fundamentais
**MÃ©todos de valor de aÃ§Ã£o** sÃ£o uma classe de algoritmos que utilizam estimativas dos valores das aÃ§Ãµes para guiar as decisÃµes de seleÃ§Ã£o de aÃ§Ã£o [^27]. A **estimativa do valor de uma aÃ§Ã£o** *a* no tempo *t*, representada por $Q_t(a)$, Ã© uma previsÃ£o da recompensa mÃ©dia que se espera receber ao selecionar a aÃ§Ã£o *a*. O **valor real da aÃ§Ã£o**, denotado por $q_*(a)$, Ã© a recompensa mÃ©dia *verdadeira* que se receberia a longo prazo ao selecionar repetidamente a aÃ§Ã£o *a* [^26].

#### Estimativa por MÃ©dia de Amostra
Um dos mÃ©todos mais intuitivos para estimar $Q_t(a)$ Ã© atravÃ©s da mÃ©dia das recompensas observadas ao longo do tempo [^27]. Formalmente, essa **mÃ©dia de amostra** Ã© definida como:

$$
Q_t(a) = \frac{\text{soma das recompensas quando 'a' Ã© tomado antes de t}}{\text{nÃºmero de vezes que 'a' foi tomado antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$

onde:
- $R_i$ Ã© a recompensa recebida no instante *i*.
- $A_i$ Ã© a aÃ§Ã£o selecionada no instante *i*.
- $\mathbb{1}_{A_i=a}$ Ã© uma funÃ§Ã£o indicadora que vale 1 se a aÃ§Ã£o $A_i$ Ã© igual Ã  aÃ§Ã£o *a*, e 0 caso contrÃ¡rio.

Se o denominador for zero (ou seja, a aÃ§Ã£o *a* nunca foi tomada antes do instante *t*), entÃ£o $Q_t(a)$ Ã© definido como um valor padrÃ£o, geralmente 0 [^27].

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos um problema de *k-armed bandit* com 3 aÃ§Ãµes. Vamos focar na aÃ§Ã£o *a* = 1. Nos primeiros 5 instantes de tempo, as aÃ§Ãµes e recompensas foram as seguintes:
>
> | Instante (i) | AÃ§Ã£o ($A_i$) | Recompensa ($R_i$) |
> |--------------|---------------|--------------------|
> | 1            | 2             | 0                  |
> | 2            | 1             | 2                  |
> | 3            | 3             | 0                  |
> | 4            | 1             | 3                  |
> | 5            | 2             | 1                  |
>
> Para calcular $Q_6(1)$, precisamos da soma das recompensas quando a aÃ§Ã£o 1 foi tomada antes do tempo 6 e do nÃºmero de vezes que a aÃ§Ã£o 1 foi tomada antes do tempo 6.
>
> $\sum_{i=1}^{5} R_i \mathbb{1}_{A_i=1} = (0 \cdot 0) + (2 \cdot 1) + (0 \cdot 0) + (3 \cdot 1) + (1 \cdot 0) = 2 + 3 = 5$
>
> $\sum_{i=1}^{5} \mathbb{1}_{A_i=1} = 0 + 1 + 0 + 1 + 0 = 2$
>
> Portanto, $Q_6(1) = \frac{5}{2} = 2.5$
>
> Este valor, 2.5, Ã© a nossa estimativa do valor da aÃ§Ã£o 1 com base nas duas vezes que a aÃ§Ã£o foi selecionada.

Para facilitar a notaÃ§Ã£o, vamos definir $N_t(a) = \sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}$ como o nÃºmero de vezes que a aÃ§Ã£o *a* foi selecionada antes do tempo *t*. Assim, podemos reescrever a estimativa por mÃ©dia de amostra como:

$$
Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{N_t(a)}
$$

quando $N_t(a) > 0$, e $Q_t(a) = 0$ quando $N_t(a) = 0$.

#### ConvergÃªncia e a Lei dos Grandes NÃºmeros
A beleza da mÃ©dia de amostra reside em sua convergÃªncia para o valor real da aÃ§Ã£o, $q_*(a)$, Ã  medida que o nÃºmero de amostras tende ao infinito [^27]. Isso Ã© garantido pela **Lei dos Grandes NÃºmeros (LLN)**. Em termos simples, a LLN afirma que a mÃ©dia de um nÃºmero suficientemente grande de amostras independentes e identicamente distribuÃ­das converge para o valor esperado da distribuiÃ§Ã£o. No contexto de mÃ©todos de valor de aÃ§Ã£o, isso significa que:

$$\lim_{t \to \infty} Q_t(a) = q_*(a)$$

sob a condiÃ§Ã£o de **estacionariedade**, que implica que a distribuiÃ§Ã£o de probabilidade das recompensas para cada aÃ§Ã£o permanece constante ao longo do tempo.

**Teorema 1:** *A estimativa por mÃ©dia de amostra $Q_t(a)$ converge para o valor real da aÃ§Ã£o $q_*(a)$ com probabilidade 1, sob a condiÃ§Ã£o de estacionariedade.*

*Prova:* A prova segue diretamente da Lei Forte dos Grandes NÃºmeros. Se as recompensas $R_i$ para a aÃ§Ã£o *a* sÃ£o independentes e identicamente distribuÃ­das com mÃ©dia $q_*(a)$, entÃ£o, pela Lei Forte dos Grandes NÃºmeros, a mÃ©dia amostral $\frac{1}{N_t(a)}\sum_{i=1}^{N_t(a)} R_i$ converge para $q_*(a)$ com probabilidade 1 quando $N_t(a) \to \infty$.  $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Imagine uma aÃ§Ã£o onde o valor real $q_*(a) = 5$. As recompensas obtidas ao longo de vÃ¡rias tentativas sÃ£o: 4, 6, 5, 4, 6, 5, 5, 4, 6, 5, ...
>
> Inicialmente, $Q_1(a) = 0$ (valor inicial).
> $Q_2(a) = 4/1 = 4$
> $Q_3(a) = (4+6)/2 = 5$
> $Q_4(a) = (4+6+5)/3 = 5$
> ...
>
> Ã€ medida que $t$ aumenta, $Q_t(a)$ se aproxima de $q_*(a) = 5$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Valor real da aÃ§Ã£o
> q_star = 5
>
> # SimulaÃ§Ã£o de recompensas
> np.random.seed(42)  # Definir a semente para reproducibilidade
> rewards = np.random.normal(q_star, 1, 1000)  # Recompensas com desvio padrÃ£o 1
>
> # Calculando a mÃ©dia amostral ao longo do tempo
> Q = np.cumsum(rewards) / np.arange(1, 1001)
>
> # Plotando a convergÃªncia
> plt.figure(figsize=(10, 6))
> plt.plot(Q, label='Estimativa Q(t)')
> plt.axhline(y=q_star, color='r', linestyle='--', label='Valor Real q*(a)')
> plt.xlabel('NÃºmero de Amostras (t)')
> plt.ylabel('Valor Estimado')
> plt.title('ConvergÃªncia da MÃ©dia Amostral para o Valor Real da AÃ§Ã£o')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> O cÃ³digo acima simula a obtenÃ§Ã£o de recompensas de uma aÃ§Ã£o com valor real de 5 e calcula a mÃ©dia amostral ao longo do tempo. O grÃ¡fico resultante mostra como a estimativa $Q_t(a)$ converge para o valor real $q_*(a)$ Ã  medida que o nÃºmero de amostras aumenta.

#### ConsideraÃ§Ãµes PrÃ¡ticas
Embora a mÃ©dia de amostra ofereÃ§a uma garantia teÃ³rica de convergÃªncia, algumas consideraÃ§Ãµes prÃ¡ticas sÃ£o importantes:

1.  **InicializaÃ§Ã£o**: O valor inicial de $Q_t(a)$ quando a aÃ§Ã£o *a* ainda nÃ£o foi selecionada pode influenciar o comportamento inicial do agente [^34]. Inicializar com valores otimistas pode encorajar a exploraÃ§Ã£o, como serÃ¡ discutido mais adiante.

2.  **NÃ£o-Estacionaridade**: A mÃ©dia de amostra atribui igual peso a todas as recompensas passadas, o que pode ser subÃ³timo em ambientes **nÃ£o-estacionÃ¡rios**, onde as distribuiÃ§Ãµes de recompensa mudam ao longo do tempo [^30]. Em tais cenÃ¡rios, mÃ©todos que dÃ£o mais peso Ã s recompensas recentes sÃ£o mais adequados, como serÃ¡ explorado em seÃ§Ãµes posteriores.

Para lidar com a nÃ£o-estacionaridade, uma tÃ©cnica comum Ã© usar uma **mÃ©dia ponderada exponencialmente**. Nesta abordagem, as recompensas mais recentes recebem mais peso do que as recompensas mais antigas. Formalmente, a atualizaÃ§Ã£o do valor da aÃ§Ã£o pode ser definida como:

$$
Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]
$$

onde $\alpha \in (0, 1]$ Ã© a **taxa de aprendizado**. Um valor de $\alpha$ prÃ³ximo de 0 dÃ¡ mais peso Ã s recompensas passadas, enquanto um valor de $\alpha$ prÃ³ximo de 1 dÃ¡ mais peso Ã  recompensa mais recente. Esta tÃ©cnica Ã© especialmente Ãºtil em ambientes onde o valor real da aÃ§Ã£o pode mudar ao longo do tempo.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que $Q_t(a) = 2$ e recebemos uma recompensa $R_t = 5$. Se escolhermos $\alpha = 0.1$, a atualizaÃ§Ã£o seria:
>
> $Q_{t+1}(a) = 2 + 0.1[5 - 2] = 2 + 0.1(3) = 2.3$
>
> Se escolhermos $\alpha = 0.9$, a atualizaÃ§Ã£o seria:
>
> $Q_{t+1}(a) = 2 + 0.9[5 - 2] = 2 + 0.9(3) = 2 + 2.7 = 4.7$
>
> Observe como um $\alpha$ maior move a estimativa do valor da aÃ§Ã£o mais rapidamente em direÃ§Ã£o Ã  recompensa recente.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # ParÃ¢metros
> alpha = 0.1
> q_true = 1.0  # Valor verdadeiro da aÃ§Ã£o
> n_steps = 100
>
> # InicializaÃ§Ã£o
> q_estimate = 0.0
> rewards = np.zeros(n_steps)
> q_estimates = np.zeros(n_steps)
>
> # Loop de aprendizado
> np.random.seed(42)
> for i in range(n_steps):
>     reward = np.random.normal(q_true, 1)  # Recompensa aleatÃ³ria com mÃ©dia q_true
>     q_estimate = q_estimate + alpha * (reward - q_estimate)
>     rewards[i] = reward
>     q_estimates[i] = q_estimate
>
> # Plotagem
> plt.figure(figsize=(10, 6))
> plt.plot(q_estimates, label='Estimativa de Q(a)')
> plt.axhline(y=q_true, color='r', linestyle='--', label='Valor Verdadeiro q*(a)')
> plt.xlabel('Passos')
> plt.ylabel('Estimativa de Valor')
> plt.title('MÃ©dia Ponderada Exponencialmente')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> Este cÃ³digo demonstra como a estimativa $Q_{t+1}(a)$ se aproxima do valor real $q_*(a)$ ao longo do tempo, utilizando a mÃ©dia ponderada exponencialmente.

**Teorema 1.1** *Se a taxa de aprendizado $\alpha$ Ã© constante, a mÃ©dia ponderada exponencialmente atribui pesos exponencialmente decrescentes Ã s recompensas passadas.*

*Prova:* Expandindo a equaÃ§Ã£o recursiva para $Q_{t+1}(a)$, obtemos:

$Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)] = (1-\alpha)Q_t(a) + \alpha R_t$
$= (1-\alpha) [(1-\alpha)Q_{t-1}(a) + \alpha R_{t-1}] + \alpha R_t = (1-\alpha)^2 Q_{t-1}(a) + \alpha(1-\alpha)R_{t-1} + \alpha R_t$

Continuando a expandir, chegamos a:

$Q_{t+1}(a) = (1-\alpha)^t Q_1(a) + \alpha \sum_{i=1}^{t} (1-\alpha)^{t-i} R_i$.

Isso mostra que o peso da recompensa $R_i$ decresce exponencialmente com o tempo, com um fator de $(1-\alpha)^{t-i}$. $\blacksquare$

**Prova detalhada do Teorema 1.1:**

I. ComeÃ§amos com a equaÃ§Ã£o de atualizaÃ§Ã£o para a mÃ©dia ponderada exponencialmente:
   $$Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]$$

II. Reorganizando a equaÃ§Ã£o, obtemos:
    $$Q_{t+1}(a) = (1 - \alpha)Q_t(a) + \alpha R_t$$

III. Aplicando a recursÃ£o novamente para $Q_t(a)$:
     $$Q_t(a) = (1 - \alpha)Q_{t-1}(a) + \alpha R_{t-1}$$

IV. Substituindo $Q_t(a)$ na equaÃ§Ã£o original:
    $$Q_{t+1}(a) = (1 - \alpha)[(1 - \alpha)Q_{t-1}(a) + \alpha R_{t-1}] + \alpha R_t$$
    $$Q_{t+1}(a) = (1 - \alpha)^2 Q_{t-1}(a) + \alpha(1 - \alpha)R_{t-1} + \alpha R_t$$

V. Generalizando para *t* passos, obtemos:
   $$Q_{t+1}(a) = (1 - \alpha)^t Q_1(a) + \alpha \sum_{i=1}^{t} (1 - \alpha)^{t-i} R_i$$

VI. Analisando a equaÃ§Ã£o resultante, podemos observar que cada recompensa $R_i$ Ã© ponderada por um fator de $\alpha (1 - \alpha)^{t-i}$. Isso indica que o peso das recompensas passadas decresce exponencialmente com o tempo, onde $(1 - \alpha)$ Ã© a taxa de decaimento exponencial.

VII. Portanto, a mÃ©dia ponderada exponencialmente atribui pesos exponencialmente decrescentes Ã s recompensas passadas, com o peso diminuindo em um fator de $(1 - \alpha)$ a cada passo no passado. â– 

#### Lemma 1: Conectando mÃ©dias amostrais com a lei dos grandes nÃºmeros
*Seja $X_1, X_2, \dots$ uma sequÃªncia de variÃ¡veis aleatÃ³rias independentes e identicamente distribuÃ­das com valor esperado $\mu$. EntÃ£o, para qualquer $\epsilon > 0$,*

$$
P\left(\lim_{n \to \infty} \left| \frac{1}{n}\sum_{i=1}^{n} X_i - \mu \right| < \epsilon \right) = 1
$$

*No contexto do problema *k*-armed bandit, se as recompensas para uma dada aÃ§Ã£o *a* sÃ£o i.i.d com valor esperado $q_*(a)$, entÃ£o $Q_t(a)$ converge para $q_*(a)$ quando $t \to \infty$. $\blacksquare$*

### ConclusÃ£o

Os **mÃ©todos de valor de aÃ§Ã£o** baseados na amostragem da mÃ©dia representam uma abordagem fundamental para estimar os valores das aÃ§Ãµes em problemas de aprendizado por reforÃ§o [^27]. A garantia de convergÃªncia fornecida pela Lei dos Grandes NÃºmeros torna essa tÃ©cnica atrativa em ambientes estacionÃ¡rios. No entanto, em situaÃ§Ãµes nÃ£o-estacionÃ¡rias, outras tÃ©cnicas que se adaptam a mudanÃ§as temporais podem ser mais eficazes, como discutido em seÃ§Ãµes posteriores, incluindo mÃ©todos incrementais que calculam a mÃ©dia de forma eficiente [^31].

### ReferÃªncias
[^26]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^27]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^30]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^31]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^34]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
<!-- END -->