## Estimativa de Valores de A√ß√£o via M√©todos de Amostragem
### Introdu√ß√£o
No contexto do aprendizado por refor√ßo em ambientes *k-armed bandit*, a estimativa precisa dos valores de a√ß√£o √© crucial para a tomada de decis√µes otimizadas. Este cap√≠tulo explora em profundidade os **m√©todos de valor de a√ß√£o**, focando na t√©cnica fundamental de **amostragem da m√©dia** para estimar o valor de cada a√ß√£o [^27]. O objetivo √© analisar como a estimativa do valor de a√ß√£o, denotada por $Q_t(a)$, evolui com o tempo e converge para o valor real da a√ß√£o, $q_*(a)$, sob condi√ß√µes de estacionariedade.

### Conceitos Fundamentais
**M√©todos de valor de a√ß√£o** s√£o uma classe de algoritmos que utilizam estimativas dos valores das a√ß√µes para guiar as decis√µes de sele√ß√£o de a√ß√£o [^27]. A **estimativa do valor de uma a√ß√£o** *a* no tempo *t*, representada por $Q_t(a)$, √© uma previs√£o da recompensa m√©dia que se espera receber ao selecionar a a√ß√£o *a*. O **valor real da a√ß√£o**, denotado por $q_*(a)$, √© a recompensa m√©dia *verdadeira* que se receberia a longo prazo ao selecionar repetidamente a a√ß√£o *a* [^26].

#### Estimativa por M√©dia de Amostra
Um dos m√©todos mais intuitivos para estimar $Q_t(a)$ √© atrav√©s da m√©dia das recompensas observadas ao longo do tempo [^27]. Formalmente, essa **m√©dia de amostra** √© definida como:

$$
Q_t(a) = \frac{\text{soma das recompensas quando 'a' √© tomado antes de t}}{\text{n√∫mero de vezes que 'a' foi tomado antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$

onde:
- $R_i$ √© a recompensa recebida no instante *i*.
- $A_i$ √© a a√ß√£o selecionada no instante *i*.
- $\mathbb{1}_{A_i=a}$ √© uma fun√ß√£o indicadora que vale 1 se a a√ß√£o $A_i$ √© igual √† a√ß√£o *a*, e 0 caso contr√°rio.

Se o denominador for zero (ou seja, a a√ß√£o *a* nunca foi tomada antes do instante *t*), ent√£o $Q_t(a)$ √© definido como um valor padr√£o, geralmente 0 [^27].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um problema de *k-armed bandit* com 3 a√ß√µes. Vamos focar na a√ß√£o *a* = 1. Nos primeiros 5 instantes de tempo, as a√ß√µes e recompensas foram as seguintes:
>
> | Instante (i) | A√ß√£o ($A_i$) | Recompensa ($R_i$) |
> |--------------|---------------|--------------------|
> | 1            | 2             | 0                  |
> | 2            | 1             | 2                  |
> | 3            | 3             | 0                  |
> | 4            | 1             | 3                  |
> | 5            | 2             | 1                  |
>
> Para calcular $Q_6(1)$, precisamos da soma das recompensas quando a a√ß√£o 1 foi tomada antes do tempo 6 e do n√∫mero de vezes que a a√ß√£o 1 foi tomada antes do tempo 6.
>
> $\sum_{i=1}^{5} R_i \mathbb{1}_{A_i=1} = (0 \cdot 0) + (2 \cdot 1) + (0 \cdot 0) + (3 \cdot 1) + (1 \cdot 0) = 2 + 3 = 5$
>
> $\sum_{i=1}^{5} \mathbb{1}_{A_i=1} = 0 + 1 + 0 + 1 + 0 = 2$
>
> Portanto, $Q_6(1) = \frac{5}{2} = 2.5$
>
> Este valor, 2.5, √© a nossa estimativa do valor da a√ß√£o 1 com base nas duas vezes que a a√ß√£o foi selecionada.

Para facilitar a nota√ß√£o, vamos definir $N_t(a) = \sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}$ como o n√∫mero de vezes que a a√ß√£o *a* foi selecionada antes do tempo *t*. Assim, podemos reescrever a estimativa por m√©dia de amostra como:

$$
Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{N_t(a)}
$$

quando $N_t(a) > 0$, e $Q_t(a) = 0$ quando $N_t(a) = 0$.

#### Converg√™ncia e a Lei dos Grandes N√∫meros
A beleza da m√©dia de amostra reside em sua converg√™ncia para o valor real da a√ß√£o, $q_*(a)$, √† medida que o n√∫mero de amostras tende ao infinito [^27]. Isso √© garantido pela **Lei dos Grandes N√∫meros (LLN)**. Em termos simples, a LLN afirma que a m√©dia de um n√∫mero suficientemente grande de amostras independentes e identicamente distribu√≠das converge para o valor esperado da distribui√ß√£o. No contexto de m√©todos de valor de a√ß√£o, isso significa que:

$$\lim_{t \to \infty} Q_t(a) = q_*(a)$$

sob a condi√ß√£o de **estacionariedade**, que implica que a distribui√ß√£o de probabilidade das recompensas para cada a√ß√£o permanece constante ao longo do tempo.

**Teorema 1:** *A estimativa por m√©dia de amostra $Q_t(a)$ converge para o valor real da a√ß√£o $q_*(a)$ com probabilidade 1, sob a condi√ß√£o de estacionariedade.*

*Prova:* A prova segue diretamente da Lei Forte dos Grandes N√∫meros. Se as recompensas $R_i$ para a a√ß√£o *a* s√£o independentes e identicamente distribu√≠das com m√©dia $q_*(a)$, ent√£o, pela Lei Forte dos Grandes N√∫meros, a m√©dia amostral $\frac{1}{N_t(a)}\sum_{i=1}^{N_t(a)} R_i$ converge para $q_*(a)$ com probabilidade 1 quando $N_t(a) \to \infty$.  $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Imagine uma a√ß√£o onde o valor real $q_*(a) = 5$. As recompensas obtidas ao longo de v√°rias tentativas s√£o: 4, 6, 5, 4, 6, 5, 5, 4, 6, 5, ...
>
> Inicialmente, $Q_1(a) = 0$ (valor inicial).
> $Q_2(a) = 4/1 = 4$
> $Q_3(a) = (4+6)/2 = 5$
> $Q_4(a) = (4+6+5)/3 = 5$
> ...
>
> √Ä medida que $t$ aumenta, $Q_t(a)$ se aproxima de $q_*(a) = 5$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Valor real da a√ß√£o
> q_star = 5
>
> # Simula√ß√£o de recompensas
> np.random.seed(42)  # Definir a semente para reproducibilidade
> rewards = np.random.normal(q_star, 1, 1000)  # Recompensas com desvio padr√£o 1
>
> # Calculando a m√©dia amostral ao longo do tempo
> Q = np.cumsum(rewards) / np.arange(1, 1001)
>
> # Plotando a converg√™ncia
> plt.figure(figsize=(10, 6))
> plt.plot(Q, label='Estimativa Q(t)')
> plt.axhline(y=q_star, color='r', linestyle='--', label='Valor Real q*(a)')
> plt.xlabel('N√∫mero de Amostras (t)')
> plt.ylabel('Valor Estimado')
> plt.title('Converg√™ncia da M√©dia Amostral para o Valor Real da A√ß√£o')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> O c√≥digo acima simula a obten√ß√£o de recompensas de uma a√ß√£o com valor real de 5 e calcula a m√©dia amostral ao longo do tempo. O gr√°fico resultante mostra como a estimativa $Q_t(a)$ converge para o valor real $q_*(a)$ √† medida que o n√∫mero de amostras aumenta.

#### Considera√ß√µes Pr√°ticas
Embora a m√©dia de amostra ofere√ßa uma garantia te√≥rica de converg√™ncia, algumas considera√ß√µes pr√°ticas s√£o importantes:

1.  **Inicializa√ß√£o**: O valor inicial de $Q_t(a)$ quando a a√ß√£o *a* ainda n√£o foi selecionada pode influenciar o comportamento inicial do agente [^34]. Inicializar com valores otimistas pode encorajar a explora√ß√£o, como ser√° discutido mais adiante.

2.  **N√£o-Estacionaridade**: A m√©dia de amostra atribui igual peso a todas as recompensas passadas, o que pode ser sub√≥timo em ambientes **n√£o-estacion√°rios**, onde as distribui√ß√µes de recompensa mudam ao longo do tempo [^30]. Em tais cen√°rios, m√©todos que d√£o mais peso √†s recompensas recentes s√£o mais adequados, como ser√° explorado em se√ß√µes posteriores.

Para lidar com a n√£o-estacionaridade, uma t√©cnica comum √© usar uma **m√©dia ponderada exponencialmente**. Nesta abordagem, as recompensas mais recentes recebem mais peso do que as recompensas mais antigas. Formalmente, a atualiza√ß√£o do valor da a√ß√£o pode ser definida como:

$$
Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]
$$

onde $\alpha \in (0, 1]$ √© a **taxa de aprendizado**. Um valor de $\alpha$ pr√≥ximo de 0 d√° mais peso √†s recompensas passadas, enquanto um valor de $\alpha$ pr√≥ximo de 1 d√° mais peso √† recompensa mais recente. Esta t√©cnica √© especialmente √∫til em ambientes onde o valor real da a√ß√£o pode mudar ao longo do tempo.

> üí° **Exemplo Num√©rico:**
>
> Suponha que $Q_t(a) = 2$ e recebemos uma recompensa $R_t = 5$. Se escolhermos $\alpha = 0.1$, a atualiza√ß√£o seria:
>
> $Q_{t+1}(a) = 2 + 0.1[5 - 2] = 2 + 0.1(3) = 2.3$
>
> Se escolhermos $\alpha = 0.9$, a atualiza√ß√£o seria:
>
> $Q_{t+1}(a) = 2 + 0.9[5 - 2] = 2 + 0.9(3) = 2 + 2.7 = 4.7$
>
> Observe como um $\alpha$ maior move a estimativa do valor da a√ß√£o mais rapidamente em dire√ß√£o √† recompensa recente.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> alpha = 0.1
> q_true = 1.0  # Valor verdadeiro da a√ß√£o
> n_steps = 100
>
> # Inicializa√ß√£o
> q_estimate = 0.0
> rewards = np.zeros(n_steps)
> q_estimates = np.zeros(n_steps)
>
> # Loop de aprendizado
> np.random.seed(42)
> for i in range(n_steps):
>     reward = np.random.normal(q_true, 1)  # Recompensa aleat√≥ria com m√©dia q_true
>     q_estimate = q_estimate + alpha * (reward - q_estimate)
>     rewards[i] = reward
>     q_estimates[i] = q_estimate
>
> # Plotagem
> plt.figure(figsize=(10, 6))
> plt.plot(q_estimates, label='Estimativa de Q(a)')
> plt.axhline(y=q_true, color='r', linestyle='--', label='Valor Verdadeiro q*(a)')
> plt.xlabel('Passos')
> plt.ylabel('Estimativa de Valor')
> plt.title('M√©dia Ponderada Exponencialmente')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> Este c√≥digo demonstra como a estimativa $Q_{t+1}(a)$ se aproxima do valor real $q_*(a)$ ao longo do tempo, utilizando a m√©dia ponderada exponencialmente.

**Teorema 1.1** *Se a taxa de aprendizado $\alpha$ √© constante, a m√©dia ponderada exponencialmente atribui pesos exponencialmente decrescentes √†s recompensas passadas.*

*Prova:* Expandindo a equa√ß√£o recursiva para $Q_{t+1}(a)$, obtemos:

$Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)] = (1-\alpha)Q_t(a) + \alpha R_t$
$= (1-\alpha) [(1-\alpha)Q_{t-1}(a) + \alpha R_{t-1}] + \alpha R_t = (1-\alpha)^2 Q_{t-1}(a) + \alpha(1-\alpha)R_{t-1} + \alpha R_t$

Continuando a expandir, chegamos a:

$Q_{t+1}(a) = (1-\alpha)^t Q_1(a) + \alpha \sum_{i=1}^{t} (1-\alpha)^{t-i} R_i$.

Isso mostra que o peso da recompensa $R_i$ decresce exponencialmente com o tempo, com um fator de $(1-\alpha)^{t-i}$. $\blacksquare$

**Prova detalhada do Teorema 1.1:**

I. Come√ßamos com a equa√ß√£o de atualiza√ß√£o para a m√©dia ponderada exponencialmente:
   $$Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]$$

II. Reorganizando a equa√ß√£o, obtemos:
    $$Q_{t+1}(a) = (1 - \alpha)Q_t(a) + \alpha R_t$$

III. Aplicando a recurs√£o novamente para $Q_t(a)$:
     $$Q_t(a) = (1 - \alpha)Q_{t-1}(a) + \alpha R_{t-1}$$

IV. Substituindo $Q_t(a)$ na equa√ß√£o original:
    $$Q_{t+1}(a) = (1 - \alpha)[(1 - \alpha)Q_{t-1}(a) + \alpha R_{t-1}] + \alpha R_t$$
    $$Q_{t+1}(a) = (1 - \alpha)^2 Q_{t-1}(a) + \alpha(1 - \alpha)R_{t-1} + \alpha R_t$$

V. Generalizando para *t* passos, obtemos:
   $$Q_{t+1}(a) = (1 - \alpha)^t Q_1(a) + \alpha \sum_{i=1}^{t} (1 - \alpha)^{t-i} R_i$$

VI. Analisando a equa√ß√£o resultante, podemos observar que cada recompensa $R_i$ √© ponderada por um fator de $\alpha (1 - \alpha)^{t-i}$. Isso indica que o peso das recompensas passadas decresce exponencialmente com o tempo, onde $(1 - \alpha)$ √© a taxa de decaimento exponencial.

VII. Portanto, a m√©dia ponderada exponencialmente atribui pesos exponencialmente decrescentes √†s recompensas passadas, com o peso diminuindo em um fator de $(1 - \alpha)$ a cada passo no passado. ‚ñ†

#### Lemma 1: Conectando m√©dias amostrais com a lei dos grandes n√∫meros
*Seja $X_1, X_2, \dots$ uma sequ√™ncia de vari√°veis aleat√≥rias independentes e identicamente distribu√≠das com valor esperado $\mu$. Ent√£o, para qualquer $\epsilon > 0$,*

$$
P\left(\lim_{n \to \infty} \left| \frac{1}{n}\sum_{i=1}^{n} X_i - \mu \right| < \epsilon \right) = 1
$$

*No contexto do problema *k*-armed bandit, se as recompensas para uma dada a√ß√£o *a* s√£o i.i.d com valor esperado $q_*(a)$, ent√£o $Q_t(a)$ converge para $q_*(a)$ quando $t \to \infty$. $\blacksquare$*

### Conclus√£o

Os **m√©todos de valor de a√ß√£o** baseados na amostragem da m√©dia representam uma abordagem fundamental para estimar os valores das a√ß√µes em problemas de aprendizado por refor√ßo [^27]. A garantia de converg√™ncia fornecida pela Lei dos Grandes N√∫meros torna essa t√©cnica atrativa em ambientes estacion√°rios. No entanto, em situa√ß√µes n√£o-estacion√°rias, outras t√©cnicas que se adaptam a mudan√ßas temporais podem ser mais eficazes, como discutido em se√ß√µes posteriores, incluindo m√©todos incrementais que calculam a m√©dia de forma eficiente [^31].

### Refer√™ncias
[^26]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^27]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^30]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^31]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^34]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
<!-- END -->