## M√©todos de Valor de A√ß√£o e Garantias Assint√≥ticas

### Introdu√ß√£o
No campo do aprendizado por refor√ßo, especificamente no contexto dos **multi-armed bandits**, um desafio central √© equilibrar **explora√ß√£o** e **explota√ß√£o** [^1]. Enquanto a explora√ß√£o permite que o agente descubra a√ß√µes potencialmente melhores, a explota√ß√£o concentra-se em utilizar o conhecimento atual para maximizar a recompensa imediata. Este cap√≠tulo se aprofunda nos m√©todos de valor de a√ß√£o, que estimam os valores das a√ß√µes para orientar as decis√µes, focando em uma an√°lise detalhada dos m√©todos **Œµ-gananciosos** e suas garantias assint√≥ticas [^1].

### Conceitos Fundamentais

**M√©todos de valor de a√ß√£o**, ou *action-value methods*, s√£o uma classe de algoritmos de aprendizado por refor√ßo que se baseiam na estimativa dos valores das a√ß√µes para selecionar a melhor a√ß√£o a cada passo. O **valor verdadeiro** de uma a√ß√£o, denotado por $q_*(a)$, √© a recompensa m√©dia esperada quando a a√ß√£o $a$ √© selecionada, ou seja, $q_*(a) = E[R_t | A_t = a]$ [^2]. Em geral, os valores de a√ß√£o s√£o desconhecidos, sendo necess√°rio estim√°-los atrav√©s da intera√ß√£o com o ambiente. Denotamos o **valor estimado** da a√ß√£o $a$ no instante $t$ por $Q_t(a)$, que buscamos que seja pr√≥ximo de $q_*(a)$ [^2].

Um m√©todo comum para estimar esses valores √© o **m√©todo da m√©dia amostral**, ou *sample-average method*, que calcula a m√©dia das recompensas recebidas quando a a√ß√£o $a$ foi selecionada:
$$
Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}},
$$
onde $\mathbb{1}_{A_i=a}$ √© uma fun√ß√£o indicadora que vale 1 se $A_i=a$ e 0 caso contr√°rio [^3]. Se o denominador for zero, $Q_t(a)$ √© definido como um valor padr√£o, como 0 [^3].

> üí° **Exemplo Num√©rico:** Imagine um cen√°rio com 3 a√ß√µes (A, B, e C). As recompensas obtidas ao longo de alguns passos s√£o:
>
> Passo 1: A√ß√£o A, Recompensa = 1
> Passo 2: A√ß√£o B, Recompensa = 0
> Passo 3: A√ß√£o A, Recompensa = 2
> Passo 4: A√ß√£o C, Recompensa = -1
> Passo 5: A√ß√£o B, Recompensa = 1
>
> Usando a m√©dia amostral, temos:
>
> $Q_1(A) = 0$ (inicializa√ß√£o), $Q_2(A) = \frac{1}{1} = 1$, $Q_2(B) = 0$, $Q_3(A) = \frac{1+2}{2} = 1.5$, $Q_3(B) = 0$, $Q_4(A) = 1.5$, $Q_4(B) = 0$, $Q_4(C) = -1$, $Q_5(A) = 1.5$, $Q_5(B) = \frac{0+1}{2} = 0.5$, $Q_5(C) = -1$.
>
> Observe que $Q_t(a)$ evolui a medida que novas recompensas s√£o observadas para cada a√ß√£o.

**Proposi√ß√£o 1** (Atualiza√ß√£o Incremental da M√©dia Amostral): O m√©todo da m√©dia amostral pode ser computado de forma incremental, o que √© computacionalmente mais eficiente para a implementa√ß√£o em sistemas que precisam de respostas em tempo real. A atualiza√ß√£o incremental √© dada por:
$$
Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a) + 1} [R_t - Q_t(a)]
$$
onde $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© o passo $t$.
```mermaid
flowchart TD
    A["$Q_t(a)$"] -->|"$N_t(a)$, $R_t$"| B{"$Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a) + 1} [R_t - Q_t(a)] $"};
    B --> C["$Q_{t+1}(a)$"];
    style B fill:#f9f,stroke:#333,stroke-width:2px
```

*Proof:*
A m√©dia amostral no passo $t+1$ √© definida como:

$$ Q_{t+1}(a) = \frac{\sum_{i=1}^{t} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t} \mathbb{1}_{A_i=a}} $$

Seja $N_{t+1}(a) = \sum_{i=1}^{t} \mathbb{1}_{A_i=a}$. Ent√£o, $N_{t+1}(a) = N_t(a) + \mathbb{1}_{A_t=a}$. Se $A_t \neq a$, ent√£o $N_{t+1}(a) = N_t(a)$. Se $A_t=a$, ent√£o $N_{t+1}(a) = N_t(a) + 1$. Podemos reescrever a soma das recompensas no numerador como:

$$ \sum_{i=1}^{t} R_i \mathbb{1}_{A_i=a} = \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a} + R_t \mathbb{1}_{A_t=a}. $$
Logo, se $A_t=a$:
$$
Q_{t+1}(a) =  \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a} + R_t}{N_t(a) + 1} = \frac{N_t(a) Q_t(a) + R_t}{N_t(a) + 1}
$$
Reorganizando essa express√£o:
$$ Q_{t+1}(a) = \frac{N_t(a) Q_t(a) + R_t}{N_t(a) + 1} = Q_t(a) + \frac{R_t - Q_t(a)}{N_t(a) + 1}. $$
Se $A_t \neq a$, ent√£o $Q_{t+1}(a) = Q_t(a)$, uma vez que a recompensa $R_t$ n√£o contribui para a m√©dia de $a$. Portanto, a atualiza√ß√£o incremental √© geral. $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando o mesmo cen√°rio com as 3 a√ß√µes (A, B, e C) e as recompensas anteriores, ilustramos a atualiza√ß√£o incremental:
>
> Passo 1: A√ß√£o A, Recompensa = 1, $Q_1(A) = 0$, $N_1(A) = 0$.
> $Q_2(A) = 0 + \frac{1}{0+1}(1-0) = 1$
>
> Passo 2: A√ß√£o B, Recompensa = 0, $Q_2(B) = 0$, $N_2(B) = 0$.
> $Q_3(B) = 0 + \frac{1}{0+1}(0-0) = 0$
>
> Passo 3: A√ß√£o A, Recompensa = 2, $Q_2(A) = 1$, $N_2(A) = 1$.
> $Q_3(A) = 1 + \frac{1}{1+1}(2-1) = 1 + \frac{1}{2} = 1.5$
>
> Passo 4: A√ß√£o C, Recompensa = -1, $Q_3(C) = 0$, $N_3(C) = 0$.
> $Q_4(C) = 0 + \frac{1}{0+1}(-1-0) = -1$
>
> Passo 5: A√ß√£o B, Recompensa = 1, $Q_3(B) = 0$, $N_3(B) = 1$.
> $Q_5(B) = 0 + \frac{1}{1+1}(1-0) = 0 + \frac{1}{2} = 0.5$
>
> Os resultados s√£o id√™nticos aos calculados usando a m√©dia amostral diretamente, mas com a vantagem de computar a nova estimativa $Q_{t+1}(a)$ usando apenas a estimativa anterior $Q_t(a)$, a recompensa $R_t$ e o contador $N_t(a)$, evitando o rec√°lculo da m√©dia completa a cada passo.

A regra de sele√ß√£o de a√ß√£o mais simples √© escolher a a√ß√£o com o maior valor estimado, tamb√©m conhecida como **a√ß√£o gananciosa**, ou *greedy action*. Essa escolha pode ser escrita como:
$$
A_t = \arg\max_a Q_t(a).
$$
Entretanto, selecionar apenas a√ß√µes gananciosas n√£o permite a explora√ß√£o de outras a√ß√µes potencialmente melhores [^3].

Para promover a explora√ß√£o, os **m√©todos Œµ-gananciosos**, ou *Œµ-greedy methods*, introduzem uma probabilidade $\epsilon$ (pequena) de selecionar uma a√ß√£o aleat√≥ria, independente dos valores estimados. Com probabilidade $1-\epsilon$ a a√ß√£o gananciosa √© escolhida. A sele√ß√£o da a√ß√£o √© definida como [^3]:
$$
A_t = \begin{cases}
\arg\max_a Q_t(a) & \text{com probabilidade } 1-\epsilon \\
\text{a√ß√£o aleat√≥ria} & \text{com probabilidade } \epsilon
\end{cases}
$$
```mermaid
flowchart TD
    A["Inicio"] --> B{{"Gerar n√∫mero aleat√≥rio 'p' entre 0 e 1"}};
    B -- "p <= $\epsilon$" --> C["A√ß√£o Aleat√≥ria"];
    B -- "p > $\epsilon$" --> D["$A_t = argmax_a Q_t(a)$"];
    C --> E["Fim"];
    D --> E;
   style B fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**  Considerando o estado no passo 5 do exemplo anterior, onde $Q_5(A) = 1.5$, $Q_5(B) = 0.5$, e $Q_5(C) = -1$.
>
> Se usarmos um m√©todo $\epsilon$-ganancioso com $\epsilon = 0.2$, ent√£o:
>
> - Com probabilidade $1 - 0.2 = 0.8$, escolher√≠amos a a√ß√£o gananciosa, que √© a A (pois tem o maior valor estimado de 1.5).
> - Com probabilidade $0.2$, escolher√≠amos uma a√ß√£o aleat√≥ria entre A, B e C. Cada a√ß√£o teria uma probabilidade de $\frac{0.2}{3} \approx 0.067$.
>
> Isto significa que, embora a a√ß√£o A seja a mais frequentemente selecionada (86.7% das vezes), ainda existe uma chance de explorar as a√ß√µes B e C.

O equil√≠brio entre explora√ß√£o e explota√ß√£o √© crucial. A explora√ß√£o permite que o agente descubra a√ß√µes melhores no longo prazo, enquanto a explota√ß√£o otimiza o ganho imediato. Este dilema √© conhecido como o "conflito entre explora√ß√£o e explota√ß√£o" [^2].

**Garantias Assint√≥ticas dos M√©todos Œµ-gananciosos:**
O m√©todo **Œµ-ganancioso** oferece uma garantia assint√≥tica importante. √Ä medida que o n√∫mero de passos aumenta, cada a√ß√£o ser√° selecionada um n√∫mero infinito de vezes. Isso ocorre porque, mesmo com a probabilidade $\epsilon$ de selecionar uma a√ß√£o aleat√≥ria, todas as a√ß√µes t√™m a chance de serem escolhidas em algum momento [^4]. Consequentemente, os valores estimados $Q_t(a)$ convergem para os valores verdadeiros $q_*(a)$, devido √† lei dos grandes n√∫meros.

Ademais, a probabilidade de selecionar a a√ß√£o √≥tima (a a√ß√£o com o maior valor verdadeiro) converge para um valor maior que $1 - \epsilon$, isto √©, para perto da certeza, √† medida que o n√∫mero de passos aumenta [^4]. A seguir, apresentamos o teorema formal que demonstra esse fato, bem como a sua demonstra√ß√£o.

**Lema 1** (Converg√™ncia dos Valores Estimados):
Seja $Q_t(a)$ a estimativa do valor de uma a√ß√£o $a$ no instante $t$, calculada pelo m√©todo da m√©dia amostral e dada pela equa√ß√£o (2.1). Se a quantidade de vezes que a a√ß√£o $a$ √© selecionada tender ao infinito, ent√£o, $Q_t(a)$ converge para $q_*(a)$.
```mermaid
flowchart TD
    subgraph "Lema 1"
    A["$N_t(a) \to \infty$"] --> B{"Lei dos Grandes N√∫meros"};
     B --> C["$Q_t(a) \to q_*(a)$"];
    end
    style A fill:#eef,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#eef,stroke:#333,stroke-width:2px
```

*Proof:*
O m√©todo da m√©dia amostral para o valor de uma a√ß√£o √© dado por:

$$
Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$

Seja $N_t(a) = \sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}$ o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© o instante $t-1$. Ent√£o, podemos reescrever $Q_t(a)$ como:

$$
Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}
$$

Pela lei dos grandes n√∫meros, se $N_t(a) \to \infty$ √† medida que $t \to \infty$, ent√£o a m√©dia amostral converge para a m√©dia esperada:

$$
\lim_{N_t(a) \to \infty} \frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a} = E[R_t|A_t = a] = q_*(a)
$$

Portanto, $Q_t(a)$ converge para $q_*(a)$ √† medida que o n√∫mero de vezes que a a√ß√£o $a$ √© selecionada tende ao infinito. $\blacksquare$

**Lema 1.1** (Converg√™ncia Uniforme da Estimativa de Valor):
Se as recompensas $R_t$ forem limitadas, ou seja, existe um $M > 0$ tal que $|R_t| \leq M$ para todo $t$, ent√£o a converg√™ncia de $Q_t(a)$ para $q_*(a)$ √© uniforme para todas as a√ß√µes $a$.

*Proof:*
Desde que $Q_t(a)$ converge para $q_*(a)$ para cada a√ß√£o $a$, pela Lei dos Grandes N√∫meros, e o conjunto de a√ß√µes √© finito, a converg√™ncia √© uniforme. Uma vez que as recompensas s√£o limitadas, o processo de converg√™ncia √© bem definido e n√£o existem condi√ß√µes extremas que levem a que algumas a√ß√µes convirjam muito mais lentamente do que outras. Assim, no limite, todas as estimativas $Q_t(a)$ convergem uniformemente para seus respectivos valores verdadeiros $q_*(a)$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos 3 a√ß√µes, e as recompensas s√£o sempre entre -1 e 1 (logo, limitadas).
>
> - A√ß√£o A: Recompensa m√©dia $q_*(A) = 0.8$
> - A√ß√£o B: Recompensa m√©dia $q_*(B) = 0.5$
> - A√ß√£o C: Recompensa m√©dia $q_*(C) = 0.2$
>
> Com o tempo, as estimativas $Q_t(A)$, $Q_t(B)$, e $Q_t(C)$ convergem para esses valores. A converg√™ncia uniforme significa que nenhuma das estimativas fica muito atr√°s das outras, ou seja, as diferen√ßas $|Q_t(a) - q_*(a)|$ v√£o para 0 em uma taxa semelhante para todas as a√ß√µes.

**Corol√°rio 1** (Probabilidade de Sele√ß√£o da A√ß√£o √ìtima):
Em um m√©todo Œµ-ganancioso, a probabilidade de selecionar a a√ß√£o √≥tima converge para um valor maior do que $1-\epsilon$, √† medida que $t \to \infty$.
```mermaid
flowchart TD
    subgraph "Corol√°rio 1"
    A["$Q_t(a) \to q_*(a)$"] --> B{"$\epsilon$-greedy"};
     B --> C["$P(A_t = a^*) \geq 1 - \epsilon$"];
    end
   style A fill:#eef,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#eef,stroke:#333,stroke-width:2px
```

*Proof:*
No m√©todo Œµ-ganancioso, a probabilidade de selecionar a a√ß√£o √≥tima √© composta por duas componentes:
1. A probabilidade $1-\epsilon$ de selecionar a a√ß√£o gananciosa, isto √©, a a√ß√£o com o maior valor estimado, $Q_t(a)$.
2. A probabilidade $\epsilon$ de selecionar uma a√ß√£o aleat√≥ria.

Seja $a^*$ a a√ß√£o √≥tima, tal que $q_*(a^*) > q_*(a)$ para todas as outras a√ß√µes $a$. Dado que $Q_t(a)$ converge para $q_*(a)$ para todas as a√ß√µes, √† medida que $t \to \infty$, e que os valores s√£o calculados com o m√©todo de m√©dia amostral, ent√£o, a a√ß√£o com maior $Q_t$ tender√° a ser $a^*$. Logo, o m√©todo vai selecionar a a√ß√£o √≥tima com probabilidade $1-\epsilon$, e as outras a√ß√µes (incluindo $a^*$) com probabilidade $\frac{\epsilon}{k}$.  A probabilidade de selecionar $a^*$ no limite de $t$ grande ser√° ent√£o
$$
\lim_{t \to \infty}  P(A_t = a^*) = (1 - \epsilon) + \frac{\epsilon}{k},
$$
ou seja, a probabilidade de selecionar a a√ß√£o √≥tima converge para um valor maior que $1-\epsilon$ [^4]. $\blacksquare$

**Corol√°rio 1.1** (Limite Inferior da Probabilidade de Sele√ß√£o da A√ß√£o √ìtima):
No limite, a probabilidade de selecionar a a√ß√£o √≥tima, usando um m√©todo $\epsilon$-ganancioso com $k$ a√ß√µes poss√≠veis, √© no m√≠nimo $1 - \epsilon + \frac{\epsilon}{k}$.
```mermaid
flowchart TD
    subgraph "Corol√°rio 1.1"
    A["$\epsilon$-greedy, k actions"] --> B{"Limite"};
    B --> C["$P(A_t = a^*) \geq 1 - \epsilon + \frac{\epsilon}{k}$"];
    end
   style A fill:#eef,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#eef,stroke:#333,stroke-width:2px
```

*Proof:*
Este resultado segue diretamente do argumento apresentado na prova do Corol√°rio 1. Especificamente, a probabilidade de selecionar a a√ß√£o √≥tima no limite √© a soma da probabilidade de selecion√°-la por meio da a√ß√£o gananciosa ($1-\epsilon$), e a probabilidade de selecion√°-la aleatoriamente $\frac{\epsilon}{k}$, que √© o limite inferior da probabilidade de selecionar a a√ß√£o √≥tima no limite. $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo com 3 a√ß√µes e $\epsilon = 0.2$.
>
> A probabilidade de selecionar a a√ß√£o √≥tima, no limite, ser√° pelo menos:
>
> $1 - 0.2 + \frac{0.2}{3} = 0.8 + 0.067 \approx 0.867$ ou 86.7%.
>
> Isso significa que, mesmo com explora√ß√£o, a probabilidade de escolher a melhor a√ß√£o tende para 86.7%, o que √© bem acima de escolher aleatoriamente (33%). A converg√™ncia para a a√ß√£o √≥tima √© garantida, ainda que o m√©todo inclua uma pequena probabilidade de explora√ß√£o.

### Conclus√£o

Os m√©todos de valor de a√ß√£o, especialmente os m√©todos Œµ-gananciosos, representam uma ferramenta fundamental no aprendizado por refor√ßo, permitindo que um agente explore e explote o ambiente para maximizar as recompensas. A propriedade de converg√™ncia dos m√©todos Œµ-gananciosos, demostrada pelas suas garantias assint√≥ticas, assegura que a explora√ß√£o eventual levar√° √† identifica√ß√£o da a√ß√£o √≥tima. Esse balan√ßo entre explora√ß√£o e explota√ß√£o, ainda que simples, √© uma base s√≥lida para algoritmos de aprendizado por refor√ßo mais avan√ßados que consideram situa√ß√µes em que a intera√ß√£o entre o agente e o ambiente √© mais complexa [^1].

### Refer√™ncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de <Multi-armed Bandits>)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action." *(Trecho de <Multi-armed Bandits>)*
[^3]: "The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section." *(Trecho de <Multi-armed Bandits>)*
[^4]: "An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the Qt(a) converge to their respective q*(a)." *(Trecho de <Multi-armed Bandits>)*
