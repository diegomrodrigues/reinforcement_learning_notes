## Upper-Confidence-Bound (UCB) Action Selection: Balancing Exploration and Exploitation

### Introdu√ß√£o
No contexto dos *k*-armed bandits, a busca por estrat√©gias eficientes para equilibrar **explora√ß√£o** e **explota√ß√£o** √© central. Anteriormente, exploramos m√©todos como *Œµ-greedy*, que introduzem aleatoriedade na sele√ß√£o de a√ß√µes para descobrir recompensas potenciais [^2]. No entanto, o *Œµ-greedy* pode ser indiscriminado na escolha de a√ß√µes n√£o-gananciosas, sem prefer√™ncia por aquelas que s√£o quase gananciosas ou particularmente incertas [^11]. Esta se√ß√£o detalha o m√©todo de **Upper-Confidence-Bound (UCB)**, uma abordagem determin√≠stica que visa selecionar a√ß√µes n√£o-gananciosas com base no seu potencial de otimalidade, considerando tanto a proximidade das suas estimativas com o m√°ximo, quanto as incertezas nessas estimativas [^11].

### Conceitos Fundamentais
O m√©todo **UCB** aborda a necessidade de explora√ß√£o reconhecendo que sempre existe incerteza sobre a precis√£o das estimativas de valor das a√ß√µes [^11]. Ao contr√°rio do m√©todo *Œµ-greedy*, que for√ßa a experimenta√ß√£o de a√ß√µes n√£o-gananciosas de forma indiscriminada [^11], o UCB busca refinar a sele√ß√£o dessas a√ß√µes com base no seu potencial de serem √≥timas.

A regra de sele√ß√£o de a√ß√£o UCB √© dada por [^11]:
$$A_t = \argmax_{a} \left[Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}\right] \qquad (2.10)$$

Onde:
*   $A_t$ representa a a√ß√£o selecionada no instante *t*.
*   $Q_t(a)$ √© a estimativa do valor da a√ß√£o *a* no instante *t*.
*   $c > 0$ √© um par√¢metro que controla o grau de explora√ß√£o. Quanto maior o valor de *c*, maior a explora√ß√£o [^36].
*   $t$ √© o instante de tempo atual.
*   $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada antes do instante *t*.
*   $\ln t$ √© o logaritmo natural de *t*.

O termo $$c \sqrt{\frac{\ln t}{N_t(a)}}$$ representa a *upper confidence bound* da a√ß√£o *a*. Este termo quantifica a incerteza ou vari√¢ncia na estimativa do valor da a√ß√£o [^36]. A ideia central √© que a a√ß√£o selecionada √© aquela que maximiza a soma da sua estimativa de valor atual ($Q_t(a)$) e a sua *upper confidence bound*.

> üí° **Exemplo Num√©rico:**
>
> Considere um problema de 3-armed bandit (k=3). Inicialmente, n√£o temos informa√ß√µes sobre as recompensas de cada bra√ßo, ent√£o $Q_1(a) = 0$ para todos os bra√ßos $a$. Vamos usar $c = 1$ para equilibrar explora√ß√£o e explota√ß√£o.
>
> **Itera√ß√£o 1 (t=1):**
>
> *   $N_1(1) = N_1(2) = N_1(3) = 0$.  Como n√£o foram selecionadas a√ß√µes, todas s√£o maximizadoras.
>
> Selecionamos o bra√ßo 1 aleatoriamente para come√ßar. Suponha que recebemos uma recompensa de 2.
>
> **Itera√ß√£o 2 (t=2):**
>
> *   $Q_2(1) = 2$ (m√©dia da recompensa do bra√ßo 1).
> *   $N_2(1) = 1$.
> *   $N_2(2) = N_2(3) = 0$.
>
> Calculamos a UCB para cada bra√ßo:
>
> *   Bra√ßo 1: $UCB_2(1) = 2 + 1 * \sqrt{\frac{\ln 2}{1}} \approx 2 + 0.83 = 2.83$
> *   Bra√ßo 2: $UCB_2(2) = 0 + 1 * \sqrt{\frac{\ln 2}{0}} = \infty$ (j√° que n√£o foi explorado).
> *   Bra√ßo 3: $UCB_2(3) = 0 + 1 * \sqrt{\frac{\ln 2}{0}} = \infty$ (j√° que n√£o foi explorado).
>
> Selecionamos o bra√ßo 2 (arbitrariamente entre 2 e 3). Suponha que recebemos uma recompensa de -1.
>
> **Itera√ß√£o 3 (t=3):**
>
> *   $Q_3(1) = 2$
> *   $Q_3(2) = -1$
> *   $N_3(1) = 1$
> *   $N_3(2) = 1$
> *   $N_3(3) = 0$
>
> Calculamos a UCB para cada bra√ßo:
>
> *   Bra√ßo 1: $UCB_3(1) = 2 + 1 * \sqrt{\frac{\ln 3}{1}} \approx 2 + 1.05 = 3.05$
> *   Bra√ßo 2: $UCB_3(2) = -1 + 1 * \sqrt{\frac{\ln 3}{1}} \approx -1 + 1.05 = 0.05$
> *   Bra√ßo 3: $UCB_3(3) = 0 + 1 * \sqrt{\frac{\ln 3}{0}} = \infty$
>
> Selecionamos o bra√ßo 3. Suponha que recebemos uma recompensa de 1.
>
> **Itera√ß√£o 4 (t=4):**
>
> *   $Q_4(1) = 2$
> *   $Q_4(2) = -1$
> *   $Q_4(3) = 1$
> *   $N_4(1) = 1$
> *   $N_4(2) = 1$
> *   $N_4(3) = 1$
>
> Calculamos a UCB para cada bra√ßo:
>
> *   Bra√ßo 1: $UCB_4(1) = 2 + 1 * \sqrt{\frac{\ln 4}{1}} \approx 2 + 1.18 = 3.18$
> *   Bra√ßo 2: $UCB_4(2) = -1 + 1 * \sqrt{\frac{\ln 4}{1}} \approx -1 + 1.18 = 0.18$
> *   Bra√ßo 3: $UCB_4(3) = 1 + 1 * \sqrt{\frac{\ln 4}{1}} \approx 1 + 1.18 = 2.18$
>
> Selecionamos o bra√ßo 1.
>
> Este processo continua, com o algoritmo UCB equilibrando a explora√ß√£o de bra√ßos menos visitados com a explota√ß√£o de bra√ßos que oferecem recompensas m√©dias mais altas. O par√¢metro $c$ controla o quanto a explora√ß√£o √© incentivada.

**Interpreta√ß√£o da F√≥rmula**
A f√≥rmula (2.10) equilibra a explora√ß√£o e a explota√ß√£o da seguinte forma:

*   **Explota√ß√£o**: O termo $Q_t(a)$ incentiva a sele√ß√£o de a√ß√µes com altas estimativas de valor, explorando o conhecimento atual [^11].
*   **Explora√ß√£o**: O termo $$c \sqrt{\frac{\ln t}{N_t(a)}}$$ incentiva a explora√ß√£o de a√ß√µes que foram selecionadas poucas vezes [^36]. Observe que:
    *   $\ln t$ aumenta com o tempo, o que significa que a explora√ß√£o √© incentivada ao longo do tempo.
    *   $N_t(a)$ est√° no denominador, o que significa que a√ß√µes menos selecionadas t√™m uma *upper confidence bound* maior e s√£o mais propensas a serem exploradas [^36].

**Funcionamento do UCB**

1.  **Inicializa√ß√£o**: Inicialmente, todas as a√ß√µes t√™m um $N_t(a)$ baixo, resultando em *upper confidence bounds* elevadas. Isso encoraja a explora√ß√£o de todas as a√ß√µes pelo menos uma vez [^36]. Se $N_t(a) = 0$, a a√ß√£o *a* √© considerada uma a√ß√£o maximizadora [^36].
2.  **Sele√ß√£o Iterativa**: A cada passo, o algoritmo UCB calcula a *upper confidence bound* para cada a√ß√£o e seleciona a a√ß√£o que maximiza a soma da sua estimativa de valor e a sua *upper confidence bound*.
3.  **Redu√ß√£o da Incerteza**: Cada vez que uma a√ß√£o *a* √© selecionada, $N_t(a)$ aumenta, o que diminui a *upper confidence bound* para essa a√ß√£o [^36]. Isso reduz a probabilidade de a a√ß√£o ser selecionada novamente, a menos que sua estimativa de valor ($Q_t(a)$) seja suficientemente alta.
4.  **Adapta√ß√£o ao Tempo**: O uso do logaritmo natural $\ln t$ garante que a explora√ß√£o continue ao longo do tempo, mas de forma mais branda, permitindo que o algoritmo se concentre gradualmente em a√ß√µes mais promissoras [^36].

Para uma melhor compreens√£o do comportamento do UCB, podemos analisar o limite superior da diferen√ßa entre o valor √≥timo e o valor da a√ß√£o selecionada.

**Teorema 1**
Seja $V^* = \max_a Q^*(a)$ o valor √≥timo esperado, onde $Q^*(a)$ √© o valor verdadeiro da a√ß√£o $a$. Seja $A_t$ a a√ß√£o selecionada no tempo $t$ usando UCB. Ent√£o, para qualquer $\delta > 0$, com probabilidade pelo menos $1 - \delta$, temos:

$$Q^*(V^*) - Q_t(A_t) \leq \sqrt{\frac{8 \ln(t/\delta)}{N_t(A_t)}}$$

*Proof strategy:* A demonstra√ß√£o envolve o uso de desigualdades de concentra√ß√£o, como a desigualdade de Hoeffding, para limitar a diferen√ßa entre a estimativa do valor da a√ß√£o e seu valor verdadeiro. A escolha de $c$ na f√≥rmula UCB est√° relacionada ao n√≠vel de confian√ßa $\delta$.

**Prova do Teorema 1:**

Para provar o Teorema 1, precisamos primeiro estabelecer alguns resultados auxiliares utilizando desigualdades de concentra√ß√£o.

I. **Desigualdade de Hoeffding:** Para uma a√ß√£o *a* e instante *t*, a desigualdade de Hoeffding nos d√°:

$$P(|Q_t(a) - Q^*(a)| > \epsilon) \leq 2e^{-2N_t(a)\epsilon^2}$$

Esta desigualdade limita a probabilidade de que a estimativa do valor da a√ß√£o $Q_t(a)$ se desvie significativamente do seu valor verdadeiro $Q^*(a)$.

II. **Escolha de $\epsilon$:** Queremos encontrar um limite superior para a diferen√ßa $Q^*(V^*) - Q_t(A_t)$. Seja $\epsilon = \sqrt{\frac{2\ln(t/\delta)}{N_t(a)}}$. Substituindo na desigualdade de Hoeffding:

$$P\left(|Q_t(a) - Q^*(a)| > \sqrt{\frac{2\ln(t/\delta)}{N_t(a)}}\right) \leq 2e^{-2N_t(a)\frac{2\ln(t/\delta)}{N_t(a)}} = 2e^{-4\ln(t/\delta)} = \frac{2\delta^4}{t^4}$$

III. **Uni√£o Limitada:** Agora, vamos aplicar a uni√£o limitada sobre todas as a√ß√µes *a* e instantes de tempo *t*:
$$P\left(\exists a, t : |Q_t(a) - Q^*(a)| > \sqrt{\frac{2\ln(t/\delta)}{N_t(a)}}\right) \leq \sum_{a=1}^{k} \sum_{t=1}^{\infty} \frac{2\delta^4}{t^4} \leq \delta$$

Isso mostra que, com probabilidade pelo menos $1 - \delta$, temos:

$$|Q_t(a) - Q^*(a)| \leq \sqrt{\frac{2\ln(t/\delta)}{N_t(a)}}$$

IV. **An√°lise da A√ß√£o Selecionada:** Pela regra UCB, $A_t = \argmax_{a} \left[Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}\right]$. Ent√£o, para qualquer a√ß√£o *a*:

$$Q_t(A_t) + c \sqrt{\frac{\ln t}{N_t(A_t)}} \geq Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}$$

V. **Limitando a Diferen√ßa:** Seja $a^* = \argmax_a Q^*(a)$. Ent√£o:

$$Q^*(a^*) - Q_t(A_t) = Q^*(a^*) - Q^*(A_t) + Q^*(A_t) - Q_t(A_t)$$

Como $Q^*(a^*) = V^*$, temos:

$$V^* - Q_t(A_t) \leq |V^* - Q_t(A_t)| \leq \sqrt{\frac{8 \ln(t/\delta)}{N_t(A_t)}}$$

Portanto, para qualquer $\delta > 0$, com probabilidade pelo menos $1 - \delta$:

$$Q^*(V^*) - Q_t(A_t) \leq \sqrt{\frac{8 \ln(t/\delta)}{N_t(A_t)}}$$ $\blacksquare$

Al√©m disso, podemos discutir uma variante do UCB que se adapta melhor a ambientes n√£o-estacion√°rios.

**Teorema 1.1** (UCB com Janela Deslizante)

Uma adapta√ß√£o do UCB para ambientes n√£o-estacion√°rios √© o UCB com janela deslizante. Neste caso, $N_t(a)$ √© calculado apenas sobre as √∫ltimas $w$ vezes em que a a√ß√£o $a$ foi selecionada, onde $w$ √© o tamanho da janela. A regra de sele√ß√£o de a√ß√£o torna-se:
$$A_t = \argmax_{a} \left[Q_t(a) + c \sqrt{\frac{\ln w}{N_t(a,w)}}\right]$$
onde $N_t(a, w)$ representa o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada nas √∫ltimas $w$ itera√ß√µes.

*Proof strategy:* Ao utilizar uma janela deslizante, o algoritmo UCB se torna mais sens√≠vel √†s mudan√ßas recentes no ambiente. A escolha do tamanho da janela $w$ √© crucial: um valor muito pequeno pode levar a uma alta vari√¢ncia nas estimativas, enquanto um valor muito grande pode tornar o algoritmo lento para responder √†s mudan√ßas.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um ambiente n√£o-estacion√°rio com 2 bra√ßos. Usaremos UCB com janela deslizante de tamanho $w = 5$ e $c=1$.
>
> **Itera√ß√µes 1-5:** (Explora√ß√£o inicial, cada bra√ßo √© selecionado ao menos uma vez)
>
> | Itera√ß√£o (t) | A√ß√£o (At) | Recompensa (Rt) | N_t(1, w) | N_t(2, w) | Q_t(1) | Q_t(2) |
> |--------------|-----------|-----------------|------------|------------|---------|---------|
> | 1            | 1         | 0.5             | 1          | 0          | 0.5     | 0       |
> | 2            | 2         | -0.2            | 1          | 1          | 0.5     | -0.2    |
> | 3            | 1         | 0.6             | 2          | 1          | 0.55    | -0.2    |
> | 4            | 2         | -0.3            | 2          | 2          | 0.55    | -0.25   |
> | 5            | 1         | 0.7             | 3          | 2          | 0.6     | -0.25   |
>
> **Itera√ß√£o 6:**
>
> *   $N_6(1, 5) = 3$ (bra√ßo 1 foi selecionado 3 vezes nas √∫ltimas 5 itera√ß√µes).
> *   $N_6(2, 5) = 2$ (bra√ßo 2 foi selecionado 2 vezes nas √∫ltimas 5 itera√ß√µes).
> *   $Q_6(1) = 0.6$ (recompensa m√©dia do bra√ßo 1).
> *   $Q_6(2) = -0.25$ (recompensa m√©dia do bra√ßo 2).
>
> Calculamos a UCB:
>
> *   Bra√ßo 1: $UCB_6(1) = 0.6 + 1 * \sqrt{\frac{\ln 5}{3}} \approx 0.6 + 0.73 = 1.33$
> *   Bra√ßo 2: $UCB_6(2) = -0.25 + 1 * \sqrt{\frac{\ln 5}{2}} \approx -0.25 + 0.90 = 0.65$
>
> Selecionamos o bra√ßo 1.
>
> **Itera√ß√£o 7:**
>
> Suponha que a recompensa do bra√ßo 1 muda repentinamente para -0.9.
>
> *   $N_7(1, 5) = 3$ (bra√ßo 1 foi selecionado 3 vezes nas √∫ltimas 5 itera√ß√µes: 3, 5, 6)
> *   $N_7(2, 5) = 2$ (bra√ßo 2 foi selecionado 2 vezes nas √∫ltimas 5 itera√ß√µes: 4, 2).
> *   $Q_7(1) \approx (0.6+0.7 - 0.9)/3= 0.133$
> *   $Q_7(2) = -0.25$
>
> A janela deslizante permite que o algoritmo se adapte rapidamente √† mudan√ßa na recompensa do bra√ßo 1. Se a mudan√ßa fosse mais gradual, a adapta√ß√£o seria igualmente suave.

### Vantagens e Desvantagens
**Vantagens:**

*   **Equil√≠brio Eficaz**: O UCB oferece um equil√≠brio eficaz entre explora√ß√£o e explota√ß√£o, adaptando-se dinamicamente √† incerteza nas estimativas de valor das a√ß√µes [^11].
*   **Desempenho Emp√≠rico**: Em muitos problemas *k*-armed bandit, o UCB supera os m√©todos *Œµ-greedy*, especialmente em cen√°rios estacion√°rios [^36].
*   **N√£o Necessidade de Ajuste Fino de Œµ**: Ao contr√°rio dos m√©todos *Œµ-greedy*, o UCB requer apenas o ajuste do par√¢metro *c*, que controla a taxa geral de explora√ß√£o.

**Desvantagens:**

*   **Sensibilidade a *c***: O desempenho do UCB pode ser sens√≠vel ao valor do par√¢metro *c*. Um valor muito alto pode levar a explora√ß√£o excessiva, enquanto um valor muito baixo pode resultar em explota√ß√£o prematura [^36].
*   **Dificuldade em Ambientes N√£o Estacion√°rios**: O UCB tem dificuldades em ambientes n√£o estacion√°rios, onde as recompensas das a√ß√µes mudam ao longo do tempo [^36]. M√©todos mais complexos de lidar com a n√£o estacionariedade, al√©m dos apresentados na se√ß√£o 2.5 [^36], podem ser necess√°rios.
*   **Escalabilidade Limitada**: O UCB pode ser dif√≠cil de escalar para espa√ßos de estados grandes, particularmente quando se utiliza aproxima√ß√£o de fun√ß√£o [^36]. Nesses casos, a ideia de sele√ß√£o de a√ß√£o UCB torna-se impratic√°vel [^36].

### Compara√ß√£o com o M√©todo *Œµ-greedy*

Enquanto o m√©todo *Œµ-greedy* for√ßa a explora√ß√£o com uma probabilidade fixa *Œµ*, independentemente do n√∫mero de vezes que uma a√ß√£o foi selecionada, o UCB ajusta dinamicamente a explora√ß√£o com base na incerteza das estimativas de valor das a√ß√µes [^11]. A Figura 2.4 [^36] ilustra o desempenho comparativo do UCB e do *Œµ-greedy* no *10-armed testbed*. O UCB geralmente supera o *Œµ-greedy*, exceto nos *k* primeiros passos, onde seleciona aleatoriamente entre as a√ß√µes ainda n√£o experimentadas [^36].



![Average performance comparison of UCB and Œµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

> üí° **Exemplo Num√©rico:**
>
> Consideremos um cen√°rio simples com 2 bra√ßos e comparemos UCB e Œµ-greedy ao longo de 100 itera√ß√µes. Suponha que o bra√ßo 1 d√° uma recompensa m√©dia de 0.6 e o bra√ßo 2, 0.4.
>
> **Œµ-greedy:** Usamos Œµ = 0.1. Isso significa que em 10% das vezes, escolhemos um bra√ßo aleatoriamente, e em 90% das vezes, escolhemos o bra√ßo com a melhor recompensa m√©dia estimada at√© o momento.
>
> **UCB:** Usamos c = 0.5.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Configura√ß√£o do problema
> n_arms = 2
> true_rewards = [0.6, 0.4]
> n_iterations = 100
>
> # Œµ-greedy
> epsilon = 0.1
> q_values_eg = [0, 0]
> n_selections_eg = [0, 0]
> rewards_eg = []
>
> # UCB
> c = 0.5
> q_values_ucb = [0, 0]
> n_selections_ucb = [0, 0]
> rewards_ucb = []
>
> # Loop principal
> for t in range(1, n_iterations + 1):
>     # Œµ-greedy
>     if np.random.rand() < epsilon:
>         arm_eg = np.random.choice(n_arms)
>     else:
>         arm_eg = np.argmax(q_values_eg)
>
>     reward_eg = np.random.normal(true_rewards[arm_eg], 0.1)  # Recompensa com ru√≠do
>     n_selections_eg[arm_eg] += 1
>     q_values_eg[arm_eg] += (reward_eg - q_values_eg[arm_eg]) / n_selections_eg[arm_eg]
>     rewards_eg.append(reward_eg)
>
>     # UCB
>     ucb_values = [q_values_ucb[a] + c * np.sqrt(np.log(t) / (n_selections_ucb[a] + 1e-6)) for a in range(n_arms)]
>     arm_ucb = np.argmax(ucb_values)
>
>     reward_ucb = np.random.normal(true_rewards[arm_ucb], 0.1)  # Recompensa com ru√≠do
>     n_selections_ucb[arm_ucb] += 1
>     q_values_ucb[arm_ucb] += (reward_ucb - q_values_ucb[arm_ucb]) / n_selections_ucb[arm_ucb]
>     rewards_ucb.append(reward_ucb)
>
> # Calcular recompensas m√©dias cumulativas
> cumulative_rewards_eg = np.cumsum(rewards_eg) / np.arange(1, n_iterations + 1)
> cumulative_rewards_ucb = np.cumsum(rewards_ucb) / np.arange(1, n_iterations + 1)
>
> # Plotar resultados
> plt.figure(figsize=(10, 6))
> plt.plot(cumulative_rewards_eg, label='Œµ-greedy (Œµ=0.1)')
> plt.plot(cumulative_rewards_ucb, label='UCB (c=0.5)')
> plt.xlabel('Iteration')
> plt.ylabel('Cumulative Average Reward')
> plt.title('Comparison of Œµ-greedy and UCB')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Neste exemplo, o gr√°fico mostra que o UCB converge para uma recompensa m√©dia cumulativa maior mais rapidamente do que o Œµ-greedy.  Isso acontece porque o UCB explora de forma mais inteligente, equilibrando a necessidade de descobrir novos bra√ßos com a explota√ß√£o daqueles que j√° mostraram ser promissores.

### Conclus√£o

O m√©todo **Upper-Confidence-Bound (UCB)** √© uma abordagem eficaz para equilibrar explora√ß√£o e explota√ß√£o em problemas *k*-armed bandit [^11]. Ao quantificar a incerteza nas estimativas de valor das a√ß√µes, o UCB permite uma sele√ß√£o de a√ß√µes mais informada, promovendo a explora√ß√£o de a√ß√µes com potencial de otimalidade [^11]. Embora o UCB apresente desafios em ambientes n√£o estacion√°rios e com grandes espa√ßos de estados [^36], ele continua sendo uma ferramenta valiosa no arsenal de algoritmos de *reinforcement learning* [^11]. M√©todos mais sofisticados e complexos podem ser necess√°rios para tratar as limita√ß√µes do UCB em determinados cen√°rios.

### Refer√™ncias
[^2]: Cap√≠tulo 2, Multi-armed Bandits, Introdu√ß√£o.
[^3]: Cap√≠tulo 2, Multi-armed Bandits, 2.6 Optimistic Initial Values
[^11]: Cap√≠tulo 2, Multi-armed Bandits, 2.7 Upper-Confidence-Bound Action Selection
[^36]: Cap√≠tulo 2, Multi-armed Bandits, 2.7 Upper-Confidence-Bound Action Selection,
<!-- END -->