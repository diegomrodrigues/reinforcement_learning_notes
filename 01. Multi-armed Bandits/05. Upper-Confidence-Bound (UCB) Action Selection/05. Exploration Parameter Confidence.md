## Ajuste da Confian√ßa nos Limites Superiores com o Par√¢metro 'c' em UCB

### Introdu√ß√£o
No contexto dos *k-armed bandit problems*, o algoritmo Upper-Confidence-Bound (UCB) representa uma abordagem para equilibrar a *explora√ß√£o* e a *explota√ß√£o* [^2]. O UCB, conforme apresentado na se√ß√£o anterior, seleciona a√ß√µes com base em uma combina√ß√£o da estimativa de valor da a√ß√£o e um termo de incerteza que incentiva a explora√ß√£o de a√ß√µes menos amostradas [^2]. Este termo de incerteza √© crucial e diretamente influenciado pelo par√¢metro de explora√ß√£o *c* [^11]. Esta se√ß√£o detalha como o ajuste do par√¢metro *c* modula a confian√ßa nos limites superiores dos valores das a√ß√µes, afetando assim a estrat√©gia de explora√ß√£o do algoritmo.

### Conceitos Fundamentais

O algoritmo UCB utiliza a seguinte f√≥rmula para selecionar a√ß√µes [^11]:

$$ A_t = \underset{a}{\text{argmax}} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right] $$

onde:
- $A_t$ √© a a√ß√£o selecionada no instante *t*
- $Q_t(a)$ √© a estimativa do valor da a√ß√£o *a* no instante *t*
- *c* √© o par√¢metro de explora√ß√£o
- *t* √© o instante atual
- $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o instante *t*

O termo $c \sqrt{\frac{\ln t}{N_t(a)}}$ representa o *upper confidence bound* da a√ß√£o *a*. Este termo √© adicionado √† estimativa de valor $Q_t(a)$ para encorajar a explora√ß√£o [^11].

A influ√™ncia do par√¢metro *c* reside na modula√ß√£o da largura deste *upper confidence bound*. Um valor de *c* maior aumenta a magnitude do termo de incerteza, tornando as a√ß√µes menos amostradas mais atraentes para sele√ß√£o [^11]. Por outro lado, um valor de *c* menor diminui a import√¢ncia do termo de incerteza, favorecendo a explota√ß√£o de a√ß√µes com estimativas de valor mais altas, mesmo que sua incerteza seja relativamente baixa [^11].

> üí° **Exemplo Num√©rico:** Considere um cen√°rio com duas a√ß√µes. Ap√≥s algumas itera√ß√µes, a a√ß√£o 1 tem uma recompensa m√©dia estimada de 0.6 e foi selecionada 10 vezes, enquanto a a√ß√£o 2 tem uma recompensa m√©dia estimada de 0.4 e foi selecionada apenas 3 vezes. Vamos calcular o UCB para ambas as a√ß√µes no instante $t=30$ para diferentes valores de $c$.
>
> Caso 1: $c = 0.1$
> $$UCB(a_1) = 0.6 + 0.1 \sqrt{\frac{\ln 30}{10}} \approx 0.6 + 0.1 \sqrt{\frac{3.4}{10}} \approx 0.6 + 0.058 \approx 0.658$$
> $$UCB(a_2) = 0.4 + 0.1 \sqrt{\frac{\ln 30}{3}} \approx 0.4 + 0.1 \sqrt{\frac{3.4}{3}} \approx 0.4 + 0.0 \sqrt{1.13} \approx 0.4 + 0.106 \approx 0.506$$
> Neste caso, a a√ß√£o 1 tem um UCB maior (0.658) e seria selecionada.
>
> Caso 2: $c = 1$
> $$UCB(a_1) = 0.6 + 1 \sqrt{\frac{\ln 30}{10}} \approx 0.6 + 1 \sqrt{\frac{3.4}{10}} \approx 0.6 + 0.583 \approx 1.183$$
> $$UCB(a_2) = 0.4 + 1 \sqrt{\frac{\ln 30}{3}} \approx 0.4 + 1 \sqrt{\frac{3.4}{3}} \approx 0.4 + 1.06 \approx 1.46$$
> Neste caso, a a√ß√£o 2 tem um UCB muito maior (1.46) devido ao alto valor de *c* e ao baixo n√∫mero de sele√ß√µes, incentivando a explora√ß√£o.
>
> Este exemplo ilustra como um *c* maior pode mudar drasticamente a a√ß√£o selecionada, priorizando a√ß√µes menos exploradas.

**An√°lise Detalhada:**

1. **Valores Altos de *c* (Maior Explora√ß√£o):**
   - Com um *c* alto, o algoritmo atribui uma alta confian√ßa nos limites superiores das a√ß√µes, independentemente de qu√£o incertas sejam suas estimativas de valor [^11].
   - Isso leva o algoritmo a explorar a√ß√µes menos amostradas mais frequentemente, pois o termo $c \sqrt{\frac{\ln t}{N_t(a)}}$ domina a decis√£o [^11].
   - A explora√ß√£o aumentada √© ben√©fica no in√≠cio do aprendizado, pois ajuda a descobrir a√ß√µes potencialmente √≥timas que inicialmente pareciam menos promissoras devido a amostras limitadas [^11].
   - No entanto, com o tempo, um *c* excessivamente alto pode levar a uma explora√ß√£o excessiva, onde o algoritmo continua a experimentar a√ß√µes sub√≥timas, mesmo depois de ter identificado a√ß√µes melhores [^11].

> üí° **Exemplo Num√©rico:** Imagine que temos 5 a√ß√µes (*k* = 5). Inicialmente, todas as a√ß√µes s√£o tentadas uma vez. Ap√≥s essas tentativas iniciais, as recompensas m√©dias s√£o: $Q(a_1) = 0.2$, $Q(a_2) = 0.3$, $Q(a_3) = 0.1$, $Q(a_4) = 0.5$, $Q(a_5) = 0.15$.
>
> Se escolhermos um valor alto para *c*, digamos *c* = 5, o algoritmo continuar√° explorando as a√ß√µes com menos amostras por um tempo consider√°vel, mesmo que a a√ß√£o 4 ($Q(a_4) = 0.5$) pare√ßa promissora.
>
> Para *t* = 10:
>
> $UCB(a) = Q(a) + 5 \sqrt{\frac{\ln 10}{N_t(a)}}$
>
> Se a a√ß√£o 4 foi selecionada 2 vezes: $UCB(a_4) = 0.5 + 5 \sqrt{\frac{\ln 10}{2}} \approx 0.5 + 5 \sqrt{\frac{2.3}{2}} \approx 0.5 + 5 * 1.07 \approx 5.85$.
>
> Se a a√ß√£o 5 foi selecionada apenas uma vez: $UCB(a_5) = 0.15 + 5 \sqrt{\frac{\ln 10}{1}} \approx 0.15 + 5 \sqrt{2.3} \approx 0.15 + 5 * 1.52 \approx 7.75$.
>
> Neste caso, a a√ß√£o 5 seria selecionada mesmo tendo um valor m√©dio muito menor, demonstrando a influ√™ncia de um *c* alto na explora√ß√£o.

2. **Valores Baixos de *c* (Maior Explota√ß√£o):**
   - Com um *c* baixo, o algoritmo confia mais nas estimativas de valor atuais das a√ß√µes e menos na necessidade de explorar incertezas [^11].
   - Isso leva a uma explota√ß√£o mais r√°pida das a√ß√µes que parecem ser as melhores com base nas informa√ß√µes dispon√≠veis [^11].
   - A explota√ß√£o √© vantajosa quando o algoritmo j√° tem uma boa compreens√£o do ambiente e pode identificar a√ß√µes √≥timas com relativa certeza [^11].
   - No entanto, um *c* excessivamente baixo pode levar √† converg√™ncia prematura para uma a√ß√£o sub√≥tima, pois o algoritmo pode n√£o explorar o suficiente para descobrir a√ß√µes melhores [^11].

> üí° **Exemplo Num√©rico:** Considerando o mesmo cen√°rio das 5 a√ß√µes, mas agora com um *c* muito baixo, como *c* = 0.01:
>
> Ap√≥s as tentativas iniciais ($Q(a_1) = 0.2$, $Q(a_2) = 0.3$, $Q(a_3) = 0.1$, $Q(a_4) = 0.5$, $Q(a_5) = 0.15$), o algoritmo rapidamente se concentrar√° na a√ß√£o 4, que tem a recompensa m√©dia mais alta.
>
> Para *t* = 10:
>
> $UCB(a) = Q(a) + 0.01 \sqrt{\frac{\ln 10}{N_t(a)}}$
>
> Se a a√ß√£o 4 foi selecionada 5 vezes: $UCB(a_4) = 0.5 + 0.01 \sqrt{\frac{\ln 10}{5}} \approx 0.5 + 0.01 \sqrt{\frac{2.3}{5}} \approx 0.5 + 0.01 * 0.67 \approx 0.5067$.
>
> Se a a√ß√£o 1 foi selecionada 2 vezes: $UCB(a_1) = 0.2 + 0.01 \sqrt{\frac{\ln 10}{2}} \approx 0.2 + 0.01 \sqrt{\frac{2.3}{2}} \approx 0.2 + 0.01 * 1.07 \approx 0.2107$.
>
> Neste caso, a a√ß√£o 4 continuar√° sendo selecionada, pois mesmo com mais sele√ß√µes, o termo de incerteza √© muito pequeno devido ao baixo valor de *c*. Isso pode impedir que o algoritmo descubra uma a√ß√£o potencialmente melhor que n√£o foi inicialmente bem amostrada.

**Ilustra√ß√£o Matem√°tica:**

Considere duas a√ß√µes, *a1* e *a2*, com as seguintes caracter√≠sticas no instante *t*:

- $Q_t(a1) = 0.5$, $N_t(a1) = 100$
- $Q_t(a2) = 0.4$, $N_t(a2) = 10$

Se $c = 0.1$:

- $UCB(a1) = 0.5 + 0.1 \sqrt{\frac{\ln t}{100}}$
- $UCB(a2) = 0.4 + 0.1 \sqrt{\frac{\ln t}{10}}$

Se $c = 1$:

- $UCB(a1) = 0.5 + 1 \sqrt{\frac{\ln t}{100}}$
- $UCB(a2) = 0.4 + 1 \sqrt{\frac{\ln t}{10}}$

Para um *t* suficientemente grande, podemos observar que com $c=0.1$, *a1* (a a√ß√£o com maior valor estimado) √© mais prov√°vel de ser selecionada. No entanto, com $c=1$, *a2* (a a√ß√£o com maior incerteza) tem uma chance maior de ser selecionada, incentivando a explora√ß√£o.

Para formalizar essa observa√ß√£o, podemos introduzir o conceito de *arrependimento* (regret) e relacion√°-lo com o par√¢metro *c*.

**Defini√ß√£o:** O *arrependimento* no instante *t* √© definido como a diferen√ßa entre a recompensa esperada da a√ß√£o √≥tima e a recompensa obtida pela a√ß√£o selecionada no instante *t*. Formalmente, se $a^*$ √© a a√ß√£o √≥tima, ent√£o o arrependimento no instante *t* √© $R_t = Q(a^*) - Q_t(A_t)$, onde $Q(a^*)$ √© o valor verdadeiro da a√ß√£o √≥tima e $Q_t(A_t)$ √© a estimativa do valor da a√ß√£o selecionada $A_t$ no instante *t*. O arrependimento total at√© o instante *T* √© $\sum_{t=1}^{T} R_t$.

**Teorema 1:** (Limite Superior para o Arrependimento do UCB)
O arrependimento total esperado do algoritmo UCB ap√≥s *T* passos √© limitado superiormente por:

$$E\left[\sum_{t=1}^{T} R_t\right] \leq \sum_{a: \Delta_a > 0} \left[ \frac{8c^2 \ln T}{\Delta_a} + (1 + \frac{\pi^2}{3})\Delta_a \right]$$

onde $\Delta_a = Q(a^*) - Q(a)$ √© a diferen√ßa entre o valor da a√ß√£o √≥tima e o valor da a√ß√£o *a*, e a soma √© sobre todas as a√ß√µes sub√≥timas *a*.

*Proof Sketch:* A prova desse teorema envolve mostrar que o n√∫mero de vezes que uma a√ß√£o sub√≥tima √© selecionada √© limitado logarithmicamente em *T*. O par√¢metro *c* aparece no limite superior, indicando que um *c* maior pode aumentar o limite superior do arrependimento se n√£o for escolhido adequadamente. A prova geralmente usa desigualdades de concentra√ß√£o para limitar a probabilidade de que uma a√ß√£o sub√≥tima pare√ßa melhor que a a√ß√£o √≥tima devido a flutua√ß√µes aleat√≥rias nas recompensas.

**Prova do Limite Superior para o Arrependimento do UCB:**

I. **Defini√ß√£o de Evento:** Seja $a^*$ a a√ß√£o √≥tima e $a$ uma a√ß√£o sub√≥tima, com $\Delta_a = Q(a^*) - Q(a) > 0$. Definimos o evento $\mathcal{E}_{t,a}$ como o evento em que a a√ß√£o sub√≥tima $a$ √© selecionada no tempo $t$, ou seja, $A_t = a$.

II. **Condi√ß√£o para Sele√ß√£o de A√ß√£o Sub√≥tima:** Para que a a√ß√£o $a$ seja selecionada no tempo $t$, √© necess√°rio que seu UCB seja maior ou igual ao UCB da a√ß√£o √≥tima $a^*$:
   $$Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \geq Q_t(a^*) + c \sqrt{\frac{\ln t}{N_t(a^*)}}$$

III. **Rearranjo da Desigualdade:** Podemos rearranjar a desigualdade acima:
   $$Q_t(a^*) - Q_t(a) \leq c \sqrt{\frac{\ln t}{N_t(a)}} + c \sqrt{\frac{\ln t}{N_t(a^*)}}$$
   Como $Q(a^*) - Q(a) = \Delta_a$, e $Q_t(a)$ √© uma estimativa de $Q(a)$, podemos usar uma desigualdade de concentra√ß√£o (como Hoeffding) para limitar a probabilidade de que $Q_t(a)$ se desvie significativamente de $Q(a)$.

IV. **Aplica√ß√£o da Desigualdade de Hoeffding:** A desigualdade de Hoeffding nos diz que:
   $$P(|Q_t(a) - Q(a)| > \epsilon) \leq 2e^{-2N_t(a)\epsilon^2}$$
   Escolhendo $\epsilon = \frac{\Delta_a}{2}$, temos:
   $$P(|Q_t(a) - Q(a)| > \frac{\Delta_a}{2}) \leq 2e^{-2N_t(a)(\frac{\Delta_a}{2})^2} = 2e^{-\frac{N_t(a)\Delta_a^2}{2}}$$
   Se $|Q_t(a) - Q(a)| \leq \frac{\Delta_a}{2}$ e $|Q_t(a^*) - Q(a^*)| \leq \frac{\Delta_a}{2}$, ent√£o $Q_t(a^*) - Q_t(a) \geq \Delta_a - \Delta_a = 0$.
   Caso contr√°rio, $a$ pode ser selecionada.

V. **Limita√ß√£o do N√∫mero de Sele√ß√µes de A√ß√µes Sub√≥timas:** Queremos limitar o n√∫mero de vezes que uma a√ß√£o sub√≥tima $a$ √© selecionada. Se $N_t(a) > \frac{8c^2 \ln t}{\Delta_a^2}$, ent√£o:
$$Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} < Q(a) + \frac{\Delta_a}{2} + c \sqrt{\frac{\ln t}{8c^2 \ln t / \Delta_a^2}} = Q(a) + \frac{\Delta_a}{2} + \frac{\Delta_a}{2\sqrt{2}} < Q(a) + \Delta_a = Q(a^*)$$

VI. **Limite Superior para o Arrependimento:** O arrependimento total √© a soma das diferen√ßas entre a recompensa √≥tima e a recompensa obtida:
   $$E\left[\sum_{t=1}^{T} R_t\right] = E\left[\sum_{t=1}^{T} (Q(a^*) - Q(A_t))\right] = \sum_{a: \Delta_a > 0} \Delta_a E[N_T(a)]$$
   Onde $N_T(a)$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© o tempo $T$.

VII. **Deriva√ß√£o do Limite Superior:** Combinando os resultados anteriores, temos que o n√∫mero esperado de vezes que uma a√ß√£o sub√≥tima $a$ √© selecionada √© limitado por:
    $$E[N_T(a)] \leq \frac{8c^2 \ln T}{\Delta_a^2} + (1 + \frac{\pi^2}{3})$$
    Substituindo isso na express√£o para o arrependimento total, obtemos:
   $$E\left[\sum_{t=1}^{T} R_t\right] \leq \sum_{a: \Delta_a > 0} \left[ \frac{8c^2 \ln T}{\Delta_a} + (1 + \frac{\pi^2}{3})\Delta_a \right]$$ ‚ñ†

Al√©m disso, podemos analisar a influ√™ncia de *c* na taxa de converg√™ncia do algoritmo.

**Proposi√ß√£o 1:** (Trade-off entre Explora√ß√£o e Explota√ß√£o)
Existe um valor √≥timo de *c* que minimiza o arrependimento total esperado. Este valor depende das caracter√≠sticas espec√≠ficas do problema, como a magnitude das diferen√ßas de valor entre as a√ß√µes ($\Delta_a$) e o horizonte de tempo *T*.

*Proof Sketch:* Essa proposi√ß√£o decorre do trade-off inerente entre explora√ß√£o e explota√ß√£o. Um *c* muito pequeno leva a uma explota√ß√£o prematura de a√ß√µes sub√≥timas, enquanto um *c* muito grande leva a uma explora√ß√£o excessiva de a√ß√µes sub√≥timas. O valor √≥timo de *c* equilibra esses dois efeitos, minimizando o arrependimento total. A determina√ß√£o anal√≠tica exata desse valor √≥timo pode ser complexa e geralmente requer conhecimento das caracter√≠sticas espec√≠ficas do problema.

**Prova do Trade-off entre Explora√ß√£o e Explota√ß√£o:**
I. **Arrependimento e o Par√¢metro c:** O teorema anterior mostrou que o arrependimento esperado do algoritmo UCB depende do par√¢metro *c*. Queremos encontrar o valor de *c* que minimiza:
$$E\left[\sum_{t=1}^{T} R_t\right] \leq \sum_{a: \Delta_a > 0} \left[ \frac{8c^2 \ln T}{\Delta_a} + (1 + \frac{\pi^2}{3})\Delta_a \right]$$

II. **An√°lise do Arrependimento:** O termo $\frac{8c^2 \ln T}{\Delta_a}$ aumenta com *c*, refletindo o custo da explora√ß√£o excessiva, enquanto o termo $(1 + \frac{\pi^2}{3})\Delta_a$ √© independente de *c*.

III. **Otimiza√ß√£o de c:** Para encontrar o valor √≥timo de *c*, poder√≠amos tentar diferenciar a express√£o do limite superior do arrependimento em rela√ß√£o a *c* e igualar a zero. No entanto, essa abordagem simplificada n√£o leva em considera√ß√£o a depend√™ncia complexa de $N_t(a)$ em rela√ß√£o a *c*.

IV. **Trade-off Intuitivo:** Intuitivamente, se *c* √© muito pequeno, o algoritmo explora pouco e pode convergir para uma a√ß√£o sub√≥tima. Se *c* √© muito grande, o algoritmo explora demais e n√£o explota as a√ß√µes que j√° sabe serem boas.

V. **Depend√™ncia do Problema:** O valor √≥timo de *c* depende das caracter√≠sticas do problema:
    - **Magnitude de $\Delta_a$**: Se as diferen√ßas entre as a√ß√µes s√£o pequenas, √© preciso explorar mais para encontrar a a√ß√£o √≥tima, justificando um *c* maior.
    - **Horizonte de Tempo T**: Para horizontes de tempo maiores, pode valer a pena explorar mais no in√≠cio para encontrar a a√ß√£o √≥tima, mesmo que isso signifique um arrependimento inicial maior.
    - **Natureza Estacion√°ria do Problema**: Em ambientes n√£o estacion√°rios, √© preciso explorar continuamente para se adaptar √†s mudan√ßas nas recompensas, o que exige um *c* maior.

VI. **Conclus√£o:** Portanto, existe um valor √≥timo de *c* que equilibra a explora√ß√£o e a explota√ß√£o, minimizando o arrependimento total esperado. Este valor √© espec√≠fico para cada problema e, em geral, n√£o pode ser determinado analiticamente sem conhecimento pr√©vio das caracter√≠sticas do problema. Em vez disso, *c* √© geralmente ajustado empiricamente. ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos simular o desempenho do UCB com diferentes valores de *c* em um problema de *k*-armed bandit com 3 a√ß√µes. As recompensas m√©dias das a√ß√µes s√£o 0.2, 0.5 e 0.7, respectivamente. Vamos rodar o algoritmo por 200 passos e plotar o arrependimento cumulativo para *c* = 0.1, *c* = 0.5 e *c* = 1.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def ucb(q_values, c, steps):
>     """
>     Executa o algoritmo UCB por um determinado n√∫mero de passos.
>     """
>     n_arms = len(q_values)
>     counts = np.zeros(n_arms)
>     values = np.zeros(n_arms)
>     rewards = []
>     cumulative_regret = [0]
>
>     # Inicializa as a√ß√µes
>     for arm in range(n_arms):
>         reward = np.random.normal(q_values[arm], 1) # Assume desvio padr√£o = 1
>         rewards.append(reward)
>         counts[arm] += 1
>         values[arm] = reward
>         cumulative_regret[0] += max(q_values) - q_values[arm]
>
>     for t in range(n_arms, steps):
>         ucb_values = values + c * np.sqrt(np.log(t) / counts)
>         chosen_arm = np.argmax(ucb_values)
>         reward = np.random.normal(q_values[chosen_arm], 1)
>         rewards.append(reward)
>         counts[chosen_arm] += 1
>         values[chosen_arm] = ((values[chosen_arm] * (counts[chosen_arm] - 1)) + reward) / counts[chosen_arm]
>         regret = max(q_values) - q_values[chosen_arm]
>         cumulative_regret.append(cumulative_regret[-1] + regret)
>
>     return cumulative_regret
>
> # Define os valores m√©dios das recompensas das a√ß√µes
> q_values = [0.2, 0.5, 0.7]
>
> # Define o n√∫mero de passos
> steps = 200
>
> # Executa o UCB com diferentes valores de c
> c_values = [0.1, 0.5, 1]
> cumulative_regrets = {}
> for c in c_values:
>     cumulative_regrets[c] = ucb(q_values, c, steps)
>
> # Plota o arrependimento cumulativo
> plt.figure(figsize=(10, 6))
> for c in c_values:
>     plt.plot(cumulative_regrets[c], label=f'c = {c}')
> plt.title('Arrependimento Cumulativo do UCB com Diferentes Valores de c')
> plt.xlabel('Passos')
> plt.ylabel('Arrependimento Cumulativo')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Este c√≥digo simula o UCB para diferentes valores de *c* e plota o arrependimento cumulativo. Podemos observar que um valor de *c* muito baixo converge rapidamente, mas para uma solu√ß√£o sub√≥tima (alto arrependimento final), enquanto um valor de *c* mais alto explora mais e pode atingir um arrependimento menor a longo prazo. O valor √≥timo de *c* depender√° do problema espec√≠fico.
>
> ```mermaid
> graph LR
>     A[In√≠cio] --> B{Inicializar A√ß√µes};
>     B --> C{Loop: Para t = 1 at√© T};
>     C --> D{Calcular UCB para cada a√ß√£o};
>     D --> E{Selecionar a√ß√£o com maior UCB};
>     E --> F{Obter recompensa da a√ß√£o selecionada};
>     F --> G{Atualizar estimativas de valor e contagem da a√ß√£o};
>     G --> H{Calcular arrependimento};
>     H --> I{Acumular arrependimento};
>     I --> J{Fim do Loop};
>     J --> K[Fim];
> ```





![Average performance comparison of UCB and Œµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

### Conclus√£o
O par√¢metro de explora√ß√£o *c* no algoritmo UCB atua como um regulador da confian√ßa nos limites superiores dos valores das a√ß√µes [^11]. Ajustar *c* permite um controle granular sobre a intensidade da explora√ß√£o versus a explota√ß√£o [^11]. Valores mais altos de *c* incentivam a explora√ß√£o, enquanto valores mais baixos favorecem a explota√ß√£o [^11]. A escolha ideal de *c* depende das caracter√≠sticas espec√≠ficas do problema *k-armed bandit*, incluindo a estacionaridade do ambiente e a toler√¢ncia ao risco [^11]. Em ambientes n√£o estacion√°rios, um valor de *c* maior pode ser prefer√≠vel para garantir que o algoritmo se adapte √†s mudan√ßas nas recompensas ao longo do tempo [^11].
<!-- END -->