## Upper-Confidence-Bound Action Selection: Balancing Exploration and Exploitation

### Introdu√ß√£o

No contexto do problema *k*-armed bandit, a busca por um equil√≠brio entre **explora√ß√£o** (descobrir a√ß√µes com potencial desconhecido) e **explota√ß√£o** (maximizar a recompensa imediata com base no conhecimento atual) √© fundamental. Vimos anteriormente m√©todos como $\epsilon$-greedy, que exploram aleatoriamente com uma certa probabilidade, e m√©todos *greedy*, que exploram exclusivamente a a√ß√£o com maior valor estimado [^27]. No entanto, esses m√©todos podem ser considerados sub√≥timos, pois n√£o levam em considera√ß√£o a **incerteza** associada √†s estimativas de valor das a√ß√µes. O m√©todo *Upper-Confidence-Bound* (UCB) surge como uma alternativa mais sofisticada, equilibrando a explora√ß√£o e explota√ß√£o de maneira inteligente, utilizando um termo de **incerteza** para encorajar a explora√ß√£o de a√ß√µes menos visitadas [^35].

> üí° **Exemplo Num√©rico:** Imagine um *k*-armed bandit com 3 bra√ßos (k=3). Inicialmente, n√£o sabemos nada sobre os bra√ßos, ent√£o atribu√≠mos valores iniciais $Q_1(a) = 0$ para todos os bra√ßos $a \in \{1, 2, 3\}$ e $N_1(a) = 0$ para todos os bra√ßos. Usando $\epsilon$-greedy com $\epsilon = 0.1$, ter√≠amos uma chance de 10% de explorar aleatoriamente. Um m√©todo *greedy* sempre escolheria o bra√ßo com valor estimado mais alto, que, nesse caso inicial, seria arbitr√°rio, j√° que todos s√£o zero. O UCB, por outro lado, calcularia o valor UCB para cada bra√ßo e escolheria o bra√ßo com o maior valor UCB. Este exemplo demonstra como o UCB pode se comportar diferentemente desde o in√≠cio em rela√ß√£o a abordagens mais simples.

Al√©m disso, √© importante notar que diferentes varia√ß√µes do problema *k*-armed bandit podem influenciar a escolha do algoritmo mais adequado. Por exemplo, em ambientes n√£o-estacion√°rios, onde as recompensas das a√ß√µes mudam ao longo do tempo, adapta√ß√µes do UCB, ou mesmo outros algoritmos como *Thompson Sampling*, podem apresentar um desempenho superior.

### Conceitos Fundamentais

O m√©todo UCB seleciona a√ß√µes com base na seguinte f√≥rmula [^35]:

$$
A_t = \underset{a}{\operatorname{argmax}} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right] \qquad (2.10)
$$

Onde:

*   $A_t$ √© a a√ß√£o selecionada no tempo *t*.
*   $Q_t(a)$ √© a estimativa do valor da a√ß√£o *a* no tempo *t*.
*   *c* > 0 √© um par√¢metro que controla o grau de explora√ß√£o.
*   $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada antes do tempo *t*.
*   $\ln t$ denota o logaritmo natural de *t*.

A f√≥rmula UCB combina a estimativa de valor $Q_t(a)$ com um termo de incerteza $c \sqrt{\frac{\ln t}{N_t(a)}}$. O termo de incerteza desempenha um papel crucial no equil√≠brio entre explora√ß√£o e explota√ß√£o [^35].

> üí° **Exemplo Num√©rico:** Suponha que estamos no tempo *t* = 10, temos tr√™s a√ß√µes, e as seguintes informa√ß√µes:
> *   A√ß√£o 1: $Q_{10}(1) = 0.5$, $N_{10}(1) = 1$
> *   A√ß√£o 2: $Q_{10}(2) = 0.6$, $N_{10}(2) = 5$
> *   A√ß√£o 3: $Q_{10}(3) = 0.4$, $N_{10}(3) = 2$
>
> Seja *c* = 1. Calculemos o valor UCB para cada a√ß√£o:
>
> *   A√ß√£o 1: $UCB_{10}(1) = 0.5 + 1 \cdot \sqrt{\frac{\ln 10}{1}} = 0.5 + \sqrt{2.30} \approx 0.5 + 1.52 = 2.02$
> *   A√ß√£o 2: $UCB_{10}(2) = 0.6 + 1 \cdot \sqrt{\frac{\ln 10}{5}} = 0.6 + \sqrt{0.46} \approx 0.6 + 0.68 = 1.28$
> *   A√ß√£o 3: $UCB_{10}(3) = 0.4 + 1 \cdot \sqrt{\frac{\ln 10}{2}} = 0.4 + \sqrt{1.15} \approx 0.4 + 1.07 = 1.47$
>
> Neste caso, a A√ß√£o 1 √© selecionada, apesar de ter uma estimativa de valor $Q_{10}(1)$ menor que a A√ß√£o 2, pois sua incerteza √© maior devido ao fato de ter sido selecionada menos vezes.

**Explora√ß√£o Encorajada por A√ß√µes Menos Visitadas:** O termo $\sqrt{\frac{\ln t}{N_t(a)}}$ diminui √† medida que $N_t(a)$ aumenta. Isso significa que a√ß√µes que foram selecionadas com menos frequ√™ncia (ou seja, t√™m um $N_t(a)$ pequeno) ter√£o um termo de incerteza maior [^35]. Este termo de incerteza inflaciona o valor da a√ß√£o, tornando-a mais propensa a ser selecionada, mesmo que sua estimativa de valor $Q_t(a)$ seja relativamente baixa. Isso incentiva a explora√ß√£o de a√ß√µes menos visitadas e que, portanto, s√£o mais incertas.

**Incerteza Decrescente com Sele√ß√µes Frequentes:** √Ä medida que uma a√ß√£o √© selecionada mais frequentemente, $N_t(a)$ aumenta e o termo de incerteza $\sqrt{\frac{\ln t}{N_t(a)}}$ diminui. Isso significa que a incerteza associada a essa a√ß√£o diminui √† medida que aprendemos mais sobre ela. Consequentemente, a influ√™ncia do termo de incerteza na sele√ß√£o da a√ß√£o diminui, permitindo que o algoritmo se concentre mais na explota√ß√£o da estimativa de valor $Q_t(a)$ [^35].

> üí° **Exemplo Num√©rico:** Continuemos o exemplo anterior. Suponha que a A√ß√£o 1 foi selecionada e rendeu uma recompensa de 0.7. Atualizamos ent√£o:
>
> *   $N_{11}(1) = N_{10}(1) + 1 = 2$
> *   $Q_{11}(1) = Q_{10}(1) + \frac{1}{N_{11}(1)}(0.7 - Q_{10}(1)) = 0.5 + \frac{1}{2}(0.7 - 0.5) = 0.5 + 0.1 = 0.6$.
>
> Agora, recalculamos o UCB no tempo *t*=11:
>
> *   A√ß√£o 1: $UCB_{11}(1) = 0.6 + 1 \cdot \sqrt{\frac{\ln 11}{2}} = 0.6 + \sqrt{\frac{2.40}{2}} \approx 0.6 + 1.10 = 1.70$
> *   A√ß√£o 2: $UCB_{11}(2) = 0.6 + 1 \cdot \sqrt{\frac{\ln 11}{5}} = 0.6 + \sqrt{\frac{2.40}{5}} \approx 0.6 + 0.69 = 1.29$
> *   A√ß√£o 3: $UCB_{11}(3) = 0.4 + 1 \cdot \sqrt{\frac{\ln 11}{2}} = 0.4 + \sqrt{\frac{2.40}{2}} \approx 0.4 + 1.10 = 1.50$
>
> Observe que o valor UCB da A√ß√£o 1 diminuiu um pouco em rela√ß√£o ao tempo *t*=10, pois a incerteza diminuiu devido ao aumento de $N_t(a)$. A a√ß√£o 3 pode ser escolhida, dependendo da recompensa obtida pelas outras a√ß√µes. Isso demonstra como o UCB ajusta dinamicamente a explora√ß√£o e explota√ß√£o.

**O par√¢metro 'c':** O par√¢metro *c* controla a for√ßa do termo de incerteza. Um valor alto de *c* incentiva mais a explora√ß√£o, enquanto um valor baixo de *c* incentiva mais a explota√ß√£o. A escolha apropriada de *c* √© crucial para um bom desempenho do algoritmo UCB. Um *parameter study* [^42] pode ajudar a encontrar o valor ideal de *c*.

> üí° **Exemplo Num√©rico:** Considere o mesmo cen√°rio anterior no tempo t=10, mas agora variamos o valor de *c*.
>
> Se *c*=0.1 (menor explora√ß√£o):
> *   A√ß√£o 1: $UCB_{10}(1) = 0.5 + 0.1 \cdot \sqrt{\frac{\ln 10}{1}} \approx 0.5 + 0.15 = 0.65$
> *   A√ß√£o 2: $UCB_{10}(2) = 0.6 + 0.1 \cdot \sqrt{\frac{\ln 10}{5}} \approx 0.6 + 0.07 = 0.67$
> *   A√ß√£o 3: $UCB_{10}(3) = 0.4 + 0.1 \cdot \sqrt{\frac{\ln 10}{2}} \approx 0.4 + 0.11 = 0.51$
>
> A A√ß√£o 2 seria escolhida, pois estamos explorando menos.
>
> Se *c*=10 (maior explora√ß√£o):
> *   A√ß√£o 1: $UCB_{10}(1) = 0.5 + 10 \cdot \sqrt{\frac{\ln 10}{1}} \approx 0.5 + 15.2 = 15.7$
> *   A√ß√£o 2: $UCB_{10}(2) = 0.6 + 10 \cdot \sqrt{\frac{\ln 10}{5}} \approx 0.6 + 6.8 = 7.4$
> *   A√ß√£o 3: $UCB_{10}(3) = 0.4 + 10 \cdot \sqrt{\frac{\ln 10}{2}} \approx 0.4 + 10.7 = 11.1$
>
> A A√ß√£o 1 seria escolhida, devido √† alta explora√ß√£o.
>
> Este exemplo demonstra o efeito dr√°stico que o par√¢metro *c* tem sobre a sele√ß√£o das a√ß√µes.

**Caso Especial: N‚Çú(a) = 0:** Se $N_t(a) = 0$, a a√ß√£o *a* nunca foi selecionada antes. Para evitar a divis√£o por zero, a a√ß√£o *a* √© considerada uma a√ß√£o maximizadora e √© prefer√≠vel a a√ß√µes j√° visitadas [^36]. Isso garante que todas as a√ß√µes sejam exploradas pelo menos uma vez.

> üí° **Exemplo Num√©rico:** No in√≠cio da execu√ß√£o do algoritmo, $N_t(a)$ ser√° 0 para todas as a√ß√µes. Neste caso, o algoritmo UCB ir√° selecionar cada a√ß√£o pelo menos uma vez para garantir que todas as a√ß√µes sejam exploradas inicialmente. Isso √© crucial para obter estimativas iniciais de $Q_t(a)$ e $N_t(a)$ para cada a√ß√£o.

O logaritmo natural $\ln t$ no termo de incerteza garante que o aumento da incerteza diminua com o tempo. Isso significa que o algoritmo explorar√° mais no in√≠cio e, gradualmente, passar√° a se concentrar mais na explora√ß√£o √† medida que o tempo passa [^36].

Para complementar a discuss√£o sobre o par√¢metro *c*, podemos analisar seu impacto na taxa de converg√™ncia do algoritmo.

**Teorema 1:** *Sob certas condi√ß√µes de regularidade nas recompensas das a√ß√µes, existe um valor √≥timo $c^*$ para o par√¢metro *c* que minimiza o arrependimento cumulativo do algoritmo UCB ao longo do tempo.*

*Prova (Esbo√ßo):* A prova desse teorema geralmente envolve a an√°lise do limite superior do arrependimento cumulativo. O arrependimento √© definido como a diferen√ßa entre a recompensa obtida pela a√ß√£o √≥tima e a recompensa obtida pela a√ß√£o selecionada pelo algoritmo. Ajustando o par√¢metro *c*, podemos controlar a taxa de explora√ß√£o e explota√ß√£o. Um valor muito alto de *c* levar√° a uma explora√ß√£o excessiva, resultando em baixo desempenho inicial. Um valor muito baixo de *c* levar√° a uma explota√ß√£o prematura, possivelmente convergindo para uma a√ß√£o sub√≥tima. O valor √≥timo $c^*$ equilibra esses dois efeitos, minimizando o arrependimento. A prova formal envolve o uso de desigualdades de concentra√ß√£o e a otimiza√ß√£o do limite superior do arrependimento em rela√ß√£o a *c*.

**Prova do fato de que o termo $\ln t$ garante que o aumento da incerteza diminua com o tempo:**

I.  Queremos demonstrar que a taxa de varia√ß√£o do termo de incerteza $c \sqrt{\frac{\ln t}{N_t(a)}}$ diminui com o tempo $t$. Para isso, analisaremos a derivada desse termo em rela√ß√£o a $t$.

II. Seja $f(t) = c \sqrt{\frac{\ln t}{N_t(a)}}$. Como *c* e $N_t(a)$ s√£o constantes em rela√ß√£o a *t*, podemos simplificar a an√°lise focando em $\sqrt{\ln t}$.  Seja $g(t) = \sqrt{\ln t} = (\ln t)^{1/2}$.

III. Calcule a derivada de $g(t)$ em rela√ß√£o a $t$:

    $$\frac{dg(t)}{dt} = \frac{1}{2} (\ln t)^{-1/2} \cdot \frac{1}{t} = \frac{1}{2t\sqrt{\ln t}}$$

IV. Agora, vamos analisar a segunda derivada para entender como a taxa de varia√ß√£o muda com o tempo:

    $$\frac{d^2g(t)}{dt^2} = \frac{d}{dt} \left(\frac{1}{2t\sqrt{\ln t}}\right)$$
    Usando a regra do quociente:

    $$\frac{d^2g(t)}{dt^2} = \frac{0 - 1 \cdot \left(2\sqrt{\ln t} + 2t \cdot \frac{1}{2} (\ln t)^{-1/2} \cdot \frac{1}{t}\right)}{(2t\sqrt{\ln t})^2}$$

    $$\frac{d^2g(t)}{dt^2} = \frac{-2\sqrt{\ln t} - (\ln t)^{-1/2}}{4t^2 \ln t} = \frac{-2\ln t - 1}{4t^2 (\ln t)^{3/2}}$$

V. Observe que para $t > 1$, $\ln t > 0$. Portanto, o numerador $-2\ln t - 1$ √© sempre negativo. O denominador $4t^2 (\ln t)^{3/2}$ √© sempre positivo. Portanto, $\frac{d^2g(t)}{dt^2} < 0$ para todo $t > 1$.

VI. Como a segunda derivada √© negativa, isso significa que a primeira derivada $\frac{dg(t)}{dt} = \frac{1}{2t\sqrt{\ln t}}$ est√° diminuindo com o tempo.  Portanto, a taxa de aumento do termo de incerteza $c \sqrt{\frac{\ln t}{N_t(a)}}$ diminui com o tempo. ‚ñ†

Al√©m disso, podemos considerar uma variante do UCB que utiliza uma estimativa da vari√¢ncia das recompensas para refinar o termo de incerteza.

**Teorema 2:** *Uma variante do UCB, chamada UCB com Vari√¢ncia (UCBV), utiliza uma estimativa da vari√¢ncia das recompensas para modular o termo de incerteza, resultando em um melhor desempenho em problemas com recompensas com alta vari√¢ncia.*

A f√≥rmula para o UCBV √© dada por:

$$
A_t = \underset{a}{\operatorname{argmax}} \left[ Q_t(a) + c \sqrt{\frac{V_t(a)}{N_t(a)}} \ln t \right]
$$

Onde $V_t(a)$ √© uma estimativa da vari√¢ncia da recompensa da a√ß√£o *a* no tempo *t*.

> üí° **Exemplo Num√©rico:** Suponha que tenhamos duas a√ß√µes. Ap√≥s algumas rodadas, as estat√≠sticas s√£o as seguintes:
>
> *   A√ß√£o 1: $Q_t(1) = 0.6$, $N_t(1) = 10$, $V_t(1) = 0.1$
> *   A√ß√£o 2: $Q_t(2) = 0.5$, $N_t(2) = 10$, $V_t(2) = 0.5$
>
> Usando UCB cl√°ssico com c=1 e t=100:
> *   A√ß√£o 1: $UCB(1) = 0.6 + \sqrt{\frac{\ln 100}{10}} = 0.6 + \sqrt{\frac{4.6}{10}} \approx 0.6 + 0.68 = 1.28$
> *   A√ß√£o 2: $UCB(2) = 0.5 + \sqrt{\frac{\ln 100}{10}} = 0.5 + \sqrt{\frac{4.6}{10}} \approx 0.5 + 0.68 = 1.18$
>
> A√ß√£o 1 seria escolhida.
>
> Usando UCBV com c=1 e t=100:
> *   A√ß√£o 1: $UCBV(1) = 0.6 + \sqrt{\frac{0.1}{10}} \ln 100 = 0.6 + \sqrt{0.01} \cdot 4.6 \approx 0.6 + 0.1 \cdot 4.6 = 0.6 + 0.46 = 1.06$
> *   A√ß√£o 2: $UCBV(2) = 0.5 + \sqrt{\frac{0.5}{10}} \ln 100 = 0.5 + \sqrt{0.05} \cdot 4.6 \approx 0.5 + 0.22 \cdot 4.6 = 0.5 + 1.01 = 1.51$
>
> A√ß√£o 2 seria escolhida pelo UCBV, pois considera a maior vari√¢ncia da recompensa. Este exemplo ilustra como UCBV pode tomar decis√µes diferentes com base na vari√¢ncia das recompensas.

*Prova (Esbo√ßo):* A prova de que o UCBV apresenta um melhor desempenho em problemas com recompensas de alta vari√¢ncia baseia-se no fato de que o termo de incerteza √© agora ponderado pela vari√¢ncia. Se uma a√ß√£o tem uma alta vari√¢ncia, o termo de incerteza ser√° maior, incentivando o algoritmo a explorar essa a√ß√£o mais extensivamente. Isso permite que o algoritmo obtenha uma estimativa mais precisa do valor da a√ß√£o, levando a um melhor desempenho a longo prazo.

A figura abaixo compara o desempenho do UCB e do Œµ-greedy em um ambiente de teste de 10 bra√ßos:

![Average performance comparison of UCB and Œµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

### Conclus√£o

O m√©todo UCB oferece uma abordagem elegante e eficaz para equilibrar a explora√ß√£o e a explota√ß√£o no problema *k*-armed bandit. Ao incorporar um termo de incerteza que diminui com o aumento da frequ√™ncia de sele√ß√£o de uma a√ß√£o, o UCB incentiva a explora√ß√£o de a√ß√µes menos visitadas e com estimativas mais incertas [^35]. Esse mecanismo permite que o algoritmo descubra a√ß√µes potencialmente √≥timas que poderiam ser negligenciadas por abordagens mais *greedy*. Embora existam m√©todos mais sofisticados, a simplicidade e o bom desempenho do UCB o tornam uma ferramenta valiosa no *toolbox* de aprendizado por refor√ßo. [^43]

### Refer√™ncias
[^35]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^36]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^27]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^42]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^43]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
<!-- END -->