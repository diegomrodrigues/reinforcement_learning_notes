## Upper-Confidence-Bound (UCB) Action Selection: Targeted Exploration Under Uncertainty

### Introdu√ß√£o
No contexto do problema do *$k$-armed bandit*, a necessidade de equilibrar **explora√ß√£o** e **explota√ß√£o** se torna crucial. Como vimos anteriormente, m√©todos *greedy* exploram apenas a a√ß√£o com maior valor estimado atual, enquanto os m√©todos *Œµ-greedy* introduzem uma explora√ß√£o aleat√≥ria [^27]. No entanto, a explora√ß√£o aleat√≥ria dos m√©todos *Œµ-greedy* pode ser ineficiente, pois n√£o prioriza a√ß√µes que s√£o quase *greedy* ou que possuem alta incerteza em suas estimativas de valor. Esta se√ß√£o explora o m√©todo **Upper-Confidence-Bound (UCB)**, que oferece uma abordagem mais direcionada para a explora√ß√£o, considerando explicitamente a incerteza nas estimativas de valor das a√ß√µes [^27].

### Conceitos Fundamentais
A ess√™ncia do **UCB** reside na premissa de que a explora√ß√£o √© essencial devido √† incerteza inerente nas estimativas de valor das a√ß√µes. Enquanto as a√ß√µes *greedy* representam as melhores op√ß√µes com base no conhecimento atual, outras a√ß√µes podem ser potencialmente melhores se explorarmos suas estimativas com maior incerteza [^27].

Ao contr√°rio da explora√ß√£o indiscriminada imposta pelos m√©todos *Œµ-greedy*, o **UCB** busca selecionar a√ß√µes n√£o *greedy* de forma mais inteligente, ponderando seu potencial para serem √≥timas. Isso √© feito levando em considera√ß√£o tanto a proximidade de suas estimativas em rela√ß√£o ao valor m√°ximo, quanto o grau de incerteza nessas estimativas [^27].

A f√≥rmula para a sele√ß√£o de a√ß√µes no **UCB** √© dada por [^27]:
$$
A_t = \argmax_a \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
$$
Onde:
*   $A_t$ representa a a√ß√£o selecionada no tempo $t$.
*   $Q_t(a)$ √© a estimativa do valor da a√ß√£o $a$ no tempo $t$.
*   $c > 0$ √© um par√¢metro que controla o grau de explora√ß√£o.
*   $\ln t$ denota o logaritmo natural de $t$.
*   $N_t(a)$ representa o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada antes do tempo $t$.

> üí° **Exemplo Num√©rico:**
>
> Considere um problema de *$k$-armed bandit* com 3 a√ß√µes. No tempo $t=10$, temos as seguintes informa√ß√µes:
>
> *   $Q_{10}(1) = 0.5$, $N_{10}(1) = 5$
> *   $Q_{10}(2) = 0.7$, $N_{10}(2) = 2$
> *   $Q_{10}(3) = 0.4$, $N_{10}(3) = 3$
>
> Suponha que $c = 2$. Vamos calcular o UCB para cada a√ß√£o:
>
> *   $\text{UCB}(1) = 0.5 + 2 \sqrt{\frac{\ln 10}{5}} \approx 0.5 + 2 \sqrt{\frac{2.3}{5}} \approx 0.5 + 2 \times 0.678 \approx 1.856$
> *   $\text{UCB}(2) = 0.7 + 2 \sqrt{\frac{\ln 10}{2}} \approx 0.7 + 2 \sqrt{\frac{2.3}{2}} \approx 0.7 + 2 \times 1.077 \approx 2.854$
> *   $\text{UCB}(3) = 0.4 + 2 \sqrt{\frac{\ln 10}{3}} \approx 0.4 + 2 \sqrt{\frac{2.3}{3}} \approx 0.4 + 2 \times 0.876 \approx 2.152$
>
> Neste caso, a a√ß√£o 2 seria selecionada ($A_{10} = 2$), pois possui o maior valor de UCB.  Note que, embora a estimativa de valor $Q_{10}(2)$ seja maior que $Q_{10}(1)$ e $Q_{10}(3)$, o n√∫mero de vezes que a a√ß√£o 2 foi selecionada √© menor, resultando em uma maior incerteza e, portanto, um maior "bonus" de explora√ß√£o.

A l√≥gica por tr√°s desta f√≥rmula √© a seguinte: o termo $Q_t(a)$ representa a **explota√ß√£o**, favorecendo a√ß√µes com altas estimativas de valor. O termo $c \sqrt{\frac{\ln t}{N_t(a)}}$ representa a **explora√ß√£o**, adicionando um *bonus* √†quelas a√ß√µes que possuem maior incerteza em suas estimativas de valor [^27]. A incerteza √© medida pela raiz quadrada do logaritmo natural do tempo atual dividido pelo n√∫mero de vezes que a a√ß√£o foi selecionada.

*Se $N_t(a) = 0$, a a√ß√£o $a$ √© considerada uma a√ß√£o maximizadora*[^36]. Isso assegura que todas as a√ß√µes sejam exploradas pelo menos uma vez.

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, se no tempo $t=1$, nenhuma a√ß√£o foi selecionada ($N_1(1) = N_1(2) = N_1(3) = 0$), todas as a√ß√µes seriam consideradas maximizadoras. O algoritmo normalmente inicializaria a sele√ß√£o de cada a√ß√£o uma vez para evitar a divis√£o por zero.

O par√¢metro $c$ controla a import√¢ncia relativa da explora√ß√£o. Um valor alto de $c$ incentiva mais explora√ß√£o, enquanto um valor baixo favorece a explota√ß√£o [^36].

> üí° **Exemplo Num√©rico:**
>
> Considere novamente o cen√°rio anterior no tempo $t=10$, mas agora compare com um valor de $c=0.5$.
>
> *   $\text{UCB}(1) = 0.5 + 0.5 \sqrt{\frac{\ln 10}{5}} \approx 0.5 + 0.5 \times 0.678 \approx 0.839$
> *   $\text{UCB}(2) = 0.7 + 0.5 \sqrt{\frac{\ln 10}{2}} \approx 0.7 + 0.5 \times 1.077 \approx 1.239$
> *   $\text{UCB}(3) = 0.4 + 0.5 \sqrt{\frac{\ln 10}{3}} \approx 0.4 + 0.5 \times 0.876 \approx 0.838$
>
> Com $c=0.5$, a a√ß√£o 2 ainda seria selecionada, mas a diferen√ßa entre os valores UCB √© menor, indicando uma menor tend√™ncia √† explora√ß√£o. Se tiv√©ssemos $c=0$, o algoritmo se comportaria como um algoritmo *greedy*, sempre selecionando a a√ß√£o com a maior estimativa de valor (neste caso, a a√ß√£o 2).

O termo $\sqrt{\frac{\ln t}{N_t(a)}}$ serve como uma medida da incerteza ou vari√¢ncia na estimativa do valor de $a$. Assim, a quantidade sendo maximizada pode ser vista como um limite superior no poss√≠vel valor verdadeiro da a√ß√£o $a$, com $c$ controlando o n√≠vel de confian√ßa [^36].

A cada vez que uma a√ß√£o $a$ √© selecionada, a incerteza presumivelmente se reduz: $N_t(a)$ incrementa e, como aparece no denominador, o termo de incerteza decresce. Por outro lado, a cada vez que uma a√ß√£o diferente de $a$ √© selecionada, $t$ aumenta, mas $N_t(a)$ n√£o; como $t$ aparece no numerador, a estimativa de incerteza aumenta [^36].

O uso do logaritmo natural significa que os incrementos se tornam menores com o tempo, mas s√£o ilimitados: todas as a√ß√µes ser√£o eventualmente selecionadas, mas a√ß√µes com estimativas de valor mais baixas, ou que j√° foram selecionadas frequentemente, ser√£o selecionadas com frequ√™ncia decrescente ao longo do tempo [^36].

Para complementar a compreens√£o do par√¢metro $c$, podemos analisar seu efeito no comportamento assint√≥tico do algoritmo UCB.

**Teorema 1** Assumindo que as recompensas s√£o limitadas no intervalo $[0, 1]$, o arrependimento cumulativo do algoritmo UCB com par√¢metro $c$ √© limitado superiormente por $O(\sqrt{K t \ln t})$, onde $K$ √© o n√∫mero de bra√ßos no problema do *$k$-armed bandit* e $t$ √© o n√∫mero de passos de tempo.

*Prova.* A prova deste teorema envolve a decomposi√ß√£o do arrependimento em termos do n√∫mero de vezes que as a√ß√µes sub√≥timas s√£o selecionadas. O termo $\sqrt{\frac{\ln t}{N_t(a)}}$ garante que as a√ß√µes sub√≥timas sejam selecionadas com frequ√™ncia decrescente ao longo do tempo. A escolha apropriada de $c$ otimiza o limite superior do arrependimento. Detalhes completos podem ser encontrados em [^37].

Para ilustrar o efeito do termo $\sqrt{\frac{\ln t}{N_t(a)}}$ no limite de arrependimento, podemos apresentar uma prova simplificada do limite superior do arrependimento.

*Prova.*

Seja $a^*$ a a√ß√£o √≥tima, e $Q(a)$ o valor verdadeiro da a√ß√£o $a$. Definimos o arrependimento no tempo $t$ como $\Delta_t = Q(a^*) - Q_t(A_t)$. O arrependimento cumulativo at√© o tempo $T$ √© ent√£o $\sum_{t=1}^{T} \Delta_t$.

I. Seja $A_t$ uma a√ß√£o sub√≥tima selecionada no tempo $t$, ou seja, $Q(A_t) < Q(a^*)$.

II. Pela defini√ß√£o do UCB, temos que:
$$Q_t(A_t) + c\sqrt{\frac{\ln t}{N_t(A_t)}} \geq Q_t(a^*) + c\sqrt{\frac{\ln t}{N_t(a^*)}}$$

III. Rearranjando os termos, obtemos:
$$Q_t(A_t) - Q_t(a^*) \geq c\sqrt{\frac{\ln t}{N_t(a^*)}} - c\sqrt{\frac{\ln t}{N_t(A_t)}}$$

IV. Como $Q(A_t) < Q(a^*)$, ent√£o $Q_t(A_t)$ deve ser superestimado ou $Q_t(a^*)$ subestimado. Para simplificar, vamos considerar que a a√ß√£o sub√≥tima $A_t$ √© selecionada porque sua estimativa $Q_t(A_t)$ √© superestimada. Isso significa que:
$$Q_t(A_t) > Q(A_t)$$
E, portanto,
$$Q(a^*) - Q(A_t) \leq Q_t(a^*) + c\sqrt{\frac{\ln t}{N_t(a^*)}} - \left(Q_t(A_t) - c\sqrt{\frac{\ln t}{N_t(A_t)}}\right)$$

V. Definimos $\Delta_a = Q(a^*) - Q(a)$, que representa a diferen√ßa entre o valor da a√ß√£o √≥tima e a a√ß√£o $a$. Assim, podemos escrever:
$$\Delta_{A_t} \leq c\sqrt{\frac{\ln t}{N_t(a^*)}} + c\sqrt{\frac{\ln t}{N_t(A_t)}}$$

VI. Assumindo que $N_t(a^*) > 0$ e $N_t(A_t) > 0$, podemos limitar o n√∫mero de vezes que uma a√ß√£o sub√≥tima $a$ √© selecionada:
$$N_t(a) \leq \frac{4c^2 \ln t}{\Delta_a^2}$$

VII. O arrependimento total √© limitado pelo n√∫mero de vezes que as a√ß√µes sub√≥timas s√£o selecionadas. Portanto, o arrependimento cumulativo at√© o tempo $T$ √©:
$$\sum_{t=1}^{T} \Delta_t \leq \sum_{a: \Delta_a > 0} \frac{4c^2 \ln T}{\Delta_a} + \text{termos constantes}$$

VIII. Simplificando, o arrependimento cumulativo √© $O(\ln T)$. No entanto, uma an√°lise mais rigorosa, como a encontrada em [^37], mostra que o arrependimento √© $O(\sqrt{K T \ln T})$ quando consideramos $K$ bra√ßos. ‚ñ†

Al√©m disso, podemos definir uma varia√ß√£o do UCB que leva em considera√ß√£o a vari√¢ncia das recompensas observadas.

**Teorema 2** (UCB com Vari√¢ncia Estimada) A a√ß√£o selecionada no tempo $t$ pode ser dada por:

$$
A_t = \argmax_a \left[ Q_t(a) + c \sqrt{\frac{V_t(a) \ln t}{N_t(a)}} \right]
$$

Onde $V_t(a)$ √© uma estimativa da vari√¢ncia das recompensas obtidas ao selecionar a a√ß√£o $a$ at√© o tempo $t$.

> üí° **Exemplo Num√©rico:**
>
> Imagine que temos duas a√ß√µes. Ap√≥s 10 intera√ß√µes ($t=10$), temos os seguintes dados:
>
> *   A√ß√£o 1: $Q_{10}(1) = 0.6$, $N_{10}(1) = 5$, Recompensas = $[0.5, 0.7, 0.6, 0.4, 0.8]$
> *   A√ß√£o 2: $Q_{10}(2) = 0.5$, $N_{10}(2) = 5$, Recompensas = $[0.1, 0.9, 0.2, 0.8, 0.5]$
>
> Vamos calcular as vari√¢ncias amostrais:
>
> *   $V_{10}(1) \approx 0.025$ (baixa variabilidade)
> *   $V_{10}(2) \approx 0.125$ (alta variabilidade)
>
> Usando $c = 1$, os UCBs seriam:
>
> *   $\text{UCB}(1) = 0.6 + 1 \sqrt{\frac{0.025 \times \ln 10}{5}} \approx 0.6 + 1 \sqrt{\frac{0.025 \times 2.3}{5}} \approx 0.6 + 0.107 \approx 0.707$
> *   $\text{UCB}(2) = 0.5 + 1 \sqrt{\frac{0.125 \times \ln 10}{5}} \approx 0.5 + 1 \sqrt{\frac{0.125 \times 2.3}{5}} \approx 0.5 + 0.240 \approx 0.740$
>
> Neste caso, a A√ß√£o 2 seria escolhida, mesmo tendo uma estimativa de valor menor ($Q_{10}(2) = 0.5 < Q_{10}(1) = 0.6$), por causa da alta variabilidade em suas recompensas. A UCB com vari√¢ncia estimada prioriza a explora√ß√£o de a√ß√µes cujas recompensas s√£o incertas devido √† sua alta variabilidade.

*Prova.* Este resultado surge ao considerar que a incerteza na estimativa de valor de uma a√ß√£o n√£o depende apenas do n√∫mero de vezes que ela foi selecionada, mas tamb√©m da variabilidade das recompensas observadas. Estimar a vari√¢ncia permite que o algoritmo UCB explore de forma mais eficiente, dando mais peso √† explora√ß√£o de a√ß√µes com alta variabilidade. A estimativa da vari√¢ncia pode ser feita de forma incremental, como a vari√¢ncia amostral.

Para formalizar, considere o seguinte:

I. Seja $Q_t(a)$ a estimativa do valor da a√ß√£o $a$ no tempo $t$, e $V_t(a)$ a estimativa da vari√¢ncia da a√ß√£o $a$ no tempo $t$.

II. A a√ß√£o selecionada $A_t$ √© aquela que maximiza o limite superior de confian√ßa, dado por:
$$A_t = \argmax_a \left[ Q_t(a) + c \sqrt{\frac{V_t(a) \ln t}{N_t(a)}} \right]$$

III. O termo $c \sqrt{\frac{V_t(a) \ln t}{N_t(a)}}$ representa a incerteza na estimativa do valor da a√ß√£o $a$, ponderada pela sua vari√¢ncia $V_t(a)$.

IV. Se a vari√¢ncia $V_t(a)$ √© alta, isso indica que as recompensas obtidas ao selecionar a a√ß√£o $a$ s√£o muito vari√°veis, o que significa que a estimativa $Q_t(a)$ √© menos confi√°vel.

V. Portanto, o algoritmo UCB com vari√¢ncia estimada explora a√ß√µes com alta vari√¢ncia, pois a incerteza em suas estimativas de valor √© maior.

VI. Consequentemente, ao maximizar o limite superior de confian√ßa, o algoritmo equilibra a explota√ß√£o (favorecendo a√ß√µes com altas estimativas de valor) e a explora√ß√£o (favorecendo a√ß√µes com alta vari√¢ncia e, portanto, alta incerteza). ‚ñ†

A inclus√£o da vari√¢ncia estimada oferece uma adapta√ß√£o mais refinada do UCB, especialmente em cen√°rios onde as recompensas das a√ß√µes exibem diferentes n√≠veis de variabilidade.

### Conclus√£o

O m√©todo **UCB** oferece uma abordagem mais sofisticada para o problema de *exploration-exploitation* no contexto do problema *$k$-armed bandit* [^36]. Ao contr√°rio dos m√©todos *Œµ-greedy*, que exploram de forma indiscriminada, o **UCB** direciona a explora√ß√£o para a√ß√µes com maior potencial de serem √≥timas, equilibrando a necessidade de adquirir conhecimento com a maximiza√ß√£o da recompensa [^36].  Embora eficaz em muitos cen√°rios, o **UCB** apresenta desafios em ambientes *nonstationary* e em problemas de *reinforcement learning* mais complexos [^36].

![Average performance comparison of UCB and Œµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

<!-- END -->