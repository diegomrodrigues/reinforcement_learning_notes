## Upper-Confidence-Bound (UCB) Action Selection: Balancing Exploration and Exploitation

### Introdu√ß√£o
No contexto do problema *$k$*-armed bandit, o desafio fundamental reside em equilibrar a **explora√ß√£o** (experimentar a√ß√µes para refinar as estimativas de seus valores) e a **explota√ß√£o** (selecionar a a√ß√£o com a maior estimativa de valor no momento) [^2]. M√©todos *$\epsilon$-greedy* for√ßam a explora√ß√£o selecionando a√ß√µes n√£o-gananciosas de forma indiscriminada, sem prefer√™ncia por aquelas que s√£o quase gananciosas ou particularmente incertas [^11]. Uma abordagem mais eficaz √© selecionar a√ß√µes n√£o-gananciosas com base em seu potencial para realmente serem √≥timas, levando em conta tanto a proximidade de suas estimativas ao valor m√°ximo quanto as incertezas nessas estimativas [^11].

### Conceitos Fundamentais

O **Upper-Confidence-Bound (UCB)** √© um m√©todo que implementa essa ideia selecionando a√ß√µes de acordo com a seguinte f√≥rmula [^11]:

$$
A_t = \underset{a}{\text{argmax}} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right] \quad (2.10)
$$

onde [^11]:

*   $A_t$ √© a a√ß√£o selecionada no tempo $t$
*   $Q_t(a)$ √© a estimativa do valor da a√ß√£o $a$ no tempo $t$
*   $c > 0$ controla o grau de explora√ß√£o
*   $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada antes do tempo $t$
*   $\ln t$ √© o logaritmo natural de $t$

A intui√ß√£o por tr√°s do UCB √© que o termo de raiz quadrada √© uma medida da **incerteza** ou **vari√¢ncia** na estimativa do valor de $a$ [^12]. A quantidade sendo maximizada √©, portanto, uma esp√©cie de *limite superior* no poss√≠vel valor verdadeiro da a√ß√£o $a$, com $c$ determinando o n√≠vel de confian√ßa [^12].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos 3 a√ß√µes (A, B, C) e estamos no tempo $t=10$. Os valores estimados e o n√∫mero de vezes que cada a√ß√£o foi selecionada s√£o:
>
> *   $Q_{10}(A) = 2.0$, $N_{10}(A) = 5$
> *   $Q_{10}(B) = 3.0$, $N_{10}(B) = 2$
> *   $Q_{10}(C) = 2.5$, $N_{10}(C) = 3$
>
> Se $c = 1$, vamos calcular o UCB para cada a√ß√£o:
>
> *   $UCB(A) = 2.0 + 1 \cdot \sqrt{\frac{\ln 10}{5}} \approx 2.0 + 1 \cdot \sqrt{\frac{2.30}{5}} \approx 2.0 + 0.68 \approx 2.68$
> *   $UCB(B) = 3.0 + 1 \cdot \sqrt{\frac{\ln 10}{2}} \approx 3.0 + 1 \cdot \sqrt{\frac{2.30}{2}} \approx 3.0 + 1.07 \approx 4.07$
> *   $UCB(C) = 2.5 + 1 \cdot \sqrt{\frac{\ln 10}{3}} \approx 2.5 + 1 \cdot \sqrt{\frac{2.30}{3}} \approx 2.5 + 0.88 \approx 3.38$
>
> Neste caso, a a√ß√£o B seria selecionada, pois tem o maior UCB (4.07). Mesmo tendo um valor estimado menor que B, a a√ß√£o C tem uma incerteza maior, o que a torna mais interessante para explorar.

**Funcionamento do UCB:**

*   Cada vez que uma a√ß√£o $a$ √© selecionada, a incerteza √© presumivelmente reduzida: $N_t(a)$ aumenta e, como aparece no denominador, o termo de incerteza diminui [^12].
*   Por outro lado, cada vez que uma a√ß√£o diferente de $a$ √© selecionada, $t$ aumenta, mas $N_t(a)$ n√£o [^12]. Como $t$ aparece no numerador, a estimativa de incerteza aumenta [^12].
*   O uso do logaritmo natural significa que os aumentos ficam menores com o tempo, mas n√£o s√£o limitados; todas as a√ß√µes eventualmente ser√£o selecionadas, mas a√ß√µes com estimativas de valor mais baixas, ou que j√° foram selecionadas com frequ√™ncia, ser√£o selecionadas com frequ√™ncia decrescente ao longo do tempo [^12].

**Caso especial:** Se $N_t(a) = 0$, ent√£o $a$ √© considerada uma a√ß√£o maximizadora [^12]. Em outras palavras, a√ß√µes que ainda n√£o foram experimentadas recebem a maior prioridade.

> üí° **Exemplo Num√©rico:**
>
> No in√≠cio da simula√ß√£o ($t=1$), nenhuma a√ß√£o foi selecionada ainda, ent√£o $N_1(A) = N_1(B) = N_1(C) = 0$. Neste caso, todas as a√ß√µes seriam consideradas igualmente maximizadoras e uma delas seria selecionada aleatoriamente. Isto garante que todas as a√ß√µes sejam exploradas pelo menos uma vez.

**Vantagens e Desvantagens:**

O UCB frequentemente tem um bom desempenho, conforme demonstrado na Figura 2.4 [^12].

![Average performance comparison of UCB and Œµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

No entanto, √© mais dif√≠cil de estender al√©m dos *bandits* para as configura√ß√µes mais gerais de aprendizado por refor√ßo consideradas no restante deste livro [^12]. Uma dificuldade reside em lidar com problemas n√£o estacion√°rios; seriam necess√°rios m√©todos mais complexos do que aqueles apresentados na Se√ß√£o 2.5 [^12]. Outra dificuldade √© lidar com grandes espa√ßos de estados, particularmente ao usar aproxima√ß√£o de fun√ß√£o, conforme desenvolvido na Parte II deste livro [^12]. Nessas configura√ß√µes mais avan√ßadas, a ideia de sele√ß√£o de a√ß√£o UCB geralmente n√£o √© pr√°tica [^12].

Para complementar a discuss√£o sobre as dificuldades do UCB em ambientes n√£o-estacion√°rios, podemos considerar uma variante que adapta a taxa de aprendizado para dar mais peso √†s recompensas recentes.

**Teorema 1:** *UCB com Janela Deslizante.* Uma poss√≠vel adapta√ß√£o do UCB para lidar com ambientes n√£o-estacion√°rios √© usar uma janela deslizante para calcular $Q_t(a)$. Em vez de usar todas as recompensas observadas para a a√ß√£o $a$, consideramos apenas as √∫ltimas $W$ recompensas.

Mais formalmente, seja $R_{i(a)}$ a $i$-√©sima recompensa obtida ao selecionar a a√ß√£o $a$. Ent√£o, a estimativa do valor da a√ß√£o $a$ no tempo $t$ √© dada por:

$$
Q_t(a) = \frac{1}{W} \sum_{i=\max(1, N_t(a) - W + 1)}^{N_t(a)} R_{i(a)}
$$

onde $W$ √© o tamanho da janela. Se $N_t(a) < W$, ent√£o $Q_t(a)$ √© calculado usando todas as $N_t(a)$ recompensas observadas.

> üí° **Exemplo Num√©rico:**
>
> Considere a a√ß√£o A com $W = 3$. Suponha que a a√ß√£o A foi selecionada 5 vezes e as recompensas obtidas foram: $R_1(A) = 1, R_2(A) = 2, R_3(A) = 3, R_4(A) = 4, R_5(A) = 5$.
>
> Neste caso, $N_t(A) = 5$. Para calcular $Q_t(A)$, consideramos apenas as √∫ltimas 3 recompensas: $R_3(A), R_4(A), R_5(A)$.
>
> $Q_t(A) = \frac{1}{3} (3 + 4 + 5) = \frac{12}{3} = 4$.
>
> Se $N_t(A)$ fosse menor que $W$, por exemplo, $N_t(A) = 2$, ent√£o $Q_t(A) = \frac{1}{2} (1 + 2) = 1.5$.

Essa abordagem permite que o algoritmo se adapte mais rapidamente a mudan√ßas no ambiente, descartando recompensas antigas que podem n√£o ser mais relevantes. No entanto, a escolha do tamanho da janela $W$ √© crucial. Um valor muito pequeno pode levar a estimativas ruidosas, enquanto um valor muito grande pode impedir que o algoritmo se adapte rapidamente √†s mudan√ßas.

**Proposi√ß√£o 1.1:** A complexidade computacional para calcular $Q_t(a)$ com a janela deslizante √© $O(W)$ no pior caso, onde $W$ √© o tamanho da janela. No entanto, se as recompensas forem armazenadas em uma fila circular, a complexidade pode ser reduzida para $O(1)$ amortizado.

*Prova.* No pior caso, a cada passo, precisamos somar $W$ recompensas para calcular $Q_t(a)$. No entanto, se mantivermos uma fila circular das √∫ltimas $W$ recompensas e a soma dessas recompensas, podemos atualizar a soma em tempo constante, removendo a recompensa mais antiga e adicionando a nova recompensa.

I. Calcular $Q_t(a)$ diretamente requer somar at√© $W$ recompensas, resultando em uma complexidade $O(W)$.

II. Ao usar uma fila circular, mantemos um registro das √∫ltimas $W$ recompensas e sua soma.

III. Quando uma nova recompensa $R_t(a)$ chega, removemos a recompensa mais antiga da fila e subtra√≠mos seu valor da soma atual.

IV. Adicionamos a nova recompensa $R_t(a)$ √† fila e somamos seu valor √† soma atual.

V. Essas opera√ß√µes de remo√ß√£o e adi√ß√£o levam tempo constante, ou seja, $O(1)$.

VI. Portanto, a complexidade amortizada para atualizar $Q_t(a)$ com uma fila circular √© $O(1)$. ‚ñ†

Outra poss√≠vel extens√£o do UCB √© considerar a vari√¢ncia amostral das recompensas observadas.

**Teorema 2:** *UCB com Vari√¢ncia Amostral.* Podemos refinar o termo de incerteza do UCB usando a vari√¢ncia amostral das recompensas observadas para cada a√ß√£o. A f√≥rmula de sele√ß√£o de a√ß√£o torna-se:

$$
A_t = \underset{a}{\text{argmax}} \left[ Q_t(a) + c \sqrt{\frac{S_t(a)}{N_t(a)}} \ln t \right]
$$

onde $S_t(a)$ √© a vari√¢ncia amostral das recompensas obtidas ao selecionar a a√ß√£o $a$ antes do tempo $t$.

A vari√¢ncia amostral √© calculada como:

$$
S_t(a) = \frac{1}{N_t(a) - 1} \sum_{i=1}^{N_t(a)} (R_{i(a)} - Q_t(a))^2
$$

onde $R_{i(a)}$ √© a $i$-√©sima recompensa obtida ao selecionar a a√ß√£o $a$.

> üí° **Exemplo Num√©rico:**
>
> Suponha que a a√ß√£o A foi selecionada 4 vezes, com as seguintes recompensas: $R_1(A) = 2, R_2(A) = 4, R_3(A) = 6, R_4(A) = 8$.  Ent√£o $N_t(A) = 4$.
>
> Primeiro, calculamos $Q_t(A)$:
>
> $Q_t(A) = \frac{1}{4} (2 + 4 + 6 + 8) = \frac{20}{4} = 5$.
>
> Agora, calculamos $S_t(A)$:
>
> $S_t(A) = \frac{1}{4 - 1} [(2-5)^2 + (4-5)^2 + (6-5)^2 + (8-5)^2] = \frac{1}{3} [9 + 1 + 1 + 9] = \frac{20}{3} \approx 6.67$.
>
> Se estivermos no tempo $t = 10$ e $c = 0.1$, o termo UCB para a a√ß√£o A seria:
>
> $UCB(A) = Q_t(A) + c \sqrt{\frac{S_t(A)}{N_t(a)}} \ln t = 5 + 0.1 \sqrt{\frac{6.67}{4}} \ln 10 \approx 5 + 0.1 \sqrt{1.667} \cdot 2.30 \approx 5 + 0.1 \cdot 1.29 \cdot 2.30 \approx 5 + 0.297 \approx 5.30$.

O uso da vari√¢ncia amostral permite que o algoritmo UCB se adapte melhor √†s diferentes escalas de recompensa e √† heterogeneidade das a√ß√µes. A√ß√µes com alta vari√¢ncia ter√£o um termo de incerteza maior, incentivando a explora√ß√£o.

**Proposi√ß√£o 2.1:** A complexidade computacional para calcular $S_t(a)$ √© $O(N_t(a))$ a cada passo. No entanto, usando uma atualiza√ß√£o incremental da vari√¢ncia, a complexidade pode ser reduzida para $O(1)$.

*Prova.* A vari√¢ncia amostral pode ser calculada incrementalmente usando as seguintes f√≥rmulas:

$$
S_t(a) = \frac{N_t(a)-2}{N_t(a)-1}S_{t-1}(a) + \frac{1}{N_t(a)}(R_t(a) - Q_t(a))^2
$$

onde $R_t(a)$ √© a recompensa observada no tempo $t$ para a a√ß√£o $a$.

I. Calcular $S_t(a)$ diretamente usando a f√≥rmula $\frac{1}{N_t(a) - 1} \sum_{i=1}^{N_t(a)} (R_{i(a)} - Q_t(a))^2$ requer iterar por $N_t(a)$ recompensas, resultando em complexidade $O(N_t(a))$.

II. A atualiza√ß√£o incremental da vari√¢ncia usa a f√≥rmula recursiva:
    $S_t(a) = \frac{N_t(a)-2}{N_t(a)-1}S_{t-1}(a) + \frac{1}{N_t(a)}(R_t(a) - Q_t(a))^2$.

III. Esta f√≥rmula s√≥ requer o valor anterior de $S_{t-1}(a)$, $N_t(a)$, $R_t(a)$ e $Q_t(a)$.

IV. Todas essas vari√°veis podem ser armazenadas e atualizadas em tempo constante, ou seja, $O(1)$.

V. Portanto, a complexidade para atualizar $S_t(a)$ incrementalmente √© $O(1)$. ‚ñ†

### Conclus√£o
O UCB representa uma abordagem sofisticada para equilibrar explora√ß√£o e explota√ß√£o, direcionando a explora√ß√£o para a√ß√µes cujos valores s√£o incertos e que, portanto, t√™m o potencial de serem √≥timas [^11, 12]. Apesar de seu desempenho promissor em problemas *$k$*-armed bandit, suas limita√ß√µes em ambientes n√£o estacion√°rios e com grandes espa√ßos de estados o tornam menos adequado para problemas de aprendizado por refor√ßo mais gerais [^12]. A chave para o sucesso do UCB est√° na medida de incerteza utilizada e na constante *$c$*, que controla o n√≠vel de explora√ß√£o. As extens√µes apresentadas, como o UCB com janela deslizante e o UCB com vari√¢ncia amostral, visam mitigar algumas dessas limita√ß√µes, permitindo que o algoritmo se adapte melhor a ambientes n√£o-estacion√°rios e √† heterogeneidade das a√ß√µes.

### Refer√™ncias
[^2]: Cap√≠tulo 2: Multi-armed Bandits, Introdu√ß√£o
[^11]: Cap√≠tulo 2: Multi-armed Bandits, Se√ß√£o 2.7, Par√°grafo 1
[^12]: Cap√≠tulo 2: Multi-armed Bandits, Se√ß√£o 2.7, Par√°grafo 2-4
$\blacksquare$
<!-- END -->