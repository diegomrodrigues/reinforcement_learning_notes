## Upper-Confidence-Bound Action Selection: Limitations and Extensions

### Introdu√ß√£o
O m√©todo de **Upper-Confidence-Bound (UCB)** √© uma abordagem para o problema do *k-armed bandit* que tenta equilibrar a **explora√ß√£o** e a **explota√ß√£o**, selecionando a√ß√µes com base n√£o apenas em suas estimativas de valor, mas tamb√©m em um termo de incerteza que incentiva a explora√ß√£o de a√ß√µes menos experimentadas [^35]. Embora o UCB demonstre bom desempenho em muitos cen√°rios de *bandit*, sua extens√£o para ambientes de *reinforcement learning* (RL) mais gerais apresenta desafios significativos [^36]. Este cap√≠tulo explora as limita√ß√µes do UCB em ambientes n√£o estacion√°rios e com grandes espa√ßos de estados, especialmente no contexto de aproxima√ß√£o de fun√ß√µes, e discute poss√≠veis abordagens para mitigar essas dificuldades.

### Conceitos Fundamentais

O algoritmo UCB seleciona a√ß√µes de acordo com a seguinte regra [^35]:

$$A_t = \underset{a}{\operatorname{argmax}} \ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}$$.

Aqui, $Q_t(a)$ √© a estimativa do valor da a√ß√£o $a$ no tempo $t$, $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© o tempo $t$, $c > 0$ √© um par√¢metro que controla o grau de explora√ß√£o e $\ln t$ √© o logaritmo natural de $t$. O termo de raiz quadrada representa a **incerteza** ou **vari√¢ncia** na estimativa do valor da a√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um problema de 3-armed bandit com as seguintes estimativas no tempo $t=100$:
>
> *   $Q_{100}(1) = 0.5$, $N_{100}(1) = 50$
> *   $Q_{100}(2) = 0.7$, $N_{100}(2) = 20$
> *   $Q_{100}(3) = 0.6$, $N_{100}(3) = 30$
>
> Vamos usar $c = 0.2$.  Calculamos o UCB para cada a√ß√£o:
>
> *   $UCB(1) = 0.5 + 0.2 \sqrt{\frac{\ln 100}{50}} \approx 0.5 + 0.2 \sqrt{\frac{4.605}{50}} \approx 0.5 + 0.2 \times 0.303 \approx 0.561$
> *   $UCB(2) = 0.7 + 0.2 \sqrt{\frac{\ln 100}{20}} \approx 0.7 + 0.2 \sqrt{\frac{4.605}{20}} \approx 0.7 + 0.2 \times 0.480 \approx 0.796$
> *   $UCB(3) = 0.6 + 0.2 \sqrt{\frac{\ln 100}{30}} \approx 0.6 + 0.2 \sqrt{\frac{4.605}{30}} \approx 0.6 + 0.2 \times 0.392 \approx 0.678$
>
> Neste caso, a a√ß√£o 2 seria selecionada ($A_{100} = 2$) porque tem o maior UCB. Embora sua estimativa de valor ($Q_{100}(2) = 0.7$) n√£o seja a mais alta, a incerteza (devido a $N_{100}(2)$ ser relativamente baixo) aumenta seu UCB, incentivando a explora√ß√£o.

√â importante notar que o par√¢metro $c$ desempenha um papel crucial no desempenho do UCB. Um valor muito pequeno de $c$ pode levar a uma explora√ß√£o insuficiente, enquanto um valor muito grande pode resultar em explora√ß√£o excessiva e, consequentemente, em um desempenho inferior.

**Proposi√ß√£o 1.** *A escolha do par√¢metro $c$ influencia diretamente a taxa de converg√™ncia do algoritmo UCB.*

*Prova (Esbo√ßo):* Um valor maior de $c$ incentiva a explora√ß√£o mais agressiva, o que pode ajudar a identificar rapidamente as a√ß√µes √≥timas, mas tamb√©m pode levar a mais sele√ß√µes sub√≥timas no in√≠cio do aprendizado. Um valor menor de $c$ leva a uma explora√ß√£o mais conservadora, o que pode resultar em uma converg√™ncia mais lenta, mas tamb√©m pode evitar a sele√ß√£o excessiva de a√ß√µes sub√≥timas. A escolha √≥tima de $c$ depende da estrutura espec√≠fica do problema *k-armed bandit* e do horizonte de tempo.

**Prova (Detalhada):**

Para ilustrar a influ√™ncia de *c* na taxa de converg√™ncia, considere o seguinte:

I. **Defini√ß√£o de Regret:** O *regret* (arrependimento) em um problema de *k-armed bandit* √© definido como a diferen√ßa cumulativa entre a recompensa esperada da a√ß√£o √≥tima e a recompensa esperada das a√ß√µes selecionadas pelo algoritmo ao longo do tempo. Formalmente, se $a^*$ √© a a√ß√£o √≥tima e $A_t$ √© a a√ß√£o selecionada no tempo $t$, ent√£o o *regret* $R_T$ ap√≥s $T$ passos √©:

$$R_T = \sum_{t=1}^{T} [q_*(a^*) - q_*(A_t)]$$,

onde $q_*(a)$ √© o valor verdadeiro da a√ß√£o $a$.

II. **An√°lise do UCB:** A an√°lise te√≥rica do UCB [^35] mostra que o *regret* cumulativo cresce logaritmicamente com o tempo, ou seja, $R_T = O(\ln T)$. A constante que multiplica o $\ln T$ depende do par√¢metro *c* e das diferen√ßas nos valores esperados das a√ß√µes.

III. **Impacto de *c* na Explora√ß√£o:**

    *   **c grande:** Um valor grande de *c* aumenta o termo de incerteza, incentivando a explora√ß√£o de a√ß√µes menos conhecidas. Isso leva a uma sele√ß√£o mais frequente de a√ß√µes sub√≥timas no in√≠cio, aumentando o *regret* inicial. No entanto, uma explora√ß√£o agressiva pode ajudar a identificar rapidamente as a√ß√µes √≥timas, levando a uma converg√™ncia mais r√°pida a longo prazo.

    *   **c pequeno:** Um valor pequeno de *c* reduz o termo de incerteza, incentivando a explota√ß√£o de a√ß√µes que j√° s√£o consideradas boas. Isso leva a um *regret* menor no in√≠cio, mas pode resultar em uma converg√™ncia mais lenta se as estimativas iniciais dos valores das a√ß√µes forem imprecisas. O algoritmo pode ficar preso em a√ß√µes sub√≥timas se n√£o explorar o suficiente.

IV. **Escolha √ìtima de *c*:** A escolha √≥tima de *c* depende das caracter√≠sticas do problema, como o n√∫mero de a√ß√µes, a magnitude das diferen√ßas nos valores esperados das a√ß√µes e o horizonte de tempo *T*. Em geral, um valor maior de *c* √© prefer√≠vel quando o horizonte de tempo √© longo e as diferen√ßas nos valores esperados das a√ß√µes s√£o grandes. Um valor menor de *c* √© prefer√≠vel quando o horizonte de tempo √© curto e as diferen√ßas nos valores esperados das a√ß√µes s√£o pequenas.

V. **Conclus√£o:** Portanto, a escolha do par√¢metro *c* influencia diretamente a taxa de converg√™ncia do algoritmo UCB, afetando o trade-off entre explora√ß√£o e explota√ß√£o. Um valor inadequado de *c* pode levar a um desempenho inferior, seja por explora√ß√£o excessiva ou insuficiente.

[^35]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.

‚ñ†

Al√©m da formula√ß√£o b√°sica do UCB, existem variantes que ajustam o termo de incerteza para melhorar o desempenho em cen√°rios espec√≠ficos. Uma dessas variantes considera a vari√¢ncia das recompensas observadas para cada a√ß√£o.

**Teorema 1.** *UCB com Vari√¢ncia Estimada:*

Seja $\sigma_t^2(a)$ a estimativa da vari√¢ncia das recompensas obtidas ao selecionar a a√ß√£o $a$ at√© o tempo $t$. Uma variante do UCB pode ser definida como:

$$A_t = \underset{a}{\operatorname{argmax}} \ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)} \sigma_t^2(a)}$$.

*Prova (Esbo√ßo):* A inclus√£o da vari√¢ncia estimada no termo de incerteza permite que o algoritmo ajuste sua explora√ß√£o com base na dispers√£o das recompensas observadas. A√ß√µes com alta vari√¢ncia nas recompensas ser√£o exploradas mais intensamente, enquanto a√ß√µes com baixa vari√¢ncia e estimativas de valor precisas ser√£o exploradas com menos frequ√™ncia. Isso pode levar a um desempenho melhorado, especialmente em ambientes onde as recompensas t√™m distribui√ß√µes n√£o-uniformes.

**Prova (Detalhada):**

I. **Justificativa Intuitiva:** A intui√ß√£o por tr√°s do uso da vari√¢ncia estimada √© que a√ß√µes com alta variabilidade nas recompensas s√£o mais incertas do que a√ß√µes com recompensas consistentes. Portanto, √© razo√°vel explorar mais a√ß√µes com alta vari√¢ncia para obter uma estimativa mais precisa de seu valor verdadeiro.

II. **Estimativa da Vari√¢ncia:** A vari√¢ncia $\sigma_t^2(a)$ pode ser estimada usando a f√≥rmula padr√£o para a vari√¢ncia amostral:

$$\sigma_t^2(a) = \frac{1}{N_t(a) - 1} \sum_{i=1}^{N_t(a)} (R_i(a) - Q_t(a))^2$$,

onde $R_i(a)$ √© a *i*-√©sima recompensa obtida ao selecionar a a√ß√£o $a$, e $Q_t(a)$ √© a estimativa do valor da a√ß√£o $a$ no tempo $t$.  Note que existem outras formas de estimar a vari√¢ncia, incluindo m√©todos incrementais.

> üí° **Exemplo Num√©rico:**
>
> Considere uma a√ß√£o $a$ que foi selecionada $N_t(a) = 10$ vezes. As recompensas obtidas foram: $R = [1, 2, 3, 2, 1, 3, 2, 3, 2, 2]$. A estimativa de valor atual √© $Q_t(a) = 2.1$. Calculamos a vari√¢ncia:
>
> $\sigma_t^2(a) = \frac{1}{10 - 1} \sum_{i=1}^{10} (R_i - 2.1)^2 = \frac{1}{9} [(1-2.1)^2 + (2-2.1)^2 + \ldots + (2-2.1)^2] \approx \frac{1}{9} \times 2.9 \approx 0.322$
>
> Uma alta vari√¢ncia como essa indica que as recompensas s√£o bastante dispersas em torno da m√©dia, sugerindo que mais explora√ß√£o dessa a√ß√£o pode ser ben√©fica.

III. **Impacto da Vari√¢ncia na Explora√ß√£o:**

*   **Alta Vari√¢ncia:** Se $\sigma_t^2(a)$ √© alta, o termo $c \sqrt{\frac{\ln t}{N_t(a)} \sigma_t^2(a)}$ aumenta, incentivando a explora√ß√£o da a√ß√£o $a$. Isso √© particularmente √∫til em ambientes onde as recompensas s√£o ruidosas ou t√™m distribui√ß√µes n√£o-gaussianas.

*   **Baixa Vari√¢ncia:** Se $\sigma_t^2(a)$ √© baixa, o termo $c \sqrt{\frac{\ln t}{N_t(a)} \sigma_t^2(a)}$ diminui, reduzindo a explora√ß√£o da a√ß√£o $a$. Isso √© apropriado quando a estimativa do valor da a√ß√£o √© precisa e confi√°vel.

IV. **Benef√≠cios em Ambientes N√£o-Uniformes:** Em ambientes onde as recompensas t√™m distribui√ß√µes n√£o-uniformes (por exemplo, distribui√ß√µes com caudas pesadas ou distribui√ß√µes multimodais), a vari√¢ncia estimada pode fornecer uma medida mais precisa da incerteza do que simplesmente usar o n√∫mero de vezes que a a√ß√£o foi selecionada $N_t(a)$. Isso permite que o algoritmo explore a√ß√µes de forma mais eficiente e se adapte melhor √†s caracter√≠sticas espec√≠ficas do ambiente.

V. **Regret Bound (Discuss√£o):** A an√°lise te√≥rica do UCB com vari√¢ncia estimada √© mais complexa do que a an√°lise do UCB padr√£o. No entanto, sob certas condi√ß√µes, √© poss√≠vel mostrar que o UCB com vari√¢ncia estimada tamb√©m tem um *regret* que cresce logaritmicamente com o tempo. A constante que multiplica o $\ln t$ no *regret bound* depende das caracter√≠sticas do ambiente, incluindo a vari√¢ncia das recompensas.

VI. **Conclus√£o:** A inclus√£o da vari√¢ncia estimada no termo de incerteza permite que o algoritmo UCB ajuste sua explora√ß√£o com base na dispers√£o das recompensas observadas, levando a um desempenho potencialmente melhorado em uma variedade de ambientes, especialmente aqueles com recompensas n√£o-uniformes.

‚ñ†

**Limita√ß√µes em Ambientes N√£o Estacion√°rios:**

Em ambientes **n√£o estacion√°rios**, as distribui√ß√µes de recompensa das a√ß√µes mudam ao longo do tempo [^30, 32]. O UCB, em sua forma b√°sica, assume que as recompensas s√£o amostradas de distribui√ß√µes estacion√°rias. Isso significa que as estimativas de valor $Q_t(a)$ convergem para os valores verdadeiros $q_*(a)$ com o tempo [^28]. No entanto, em ambientes n√£o estacion√°rios, os valores verdadeiros $q_*(a)$ podem mudar, tornando as estimativas de valor obsoletas.

Uma dificuldade √© que o termo de incerteza $\sqrt{\frac{\ln t}{N_t(a)}}$ diminui √† medida que $t$ aumenta e $N_t(a)$ cresce, levando o algoritmo a explorar menos ao longo do tempo. Em um ambiente n√£o estacion√°rio, essa diminui√ß√£o da explora√ß√£o pode ser prejudicial porque o algoritmo pode n√£o se adaptar rapidamente √†s mudan√ßas nas distribui√ß√µes de recompensa.

**Teorema 1.1.** *O UCB padr√£o falha em ambientes n√£o-estacion√°rios.*

*Prova (Esbo√ßo):* Como mencionado, o termo de incerteza decresce com o tempo. Em um ambiente n√£o-estacion√°rio, a a√ß√£o √≥tima pode mudar ao longo do tempo. Se o algoritmo explorar muito pouco, ele n√£o ser√° capaz de detectar essa mudan√ßa e continuar√° explorando a√ß√µes sub-√≥timas. Formalmente, o *regret* (arrependimento) do algoritmo UCB padr√£o crescer√° linearmente com o tempo em ambientes n√£o-estacion√°rios.

> üí° **Exemplo Num√©rico:**
>
> Imagine um *bandit* de duas armas, onde a recompensa da A√ß√£o 1 √© inicialmente melhor do que a A√ß√£o 2. O UCB explora inicialmente e converge para a A√ß√£o 1. No entanto, ap√≥s um certo ponto no tempo (digamos, $t=500$), a recompensa da A√ß√£o 2 se torna melhor. O UCB padr√£o, com sua explora√ß√£o decrescente, pode n√£o revisitar a A√ß√£o 2 com frequ√™ncia suficiente para detectar essa mudan√ßa. Como resultado, ele continua explorando a A√ß√£o 1 sub√≥tima, acumulando *regret* linearmente ao longo do tempo.

Para lidar com ambientes n√£o estacion√°rios, √© crucial adaptar o algoritmo UCB para que ele possa "esquecer" informa√ß√µes antigas e se adaptar rapidamente √†s mudan√ßas nas distribui√ß√µes de recompensa.

**Limita√ß√µes em Grandes Espa√ßos de Estados com Aproxima√ß√£o de Fun√ß√µes:**

Em problemas de RL com grandes espa√ßos de estados, √© frequentemente impratic√°vel manter estimativas de valor separadas $Q_t(s, a)$ para cada estado $s$ e a√ß√£o $a$ [^36]. Em vez disso, utiliza-se a **aproxima√ß√£o de fun√ß√µes** para generalizar entre estados similares. Por exemplo, redes neurais ou outras fun√ß√µes param√©tricas podem ser usadas para aproximar a fun√ß√£o de valor $Q(s, a; \mathbf{w})$, onde $\mathbf{w}$ s√£o os pesos da fun√ß√£o de aproxima√ß√£o.

Quando a aproxima√ß√£o de fun√ß√µes √© usada com UCB, v√°rias dificuldades surgem:

1.  **Estimativas de Incerteza Imprecisas:** O termo de incerteza $\sqrt{\frac{\ln t}{N_t(a)}}$ assume que cada a√ß√£o √© independente das outras. Com a aproxima√ß√£o de fun√ß√µes, as estimativas de valor de a√ß√µes similares est√£o correlacionadas, e o n√∫mero de vezes que uma a√ß√£o espec√≠fica $a$ foi selecionada $N_t(a)$ pode n√£o refletir adequadamente a incerteza sobre seu valor verdadeiro.

2.  **Explora√ß√£o Ineficiente:** A explora√ß√£o dirigida pela incerteza pode se tornar ineficiente em grandes espa√ßos de estados. A explora√ß√£o em um estado pode n√£o generalizar bem para outros estados, especialmente se a fun√ß√£o de aproxima√ß√£o for limitada em sua capacidade de representar a fun√ß√£o de valor verdadeira.

3.  **Dificuldade de Implementa√ß√£o:** Calcular e manter estimativas de incerteza precisas para cada a√ß√£o em cada estado pode ser computacionalmente caro, especialmente com fun√ß√µes de aproxima√ß√£o complexas.

Uma abordagem para lidar com a imprecis√£o das estimativas de incerteza em grandes espa√ßos de estados √© usar **conjuntos de fun√ß√µes de valor (value function ensembles)**.

**Teorema 2.** *UCB com Conjuntos de Fun√ß√µes de Valor:*

Seja $\{Q_t^{(i)}(s, a)\}_{i=1}^m$ um conjunto de $m$ fun√ß√µes de valor, cada uma treinada independentemente com diferentes inicializa√ß√µes ou conjuntos de dados. O UCB com conjuntos de fun√ß√µes de valor seleciona a√ß√µes de acordo com:

$$A_t = \underset{a}{\operatorname{argmax}} \ \bar{Q}_t(s, a) + c \sqrt{\frac{\ln t}{N_t(s, a)} + \frac{1}{m} \sum_{i=1}^m (Q_t^{(i)}(s, a) - \bar{Q}_t(s, a))^2}$$.

Onde $\bar{Q}_t(s, a) = \frac{1}{m} \sum_{i=1}^m Q_t^{(i)}(s, a)$ √© a m√©dia das estimativas de valor das $m$ fun√ß√µes de valor e $N_t(s, a)$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada no estado $s$ at√© o tempo $t$.

*Prova (Esbo√ßo):* A adi√ß√£o do termo $\frac{1}{m} \sum_{i=1}^m (Q_t^{(i)}(s, a) - \bar{Q}_t(s, a))^2$ representa a vari√¢ncia das estimativas de valor entre as diferentes fun√ß√µes de valor. Isso fornece uma medida mais robusta da incerteza do que simplesmente usar $N_t(s, a)$. Se as fun√ß√µes de valor concordarem em suas estimativas, a incerteza ser√° baixa e a explora√ß√£o ser√° menos incentivada. Se as fun√ß√µes de valor discordarem, a incerteza ser√° alta e a explora√ß√£o ser√° mais incentivada.

**Prova (Detalhada):**

I. **Intui√ß√£o por tr√°s dos Conjuntos:** A ideia central √© que, ao treinar m√∫ltiplas fun√ß√µes de valor independentemente, cada uma captura diferentes aspectos do ambiente e tem diferentes fontes de erro. A vari√¢ncia entre essas fun√ß√µes de valor pode ser usada como uma medida da incerteza sobre a estimativa de valor.

II. **C√°lculo da M√©dia:** A m√©dia das estimativas de valor, $\bar{Q}_t(s, a)$, fornece uma estimativa mais robusta do valor verdadeiro do que uma √∫nica fun√ß√£o de valor. Isso porque a m√©dia tende a reduzir o impacto de erros individuais nas estimativas de valor.

III. **C√°lculo da Vari√¢ncia:** O termo $\frac{1}{m} \sum_{i=1}^m (Q_t^{(i)}(s, a) - \bar{Q}_t(s, a))^2$ calcula a vari√¢ncia das estimativas de valor entre as diferentes fun√ß√µes de valor. Essa vari√¢ncia quantifica o grau de discord√¢ncia entre as fun√ß√µes de valor.

IV. **Impacto na Explora√ß√£o:**

*   **Baixa Vari√¢ncia:** Se as fun√ß√µes de valor concordam em suas estimativas (ou seja, a vari√¢ncia √© baixa), isso indica que h√° uma alta confian√ßa na estimativa do valor verdadeiro. Nesse caso, o termo de incerteza diminui, reduzindo a explora√ß√£o.

*   **Alta Vari√¢ncia:** Se as fun√ß√µes de valor discordam (ou seja, a vari√¢ncia √© alta), isso indica que h√° uma alta incerteza sobre a estimativa do valor verdadeiro. Nesse caso, o termo de incerteza aumenta, incentivando a explora√ß√£o.

V. **Racionalidade da Explora√ß√£o:** A explora√ß√£o incentivada pela vari√¢ncia entre as fun√ß√µes de valor √© racional porque direciona o algoritmo para estados e a√ß√µes onde as estimativas de valor s√£o mais incertas. Ao explorar essas √°reas, o algoritmo pode reduzir a incerteza e melhorar suas estimativas de valor.

VI. **Vantagens sobre o UCB Padr√£o:** O UCB com conjuntos de fun√ß√µes de valor tem v√°rias vantagens sobre o UCB padr√£o em grandes espa√ßos de estados com aproxima√ß√£o de fun√ß√µes:

*   **Estimativas de Incerteza Mais Precisas:** A vari√¢ncia entre as fun√ß√µes de valor fornece uma medida mais precisa da incerteza do que simplesmente usar o n√∫mero de vezes que a a√ß√£o foi selecionada.
*   **Explora√ß√£o Mais Eficiente:** A explora√ß√£o √© direcionada para √°reas onde a incerteza √© alta, levando a uma explora√ß√£o mais eficiente.
*   **Robustez:** A m√©dia das fun√ß√µes de valor torna o algoritmo mais robusto a erros individuais nas estimativas de valor.

VII. **Complexidade Computacional:** Uma desvantagem do UCB com conjuntos de fun√ß√µes de valor √© que ele requer o treinamento e a manuten√ß√£o de m√∫ltiplas fun√ß√µes de valor, o que pode aumentar a complexidade computacional.

VIII. **Conclus√£o:** O UCB com conjuntos de fun√ß√µes de valor √© uma abordagem promissora para lidar com a imprecis√£o das estimativas de incerteza em grandes espa√ßos de estados com aproxima√ß√£o de fun√ß√µes. Ao usar a vari√¢ncia entre as fun√ß√µes de valor como uma medida da incerteza, o algoritmo pode direcionar a explora√ß√£o de forma mais eficiente e melhorar seu desempenho.

‚ñ†

### Poss√≠veis Abordagens

Para mitigar as limita√ß√µes do UCB em ambientes n√£o estacion√°rios e com aproxima√ß√£o de fun√ß√µes, v√°rias abordagens t√™m sido exploradas:

1.  **Janelas Deslizantes:** Modificar o c√°lculo das estimativas de valor para dar mais peso √†s recompensas recentes, por exemplo, usando uma **m√©dia ponderada exponencialmente** ou uma **janela deslizante** das √∫ltimas $n$ recompensas [^32]. Isso permite que o algoritmo se adapte mais rapidamente √†s mudan√ßas nas distribui√ß√µes de recompensa.

    > üí° **Exemplo Num√©rico:**
    >
    > Suponha que estamos usando uma janela deslizante de tamanho $n=5$. As √∫ltimas 5 recompensas para a a√ß√£o $a$ s√£o: $R = [1, 2, 3, 4, 5]$. A estimativa de valor $Q_t(a)$ seria a m√©dia dessas recompensas: $Q_t(a) = \frac{1 + 2 + 3 + 4 + 5}{5} = 3$. Se a pr√≥xima recompensa for 10, a janela desliza e as recompensas se tornam: $R = [2, 3, 4, 5, 10]$. A nova estimativa de valor √©: $Q_{t+1}(a) = \frac{2 + 3 + 4 + 5 + 10}{5} = 4.8$. Isso demonstra como a janela deslizante permite que o algoritmo se adapte rapidamente a mudan√ßas recentes nas recompensas.

2.  **UCB com Esquecimentos:** Introduzir um fator de "esquecimento" nas contagens $N_t(a)$. Em vez de simplesmente incrementar $N_t(a)$ cada vez que a a√ß√£o $a$ √© selecionada, aplicar um fator de decaimento para reduzir gradualmente o peso de sele√ß√µes passadas [^30, 32]. Isso permite que o algoritmo "esque√ßa" informa√ß√µes obsoletas e explore a√ß√µes mais frequentemente.

    Formalmente, podemos definir a atualiza√ß√£o de $N_t(a)$ com um fator de esquecimento $\lambda \in [0, 1]$ como:

    $$N_{t+1}(a) = \lambda N_t(a) + \mathbb{I}(A_t = a)$$,

    onde $\mathbb{I}(A_t = a)$ √© uma fun√ß√£o indicadora que vale 1 se a a√ß√£o $a$ foi selecionada no tempo $t$ e 0 caso contr√°rio.

    > üí° **Exemplo Num√©rico:**
    >
    > Suponha que $\lambda = 0.9$ e $N_t(a) = 10$. Se a a√ß√£o $a$ n√£o for selecionada no tempo $t+1$, ent√£o $N_{t+1}(a) = 0.9 \times 10 + 0 = 9$. Se a a√ß√£o $a$ for selecionada no tempo $t+1$, ent√£o $N_{t+1}(a) = 0.9 \times 10 + 1 = 10$. Observe que o fator de esquecimento reduz gradualmente a influ√™ncia de sele√ß√µes passadas, permitindo que o algoritmo se adapte mais rapidamente a mudan√ßas nas recompensas.
    >
    > Se repetirmos essa atualiza√ß√£o por v√°rios passos sem selecionar a a√ß√£o *a*, $N_t(a)$ decair√° exponencialmente. Por exemplo, ap√≥s 5 passos sem selecionar *a*:
    >
    > $N_{t+5}(a) = \lambda^5 N_t(a) = 0.9^5 \times 10 \approx 5.9$.
    >
    > Isso significa que, ap√≥s alguns passos, a contagem efetiva de sele√ß√µes passadas √© significativamente reduzida, incentivando a reexplora√ß√£o da a√ß√£o *a*.

3.  **Abordagens Bayesianas:** Usar m√©todos **Bayesianos** para manter distribui√ß√µes de probabilidade sobre os valores das a√ß√µes [^43]. Isso permite uma representa√ß√£o mais precisa da incerteza, que pode ser usada para dirigir a explora√ß√£o de forma mais eficiente. A **amostragem posterior (posterior sampling)** ou **amostragem de Thompson (Thompson sampling)**, onde as a√ß√µes s√£o selecionadas com base em amostras retiradas das distribui√ß√µes posteriores, √© um exemplo de abordagem Bayesiana.

    > üí° **Exemplo Num√©rico:**
    >
    > Em Thompson Sampling, para cada a√ß√£o, mantemos uma distribui√ß√£o *a priori* sobre sua recompensa m√©dia (por exemplo, uma distribui√ß√£o Gaussiana). Ap√≥s cada intera√ß√£o, atualizamos essa distribui√ß√£o *a posteriori* com base na recompensa observada. Para selecionar uma a√ß√£o, amostramos um valor da distribui√ß√£o *a posteriori* de cada a√ß√£o e escolhemos a a√ß√£o com o maior valor amostrado. Isso naturalmente equilibra explora√ß√£o e explota√ß√£o: a√ß√µes com distribui√ß√µes *a posteriori* mais incertas (alta vari√¢ncia) t√™m uma maior probabilidade de serem amostradas com um valor alto, incentivando a explora√ß√£o.
    >
    > Por exemplo, se temos duas a√ß√µes, A e B, e suas distribui√ß√µes *a posteriori* s√£o Gaussianas:
    >
    > *   A√ß√£o A: $\mathcal{N}(\mu_A = 0.6, \sigma_A^2 = 0.1)$
    > *   A√ß√£o B: $\mathcal{N}(\mu_B = 0.4, \sigma_B^2 = 0.5)$
    >
    > Embora a m√©dia da A√ß√£o A seja maior, a A√ß√£o B tem uma vari√¢ncia muito maior, o que significa que √© mais incerta. Ao amostrar de cada distribui√ß√£o, existe uma probabilidade razo√°vel de amostrar um valor maior da A√ß√£o B do que da A√ß√£o A, levando √† sua sele√ß√£o e, portanto, √† explora√ß√£o.

4.  **Explora√ß√£o Dirigida por Modelos:** Aprender um modelo do ambiente e usar o modelo para planejar a√ß√µes que equilibram a explora√ß√£o e a explota√ß√£o [^36]. Por exemplo, pode-se aprender um modelo da fun√ß√£o de transi√ß√£o e da fun√ß√£o de recompensa e usar o modelo para simular diferentes sequ√™ncias de a√ß√µes e escolher a a√ß√£o que maximiza a recompensa esperada, levando em conta a incerteza sobre as estimativas do modelo.

5.  **Estimativas de Incerteza Aprimoradas:** Desenvolver m√©todos para estimar a incerteza de forma mais precisa no contexto da aproxima√ß√£o de fun√ß√µes [^36]. Isso pode envolver o uso de **redes neurais Bayesianas** ou outras t√©cnicas para quantificar a incerteza nas estimativas de valor da fun√ß√£o de aproxima√ß√£o.

### Conclus√£o
O UCB √© uma estrat√©gia eficaz para balancear explora√ß√£o e explota√ß√£o em problemas de *k-armed bandit* [^35]. No entanto, sua extens√£o direta para ambientes de RL mais complexos enfrenta desafios significativos, particularmente em ambientes n√£o estacion√°rios e com grandes espa√ßos de estados onde a aproxima√ß√£o de fun√ß√µes √© necess√°ria [^36]. As abordagens discutidas, como janelas deslizantes, UCB com esquecimentos, m√©todos Bayesianos, explora√ß√£o dirigida por modelos e estimativas de incerteza aprimoradas, representam dire√ß√µes promissoras para superar essas limita√ß√µes e desenvolver algoritmos de RL mais robustos e adapt√°veis [^30, 32, 43].

Para complementar a discuss√£o sobre UCB, a Figura 2.4, retirada do Cap√≠tulo 2 do documento, oferece uma compara√ß√£o visual do desempenho do UCB com o m√©todo Œµ-greedy em um testbed de 10 bra√ßos.

![Average performance comparison of UCB and Œµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

Esta figura mostra que o UCB (com c=2) geralmente supera o Œµ-greedy (com Œµ=0.1) ap√≥s uma fase inicial em que o UCB seleciona randomicamente entre as a√ß√µes ainda n√£o experimentadas.

### Refer√™ncias
[^35]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
[^36]: Sutton, R. S. (1996). Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding. *Advances in Neural Information Processing Systems*, 1038‚Äì1044.
[^30]: Sutton, R.S. (1984). Temporal Credit Assignment in Reinforcement Learning. *Doctoral Dissertation, Department of Computer and Information Science, University of Massachusetts, Amherst*.
[^32]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.
[^43]: Russo, D., Van Roy, B., Kazerouni, A., Osband, I., & Wen, Z. (2018). A Tutorial on Thompson Sampling. *Foundations and Trends in Machine Learning, 11*(1), 1-96.
<!-- END -->