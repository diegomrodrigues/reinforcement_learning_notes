## Upper-Confidence-Bound Action Selection e a Limita√ß√£o dos M√©todos Œµ-gananciosos

### Introdu√ß√£o

O aprendizado por refor√ßo (reinforcement learning) se distingue de outros tipos de aprendizado por utilizar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas em vez de instruir atrav√©s da indica√ß√£o de a√ß√µes corretas [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Essa caracter√≠stica fundamental cria a necessidade de explora√ß√£o ativa, uma busca expl√≠cita por um bom comportamento [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Em um contexto simplificado, como o do problema do bandido k-bra√ßos (k-armed bandit problem), onde n√£o h√° m√∫ltiplas situa√ß√µes a serem consideradas, o desafio de equilibrar explora√ß√£o e explota√ß√£o se torna mais evidente [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Este cap√≠tulo explora como o feedback avaliativo √© utilizado em um ambiente n√£o associativo e apresenta m√©todos para otimizar a sele√ß√£o de a√ß√µes, com foco particular nas limita√ß√µes dos m√©todos Œµ-gananciosos e na introdu√ß√£o do m√©todo de Upper-Confidence-Bound (UCB).

### Conceitos Fundamentais

O problema do bandido k-bra√ßos envolve tomar decis√µes repetidamente entre *k* op√ß√µes, ou a√ß√µes [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Cada a√ß√£o resulta em uma recompensa num√©rica proveniente de uma distribui√ß√£o de probabilidade estacion√°ria [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). O objetivo √© maximizar a recompensa total esperada ao longo do tempo [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Cada a√ß√£o possui um valor esperado, denotado por $q_*(a)$, que √© a recompensa m√©dia obtida ao selecionar a a√ß√£o *a*:

$$
q_*(a) = E[R_t | A_t = a]
$$
onde $A_t$ √© a a√ß√£o selecionada no instante *t*, e $R_t$ √© a recompensa correspondente [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

No entanto, esses valores s√£o desconhecidos, levando √† necessidade de estimativas, denotadas por $Q_t(a)$. A diferen√ßa entre a explora√ß√£o (selecionar a√ß√µes n√£o-gananciosas) e a explota√ß√£o (selecionar a√ß√µes gananciosas) √© fundamental [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). M√©todos de valor de a√ß√£o, como a m√©dia amostral, estimam o valor de cada a√ß√£o, calculando a m√©dia das recompensas recebidas por cada a√ß√£o [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). A m√©dia amostral √© definida por:

$$
Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}
$$

onde $\mathbb{1}_{A_i = a}$ √© uma fun√ß√£o indicadora que vale 1 se a a√ß√£o $a$ foi selecionada no passo $i$, e 0 caso contr√°rio [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3).

> üí° **Exemplo Num√©rico:**
> Vamos considerar um problema de bandido com 3 bra√ßos (k=3). As recompensas obtidas para cada bra√ßo ao longo de 5 itera√ß√µes s√£o:
>
> - Bra√ßo 1: [1, 0, 2, 1, 0]
> - Bra√ßo 2: [0, 0, 1, 0, 1]
> - Bra√ßo 3: [0, 1, 0, 0, 2]
>
> Usando a f√≥rmula da m√©dia amostral, $Q_t(a)$, podemos calcular as estimativas de valor para cada bra√ßo ap√≥s essas 5 itera√ß√µes:
>
> - $Q_5(1) = (1 + 0 + 2 + 1 + 0) / 5 = 0.8$
> - $Q_5(2) = (0 + 0 + 1 + 0 + 1) / 5 = 0.4$
> - $Q_5(3) = (0 + 1 + 0 + 0 + 2) / 5 = 0.6$
>
> Essas s√£o as estimativas de valor que o agente utilizaria para decidir qual a√ß√£o tomar. Perceba que, inicialmente, as estimativas podem ser muito diferentes dos verdadeiros valores esperados $q_*(a)$, especialmente quando o n√∫mero de amostras √© pequeno.

A escolha de a√ß√£o mais simples √© a sele√ß√£o gananciosa, que escolhe a a√ß√£o com a maior estimativa de valor [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3):

$$
A_t = \text{argmax}_a Q_t(a)
$$

No entanto, a explora√ß√£o √© crucial para identificar a√ß√µes com valores potencialmente maiores.

Os m√©todos Œµ-gananciosos tentam equilibrar a explora√ß√£o e a explota√ß√£o ao selecionar uma a√ß√£o aleat√≥ria com uma pequena probabilidade Œµ e, com probabilidade 1-Œµ, selecionar a a√ß√£o gananciosa [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). Embora esses m√©todos garantam que todas as a√ß√µes sejam amostradas infinitamente no limite, eles exploram a√ß√µes n√£o gananciosas indiscriminadamente, sem levar em considera√ß√£o suas incertezas ou proximidades com a a√ß√£o gananciosa [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3).

**Proposi√ß√£o 1**
*Um m√©todo $\epsilon$-ganancioso garante que, no limite, todas as a√ß√µes ser√£o visitadas infinitas vezes.*

*Proof*. A probabilidade de selecionar qualquer a√ß√£o espec√≠fica em um m√©todo $\epsilon$-ganancioso √© sempre maior ou igual a $\frac{\epsilon}{k}$, onde *k* √© o n√∫mero de a√ß√µes. Como $\epsilon > 0$, esta probabilidade √© sempre positiva. Portanto, com um n√∫mero infinito de etapas, todas as a√ß√µes ser√£o selecionadas infinitas vezes, garantindo a explora√ß√£o de todo o espa√ßo de a√ß√µes.

```mermaid
graph LR
    A[In√≠cio] --> B{Explora√ß√£o Œµ-gananciosa};
    B -->|Œµ| C[A√ß√£o Aleat√≥ria];
    B -->|1-Œµ| D{A√ß√£o Gananciosa};
    C --> E[Fim];
    D --> E;
    style B fill:#f9f,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
> Suponha que temos um problema de 5 bra√ßos (k=5) e que utilizamos um m√©todo Œµ-ganancioso com Œµ = 0.1. Em cada passo, h√° uma probabilidade de 10% (0.1) de selecionar uma a√ß√£o aleat√≥ria entre as 5, e uma probabilidade de 90% (0.9) de selecionar a a√ß√£o com a maior estimativa de valor. Isso garante que, mesmo que uma a√ß√£o n√£o pare√ßa promissora no momento, ela ainda ser√° explorada com uma frequ√™ncia m√≠nima de 2% (0.1 / 5), o que garante que, com o tempo, todas as a√ß√µes ser√£o exploradas.

### A Limita√ß√£o da Explora√ß√£o Indiscriminada

Os m√©todos Œµ-gananciosos, apesar de sua simplicidade e capacidade de garantir a explora√ß√£o, sofrem de uma explora√ß√£o indiscriminada. Eles n√£o fazem distin√ß√£o entre a√ß√µes n√£o-gananciosas que s√£o claramente inferiores e aquelas que t√™m o potencial de serem √≥timas, mas que ainda n√£o foram suficientemente exploradas [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). Essa abordagem leva a um uso ineficiente dos passos de explora√ß√£o. Em vez de direcionar a explora√ß√£o para a√ß√µes incertas ou com potencial, o m√©todo gasta tempo explorando a√ß√µes que podem j√° ter sido reconhecidas como sub√≥timas, o que resulta em um aprendizado mais lento.

**Lema 2**
*Em um problema de bandido k-bra√ßos com recompensas n√£o-negativas, a explora√ß√£o indiscriminada do m√©todo Œµ-ganancioso pode levar a um arrependimento cumulativo maior em compara√ß√£o com uma estrat√©gia de explora√ß√£o mais direcionada.*

*Proof.* Considere um caso em que uma a√ß√£o tem um valor esperado muito alto e todas as outras tem um valor esperado muito baixo. Um m√©todo Œµ-ganancioso explorar√° com uma frequ√™ncia de $\epsilon$, tanto a a√ß√£o de alto valor quanto as de baixo valor. Se fosse poss√≠vel direcionar a explora√ß√£o para a√ß√µes que tem um valor esperado mais pr√≥ximo da a√ß√£o √≥tima, o aprendizado seria mais r√°pido e o arrependimento cumulativo menor. M√©todos direcionados, como o UCB, buscam esse objetivo.

```mermaid
graph LR
    subgraph "Explora√ß√£o Œµ-gananciosa"
        A["A√ß√£o de Alto Valor"]
        B["A√ß√µes de Baixo Valor 1"]
        C["A√ß√µes de Baixo Valor 2"]
        D["A√ß√µes de Baixo Valor N"]
    end
    A -- "Probabilidade Œµ/k" --> Explora√ß√£o
    B -- "Probabilidade Œµ/k" --> Explora√ß√£o
    C -- "Probabilidade Œµ/k" --> Explora√ß√£o
    D -- "Probabilidade Œµ/k" --> Explora√ß√£o
    Explora√ß√£o[Explora√ß√£o Indiscriminada]
    style A fill:#ccf,stroke:#333,stroke-width:1px
    style B fill:#fcc,stroke:#333,stroke-width:1px
    style C fill:#fcc,stroke:#333,stroke-width:1px
        style D fill:#fcc,stroke:#333,stroke-width:1px
    style Explora√ß√£o fill:#ff9,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
> Imagine um problema de 3 bra√ßos onde o bra√ßo 1 tem uma recompensa m√©dia de 1, o bra√ßo 2 tem uma recompensa m√©dia de 0.1, e o bra√ßo 3 tem uma recompensa m√©dia de 0.2. Em um m√©todo Œµ-ganancioso com Œµ=0.1, o agente explorar√° os bra√ßos 2 e 3 com a mesma probabilidade (0.1/3), mesmo que o bra√ßo 2 seja claramente inferior ao 3. Essa explora√ß√£o indiscriminada faz com que o agente perca tempo e recursos explorando a√ß√µes menos promissoras, atrasando a converg√™ncia para a melhor a√ß√£o (bra√ßo 1).

### Upper-Confidence-Bound (UCB) Action Selection

O m√©todo de Upper-Confidence-Bound (UCB) surge como uma alternativa mais sofisticada ao m√©todo Œµ-ganancioso. O UCB busca a√ß√µes que s√£o potencialmente √≥timas, considerando tanto a sua estimativa de valor quanto a incerteza associada a essa estimativa [11](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-11). A sele√ß√£o de a√ß√£o no UCB √© definida como:

$$
A_t = \text{argmax}_a \left[ Q_t(a) + c \sqrt{\frac{\ln{t}}{N_t(a)}} \right]
$$

onde $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o instante *t*, e *c* √© um par√¢metro que controla o grau de explora√ß√£o [11](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-11). O termo $c\sqrt{\frac{\ln{t}}{N_t(a)}}$ representa a incerteza da estimativa de valor da a√ß√£o. A utiliza√ß√£o do logaritmo natural de *t* garante que a explora√ß√£o diminua com o tempo, mas de forma controlada, incentivando a√ß√µes pouco exploradas e que tenham potencial para otimiza√ß√£o [11](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-11).

**Lema 2.1**
*O termo de explora√ß√£o no UCB, $c\sqrt{\frac{\ln{t}}{N_t(a)}}$,  diminui monotonicamente com o tempo para a√ß√µes que s√£o selecionadas frequentemente.*

*Proof.* Para uma a√ß√£o *a* que √© selecionada frequentemente, $N_t(a)$ aumenta com o tempo *t*. Como o logaritmo natural $\ln{t}$ cresce mais lentamente do que *t*, o termo $\frac{\ln{t}}{N_t(a)}$ diminui conforme $t$ aumenta. Consequentemente,  $c\sqrt{\frac{\ln{t}}{N_t(a)}}$ tamb√©m diminui monotonicamente com o tempo para a√ß√µes que s√£o selecionadas frequentemente, o que leva √† redu√ß√£o da explora√ß√£o dessas a√ß√µes.

```mermaid
graph LR
    A["Estimativa de Valor Q_t(a)"]
    B["Incerteza: c * sqrt(ln(t) / N_t(a))"]
    C["Valor UCB: Q_t(a) + Incerteza"]
    A --> C
    B --> C
    style A fill:#ccf,stroke:#333,stroke-width:1px
    style B fill:#fcc,stroke:#333,stroke-width:1px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    subgraph "C√°lculo UCB"
      A
      B
      C
    end
    C --> D[Sele√ß√£o da A√ß√£o com Maior Valor UCB]
    D --> E[Fim]

```

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o mesmo problema de 3 bra√ßos do exemplo anterior. Suponha que ap√≥s 10 itera√ß√µes, tenhamos:
>
> - $Q_{10}(1) = 0.7$, $N_{10}(1) = 4$
> - $Q_{10}(2) = 0.2$, $N_{10}(2) = 3$
> - $Q_{10}(3) = 0.4$, $N_{10}(3) = 3$
>
> Usando $c=1$, podemos calcular o valor de UCB para cada a√ß√£o:
>
> - $UCB_{10}(1) = 0.7 + 1 \times \sqrt{\frac{\ln{10}}{4}} \approx 0.7 + 1.07 \approx 1.77$
> - $UCB_{10}(2) = 0.2 + 1 \times \sqrt{\frac{\ln{10}}{3}} \approx 0.2 + 1.22 \approx 1.42$
> - $UCB_{10}(3) = 0.4 + 1 \times \sqrt{\frac{\ln{10}}{3}} \approx 0.4 + 1.22 \approx 1.62$
>
> Nesse caso, o UCB selecionaria a a√ß√£o 1, mesmo n√£o sendo a a√ß√£o que parece melhor somente com base na m√©dia amostral $Q_t(a)$. Isso ocorre porque o UCB leva em considera√ß√£o a incerteza, que nesse caso √© maior para as a√ß√µes 2 e 3. Se a a√ß√£o 1 continuar a ser selecionada, $N_t(1)$ aumentar√°, e o termo de incerteza diminuir√°.
>
> Agora, vamos calcular o valor de UCB ap√≥s 100 itera√ß√µes, assumindo que a a√ß√£o 1 foi selecionada muitas vezes e que os valores de Q(a) mudaram pouco:
>
> - $Q_{100}(1) = 0.75$, $N_{100}(1) = 50$
> - $Q_{100}(2) = 0.25$, $N_{100}(2) = 25$
> - $Q_{100}(3) = 0.35$, $N_{100}(3) = 25$
>
> - $UCB_{100}(1) = 0.75 + 1 \times \sqrt{\frac{\ln{100}}{50}} \approx 0.75 + 0.30 \approx 1.05$
> - $UCB_{100}(2) = 0.25 + 1 \times \sqrt{\frac{\ln{100}}{25}} \approx 0.25 + 0.42 \approx 0.67$
> - $UCB_{100}(3) = 0.35 + 1 \times \sqrt{\frac{\ln{100}}{25}} \approx 0.35 + 0.42 \approx 0.77$
>
> Veja como o termo de explora√ß√£o para a a√ß√£o 1 diminuiu consideravelmente, pois ela foi selecionada mais vezes, e as a√ß√µes 2 e 3, menos exploradas, t√™m um termo de explora√ß√£o maior, o que pode fazer com que elas sejam selecionadas no futuro.

Ao contr√°rio do m√©todo Œµ-ganancioso, o UCB seleciona a√ß√µes que n√£o apenas parecem boas no momento, mas tamb√©m aquelas que ainda n√£o foram bem exploradas e, por isso, podem ter um valor real mais alto do que o estimado [11](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-11). A cada sele√ß√£o de uma a√ß√£o, o n√∫mero de vezes que ela foi selecionada aumenta, diminuindo o termo de incerteza e incentivando que outras a√ß√µes sejam exploradas, resultando em uma explora√ß√£o mais direcionada e eficiente.

**Teorema 3**
*Em um ambiente estacion√°rio, o algoritmo UCB com um par√¢metro de explora√ß√£o *c* adequadamente escolhido garante um arrependimento cumulativo sublinear, isto √©, o arrependimento cresce mais lentamente do que linearmente com o tempo.*

*Proof Outline.* A prova do arrependimento sublinear do UCB √© complexa e envolve an√°lise de desigualdades de probabilidade e argumenta√ß√£o sobre a frequ√™ncia com que a√ß√µes sub√≥timas s√£o selecionadas. Essencialmente, a prova mostra que, com o termo de confian√ßa do UCB, a√ß√µes sub√≥timas s√£o exploradas com uma frequ√™ncia que diminui com o tempo, resultando em um n√∫mero limitado de sele√ß√µes dessas a√ß√µes e, portanto, em um arrependimento cumulativo sublinear. Uma prova detalhada pode ser encontrada em [Auer et al. 2002](https://link.springer.com/article/10.1023/A:1013689704352).

### Conclus√£o

A escolha entre explora√ß√£o e explota√ß√£o √© fundamental no aprendizado por refor√ßo. Enquanto os m√©todos Œµ-gananciosos garantem a explora√ß√£o, eles o fazem de maneira indiscriminada, explorando tanto a√ß√µes promissoras quanto a√ß√µes sub√≥timas [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). O m√©todo de Upper-Confidence-Bound (UCB) oferece uma abordagem mais eficiente, direcionando a explora√ß√£o para a√ß√µes que t√™m maior potencial de serem √≥timas e explorando a√ß√µes pouco experimentadas, equilibrando a explora√ß√£o e explota√ß√£o de maneira mais sofisticada [11](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-11). Embora o UCB apresente um desempenho promissor em cen√°rios estacion√°rios, ele pode se tornar menos adequado em ambientes n√£o estacion√°rios ou com grandes espa√ßos de estados, indicando a necessidade de m√©todos mais avan√ßados para cen√°rios complexos [11](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-11).

**Corol√°rio 3.1**
*O sucesso do UCB em ambientes estacion√°rios depende da escolha adequada do par√¢metro *c*. Valores muito baixos de *c* levam a explora√ß√£o insuficiente, enquanto valores muito altos levam a uma explora√ß√£o excessiva.*

*Proof.* O par√¢metro *c* controla a magnitude do termo de incerteza no UCB. Se *c* for muito baixo, o termo de incerteza ser√° pequeno e o algoritmo se comportar√° de maneira mais gananciosa, n√£o explorando suficientemente a√ß√µes menos conhecidas. Se *c* for muito alto, o termo de incerteza dominar√° a sele√ß√£o de a√ß√µes, levando a uma explora√ß√£o excessiva e, portanto, tamb√©m a um aprendizado mais lento. A escolha ideal de *c* depende da escala das recompensas e da natureza do problema, e frequentemente √© feita atrav√©s de uma busca de hiperpar√¢metros.

### Refer√™ncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions."
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action."
[^3]: "The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section."
[^11]: "Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. The greedy actions are those that look best at present, but some of the other actions may actually be better."

**Observa√ß√£o:** A refer√™ncia [Auer et al. 2002](https://link.springer.com/article/10.1023/A:1013689704352) foi adicionada na prova do Teorema 3 para fornecer um embasamento mais detalhado.
