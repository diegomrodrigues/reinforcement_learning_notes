## Upper-Confidence-Bound Action Selection e suas Limita√ß√µes na Generaliza√ß√£o
### Introdu√ß√£o
O aprendizado por refor√ßo se distingue de outras formas de aprendizado por utilizar informa√ß√£o de treinamento que avalia as a√ß√µes tomadas ao inv√©s de instruir a√ß√µes corretas [1]. Isso cria uma necessidade de explora√ß√£o ativa, uma busca expl√≠cita por bom comportamento. O feedback puramente avaliativo indica o qu√£o boa a a√ß√£o tomada foi, mas n√£o se foi a melhor ou a pior poss√≠vel. Em contraste, o feedback instrutivo indica a a√ß√£o correta a ser tomada, independentemente da a√ß√£o realmente realizada [1]. Este cap√≠tulo foca no aspecto avaliativo do aprendizado por refor√ßo em um cen√°rio simplificado, o problema do *k-armed bandit*, que n√£o envolve aprender a agir em diversas situa√ß√µes [1]. Dentro deste contexto, exploramos diferentes m√©todos para equilibrar explora√ß√£o e explota√ß√£o, incluindo o m√©todo Upper-Confidence-Bound (UCB) [1].

### Conceitos Fundamentais
No problema do *k-armed bandit*, o objetivo √© maximizar a recompensa total esperada ao longo de um per√≠odo de tempo, escolhendo repetidamente entre *k* op√ß√µes diferentes, ou a√ß√µes [1]. Cada a√ß√£o resulta em uma recompensa num√©rica selecionada a partir de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o escolhida [1]. O valor de uma a√ß√£o *a*, denotado por $q_*(a)$, √© a recompensa esperada ao selecionar essa a√ß√£o, $q_*(a) = E[R_t | A_t = a]$ [1]. Como esses valores s√£o desconhecidos, √© necess√°rio estim√°-los, com $Q_t(a)$ denotando o valor estimado da a√ß√£o *a* no tempo *t*.

A sele√ß√£o da a√ß√£o pode ser feita por explora√ß√£o ou explota√ß√£o [2]. **Explota√ß√£o** significa escolher a a√ß√£o que atualmente tem a maior estimativa de valor, ou seja, uma a√ß√£o gulosa. **Explora√ß√£o**, por outro lado, envolve a sele√ß√£o de uma a√ß√£o n√£o-gulosa para melhorar a estimativa de valor dessas a√ß√µes [2]. O desafio √© equilibrar esses dois objetivos, pois a explota√ß√£o maximiza a recompensa imediata, enquanto a explora√ß√£o pode levar a maiores recompensas a longo prazo [2].

```mermaid
graph LR
    A("A√ß√µes") -->|Explora√ß√£o| B("A√ß√µes n√£o-gulosas");
    A -->|Explota√ß√£o| C("A√ß√£o gulosa");
    B --> D("Melhora a estimativa");
    C --> E("Recompensa imediata");
    D --> F("Recompensa a longo prazo");
    E --> G("Desafio: Equil√≠brio");
    F --> G
```

Uma maneira de equilibrar explora√ß√£o e explota√ß√£o √© atrav√©s do m√©todo *…õ-greedy*, que seleciona a a√ß√£o com maior valor estimado na maioria das vezes, mas aleatoriamente seleciona outras a√ß√µes com probabilidade *…õ* [3]. Em contrapartida, o m√©todo **Upper-Confidence-Bound (UCB)** seleciona a√ß√µes de acordo com a seguinte regra [11]:
$$A_t = \underset{a}{\operatorname{argmax}} \left[ Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}} \right],$$
onde $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o tempo *t*, e $c>0$ controla o grau de explora√ß√£o [11]. O termo de raiz quadrada √© uma medida da incerteza do valor da a√ß√£o, e a quantidade que est√° sendo maximizada √© um limite superior do poss√≠vel valor verdadeiro da a√ß√£o, determinado pelo n√≠vel de confian√ßa *c* [11].
> üí° **Exemplo Num√©rico:**
> Vamos considerar um cen√°rio com 3 a√ß√µes (k=3) e assumir que o valor estimado inicial de cada a√ß√£o √© 0, ou seja, $Q_1(a) = 0$ para $a \in \{1, 2, 3\}$. Vamos usar $c=1$ no UCB. No primeiro passo (t=1), $N_1(a)=0$ para todas as a√ß√µes, o que resulta em uma divis√£o por zero dentro do termo de incerteza. Para evitar isso na pr√°tica, iniciaremos $N_1(a)$ com 1 para todas as a√ß√µes. Agora, a f√≥rmula do UCB se torna:
>
>  $A_t = \underset{a}{\operatorname{argmax}} \left[ Q_t(a) + 1\sqrt{\frac{\ln t}{N_t(a)}} \right]$
>
>  Para $t=1$, temos:
>
>   $UCB(a) = 0 + 1\sqrt{\frac{\ln 1}{1}} = 0$
>
>  Como todos os UCBs s√£o iguais, vamos escolher a a√ß√£o 1 arbitrariamente. Suponha que ela retorna uma recompensa de $R_1=0.5$. Atualizamos $Q_2(1) = 0.5$ e $N_2(1) = 2$. Para $t=2$, as contas se tornam:
>
>   $UCB(1) = 0.5 + 1\sqrt{\frac{\ln 2}{2}} \approx 0.5 + 0.5887 \approx 1.0887$
>
>   $UCB(2) = 0 + 1\sqrt{\frac{\ln 2}{1}} \approx 0 + 0.8326 \approx 0.8326$
>
>   $UCB(3) = 0 + 1\sqrt{\frac{\ln 2}{1}} \approx 0 + 0.8326 \approx 0.8326$
>
>  A a√ß√£o 1 √© escolhida, e suponha que a recompensa √© $R_2=0.6$. Ent√£o, $Q_3(1) = \frac{0.5 + 0.6}{2} = 0.55$ e $N_3(1) = 3$.  A a√ß√£o com o maior UCB √© selecionada a cada passo. Este exemplo ilustra que, inicialmente, a incerteza for√ßa a explora√ß√£o de todas as a√ß√µes. Com o tempo, a estimativa da a√ß√£o 1 se torna mais precisa, reduzindo a incerteza e incentivando a explora√ß√£o de outras a√ß√µes.

```mermaid
sequenceDiagram
    participant Agente
    participant Ambiente
    Agente->>Ambiente: Escolhe a√ß√£o "a"
    Ambiente-->>Agente: Retorna recompensa "R_t"
    Agente->>Agente: Calcula "Q_t(a)"
    Agente->>Agente: Calcula "N_t(a)"
    Agente->>Agente: Calcula "UCB(a) = Q_t(a) + c * sqrt(ln(t) / N_t(a))"
    Agente->>Agente: Seleciona a a√ß√£o com maior "UCB(a)"
    loop Continua at√© converg√™ncia
        Agente->>Ambiente: Escolhe a√ß√£o "a"
        Ambiente-->>Agente: Retorna recompensa "R_t"
        Agente->>Agente: Calcula "Q_t(a)"
        Agente->>Agente: Calcula "N_t(a)"
        Agente->>Agente: Calcula "UCB(a) = Q_t(a) + c * sqrt(ln(t) / N_t(a))"
        Agente->>Agente: Seleciona a a√ß√£o com maior "UCB(a)"
    end
```

O m√©todo UCB funciona de forma que, quanto mais vezes uma a√ß√£o √© selecionada, menor o seu termo de incerteza e, portanto, torna-se menos prov√°vel que ela seja escolhida, incentivando a explora√ß√£o de outras a√ß√µes [11]. M√©todos como o *…õ-greedy* exploram a√ß√µes n√£o-gulosas de maneira indiscriminada. O UCB √© mais inteligente, selecionando a√ß√µes n√£o-gulosas com base em seu potencial de serem √≥timas [11].
**Proposi√ß√£o 1**
A escolha do par√¢metro *c* no m√©todo UCB afeta diretamente a taxa de explora√ß√£o. Um valor maior para *c* aumenta a explora√ß√£o, enquanto um valor menor tende a favorecer a explota√ß√£o.
> üí° **Exemplo Num√©rico:**
> Suponha que temos duas a√ß√µes, A e B, com estimativas $Q_t(A) = 0.7$ e $Q_t(B) = 0.6$. No tempo t=10, a a√ß√£o A foi selecionada 5 vezes ($N_t(A) = 5$) e a a√ß√£o B 2 vezes ($N_t(B) = 2$). Se utilizarmos c=0.1, os UCBs ser√£o:
>
>   $UCB(A) = 0.7 + 0.1 \sqrt{\frac{\ln 10}{5}} \approx 0.7 + 0.1 \sqrt{0.46} \approx 0.7 + 0.067 = 0.767$
>   $UCB(B) = 0.6 + 0.1 \sqrt{\frac{\ln 10}{2}} \approx 0.6 + 0.1 \sqrt{1.15} \approx 0.6 + 0.107 = 0.707$
>
> Nesse caso, a a√ß√£o A seria selecionada. Agora, se utilizarmos um valor maior, c=1, os UCBs ser√£o:
>
>  $UCB(A) = 0.7 + 1 \sqrt{\frac{\ln 10}{5}} \approx 0.7 + 1 \sqrt{0.46} \approx 0.7 + 0.67 = 1.37$
>   $UCB(B) = 0.6 + 1 \sqrt{\frac{\ln 10}{2}} \approx 0.6 + 1 \sqrt{1.15} \approx 0.6 + 1.07 = 1.67$
>
> Com c=1, a a√ß√£o B seria selecionada. Isso ilustra que um valor maior de c favorece a explora√ß√£o da a√ß√£o B, apesar de sua estimativa ser menor que a de A.

```mermaid
graph LR
    A("Par√¢metro c") --> B("Taxa de Explora√ß√£o");
    B --> C{Maior "c"};
    B --> D{Menor "c"};
    C --> E("Mais Explora√ß√£o");
    D --> F("Mais Explota√ß√£o");
```

*Proof Sketch:* This follows directly from the UCB formula. A larger *c* amplifies the uncertainty term, making it more likely that less-explored actions will be selected. Conversely, a smaller *c* diminishes the impact of the uncertainty term, favoring actions with high estimated values $Q_t(a)$.

**Lema 1**
Se a recompensa √© limitada no intervalo $[0,1]$, ent√£o $q_*(a) \in [0,1]$ para toda a√ß√£o *a*.
*Proof:* Dado que $q_*(a) = E[R_t | A_t = a]$, e $0 \le R_t \le 1$ para qualquer tempo *t*, ent√£o, pela linearidade da esperan√ßa, $0 \le E[R_t | A_t = a] \le 1$, o que implica $0 \le q_*(a) \le 1$.
> üí° **Exemplo Num√©rico:**
> Suponha que uma a√ß√£o *a* tem as seguintes recompensas em 3 tentativas: 0.3, 0.8 e 0.5. O valor esperado $q_*(a)$ √© a m√©dia dessas recompensas: $q_*(a) = (0.3+0.8+0.5)/3 = 0.533$. Como todas as recompensas est√£o no intervalo [0,1], o valor esperado tamb√©m est√° dentro desse intervalo.

```mermaid
graph LR
    A("Recompensa R_t") --> B("Intervalo [0, 1]");
    B --> C("Valor Esperado q_*(a) = E[R_t | A_t = a]");
    C --> D("q_*(a) tamb√©m em [0, 1]");
```

**Lema 1.1**
Se as recompensas s√£o limitadas por um intervalo $[R_{min}, R_{max}]$, ent√£o $q_*(a) \in [R_{min}, R_{max}]$ para toda a√ß√£o *a*.
*Proof:* An√°logo ao Lema 1, aplicando a linearidade da esperan√ßa e os limites nas recompensas, temos $R_{min} \le E[R_t | A_t = a] \le R_{max}$ para todas as a√ß√µes *a*.
> üí° **Exemplo Num√©rico:**
> Se as recompensas forem limitadas no intervalo [-1, 2], e uma a√ß√£o *b* fornece as recompensas -0.5, 1.5, e 0.0 em 3 tentativas, ent√£o $q_*(b) = (-0.5 + 1.5 + 0.0)/3 = 0.333$. Este valor est√° dentro do intervalo [-1, 2], como esperado.

```mermaid
graph LR
    A("Recompensa R_t") --> B("Intervalo [R_min, R_max]");
     B --> C("Valor Esperado q_*(a) = E[R_t | A_t = a]");
    C --> D("q_*(a) tamb√©m em [R_min, R_max]");
```

**Teorema 1**
O algoritmo UCB garante que todas as a√ß√µes ser√£o exploradas infinitamente em um problema *k*-armed bandit.

*Proof Sketch:* The UCB action selection rule includes a term that increases with time, $\sqrt{\frac{\ln t}{N_t(a)}}$. As $t$ increases, if $N_t(a)$ remains bounded for any action *a*, the uncertainty term grows without bound. This guarantees that any action will eventually be explored. Moreover, as the number of plays for an action increases, $N_t(a)$ increases, making the uncertainty term associated with that action decrease, which then allows exploration to other actions to occur.
> üí° **Exemplo Num√©rico:**
> Imagine um problema com 2 a√ß√µes. A a√ß√£o 1 √© inicialmente muito explorada, mas mesmo que sua estimativa seja alta, o termo $\sqrt{\frac{\ln t}{N_t(a)}}$  para a a√ß√£o 2 aumentar√° com o tempo. Isso acontecer√° at√© o ponto em que o UCB da a√ß√£o 2 seja maior do que o da a√ß√£o 1, levando √† explora√ß√£o da a√ß√£o 2. A a√ß√£o 2 ser√° explorada e, com o tempo, a a√ß√£o 1 ser√° explorada novamente, garantindo que todas as a√ß√µes sejam exploradas infinitamente.

```mermaid
graph LR
    A("UCB: Termo de incerteza") --> B("sqrt(ln(t) / N_t(a))");
    B --> C("Se N_t(a) limitado");
    C --> D("Incerteza cresce com 't'");
    D --> E("A√ß√£o 'a' explorada");
    E --> F("N_t(a) aumenta, incerteza diminui");
     F --> G("Outras a√ß√µes exploradas");
     G --> H("Todas a√ß√µes exploradas infinitamente");
```

### UCB e suas Limita√ß√µes na Generaliza√ß√£o
Apesar do bom desempenho do UCB, como demonstrado no *10-armed testbed* [12], ele apresenta dificuldades ao ser estendido para cen√°rios mais gerais de aprendizado por refor√ßo. Uma das dificuldades √© lidar com problemas n√£o-estacion√°rios, nos quais as propriedades do ambiente mudam ao longo do tempo [12]. Nesses casos, m√©todos mais complexos para lidar com valores de a√ß√£o que mudam ao longo do tempo seriam necess√°rios, superando os m√©todos apresentados para o problema *k-armed bandit* [12]. Outra dificuldade surge ao lidar com espa√ßos de estados grandes, especialmente quando se utiliza aproxima√ß√£o de fun√ß√µes [12]. Em configura√ß√µes mais avan√ßadas, a ideia de selecionar a√ß√µes UCB geralmente n√£o √© pr√°tica. O m√©todo UCB, apesar de suas vantagens no cen√°rio simplificado, n√£o se traduz bem para a complexidade de problemas gerais de aprendizado por refor√ßo. Ele se mostra inadequado para lidar com n√£o-estacionariedade e complexidade de espa√ßos de estados maiores [12].
**Observa√ß√£o 1:**
O m√©todo UCB n√£o considera a possibilidade de as recompensas associadas √†s a√ß√µes mudarem ao longo do tempo. Se a distribui√ß√£o de recompensas de uma a√ß√£o mudar, o m√©todo UCB pode n√£o ser capaz de se adaptar rapidamente a essas mudan√ßas.
> üí° **Exemplo Num√©rico:**
> Imagine que a a√ß√£o 1 inicialmente tem uma recompensa m√©dia alta, mas ap√≥s 100 passos, a recompensa m√©dia cai drasticamente. O UCB, ao basear sua sele√ß√£o no hist√≥rico de recompensas, ainda tender√° a escolher a a√ß√£o 1 por conta do seu hist√≥rico. Isso ilustra a dificuldade de adapta√ß√£o do UCB em cen√°rios n√£o-estacion√°rios.

```mermaid
graph LR
    A("UCB") --> B("Problemas n√£o-estacion√°rios");
    A --> C("Espa√ßos de estados grandes");
    B --> D("Dificuldade de adapta√ß√£o");
    C --> E("Complexidade em aproxima√ß√£o de fun√ß√µes");
    D --> F("Recompensas mudam, UCB n√£o se adapta");
     E --> G("N√£o pr√°tico em cen√°rios complexos");
```

### Conclus√£o
O m√©todo Upper-Confidence-Bound (UCB) oferece uma abordagem eficaz para equilibrar explora√ß√£o e explota√ß√£o em problemas do tipo *k-armed bandit*, selecionando a√ß√µes que maximizam um limite superior de seus valores reais, levando em considera√ß√£o a incerteza associada a essas estimativas [11]. Apesar do bom desempenho demonstrado, o m√©todo UCB apresenta limita√ß√µes quando se trata de estend√™-lo para problemas mais gerais de aprendizado por refor√ßo. As dificuldades surgem ao lidar com a n√£o-estacionariedade do ambiente e com espa√ßos de estado grandes. Apesar disso, m√©todos como o UCB servem como ponto de partida para o entendimento da din√¢mica entre explora√ß√£o e explota√ß√£o, conceitos fundamentais no campo de aprendizado por refor√ßo [11].

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions."
[^2]: "When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action's value."
[^3]: "A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability …õ, instead select randomly from among all the actions with equal probability, independently of the action-value estimates."
[^11]: "One effective way of doing this is to select actions according to  $A_t = \operatorname{argmax}_a [Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}]$."
[^12]: "UCB often performs well, as shown here, but is more difficult than …õ-greedy to extend beyond bandits to the more general reinforcement learning settings considered in the rest of this book."
