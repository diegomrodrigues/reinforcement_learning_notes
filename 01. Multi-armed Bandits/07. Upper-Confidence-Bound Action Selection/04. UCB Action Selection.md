### Upper-Confidence-Bound Action Selection: Uma Abordagem Explorat√≥ria Avan√ßada

### Introdu√ß√£o
O aprendizado por refor√ßo (reinforcement learning) se distingue de outras formas de aprendizado por utilizar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas em vez de instruir atrav√©s de a√ß√µes corretas [^1]. Essa caracter√≠stica fundamental cria a necessidade de explora√ß√£o ativa, buscando explicitamente por um bom comportamento [^1]. Em um cen√°rio simplificado, como o problema do k-armed bandit, que foca no aspecto avaliativo do aprendizado por refor√ßo em um ambiente n√£o associativo, ou seja, sem a necessidade de aprendizado de a√ß√µes em m√∫ltiplas situa√ß√µes, podemos analisar em detalhes como o feedback avaliativo difere do instrutivo [^1]. A explora√ß√£o desempenha um papel crucial, pois o objetivo √© maximizar a recompensa total esperada ao longo de um per√≠odo [^2]. Dada a incerteza inerente aos valores de a√ß√£o, torna-se necess√°rio n√£o apenas explorar as a√ß√µes que parecem promissoras no momento, mas tamb√©m considerar a√ß√µes menos exploradas que podem vir a ser √≥timas. Este cap√≠tulo se dedica ao estudo detalhado de m√©todos que equilibram essa dualidade entre explora√ß√£o e aproveitamento (exploitation), com foco especial no m√©todo de sele√ß√£o de a√ß√£o Upper Confidence Bound (UCB), explorando suas nuances e aplica√ß√µes em problemas de aprendizado por refor√ßo [^2].

### Conceitos Fundamentais
A ess√™ncia do problema do **k-armed bandit** reside na escolha repetida entre *k* diferentes op√ß√µes ou a√ß√µes, em que cada a√ß√£o resulta em uma recompensa num√©rica proveniente de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o selecionada [^1]. O objetivo central √© maximizar a recompensa total esperada ao longo do tempo [^2]. √â importante ressaltar que, em um cen√°rio ideal, se conhec√™ssemos os valores de cada a√ß√£o, a solu√ß√£o seria trivial: selecionar sempre a a√ß√£o de maior valor [^2]. No entanto, na maioria dos casos, n√£o temos conhecimento pr√©vio desses valores, embora possamos obter estimativas [^2]. A nota√ß√£o crucial aqui inclui:

*   **$A_t$**: A a√ß√£o selecionada no passo de tempo *t* [^2].
*   **$R_t$**: A recompensa correspondente √† a√ß√£o $A_t$ [^2].
*   **$q_*(a)$**: O valor esperado da a√ß√£o *a*, calculado como $q_*(a) = E[R_t | A_t=a]$ [^2].
*   **$Q_t(a)$**: A estimativa do valor da a√ß√£o *a* no passo de tempo *t* [^2].

> üí° **Exemplo Num√©rico:** Imagine um cen√°rio de 3-armed bandit (k=3), onde as recompensas de cada a√ß√£o s√£o determinadas por distribui√ß√µes normais. A a√ß√£o 1 tem uma distribui√ß√£o com m√©dia 1 e desvio padr√£o 1, a a√ß√£o 2 tem m√©dia 2 e desvio padr√£o 1, e a a√ß√£o 3 tem m√©dia 3 e desvio padr√£o 1. Assim, $q_*(1) = 1$, $q_*(2) = 2$, e $q_*(3) = 3$. Inicialmente, n√£o sabemos esses valores e precisamos estim√°-los, ent√£o, para t=1, $Q_1(1)$, $Q_1(2)$, e $Q_1(3)$ podem ser valores aleat√≥rios pr√≥ximos de 0. A cada itera√ß√£o, atualizamos $Q_t(a)$ baseado nas recompensas observadas.
```mermaid
graph LR
    A[ "A√ß√£o 'a'" ] -->| "Recompensa R_t" | B( "Distribui√ß√£o de Probabilidade" )
    B -->| "Valor Esperado q*(a) = E[R_t|A_t=a]" | C[ "Valor Verdadeiro de 'a'" ]
    C -->| "Estimativa Q_t(a)" | D[ "Valor Estimado de 'a' no tempo t" ]
```

O dilema central entre **explora√ß√£o e aproveitamento (exploitation)** emerge do fato de que a explora√ß√£o (escolha de a√ß√µes n√£o gananciosas) permite melhorar as estimativas dos valores de a√ß√£o, enquanto o aproveitamento (escolha da a√ß√£o com maior estimativa atual) maximiza a recompensa imediata [^2]. O m√©todo **$\epsilon$-greedy**, por exemplo, explora uma fra√ß√£o $\epsilon$ das vezes, enquanto o restante do tempo explora [^3]. No entanto, o m√©todo $\epsilon$-greedy explora a√ß√µes n√£o gananciosas indiscriminadamente [^3]. Uma abordagem mais refinada √© o **Upper Confidence Bound (UCB)**.

**Proposi√ß√£o 1** *O m√©todo $\epsilon$-greedy garante que todas as a√ß√µes ser√£o visitadas infinitas vezes no limite, assumindo que $\epsilon > 0$ e que o n√∫mero de itera√ß√µes tende ao infinito.*

*Prova:* Dado que o m√©todo $\epsilon$-greedy escolhe uma a√ß√£o aleat√≥ria com probabilidade $\epsilon$ em cada passo, e que essa escolha √© uniforme sobre todas as $k$ a√ß√µes, a probabilidade de escolher uma a√ß√£o espec√≠fica $a$ em um passo √© $\frac{\epsilon}{k}$. Como h√° um n√∫mero infinito de passos, a soma das probabilidades de escolher uma a√ß√£o espec√≠fica diverge, o que implica que todas as a√ß√µes ser√£o selecionadas infinitas vezes.
```mermaid
graph LR
    subgraph "Œµ-greedy"
    A[ "A√ß√£o a" ] -->| "Probabilidade Œµ/k" | B("Explora√ß√£o Aleat√≥ria");
    C[ "A√ß√µes n√£o gananciosas" ] --> | "Probabilidade 1 - Œµ" | D("Aproveitamento");
    B --> E("Sele√ß√£o da A√ß√£o")
    D --> E
    end
    E --> F[ "Realizar A√ß√£o e Obter Recompensa" ]
```

> üí° **Exemplo Num√©rico:** Considere um 4-armed bandit (k=4) e $\epsilon = 0.2$. Em cada passo, h√° uma probabilidade de 0.2 de escolher uma a√ß√£o aleat√≥ria e 0.8 de escolher a a√ß√£o com a melhor estimativa atual. Se o algoritmo optar por explorar, cada uma das 4 a√ß√µes tem uma probabilidade de 0.2/4 = 0.05 de ser escolhida. Isso garante que, ao longo de muitas itera√ß√µes, cada a√ß√£o ser√° explorada uma quantidade n√£o nula de vezes.

O m√©todo de **sele√ß√£o de a√ß√£o UCB** emprega uma estrat√©gia mais criteriosa de explora√ß√£o, selecionando a√ß√µes de acordo com a seguinte regra:
$$A_t = \underset{a}{\operatorname{argmax}} \left[Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}\right]$$ [^11]
```mermaid
flowchart LR
    A["Para cada a√ß√£o a"] --> B{"Calcula Q_t(a)"};
    B --> C{"Calcula N_t(a)"};
    C --> D{"Calcula  " + "$c \\sqrt{\\frac{\\ln t}{N_t(a)}}$"};
    D --> E{"Soma Q_t(a) +  " + "$c \\sqrt{\\frac{\\ln t}{N_t(a)}}$"};
    E --> F{"Seleciona a a√ß√£o 'a' que maximiza a soma"};
    F --> G["A_t"]
```

Nessa express√£o, $N_t(a)$ representa o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o instante *t* [^11], e *c > 0* controla o n√≠vel de explora√ß√£o [^11]. O termo $\sqrt{\frac{\ln t}{N_t(a)}}$ mede a incerteza ou vari√¢ncia na estimativa do valor de *a* [^12]. A ideia central √© que a quantidade que √© maximizada √© uma esp√©cie de limite superior do poss√≠vel valor verdadeiro de *a*, com *c* determinando o n√≠vel de confian√ßa [^12]. A cada vez que uma a√ß√£o √© selecionada, sua incerteza presumivelmente diminui, pois $N_t(a)$ aumenta, enquanto a incerteza das outras a√ß√µes aumenta, pois o numerador $t$ cresce, mas $N_t(a)$ n√£o, tornando o m√©todo UCB eficiente na sele√ß√£o de a√ß√µes [^12]. O logaritmo natural garante que esses aumentos diminuam com o tempo, mas continuem indefinidamente, garantindo que todas as a√ß√µes sejam exploradas eventualmente [^12]. No entanto, a frequ√™ncia com que a√ß√µes menos valorizadas s√£o selecionadas diminui ao longo do tempo [^12].
```mermaid
graph LR
    A["Tempo t aumenta"] --> B{"N_t(a) aumenta para a a√ß√£o selecionada"};
    B --> C{"Incerteza da a√ß√£o selecionada diminui  " + "$\\sqrt{\\frac{\\ln t}{N_t(a)}}$"};
    A --> D{"Incerteza das outras a√ß√µes aumenta,  " + "$\\sqrt{\\frac{\\ln t}{N_t(a)}}$"};
    C --> F("Sele√ß√£o da A√ß√£o UCB")
    D --> F
```
> üí° **Exemplo Num√©rico:** Vamos considerar um caso com 3 a√ß√µes (k=3) e um valor de c=1. Suponha que no tempo t=10, temos as seguintes informa√ß√µes:
>
> *   A√ß√£o 1: $Q_{10}(1) = 2.5$, $N_{10}(1) = 5$
> *   A√ß√£o 2: $Q_{10}(2) = 1.8$, $N_{10}(2) = 2$
> *   A√ß√£o 3: $Q_{10}(3) = 2.0$, $N_{10}(3) = 3$
>
> Aplicando a f√≥rmula UCB:
>
> *   A√ß√£o 1: $2.5 + 1 \cdot \sqrt{\frac{\ln 10}{5}} \approx 2.5 + 1 \cdot \sqrt{\frac{2.3}{5}} \approx 2.5 + 0.68 \approx 3.18$
> *   A√ß√£o 2: $1.8 + 1 \cdot \sqrt{\frac{\ln 10}{2}} \approx 1.8 + 1 \cdot \sqrt{\frac{2.3}{2}} \approx 1.8 + 1.07 \approx 2.87$
> *   A√ß√£o 3: $2.0 + 1 \cdot \sqrt{\frac{\ln 10}{3}} \approx 2.0 + 1 \cdot \sqrt{\frac{2.3}{3}} \approx 2.0 + 0.88 \approx 2.88$
>
> Neste caso, a a√ß√£o 1 teria o maior valor UCB, indicando que, apesar de n√£o ter a maior estimativa de valor ($Q_{10}(a)$), a incerteza associada √† a√ß√£o 1 (devido a um $N_{10}(1)$ relativamente menor) faz com que seja a a√ß√£o mais promissora para explora√ß√£o.

**Lema 1** *No m√©todo UCB, para qualquer a√ß√£o $a$, $N_t(a)$ tende ao infinito quando $t$ tende ao infinito.*

*Prova:* O termo de explora√ß√£o em UCB √© dado por $c\sqrt{\frac{\ln t}{N_t(a)}}$. Este termo influencia a sele√ß√£o da a√ß√£o, fazendo com que a√ß√µes com baixa contagem de sele√ß√£o sejam escolhidas com maior frequ√™ncia. Como o numerador $\ln t$ cresce lentamente, todas as a√ß√µes ser√£o eventualmente selecionadas. Para provar formalmente, suponha por contradi√ß√£o que existe uma a√ß√£o $a^*$ tal que $N_t(a^*)$ permanece limitada. Isso implicaria que existe um tempo $t_0$ tal que para todos $t > t_0$, $N_t(a^*) < M$, onde $M$ √© uma constante. Mas como $\ln t$ cresce indefinidamente, em algum momento o termo de explora√ß√£o de $a^*$ dominar√° as outras a√ß√µes, for√ßando-a a ser selecionada, contrariando a hip√≥tese de que sua contagem permanece limitada. Portanto, $N_t(a)$ tende ao infinito para todas as a√ß√µes.

> üí° **Exemplo Num√©rico:** Imagine que a a√ß√£o 2 √© uma a√ß√£o sub√≥tima. No in√≠cio, ela pode ser selecionada algumas vezes devido √† alta incerteza, mas conforme t aumenta e $N_t(2)$ tamb√©m, o termo de explora√ß√£o $\sqrt{\frac{\ln t}{N_t(2)}}$ diminui, mas como o termo $\ln t$ tamb√©m aumenta (embora lentamente), a a√ß√£o 2 vai ser selecionada novamente, garantindo que, mesmo a√ß√µes sub√≥timas, ter√£o $N_t(a)$ tendendo ao infinito.

**Teorema 1** *O m√©todo UCB garante que todas as a√ß√µes ser√£o exploradas infinitamente, em um contexto de k-armed bandits com n√∫mero finito de a√ß√µes.*

*Prova:* Pelo Lema 1, sabemos que para qualquer a√ß√£o $a$, $N_t(a)$ tende ao infinito quando $t$ tende ao infinito. Isso significa que, com o tempo, cada a√ß√£o ser√° selecionada um n√∫mero infinito de vezes. Portanto, o m√©todo UCB garante a explora√ß√£o infinita de todas as a√ß√µes. A prova deriva diretamente do Lema 1.

O m√©todo UCB se diferencia do $\epsilon$-greedy por direcionar a explora√ß√£o, priorizando a√ß√µes com maior potencial, n√£o simplesmente escolhendo a√ß√µes aleatoriamente [^11]. Ao contr√°rio do m√©todo $\epsilon$-greedy, o m√©todo UCB tenta balancear tanto o qu√£o pr√≥ximo a a√ß√£o est√° de ser √≥tima, quanto a incerteza associada a sua estimativa de valor [^11].
```mermaid
graph LR
    subgraph "UCB"
        A["Estimativa do valor Q_t(a)"] --> B("Incerteza  " + "$c\\sqrt{\\frac{\\ln t}{N_t(a)}}$");
        B --> C("Sele√ß√£o de A√ß√£o");
        A-->C
    end
    subgraph "Œµ-greedy"
        D["Estimativa do valor"] --> E("Explora√ß√£o aleat√≥ria");
        D --> F("Aproveitamento ganancioso");
     end
     C --> G("Seleciona A√ß√£o");
     E --> G;
     F --> G
```

**Observa√ß√£o:** √â importante notar que o par√¢metro $c$ no m√©todo UCB √© crucial para balancear a explora√ß√£o e o aproveitamento. Um valor muito pequeno de $c$ resultar√° em um comportamento muito ganancioso, e a explora√ß√£o ser√° insuficiente. Por outro lado, um valor muito alto de $c$ pode resultar em muita explora√ß√£o, prejudicando o aprendizado. Portanto, a escolha do valor de $c$ √© um fator importante a ser considerado.

> üí° **Exemplo Num√©rico:** Se usarmos $c=0.1$ no exemplo anterior com t=10, as a√ß√µes seriam selecionadas muito mais baseadas em sua estimativa de valor ($Q_t(a)$), e o impacto da incerteza seria bem menor. A a√ß√£o 1 ainda teria o maior valor, aproximadamente 2.56, comparado com 2.03 para a a√ß√£o 3. Isso faria o agente ser mais ganancioso. Por outro lado, se usarmos $c=2$, a a√ß√£o 2 teria o maior valor, 1.8 + 2.14 = 3.94, mesmo com uma estimativa de valor mais baixa, mostrando como um c alto leva a uma explora√ß√£o mais agressiva.

### Conclus√£o
Em resumo, o m√©todo de sele√ß√£o de a√ß√£o UCB representa um avan√ßo em rela√ß√£o √†s abordagens explorat√≥rias mais simples, como $\epsilon$-greedy, ao direcionar a explora√ß√£o com base na incerteza das estimativas de valor das a√ß√µes [^11]. Este m√©todo se destaca por sua capacidade de ponderar a import√¢ncia tanto de a√ß√µes promissoras quanto de a√ß√µes menos exploradas, resultando em um aprendizado mais eficiente em cen√°rios estacion√°rios [^12]. Apesar de sua efic√°cia em problemas de bandit, sua extens√£o para ambientes n√£o estacion√°rios e outros problemas mais complexos ainda representam desafios [^12].

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken." *(Trecho de Cap√≠tulo 2: Multi-armed Bandits)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected:
$q_*(a) = E[R_t | A_t=a]$.
If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time stept as Qt(a). We would like Qt(a) to be close to q*(a)." *(Trecho de Cap√≠tulo 2: Multi-armed Bandits)*
[^3]: "The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as
$A_t = \operatorname{argmax}_a Q_t(a)$,
where $\operatorname{argmax}_a$ denotes the action a for which the expression that follows is maximized (with ties broken arbitrarily). Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability …õ, instead select randomly" *(Trecho de Cap√≠tulo 2: Multi-armed Bandits)*
[^11]: "One effective way of doing this is to select actions according to
$A_t = \underset{a}{\operatorname{argmax}} Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}$
where Int denotes the natural logarithm of t (the number that e ‚âà 2.71828 would have to be raised to in order to equal t), Nt(a) denotes the number of times that action a has" *(Trecho de Cap√≠tulo 2: Multi-armed Bandits)*
[^12]: "been selected prior to time t (the denominator in (2.1)), and the number c > 0 controls the degree of exploration. If N≈Ç(a) = 0, then a is considered to be a maximizing action.
The idea of this upper confidence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a's value. The quantity being max'ed over is thus a sort of upper bound on the possible true value of action a, with c determining the confidence level. Each time a is selected the uncertainty is presumably reduced: N≈Ç(a) increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than a is selected, t increases but Nt(a) does not; because t appears in the numerator, the uncertainty estimate increases.
The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time." *(Trecho de Cap√≠tulo 2: Multi-armed Bandits)*
