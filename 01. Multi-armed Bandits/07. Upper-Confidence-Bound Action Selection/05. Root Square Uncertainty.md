## Upper-Confidence-Bound Action Selection: A Profundidade da Incerteza

### Introdu√ß√£o
A aprendizagem por refor√ßo distingue-se de outras formas de aprendizado por utilizar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de instruir atrav√©s de a√ß√µes corretas [^1]. Isso cria a necessidade de explora√ß√£o ativa para comportamentos desejados. No contexto da explora√ß√£o em **multi-armed bandits**, √© crucial equilibrar entre **explora√ß√£o** (tentar novas a√ß√µes para descobrir melhores recompensas) e **explota√ß√£o** (utilizar o conhecimento atual para obter recompensas m√°ximas) [^2]. Este cap√≠tulo foca no problema de *k-armed bandit*, uma configura√ß√£o n√£o associativa onde a melhor a√ß√£o n√£o depende da situa√ß√£o, mas sim de suas recompensas m√©dias [^1]. O problema √© escolher, em cada passo, uma entre $k$ a√ß√µes, cada uma com uma distribui√ß√£o de probabilidade estacion√°ria diferente. O objetivo √© maximizar a recompensa total esperada ao longo do tempo [^1]. Dentre os m√©todos para equilibrar a explora√ß√£o e explota√ß√£o, destaca-se o *Upper-Confidence-Bound (UCB) action selection*, que vamos explorar em detalhes a seguir.

### Conceitos Fundamentais
O m√©todo de sele√ß√£o de a√ß√£o *Upper-Confidence-Bound* (UCB) √© uma abordagem determin√≠stica para balancear explora√ß√£o e explota√ß√£o em problemas de *k-armed bandit* [^2]. Ao contr√°rio dos m√©todos $\epsilon$-greedy, que exploram a√ß√µes n√£o-greedy indiscriminadamente, o UCB seleciona a√ß√µes considerando o potencial para serem √≥timas, dada a incerteza de suas estimativas de valor [^2]. A formula√ß√£o do m√©todo UCB √© dada por:

$$A_t = \underset{a}{\operatorname{argmax}} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]$$

Onde:
-   $A_t$ √© a a√ß√£o selecionada no instante de tempo $t$ [^2].
-   $Q_t(a)$ √© a estimativa do valor da a√ß√£o $a$ no instante de tempo $t$ [^2].
-   $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada antes do tempo $t$ [^2].
-   $c > 0$ √© um par√¢metro que controla o grau de explora√ß√£o [^2].
-   $\ln t$ √© o logaritmo natural do instante de tempo $t$ [^2].

**O termo da raiz quadrada,**  $c \sqrt{\frac{\ln t}{N_t(a)}}$, √© uma **medida da incerteza ou vari√¢ncia na estimativa do valor da a√ß√£o $a$**. O conceito central do UCB √© que a sele√ß√£o de a√ß√µes √© baseada em uma esp√©cie de "limite superior" do valor verdadeiro da a√ß√£o $a$ [^2]. Este termo aumenta a estimativa do valor da a√ß√£o com a incerteza, incentivando a explora√ß√£o de a√ß√µes pouco exploradas [^2]. O par√¢metro $c$ controla a confian√ßa nesse limite superior, influenciando diretamente o balan√ßo entre explora√ß√£o e explota√ß√£o.

```mermaid
graph LR
    subgraph "Componentes do UCB"
    A[A_t: "A√ß√£o Selecionada"]
    Q[Q_t(a): "Estimativa do Valor da A√ß√£o"]
    N[N_t(a): "N√∫mero de Sele√ß√µes da A√ß√£o"]
    c["c: Par√¢metro de Explora√ß√£o"]
    ln_t["ln t: Logaritmo do Tempo"]
    UCB[UCB(a): "Valor UCB"]
    end
    Q --> UCB
    N --> UCB
    c --> UCB
    ln_t --> UCB
    UCB --> A
   style UCB fill:#f9f,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Consideremos um cen√°rio com 3 a√ß√µes (k=3). Vamos analisar como o UCB funciona em alguns passos. Inicialmente, todas as a√ß√µes foram escolhidas zero vezes, e assumimos $c=1$.
>
> **Passo 1:**
>
>   - $N_1(1) = 0$, $N_1(2) = 0$, $N_1(3) = 0$
>   - $Q_1(1) = 0$, $Q_1(2) = 0$, $Q_1(3) = 0$ (inicializado com 0)
>   - UCB Values:
>     - Action 1: $0 + 1 \sqrt{\frac{\ln 1}{0}}$ = $\infty$ (inicialmente, qualquer a√ß√£o que n√£o foi selecionada possui um valor muito alto devido √† divis√£o por zero. Na pr√°tica, inicializa-se $N_t(a)$ com 1 ou um valor pequeno para evitar a divis√£o por zero). Para fins de ilustra√ß√£o, vamos considerar um $N_t(a)$ inicial de 1 para o primeiro passo, em vez de 0. Nesse caso, $\sqrt{\frac{\ln 1}{1}}=0$, e o valor UCB se torna 0 para todas as a√ß√µes.
>     - Action 2: $0 + 1 \sqrt{\frac{\ln 1}{1}} = 0$
>     - Action 3: $0 + 1 \sqrt{\frac{\ln 1}{1}} = 0$
>
>  Todas as a√ß√µes tem o mesmo valor UCB, ent√£o a primeira a√ß√£o √© escolhida aleatoriamente.
>
>  **Passo 2:**
>
>  Assumindo que a a√ß√£o 1 foi escolhida no passo 1 e recebemos uma recompensa de 0.5:
>  - $N_2(1) = 1$, $N_2(2) = 0$, $N_2(3) = 0$
>  - $Q_2(1) = 0.5$, $Q_2(2) = 0$, $Q_2(3) = 0$
> - UCB Values:
>     - Action 1: $0.5 + 1 \sqrt{\frac{\ln 2}{1}} \approx 0.5 + 0.83 = 1.33$
>     - Action 2: $0 + 1 \sqrt{\frac{\ln 2}{1}} \approx 0 + 0.83 = 0.83$
>     - Action 3: $0 + 1 \sqrt{\frac{\ln 2}{1}} \approx 0 + 0.83 = 0.83$
>  Agora a a√ß√£o 1 tem o maior valor UCB e ser√° selecionada.
>  **Passo 3:**
>
>  Assumindo que a a√ß√£o 1 foi escolhida novamente e recebemos uma recompensa de 0.6:
>   - $N_3(1) = 2$, $N_3(2) = 0$, $N_3(3) = 0$
>   - $Q_3(1) = \frac{0.5 + 0.6}{2} = 0.55$, $Q_3(2) = 0$, $Q_3(3) = 0$
>   - UCB Values:
>     - Action 1: $0.55 + 1 \sqrt{\frac{\ln 3}{2}} \approx 0.55 + 0.74 = 1.29$
>     - Action 2: $0 + 1 \sqrt{\frac{\ln 3}{1}} \approx 0 + 1.09 = 1.09$
>     - Action 3: $0 + 1 \sqrt{\frac{\ln 3}{1}} \approx 0 + 1.09 = 1.09$
> A a√ß√£o 1 ainda tem o maior valor UCB. Observe como o termo de incerteza diminui para a a√ß√£o 1 ao ser selecionada mais vezes, incentivando eventualmente a explora√ß√£o de outras a√ß√µes.
>
>  Este exemplo ilustra como o UCB direciona a explora√ß√£o, favorecendo a√ß√µes com alta incerteza e recompensas potencialmente boas.

**Explica√ß√£o detalhada do termo de incerteza:**
-   $N_t(a)$: Como $N_t(a)$ est√° no denominador, quanto mais a a√ß√£o $a$ foi selecionada, menor ser√° o valor do termo de incerteza, o que √© intuitivo. Se uma a√ß√£o foi selecionada muitas vezes, temos mais confian√ßa no seu valor estimado, o que diminui a necessidade de explora√ß√£o adicional [^2].
-   $\ln t$: O logaritmo natural do tempo, $\ln t$, garante que a incerteza aumente com o tempo, incentivando a explora√ß√£o de a√ß√µes menos exploradas [^2]. O aumento de $\ln t$ √© desacelerado com o tempo, o que permite que o algoritmo eventualmente se concentre em explorar as a√ß√µes que mais prometem recompensa.
-   $c$: O par√¢metro $c$ controla a import√¢ncia da incerteza na sele√ß√£o das a√ß√µes. Um $c$ maior resultar√° em maior explora√ß√£o, enquanto um $c$ menor diminuir√° a explora√ß√£o, favorecendo a√ß√µes com valores j√° estimados mais altos [^2].

```mermaid
graph LR
    subgraph "Impacto de N_t(a) no termo de incerteza"
    N_t_a_baixo["N_t(a) Baixo (A√ß√£o Pouco Explorada)"]
    N_t_a_alto["N_t(a) Alto (A√ß√£o Muito Explorada)"]
    Incerteza_alta["Incerteza Alta"]
    Incerteza_baixa["Incerteza Baixa"]
    end
    N_t_a_baixo --> Incerteza_alta
    N_t_a_alto --> Incerteza_baixa
    Incerteza_alta --> Explora√ß√£o["Incentiva Explora√ß√£o"]
    Incerteza_baixa --> Explota√ß√£o["Incentiva Explota√ß√£o"]
```

> üí° **Exemplo Num√©rico:**
> Vamos comparar o efeito de diferentes valores de $c$. Considere duas a√ß√µes, onde ap√≥s 10 intera√ß√µes, a a√ß√£o 1 foi escolhida 7 vezes com uma m√©dia de recompensa $Q_t(1) = 0.7$, e a a√ß√£o 2 foi escolhida 3 vezes com uma m√©dia de recompensa $Q_t(2) = 0.5$.
>
> **Caso 1: $c = 0.5$ (Menos Explora√ß√£o)**
> - UCB(A√ß√£o 1) = $0.7 + 0.5 \sqrt{\frac{\ln 10}{7}} \approx 0.7 + 0.5 * 0.66 \approx 1.03$
> - UCB(A√ß√£o 2) = $0.5 + 0.5 \sqrt{\frac{\ln 10}{3}} \approx 0.5 + 0.5 * 0.92 \approx 0.96$
> A a√ß√£o 1 √© selecionada devido ao seu valor UCB ligeiramente superior.
>
> **Caso 2: $c = 2$ (Mais Explora√ß√£o)**
> - UCB(A√ß√£o 1) = $0.7 + 2 \sqrt{\frac{\ln 10}{7}} \approx 0.7 + 2 * 0.66 \approx 2.02$
> - UCB(A√ß√£o 2) = $0.5 + 2 \sqrt{\frac{\ln 10}{3}} \approx 0.5 + 2 * 0.92 \approx 2.34$
> A a√ß√£o 2 √© selecionada devido ao seu alto termo de incerteza, apesar de sua recompensa m√©dia ser menor.
>
> Isso demonstra que valores maiores de $c$ favorecem a explora√ß√£o de a√ß√µes menos exploradas, mesmo que as a√ß√µes mais exploradas tenham um valor m√©dio ligeiramente superior.

**Lema 1:** *A fun√ß√£o  $f(t) = \frac{\ln t}{N_t(a)}$ tem um crescimento sublinear com rela√ß√£o a t, demonstrando que a explora√ß√£o √© reduzida com o tempo.*
*Prova:*
Dado que $N_t(a)$ n√£o cresce mais r√°pido que $t$, ent√£o $\lim_{t \to \infty} \frac{N_t(a)}{t}$ √© um valor menor ou igual a 1. Portanto, temos que:
$$ \lim_{t \to \infty} \frac{\ln t}{N_t(a)} \leq \lim_{t \to \infty} \frac{\ln t}{t} = 0 $$

Com isso, provamos que o termo de incerteza diminui com o tempo, embora de forma sublinear.

**Lema 1.1:** *Se $N_t(a)$ cresce proporcionalmente a $t$ (i.e., $N_t(a) = \alpha t$ para alguma constante $0 < \alpha \leq 1$), ent√£o  $f(t) = \frac{\ln t}{N_t(a)}$ decresce asimptoticamente como $\frac{\ln t}{\alpha t}$, demonstrando um decr√©scimo mais r√°pido da incerteza com o tempo em compara√ß√£o com a an√°lise geral.*
*Prova:*
Substituindo $N_t(a) = \alpha t$ na fun√ß√£o $f(t)$, obtemos:
$$f(t) = \frac{\ln t}{\alpha t} = \frac{1}{\alpha} \frac{\ln t}{t}$$
Como $\lim_{t \to \infty} \frac{\ln t}{t} = 0$, temos que $\lim_{t \to \infty} \frac{1}{\alpha} \frac{\ln t}{t} = 0$. A compara√ß√£o com o Lema 1 mostra que, quando a frequ√™ncia de sele√ß√£o cresce linearmente com o tempo, a incerteza decresce mais rapidamente em rela√ß√£o ao cen√°rio geral onde $N_t(a)$ cresce arbitrariamente devagar.
This demonstrates that the uncertainty term can decay more quickly if actions are selected more often.

```mermaid
graph LR
    subgraph "Decaimento da Incerteza"
    f_t["f(t) = ln(t) / N_t(a)"]
    N_t_a["N_t(a) Cresce Lento"]
    N_t_a_alpha["N_t(a) = alpha * t"]
    Decaimento_sublinear["Decaimento Sublinear"]
    Decaimento_rapido["Decaimento Mais R√°pido"]
    end
    f_t --> N_t_a
    f_t --> N_t_a_alpha
     N_t_a --> Decaimento_sublinear
     N_t_a_alpha --> Decaimento_rapido
```

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar o Lema 1.1, vamos comparar duas a√ß√µes em um cen√°rio simplificado com $\alpha = 0.5$ para a a√ß√£o 1 e $\alpha = 0.1$ para a a√ß√£o 2. Suponha que $N_t(a) = \alpha t$.
>
> **A√ß√£o 1:**
>  - $N_t(1) = 0.5t$
>  - $f(t) = \frac{\ln t}{0.5t} = 2 \frac{\ln t}{t}$
>
> **A√ß√£o 2:**
>  - $N_t(2) = 0.1t$
>  - $f(t) = \frac{\ln t}{0.1t} = 10 \frac{\ln t}{t}$
>
> O fator $\frac{\ln t}{t}$ decresce com o tempo, mas como a√ß√£o 1 tem $\alpha$ maior, sua incerteza $f(t)$ decresce mais rapidamente do que a√ß√£o 2. Ou seja, a a√ß√£o 1 ser√° explorada menos rapidamente do que a√ß√£o 2 em passos futuros. Se ambas as a√ß√µes tem uma recompensa m√©dia similar, eventualmente a√ß√£o 1 vai ter uma maior probabilidade de ser selecionada.

A formula√ß√£o do UCB garante que a explora√ß√£o √© dirigida, n√£o aleat√≥ria. A√ß√µes que parecem boas (alto $Q_t(a)$) e t√™m alta incerteza (baixo $N_t(a)$) ser√£o mais favorecidas, o que √© desej√°vel em situa√ß√µes de aprendizado [^2]. A efic√°cia do UCB depende da escolha apropriada do par√¢metro $c$ e da natureza do problema [^2]. Embora eficaz em problemas estacion√°rios, a vers√£o b√°sica do UCB pode n√£o se adaptar t√£o bem a ambientes n√£o-estacion√°rios como os m√©todos de passo de tamanho vari√°vel [^2].

**Proposi√ß√£o 1:** *Se a recompensa de cada a√ß√£o √© limitada no intervalo $[0, 1]$, ent√£o o arrependimento do UCB √© limitado por $O(\sqrt{T \ln T})$, onde T √© o n√∫mero total de passos.*

*Discuss√£o:* Essa proposi√ß√£o √© uma vers√£o simplificada do limite de arrependimento para o UCB. A prova formal envolve analisar o n√∫mero de vezes que o algoritmo escolhe a√ß√µes sub√≥timas e o custo cumulativo dessas escolhas. O fator $\sqrt{T}$ surge da rela√ß√£o entre explora√ß√£o e o n√∫mero de passos, enquanto o termo $\sqrt{\ln T}$ surge do termo de incerteza do UCB. Este limite de arrependimento sublinear em rela√ß√£o a T indica que, no longo prazo, o UCB converge para a√ß√µes √≥timas. Embora a demonstra√ß√£o completa seja mais complexa e envolva argumentos probabil√≠sticos e combinat√≥rios, entender a intui√ß√£o por tr√°s deste limite √© fundamental.

```mermaid
graph LR
    subgraph "Arrependimento do UCB"
    T["T: N√∫mero total de passos"]
    Limite_arrependimento["O(sqrt(T * ln(T)))"]
    sqrt_T["sqrt(T): Rela√ß√£o com Explora√ß√£o"]
    sqrt_ln_T["sqrt(ln(T)): Termo de Incerteza"]
    end
    T --> Limite_arrependimento
    Limite_arrependimento --> sqrt_T
    Limite_arrependimento --> sqrt_ln_T
```

**Teorema 1:** *O UCB1 algorithm, que √© uma vers√£o espec√≠fica do UCB, garante que o n√∫mero de vezes que uma a√ß√£o sub√≥tima √© selecionada √© logar√≠tmico em rela√ß√£o ao tempo, o que implica uma converg√™ncia r√°pida para a√ß√µes √≥timas.*

*Discuss√£o:* O teorema 1 estabelece uma propriedade fundamental do UCB1. A prova do teorema envolve analisar a rela√ß√£o entre o n√∫mero de sele√ß√µes das a√ß√µes √≥timas e sub√≥timas. A demonstra√ß√£o do limite logar√≠tmico no n√∫mero de escolhas sub√≥timas depende de argumentos probabil√≠sticos e de an√°lise assint√≥tica, destacando a efic√°cia do algoritmo em focar na explora√ß√£o inicial, e rapidamente convergir para explora√ß√£o da a√ß√£o √≥tima.

**Corol√°rio 1.1** *Dado que o arrependimento √© limitado por $O(\sqrt{T \ln T})$ e o n√∫mero de vezes que uma a√ß√£o sub√≥tima √© escolhida cresce logaritmicamente, podemos concluir que o UCB √© uma estrat√©gia eficiente para lidar com o dilema de explora√ß√£o-explota√ß√£o em cen√°rios de bandit.*
*Discuss√£o*: Este corol√°rio resume as propriedades importantes do algoritmo UCB e conecta os limites de arrependimento com o n√∫mero de escolhas sub√≥timas. Ele destaca que o algoritmo n√£o apenas garante um arrependimento sublinear, mas tamb√©m converge rapidamente para a√ß√µes √≥timas, o que justifica sua efic√°cia em ambientes de aprendizado por refor√ßo.

### Conclus√£o
A sele√ß√£o de a√ß√£o *Upper-Confidence-Bound* (UCB) oferece uma abordagem eficaz e determin√≠stica para equilibrar explora√ß√£o e explota√ß√£o em problemas de *k-armed bandit*. O termo da raiz quadrada na f√≥rmula do UCB quantifica a incerteza das estimativas de valor das a√ß√µes, incentivando a explora√ß√£o de a√ß√µes menos exploradas, ao mesmo tempo em que explora as a√ß√µes que apresentam maior potencial de recompensa [^2]. A an√°lise detalhada do termo de incerteza demonstra como a explora√ß√£o √© controlada e ajustada ao longo do tempo. Embora o m√©todo UCB seja robusto e eficaz em ambientes estacion√°rios, √© importante considerar suas limita√ß√µes em ambientes n√£o-estacion√°rios. Em tais cen√°rios, pode ser mais adequado empregar m√©todos adaptativos e mais sofisticados para obter um melhor balan√ßo entre explora√ß√£o e explota√ß√£o. A compreens√£o do termo de incerteza e da sua influ√™ncia no algoritmo UCB √© crucial para a implementa√ß√£o eficaz deste m√©todo em problemas reais de aprendizado por refor√ßo.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior."
[^2]: "The idea of this upper confidence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a‚Äôs value. The quantity being max‚Äôed over is thus a sort of upper bound on the possible true value of action a, with c determining the confidence level."
