## Upper-Confidence-Bound Action Selection
### IntroduÃ§Ã£o
Em problemas de **reinforcement learning**, a exploraÃ§Ã£o Ã© essencial, dado que sempre existe incerteza acerca da precisÃ£o das estimativas de valor de aÃ§Ã£o. As aÃ§Ãµes *greedy* sÃ£o aquelas que, no momento, parecem ser as melhores, mas aÃ§Ãµes alternativas podem realmente ser superiores. A seleÃ§Ã£o de aÃ§Ãµes $\epsilon$-greedy forÃ§a a experimentaÃ§Ã£o de aÃ§Ãµes nÃ£o-gananciosas, embora de forma indiscriminada, sem preferÃªncia pelas que sÃ£o quase *greedy* ou mais incertas. Uma abordagem mais inteligente Ã© selecionar aÃ§Ãµes nÃ£o-gananciosas com base no seu potencial de otimalidade, considerando tanto as estimativas quanto as incertezas [^1].

### Conceitos Fundamentais
A **Upper-Confidence-Bound (UCB)** action selection aborda esta necessidade de forma eficaz [^1]. A ideia central Ã© que o termo de raiz quadrada na seguinte expressÃ£o:
$$
A_t = \underset{a}{\text{argmax}} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
$$
funciona como uma medida de incerteza ou variaÃ§Ã£o na estimativa do valor de uma aÃ§Ã£o $a$. O valor maximizado Ã©, portanto, um limite superior para o valor real da aÃ§Ã£o $a$, com $c$ controlando o nÃ­vel de confianÃ§a [^1].
```mermaid
graph LR
    A[ "AÃ§Ã£o a" ]
    Q[ "Q_t(a): EstimaÃ§Ã£o do valor da aÃ§Ã£o a" ]
    U[ "c * sqrt(ln(t) / N_t(a)): Termo de Incerteza" ]
    S[ "Soma: Q_t(a) + Termo de Incerteza" ]
    M[ "A_t: AÃ§Ã£o selecionada" ]
    A --> Q
    A --> U
    Q --> S
    U --> S
    S --> M
    style S fill:#f9f,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3 stroke:#333,stroke-width:1px
```

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Imagine que temos 3 aÃ§Ãµes (a1, a2, a3) e estamos no instante de tempo t=5. As estimativas de valor e contagens das aÃ§Ãµes sÃ£o:
> - $Q_5(a1) = 10$, $N_5(a1) = 2$
> - $Q_5(a2) = 12$, $N_5(a2) = 1$
> - $Q_5(a3) = 8$, $N_5(a3) = 1$
>
> Usando a UCB com $c = 1$, calculamos:
>
> - $UCB(a1) = 10 + 1 \cdot \sqrt{\frac{\ln 5}{2}} = 10 + \sqrt{\frac{1.609}{2}} \approx 10 + 0.897 = 10.897$
> - $UCB(a2) = 12 + 1 \cdot \sqrt{\frac{\ln 5}{1}} = 12 + \sqrt{1.609} \approx 12 + 1.268 = 13.268$
> - $UCB(a3) = 8 + 1 \cdot \sqrt{\frac{\ln 5}{1}} = 8 + \sqrt{1.609} \approx 8 + 1.268 = 9.268$
>
> A aÃ§Ã£o selecionada serÃ¡ $a2$, pois $UCB(a2)$ Ã© o maior. Note que, apesar de $Q_5(a2) > Q_5(a1)$, a incerteza de $a2$ Ã© maior.

**ProposiÃ§Ã£o 1:**  *A aÃ§Ã£o selecionada $A_t$ pela UCB Ã© sempre uma aÃ§Ã£o cujo valor estimado mais o termo de incerteza Ã© mÃ¡ximo dentre todas as aÃ§Ãµes.  Ou seja, para qualquer aÃ§Ã£o $b$ diferente de $A_t$, temos $Q_t(A_t) + c \sqrt{\frac{\ln t}{N_t(A_t)}} \geq Q_t(b) + c \sqrt{\frac{\ln t}{N_t(b)}}$.*
*Prova:*  Esta propriedade Ã© uma consequÃªncia direta da definiÃ§Ã£o da UCB, que seleciona a aÃ§Ã£o que maximiza a expressÃ£o $Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}$. Portanto, se $A_t$ Ã© a aÃ§Ã£o que maximiza essa expressÃ£o, entÃ£o ela deve ser maior ou igual a qualquer outra aÃ§Ã£o $b$ nessa mesma expressÃ£o. $\blacksquare$

**Detalhes da EquaÃ§Ã£o 2.10:**
- $A_t$: Representa a aÃ§Ã£o selecionada no instante de tempo $t$ [^1].
- $Q_t(a)$: Ã‰ a estimativa do valor da aÃ§Ã£o $a$ no instante de tempo $t$ [^1].
- $c$: Ã‰ um parÃ¢metro que controla o grau de exploraÃ§Ã£o. Um valor mais alto de $c$ incentiva mais a exploraÃ§Ã£o [^1].
- $t$: Ã‰ o instante de tempo atual [^1].
- $N_t(a)$: Ã‰ o nÃºmero de vezes que a aÃ§Ã£o $a$ foi selecionada antes do instante de tempo $t$ [^1].
- $ln t$: Ã‰ o logaritmo natural de $t$. Este termo garante que a exploraÃ§Ã£o diminua com o tempo [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Se temos $t=10$ e a aÃ§Ã£o $a$ foi selecionada $N_t(a)=2$ vezes, o termo de incerteza Ã© $\sqrt{\frac{\ln 10}{2}} = \sqrt{\frac{2.303}{2}} \approx 1.074$. Se $t=100$ e $N_t(a)=20$, o termo de incerteza Ã© $\sqrt{\frac{\ln 100}{20}} = \sqrt{\frac{4.605}{20}} \approx 0.48$. Note que, mesmo com mais seleÃ§Ãµes da aÃ§Ã£o, o aumento de $t$ reduz o termo de incerteza.

**Lemma 1:** *O termo de incerteza $\sqrt{\frac{\ln t}{N_t(a)}}$ decresce com o aumento de $N_t(a)$ e cresce com o aumento de $t$.*
*Prova:*
A derivada de $\sqrt{\frac{\ln t}{N_t(a)}}$ com relaÃ§Ã£o a $N_t(a)$ Ã©:
$$
\frac{\partial}{\partial N_t(a)} \sqrt{\frac{\ln t}{N_t(a)}} = \sqrt{\ln t} \cdot \frac{-1}{2} {N_t(a)}^{-\frac{3}{2}} = -\frac{\sqrt{\ln t}}{2 {N_t(a)}^{\frac{3}{2}}}
$$
Como essa derivada Ã© negativa, o termo diminui com o aumento de $N_t(a)$. A derivada com relaÃ§Ã£o a $t$ Ã©:
$$
\frac{\partial}{\partial t} \sqrt{\frac{\ln t}{N_t(a)}} = \frac{1}{2} \sqrt{\frac{1}{N_t(a) \ln t}} \frac{1}{t}  = \frac{1}{2t} \sqrt{\frac{1}{N_t(a) \ln t}}
$$
Como essa derivada Ã© positiva, o termo aumenta com o aumento de $t$. $\blacksquare$
```mermaid
graph LR
subgraph "Termo de Incerteza"
    A["sqrt(ln(t) / N_t(a))"]
end
    B["N_t(a) aumenta"]
    C["t aumenta"]
    D["Incerteza diminui"]
    E["Incerteza aumenta"]

    B --> D
    C --> E
    A --> B
    A --> C
    style A fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3 stroke:#333,stroke-width:1px
```

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Vamos analisar o impacto de $N_t(a)$ no termo de incerteza com $t=100$ e $c=1$:
>
> | $N_t(a)$ | Incerteza: $\sqrt{\frac{\ln t}{N_t(a)}}$ |
> |----------|-----------------------------------|
> | 1        | $\sqrt{\frac{\ln 100}{1}} \approx 2.145$    |
> | 5        | $\sqrt{\frac{\ln 100}{5}} \approx 0.96$    |
> | 25       | $\sqrt{\frac{\ln 100}{25}} \approx 0.429$  |
> | 50       | $\sqrt{\frac{\ln 100}{50}} \approx 0.303$  |
>
> Como podemos ver, quanto maior o $N_t(a)$, menor a incerteza.

**Lemma 1.1:** *Assumindo que $N_t(a) > 0$, o termo de incerteza $\sqrt{\frac{\ln t}{N_t(a)}}$ converge para zero quando $t$ tende a infinito e $N_t(a)$ cresce na mesma ordem de $t$.*

*Prova:* Seja $N_t(a) = k t$ para uma constante $k > 0$. EntÃ£o, o termo de incerteza se torna $\sqrt{\frac{\ln t}{k t}} = \frac{\sqrt{\ln t}}{\sqrt{kt}}$.  Para mostrar que converge para zero, basta verificar que $\lim_{t\to\infty} \frac{\sqrt{\ln t}}{\sqrt{kt}} = \lim_{t\to\infty} \frac{\sqrt{\ln t}}{\sqrt{k}\sqrt{t}} = 0$. De fato, aplicando a regra de L'HÃ´pital para o limite $\lim_{t \to \infty} \frac{\sqrt{\ln t}}{\sqrt{t}}$, temos: $\lim_{t \to \infty} \frac{\frac{1}{2\sqrt{\ln t}}\cdot \frac{1}{t}}{\frac{1}{2\sqrt{t}}} = \lim_{t \to \infty} \frac{\sqrt{t}}{t\sqrt{\ln t}} = \lim_{t \to \infty} \frac{1}{\sqrt{t\ln t}}= 0$. Como $k$ Ã© uma constante positiva,  $\lim_{t\to\infty} \frac{\sqrt{\ln t}}{\sqrt{kt}} = 0$. $\blacksquare$
```mermaid
graph LR
subgraph "Limite do Termo de Incerteza"
    A["Limite: lim t->inf sqrt(ln(t)/N_t(a))"]
    B["N_t(a) = k*t"]
    C["lim t->inf sqrt(ln(t) / k*t)"]
    D["lim t->inf sqrt(ln(t))/sqrt(k*t)"]
    E["Regra de L'HÃ´pital"]
    F["lim t->inf 1/sqrt(t*ln(t)) = 0"]
    G["Converge para zero"]
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    style A fill:#ccf,stroke:#333,stroke-width:2px
  linkStyle 0,1,2,3,4,5,6 stroke:#333,stroke-width:1px
end
```

**CorolÃ¡rio 1:** *A UCB seleciona aÃ§Ãµes menos exploradas com mais frequÃªncia, dado que o denominador $N_t(a)$ diminui a incerteza quando a aÃ§Ã£o $a$ jÃ¡ foi selecionada vÃ¡rias vezes. A seleÃ§Ã£o de uma aÃ§Ã£o $a$ reduz a incerteza associada a essa aÃ§Ã£o e, ao mesmo tempo, o aumento do tempo $t$ aumenta a incerteza para todas as aÃ§Ãµes* [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Vamos supor que temos duas aÃ§Ãµes: $a1$ e $a2$. Inicialmente, $Q(a1) = 5$, $Q(a2) = 7$, e $N(a1)=N(a2)=1$, com $c=1$.
>
> - **t=2:** Calculamos $UCB(a1)$ e $UCB(a2)$:
>  - $UCB(a1) = 5 + \sqrt{\frac{\ln 2}{1}} \approx 5 + 0.833 \approx 5.833$
>  - $UCB(a2) = 7 + \sqrt{\frac{\ln 2}{1}} \approx 7 + 0.833 \approx 7.833$
>   $a2$ Ã© selecionada. $N_2(a2) = 2$
> - **t=3:**
>  - $UCB(a1) = 5 + \sqrt{\frac{\ln 3}{1}} \approx 5 + 1.04 \approx 6.04$
>  - $UCB(a2) = 7 + \sqrt{\frac{\ln 3}{2}} \approx 7 + 0.74 \approx 7.74$
>   $a2$ Ã© selecionada novamente. $N_3(a2) = 3$
> - **t=4:**
>  - $UCB(a1) = 5 + \sqrt{\frac{\ln 4}{1}} \approx 5 + 1.17 \approx 6.17$
>  - $UCB(a2) = 7 + \sqrt{\frac{\ln 4}{3}} \approx 7 + 0.68 \approx 7.68$
> Mesmo $a2$ tendo um valor inicial maior, o aumento de $t$ e a reduÃ§Ã£o da incerteza de $a2$ devido a repetidas seleÃ§Ãµes levarÃ£o $a1$ a ser selecionada em algum momento futuro.

**CorolÃ¡rio 1.1:** *Quando o tempo $t$ tende a infinito, e se todas as aÃ§Ãµes sÃ£o selecionadas infinitamente, o termo de incerteza $\sqrt{\frac{\ln t}{N_t(a)}}$ converge para zero para todas as aÃ§Ãµes.*

*Prova:* Do Lema 1.1, sabemos que se $N_t(a)$ crescer na mesma ordem de $t$, o termo de incerteza converge para zero. Se todas as aÃ§Ãµes sÃ£o selecionadas infinitamente, entÃ£o para cada aÃ§Ã£o $a$,  $N_t(a)$ tende a infinito,  logo $N_t(a)$ cresce na mesma ordem de $t$ e o termo de incerteza converge para zero. $\blacksquare$
```mermaid
graph LR
subgraph "ConvergÃªncia do Termo de Incerteza"
    A["t -> inf e todas as aÃ§Ãµes sÃ£o selecionadas inf"]
    B["Para cada aÃ§Ã£o a, N_t(a) -> inf"]
    C["N_t(a) cresce na mesma ordem de t"]
    D["sqrt(ln(t) / N_t(a)) converge para zero"]
    A --> B
    B --> C
    C --> D
    style A fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2 stroke:#333,stroke-width:1px
end
```

Sendo assim, toda vez que uma aÃ§Ã£o $a$ Ã© selecionada, a incerteza Ã© presumivelmente reduzida, pois $N_t(a)$ aumenta. O inverso ocorre quando uma aÃ§Ã£o diferente Ã© selecionada, jÃ¡ que $t$ aumenta enquanto $N_t(a)$ nÃ£o, aumentando a incerteza [^1]. O uso do logaritmo natural garante que o aumento da incerteza diminua com o tempo, mas permanece ilimitado, o que implica que todas as aÃ§Ãµes serÃ£o eventualmente selecionadas. No entanto, aÃ§Ãµes com baixas estimativas de valor ou selecionadas com frequÃªncia, serÃ£o menos selecionadas ao longo do tempo [^1].

**ObservaÃ§Ã£o 1:** *A constante $c$ na equaÃ§Ã£o da UCB ajusta a taxa de exploraÃ§Ã£o.  Um valor maior de $c$ leva a uma maior exploraÃ§Ã£o, pois aumenta o peso do termo de incerteza, enquanto um valor menor de $c$ favorece a explotaÃ§Ã£o.*

> ðŸ’¡ **Exemplo NumÃ©rico:**
> Considere que temos duas aÃ§Ãµes, $a1$ e $a2$, com $Q_t(a1) = 5$ e $Q_t(a2) = 7$. Se $t=10$ e $N_t(a1) = 2$ e $N_t(a2) = 5$.
>
> - Com $c=0.5$:
>   - $UCB(a1) = 5 + 0.5 \cdot \sqrt{\frac{\ln 10}{2}} \approx 5 + 0.5 \cdot 1.074 \approx 5.537$
>   - $UCB(a2) = 7 + 0.5 \cdot \sqrt{\frac{\ln 10}{5}} \approx 7 + 0.5 \cdot 0.678 \approx 7.339$
>   $a2$ Ã© selecionada.
> - Com $c=2$:
>   - $UCB(a1) = 5 + 2 \cdot \sqrt{\frac{\ln 10}{2}} \approx 5 + 2 \cdot 1.074 \approx 7.148$
>   - $UCB(a2) = 7 + 2 \cdot \sqrt{\frac{\ln 10}{5}} \approx 7 + 2 \cdot 0.678 \approx 8.356$
>   $a2$ Ã© selecionada.
>
>   Note que o maior valor de $c$ favorece a exploraÃ§Ã£o. Se $N_t(a1)$ fosse muito menor, o valor de $UCB(a1)$ poderia ultrapassar $UCB(a2)$ com $c=2$ devido ao alto fator de incerteza.
```mermaid
graph LR
    A["Valor de c"]
    B["c maior"]
    C["c menor"]
    D["Maior exploraÃ§Ã£o"]
    E["Maior explotaÃ§Ã£o"]
    A --> B
    A --> C
    B --> D
    C --> E
    style A fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3 stroke:#333,stroke-width:1px
```

### ConclusÃ£o
A UCB Ã© eficaz em diversas situaÃ§Ãµes e frequentemente apresenta bom desempenho, mas apresenta limitaÃ§Ãµes em cenÃ¡rios mais complexos, como em problemas nÃ£o-estacionÃ¡rios ou com grandes espaÃ§os de estados, especialmente quando se usa aproximaÃ§Ã£o de funÃ§Ãµes. Nestes casos, as ideias da UCB tornam-se menos prÃ¡ticas [^1]. Apesar das limitaÃ§Ãµes, a abordagem UCB representa um avanÃ§o significativo ao equilibrar de forma mais inteligente a exploraÃ§Ã£o e a explotaÃ§Ã£o em problemas *multi-armed bandit*, oferecendo uma forma de selecionar aÃ§Ãµes nÃ£o-gananciosas baseada no potencial de otimalidade, considerando tanto as estimativas quanto as incertezas [^1].

### ReferÃªncias
[^1]: "Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. The greedy actions are those that look best at present, but some of the other actions may actually be better. É›-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions according to $A_t = \text{argmax}_a Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}$ where Int denotes the natural logarithm of t (the number that e â‰ˆ 2.71828 would have to be raised to in order to equal t), Nt(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c > 0 controls the degree of exploration. If Nt(a) = 0, then a is considered to be a maximizing action." *(Trecho de Multi-armed Bandits)*
