## Upper-Confidence-Bound Action Selection
### Introdu√ß√£o
Em problemas de **reinforcement learning**, a explora√ß√£o √© essencial, dado que sempre existe incerteza acerca da precis√£o das estimativas de valor de a√ß√£o. As a√ß√µes *greedy* s√£o aquelas que, no momento, parecem ser as melhores, mas a√ß√µes alternativas podem realmente ser superiores. A sele√ß√£o de a√ß√µes $\epsilon$-greedy for√ßa a experimenta√ß√£o de a√ß√µes n√£o-gananciosas, embora de forma indiscriminada, sem prefer√™ncia pelas que s√£o quase *greedy* ou mais incertas. Uma abordagem mais inteligente √© selecionar a√ß√µes n√£o-gananciosas com base no seu potencial de otimalidade, considerando tanto as estimativas quanto as incertezas [^1].

### Conceitos Fundamentais
A **Upper-Confidence-Bound (UCB)** action selection aborda esta necessidade de forma eficaz [^1]. A ideia central √© que o termo de raiz quadrada na seguinte express√£o:
$$
A_t = \underset{a}{\text{argmax}} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
$$
funciona como uma medida de incerteza ou varia√ß√£o na estimativa do valor de uma a√ß√£o $a$. O valor maximizado √©, portanto, um limite superior para o valor real da a√ß√£o $a$, com $c$ controlando o n√≠vel de confian√ßa [^1].
```mermaid
graph LR
    A[ "A√ß√£o a" ]
    Q[ "Q_t(a): Estima√ß√£o do valor da a√ß√£o a" ]
    U[ "c * sqrt(ln(t) / N_t(a)): Termo de Incerteza" ]
    S[ "Soma: Q_t(a) + Termo de Incerteza" ]
    M[ "A_t: A√ß√£o selecionada" ]
    A --> Q
    A --> U
    Q --> S
    U --> S
    S --> M
    style S fill:#f9f,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3 stroke:#333,stroke-width:1px
```

> üí° **Exemplo Num√©rico:**
> Imagine que temos 3 a√ß√µes (a1, a2, a3) e estamos no instante de tempo t=5. As estimativas de valor e contagens das a√ß√µes s√£o:
> - $Q_5(a1) = 10$, $N_5(a1) = 2$
> - $Q_5(a2) = 12$, $N_5(a2) = 1$
> - $Q_5(a3) = 8$, $N_5(a3) = 1$
>
> Usando a UCB com $c = 1$, calculamos:
>
> - $UCB(a1) = 10 + 1 \cdot \sqrt{\frac{\ln 5}{2}} = 10 + \sqrt{\frac{1.609}{2}} \approx 10 + 0.897 = 10.897$
> - $UCB(a2) = 12 + 1 \cdot \sqrt{\frac{\ln 5}{1}} = 12 + \sqrt{1.609} \approx 12 + 1.268 = 13.268$
> - $UCB(a3) = 8 + 1 \cdot \sqrt{\frac{\ln 5}{1}} = 8 + \sqrt{1.609} \approx 8 + 1.268 = 9.268$
>
> A a√ß√£o selecionada ser√° $a2$, pois $UCB(a2)$ √© o maior. Note que, apesar de $Q_5(a2) > Q_5(a1)$, a incerteza de $a2$ √© maior.

**Proposi√ß√£o 1:**  *A a√ß√£o selecionada $A_t$ pela UCB √© sempre uma a√ß√£o cujo valor estimado mais o termo de incerteza √© m√°ximo dentre todas as a√ß√µes.  Ou seja, para qualquer a√ß√£o $b$ diferente de $A_t$, temos $Q_t(A_t) + c \sqrt{\frac{\ln t}{N_t(A_t)}} \geq Q_t(b) + c \sqrt{\frac{\ln t}{N_t(b)}}$.*
*Prova:*  Esta propriedade √© uma consequ√™ncia direta da defini√ß√£o da UCB, que seleciona a a√ß√£o que maximiza a express√£o $Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}$. Portanto, se $A_t$ √© a a√ß√£o que maximiza essa express√£o, ent√£o ela deve ser maior ou igual a qualquer outra a√ß√£o $b$ nessa mesma express√£o. $\blacksquare$

**Detalhes da Equa√ß√£o 2.10:**
- $A_t$: Representa a a√ß√£o selecionada no instante de tempo $t$ [^1].
- $Q_t(a)$: √â a estimativa do valor da a√ß√£o $a$ no instante de tempo $t$ [^1].
- $c$: √â um par√¢metro que controla o grau de explora√ß√£o. Um valor mais alto de $c$ incentiva mais a explora√ß√£o [^1].
- $t$: √â o instante de tempo atual [^1].
- $N_t(a)$: √â o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada antes do instante de tempo $t$ [^1].
- $ln t$: √â o logaritmo natural de $t$. Este termo garante que a explora√ß√£o diminua com o tempo [^1].

> üí° **Exemplo Num√©rico:**
> Se temos $t=10$ e a a√ß√£o $a$ foi selecionada $N_t(a)=2$ vezes, o termo de incerteza √© $\sqrt{\frac{\ln 10}{2}} = \sqrt{\frac{2.303}{2}} \approx 1.074$. Se $t=100$ e $N_t(a)=20$, o termo de incerteza √© $\sqrt{\frac{\ln 100}{20}} = \sqrt{\frac{4.605}{20}} \approx 0.48$. Note que, mesmo com mais sele√ß√µes da a√ß√£o, o aumento de $t$ reduz o termo de incerteza.

**Lemma 1:** *O termo de incerteza $\sqrt{\frac{\ln t}{N_t(a)}}$ decresce com o aumento de $N_t(a)$ e cresce com o aumento de $t$.*
*Prova:*
A derivada de $\sqrt{\frac{\ln t}{N_t(a)}}$ com rela√ß√£o a $N_t(a)$ √©:
$$
\frac{\partial}{\partial N_t(a)} \sqrt{\frac{\ln t}{N_t(a)}} = \sqrt{\ln t} \cdot \frac{-1}{2} {N_t(a)}^{-\frac{3}{2}} = -\frac{\sqrt{\ln t}}{2 {N_t(a)}^{\frac{3}{2}}}
$$
Como essa derivada √© negativa, o termo diminui com o aumento de $N_t(a)$. A derivada com rela√ß√£o a $t$ √©:
$$
\frac{\partial}{\partial t} \sqrt{\frac{\ln t}{N_t(a)}} = \frac{1}{2} \sqrt{\frac{1}{N_t(a) \ln t}} \frac{1}{t}  = \frac{1}{2t} \sqrt{\frac{1}{N_t(a) \ln t}}
$$
Como essa derivada √© positiva, o termo aumenta com o aumento de $t$. $\blacksquare$
```mermaid
graph LR
subgraph "Termo de Incerteza"
    A["sqrt(ln(t) / N_t(a))"]
end
    B["N_t(a) aumenta"]
    C["t aumenta"]
    D["Incerteza diminui"]
    E["Incerteza aumenta"]

    B --> D
    C --> E
    A --> B
    A --> C
    style A fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3 stroke:#333,stroke-width:1px
```

> üí° **Exemplo Num√©rico:**
> Vamos analisar o impacto de $N_t(a)$ no termo de incerteza com $t=100$ e $c=1$:
>
> | $N_t(a)$ | Incerteza: $\sqrt{\frac{\ln t}{N_t(a)}}$ |
> |----------|-----------------------------------|
> | 1        | $\sqrt{\frac{\ln 100}{1}} \approx 2.145$    |
> | 5        | $\sqrt{\frac{\ln 100}{5}} \approx 0.96$    |
> | 25       | $\sqrt{\frac{\ln 100}{25}} \approx 0.429$  |
> | 50       | $\sqrt{\frac{\ln 100}{50}} \approx 0.303$  |
>
> Como podemos ver, quanto maior o $N_t(a)$, menor a incerteza.

**Lemma 1.1:** *Assumindo que $N_t(a) > 0$, o termo de incerteza $\sqrt{\frac{\ln t}{N_t(a)}}$ converge para zero quando $t$ tende a infinito e $N_t(a)$ cresce na mesma ordem de $t$.*

*Prova:* Seja $N_t(a) = k t$ para uma constante $k > 0$. Ent√£o, o termo de incerteza se torna $\sqrt{\frac{\ln t}{k t}} = \frac{\sqrt{\ln t}}{\sqrt{kt}}$.  Para mostrar que converge para zero, basta verificar que $\lim_{t\to\infty} \frac{\sqrt{\ln t}}{\sqrt{kt}} = \lim_{t\to\infty} \frac{\sqrt{\ln t}}{\sqrt{k}\sqrt{t}} = 0$. De fato, aplicando a regra de L'H√¥pital para o limite $\lim_{t \to \infty} \frac{\sqrt{\ln t}}{\sqrt{t}}$, temos: $\lim_{t \to \infty} \frac{\frac{1}{2\sqrt{\ln t}}\cdot \frac{1}{t}}{\frac{1}{2\sqrt{t}}} = \lim_{t \to \infty} \frac{\sqrt{t}}{t\sqrt{\ln t}} = \lim_{t \to \infty} \frac{1}{\sqrt{t\ln t}}= 0$. Como $k$ √© uma constante positiva,  $\lim_{t\to\infty} \frac{\sqrt{\ln t}}{\sqrt{kt}} = 0$. $\blacksquare$
```mermaid
graph LR
subgraph "Limite do Termo de Incerteza"
    A["Limite: lim t->inf sqrt(ln(t)/N_t(a))"]
    B["N_t(a) = k*t"]
    C["lim t->inf sqrt(ln(t) / k*t)"]
    D["lim t->inf sqrt(ln(t))/sqrt(k*t)"]
    E["Regra de L'H√¥pital"]
    F["lim t->inf 1/sqrt(t*ln(t)) = 0"]
    G["Converge para zero"]
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    style A fill:#ccf,stroke:#333,stroke-width:2px
  linkStyle 0,1,2,3,4,5,6 stroke:#333,stroke-width:1px
end
```

**Corol√°rio 1:** *A UCB seleciona a√ß√µes menos exploradas com mais frequ√™ncia, dado que o denominador $N_t(a)$ diminui a incerteza quando a a√ß√£o $a$ j√° foi selecionada v√°rias vezes. A sele√ß√£o de uma a√ß√£o $a$ reduz a incerteza associada a essa a√ß√£o e, ao mesmo tempo, o aumento do tempo $t$ aumenta a incerteza para todas as a√ß√µes* [^1].

> üí° **Exemplo Num√©rico:**
> Vamos supor que temos duas a√ß√µes: $a1$ e $a2$. Inicialmente, $Q(a1) = 5$, $Q(a2) = 7$, e $N(a1)=N(a2)=1$, com $c=1$.
>
> - **t=2:** Calculamos $UCB(a1)$ e $UCB(a2)$:
>  - $UCB(a1) = 5 + \sqrt{\frac{\ln 2}{1}} \approx 5 + 0.833 \approx 5.833$
>  - $UCB(a2) = 7 + \sqrt{\frac{\ln 2}{1}} \approx 7 + 0.833 \approx 7.833$
>   $a2$ √© selecionada. $N_2(a2) = 2$
> - **t=3:**
>  - $UCB(a1) = 5 + \sqrt{\frac{\ln 3}{1}} \approx 5 + 1.04 \approx 6.04$
>  - $UCB(a2) = 7 + \sqrt{\frac{\ln 3}{2}} \approx 7 + 0.74 \approx 7.74$
>   $a2$ √© selecionada novamente. $N_3(a2) = 3$
> - **t=4:**
>  - $UCB(a1) = 5 + \sqrt{\frac{\ln 4}{1}} \approx 5 + 1.17 \approx 6.17$
>  - $UCB(a2) = 7 + \sqrt{\frac{\ln 4}{3}} \approx 7 + 0.68 \approx 7.68$
> Mesmo $a2$ tendo um valor inicial maior, o aumento de $t$ e a redu√ß√£o da incerteza de $a2$ devido a repetidas sele√ß√µes levar√£o $a1$ a ser selecionada em algum momento futuro.

**Corol√°rio 1.1:** *Quando o tempo $t$ tende a infinito, e se todas as a√ß√µes s√£o selecionadas infinitamente, o termo de incerteza $\sqrt{\frac{\ln t}{N_t(a)}}$ converge para zero para todas as a√ß√µes.*

*Prova:* Do Lema 1.1, sabemos que se $N_t(a)$ crescer na mesma ordem de $t$, o termo de incerteza converge para zero. Se todas as a√ß√µes s√£o selecionadas infinitamente, ent√£o para cada a√ß√£o $a$,  $N_t(a)$ tende a infinito,  logo $N_t(a)$ cresce na mesma ordem de $t$ e o termo de incerteza converge para zero. $\blacksquare$
```mermaid
graph LR
subgraph "Converg√™ncia do Termo de Incerteza"
    A["t -> inf e todas as a√ß√µes s√£o selecionadas inf"]
    B["Para cada a√ß√£o a, N_t(a) -> inf"]
    C["N_t(a) cresce na mesma ordem de t"]
    D["sqrt(ln(t) / N_t(a)) converge para zero"]
    A --> B
    B --> C
    C --> D
    style A fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2 stroke:#333,stroke-width:1px
end
```

Sendo assim, toda vez que uma a√ß√£o $a$ √© selecionada, a incerteza √© presumivelmente reduzida, pois $N_t(a)$ aumenta. O inverso ocorre quando uma a√ß√£o diferente √© selecionada, j√° que $t$ aumenta enquanto $N_t(a)$ n√£o, aumentando a incerteza [^1]. O uso do logaritmo natural garante que o aumento da incerteza diminua com o tempo, mas permanece ilimitado, o que implica que todas as a√ß√µes ser√£o eventualmente selecionadas. No entanto, a√ß√µes com baixas estimativas de valor ou selecionadas com frequ√™ncia, ser√£o menos selecionadas ao longo do tempo [^1].

**Observa√ß√£o 1:** *A constante $c$ na equa√ß√£o da UCB ajusta a taxa de explora√ß√£o.  Um valor maior de $c$ leva a uma maior explora√ß√£o, pois aumenta o peso do termo de incerteza, enquanto um valor menor de $c$ favorece a explota√ß√£o.*

> üí° **Exemplo Num√©rico:**
> Considere que temos duas a√ß√µes, $a1$ e $a2$, com $Q_t(a1) = 5$ e $Q_t(a2) = 7$. Se $t=10$ e $N_t(a1) = 2$ e $N_t(a2) = 5$.
>
> - Com $c=0.5$:
>   - $UCB(a1) = 5 + 0.5 \cdot \sqrt{\frac{\ln 10}{2}} \approx 5 + 0.5 \cdot 1.074 \approx 5.537$
>   - $UCB(a2) = 7 + 0.5 \cdot \sqrt{\frac{\ln 10}{5}} \approx 7 + 0.5 \cdot 0.678 \approx 7.339$
>   $a2$ √© selecionada.
> - Com $c=2$:
>   - $UCB(a1) = 5 + 2 \cdot \sqrt{\frac{\ln 10}{2}} \approx 5 + 2 \cdot 1.074 \approx 7.148$
>   - $UCB(a2) = 7 + 2 \cdot \sqrt{\frac{\ln 10}{5}} \approx 7 + 2 \cdot 0.678 \approx 8.356$
>   $a2$ √© selecionada.
>
>   Note que o maior valor de $c$ favorece a explora√ß√£o. Se $N_t(a1)$ fosse muito menor, o valor de $UCB(a1)$ poderia ultrapassar $UCB(a2)$ com $c=2$ devido ao alto fator de incerteza.
```mermaid
graph LR
    A["Valor de c"]
    B["c maior"]
    C["c menor"]
    D["Maior explora√ß√£o"]
    E["Maior explota√ß√£o"]
    A --> B
    A --> C
    B --> D
    C --> E
    style A fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3 stroke:#333,stroke-width:1px
```

### Conclus√£o
A UCB √© eficaz em diversas situa√ß√µes e frequentemente apresenta bom desempenho, mas apresenta limita√ß√µes em cen√°rios mais complexos, como em problemas n√£o-estacion√°rios ou com grandes espa√ßos de estados, especialmente quando se usa aproxima√ß√£o de fun√ß√µes. Nestes casos, as ideias da UCB tornam-se menos pr√°ticas [^1]. Apesar das limita√ß√µes, a abordagem UCB representa um avan√ßo significativo ao equilibrar de forma mais inteligente a explora√ß√£o e a explota√ß√£o em problemas *multi-armed bandit*, oferecendo uma forma de selecionar a√ß√µes n√£o-gananciosas baseada no potencial de otimalidade, considerando tanto as estimativas quanto as incertezas [^1].

### Refer√™ncias
[^1]: "Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. The greedy actions are those that look best at present, but some of the other actions may actually be better. …õ-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions according to $A_t = \text{argmax}_a Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}$ where Int denotes the natural logarithm of t (the number that e ‚âà 2.71828 would have to be raised to in order to equal t), Nt(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c > 0 controls the degree of exploration. If Nt(a) = 0, then a is considered to be a maximizing action." *(Trecho de Multi-armed Bandits)*
