## Upper-Confidence-Bound Action Selection e suas Limita√ß√µes em Ambientes Complexos

### Introdu√ß√£o

Este cap√≠tulo aborda o **problema do multi-armed bandit** como um caso simplificado de **reinforcement learning** focado no feedback avaliativo [^1]. A import√¢ncia do reinforcement learning reside em sua capacidade de utilizar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, ao inv√©s de instruir por meio de a√ß√µes corretas, como em aprendizado supervisionado. O cap√≠tulo introduz o conceito de **explora√ß√£o ativa**, essencial para a busca de um bom comportamento [^1]. Em particular, explora-se a **a√ß√£o-valor**, que estima o valor de uma a√ß√£o como a m√©dia da recompensa obtida ao execut√°-la [^3]. O m√©todo **Upper-Confidence-Bound (UCB)** √© discutido como uma abordagem para equilibrar a explora√ß√£o e a explota√ß√£o [^11]. No entanto, o cap√≠tulo tamb√©m aborda as dificuldades do m√©todo UCB em ambientes mais complexos, como problemas n√£o estacion√°rios e grandes espa√ßos de estados.

### Conceitos Fundamentais

O problema do **k-armed bandit** envolve a sele√ß√£o repetida de a√ß√µes entre *k* op√ß√µes, com o objetivo de maximizar a recompensa total esperada ao longo do tempo [^1]. A dificuldade reside no fato de que os valores reais das a√ß√µes s√£o desconhecidos, exigindo uma explora√ß√£o para estim√°-los [^2]. Os m√©todos de **a√ß√£o-valor** estimam os valores das a√ß√µes com base nas recompensas recebidas e a partir delas s√£o tomadas as decis√µes [^3]. A a√ß√£o **greedy** seleciona a a√ß√£o com a maior estimativa de valor, enquanto a a√ß√£o **explorat√≥ria** busca alternativas para melhorar as estimativas, como as a√ß√µes n√£o-greedy [^2]. O m√©todo **sample-average** estima o valor de uma a√ß√£o como a m√©dia das recompensas obtidas ao selecionar essa a√ß√£o, como dado por [^3]:
$$
Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$
onde $\mathbb{1}_{A_i=a}$ √© 1 se $A_i=a$ e 0 caso contr√°rio, e $R_i$ √© a recompensa recebida ap√≥s selecionar a a√ß√£o $A_i$. Em particular, este m√©todo faz uma m√©dia das recompensas obtidas ao longo do tempo. O m√©todo **$\epsilon$-greedy** equilibra explora√ß√£o e explota√ß√£o, escolhendo a a√ß√£o greedy com probabilidade $1-\epsilon$ e uma a√ß√£o aleat√≥ria com probabilidade $\epsilon$ [^3].
  ```mermaid
  flowchart LR
      A["A√ß√£o-Valor Q(a)"] --> B{"Avalia√ß√£o"}
      B -- "Recompensa R" --> C["Atualiza√ß√£o Q(a)"]
      C --> D{"Decis√£o"}
      D --> E{"A√ß√£o Greedy"}
      D --> F{"A√ß√£o Explorat√≥ria"}
      E --> A
      F --> A
      subgraph "M√©todo Sample-Average"
      C
      end
  ```

> üí° **Exemplo Num√©rico:** Considere um problema de 3-armed bandit (k=3), com a√ß√µes A, B e C. Ap√≥s algumas itera√ß√µes, temos os seguintes resultados:
>  - A√ß√£o A foi selecionada 5 vezes, com recompensas [1, 2, 1, 3, 2].
>  - A√ß√£o B foi selecionada 3 vezes, com recompensas [0, 1, 0].
>  - A√ß√£o C foi selecionada 2 vezes, com recompensas [3, 4].
>
> Usando o m√©todo *sample-average*:
>   - $Q(A) = (1 + 2 + 1 + 3 + 2) / 5 = 9 / 5 = 1.8$
>   - $Q(B) = (0 + 1 + 0) / 3 = 1 / 3 \approx 0.33$
>   - $Q(C) = (3 + 4) / 2 = 7 / 2 = 3.5$
>
>  A a√ß√£o *greedy* nesse momento seria a a√ß√£o C, pois tem a maior estimativa de valor. Se $\epsilon = 0.2$ em um m√©todo $\epsilon$-greedy, a cada passo existe uma chance de 80% de escolher a a√ß√£o C e 20% de escolher A ou B aleatoriamente.

**Lema 1** O m√©todo sample-average, como definido acima, pode ser implementado de forma incremental, atualizando a estimativa $Q_t(a)$ a cada passo $t$ sem a necessidade de recalcular a m√©dia de todas as recompensas anteriores.

*Proof:* Para demonstrar isso, vamos considerar a estimativa $Q_t(a)$ no tempo $t$ como a m√©dia de todas as recompensas anteriores recebidas quando a a√ß√£o $a$ foi selecionada. Seja $n_t(a) = \sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}$ o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© o tempo $t-1$. Ent√£o, $Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{n_t(a)}$. Quando a a√ß√£o $a$ √© selecionada no passo $t$ e recebemos a recompensa $R_t$, o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada √© incrementado para $n_{t+1}(a) = n_t(a) + 1$, e a nova estimativa $Q_{t+1}(a)$ pode ser escrita como:

$Q_{t+1}(a) = \frac{\sum_{i=1}^{t} R_i \mathbb{1}_{A_i=a}}{n_{t+1}(a)} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a} + R_t \mathbb{1}_{A_t=a}}{n_t(a) + 1} = \frac{n_t(a) Q_t(a) + R_t}{n_t(a) + 1}$.

Esta formula√ß√£o permite calcular a nova estimativa de valor $Q_{t+1}(a)$ utilizando a estimativa anterior $Q_t(a)$, o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© ent√£o, $n_t(a)$, e a recompensa $R_t$. Portanto, $Q_{t+1}(a)$ pode ser calculado incrementalmente, sem a necessidade de armazenar ou recalcular todas as recompensas anteriores.
  ```mermaid
  flowchart LR
      A["Q_t(a)"] --> B["n_t(a)"]
      A --> C["R_t"]
      B --> D["n_t(a) + 1"]
      C --> D
      D --> E["Q_{t+1}(a) = (n_t(a) * Q_t(a) + R_t) / (n_t(a) + 1)"]
      subgraph "Atualiza√ß√£o Incremental"
      E
      end
  ```

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, vamos supor que selecionamos a a√ß√£o A novamente e recebemos uma recompensa de 3. Usando a atualiza√ß√£o incremental:
>
> - $n_t(A) = 5$
> - $Q_t(A) = 1.8$
> - $R_t = 3$
> - $n_{t+1}(A) = 6$
>
> $Q_{t+1}(A) = \frac{5 \cdot 1.8 + 3}{6} = \frac{9 + 3}{6} = \frac{12}{6} = 2$
>
> Portanto, a nova estimativa de valor para a a√ß√£o A √© 2, sem precisar recalcular a m√©dia de todos os 6 retornos.

O m√©todo **Upper-Confidence-Bound (UCB)**, por sua vez, seleciona a√ß√µes com base na estimativa de valor e na incerteza associada. √â expresso por [^11]:
$$
A_t = \text{argmax}_a \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
$$
onde $Q_t(a)$ √© a estimativa do valor da a√ß√£o $a$ no tempo $t$, $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© o tempo $t$, e $c>0$ controla o grau de explora√ß√£o. O termo $\sqrt{\frac{\ln t}{N_t(a)}}$ representa a incerteza na estimativa do valor da a√ß√£o. O algoritmo UCB, ao contr√°rio do $\epsilon$-greedy, faz uma explora√ß√£o direcional, selecionando aquelas a√ß√µes que possuem um maior potencial de serem √≥timas, balanceando, assim, a busca por recompensas imediatas (explota√ß√£o) com a busca por melhores recompensas futuras (explora√ß√£o). A forma como a explora√ß√£o √© conduzida √© que difere o UCB do $\epsilon$-greedy, pois neste √∫ltimo, todas as a√ß√µes n√£o-greedy t√™m a mesma probabilidade de serem selecionadas.
  ```mermaid
  flowchart LR
      A["Q_t(a)"] --> B["Incerteza: c * sqrt(ln(t) / N_t(a))"]
      B --> C["Soma"]
      A --> C
      C --> D["argmax_a"]
      D --> E["A_t"]
    subgraph "M√©todo UCB"
        E
    end
  ```

> üí° **Exemplo Num√©rico:** Usando os mesmos dados, vamos calcular a a√ß√£o a ser selecionada pelo m√©todo UCB no passo 11 (t=11) e assumindo c=1:
>
>  - A√ß√£o A: $N_{10}(A) = 6$, $Q_{10}(A) = 2$
>  - A√ß√£o B: $N_{10}(B) = 3$, $Q_{10}(B) \approx 0.33$
>  - A√ß√£o C: $N_{10}(C) = 2$, $Q_{10}(C) = 3.5$
>
>  Calculando o UCB para cada a√ß√£o:
>  - $UCB(A) = 2 + 1 * \sqrt{\frac{\ln 11}{6}} \approx 2 + 1 * \sqrt{0.415} \approx 2 + 0.64 \approx 2.64$
>  - $UCB(B) = 0.33 + 1 * \sqrt{\frac{\ln 11}{3}} \approx 0.33 + 1 * \sqrt{0.83} \approx 0.33 + 0.91 \approx 1.24$
>  - $UCB(C) = 3.5 + 1 * \sqrt{\frac{\ln 11}{2}} \approx 3.5 + 1 * \sqrt{1.2} \approx 3.5 + 1.1 \approx 4.6$
>
>  Neste caso, o UCB selecionaria a a√ß√£o C pois tem o maior valor de UCB, apesar da incerteza nas estimativas das a√ß√µes A e B serem maiores.

**Proposi√ß√£o 1** O termo de incerteza no UCB, $\sqrt{\frac{\ln t}{N_t(a)}}$, garante que, com o tempo, todas as a√ß√µes ser√£o exploradas infinitamente, a menos que uma a√ß√£o seja selecionada infinitamente mais do que outras.

*Proof:* Considere que para alguma a√ß√£o $a$, $N_t(a)$ n√£o cresce para o infinito. Ent√£o, existe um $M$ tal que $N_t(a) < M$ para todo $t$. Nesse caso, o termo $\sqrt{\frac{\ln t}{N_t(a)}}$ crescer√° para infinito conforme $t \rightarrow \infty$. Eventualmente, esse termo dominar√° a estimativa de valor $Q_t(a)$ de todas as a√ß√µes, garantindo que a a√ß√£o $a$ seja selecionada novamente, o que contradiz a hip√≥tese que $N_t(a)$ n√£o cresce para o infinito. Isso implica que, para cada a√ß√£o $a$, ou $N_t(a) \rightarrow \infty$, ou a a√ß√£o $a$ ser√° selecionada infinitamente vezes.
  ```mermaid
  flowchart LR
      A["N_t(a) n√£o cresce para infinito"] --> B{"Existe M tal que N_t(a) < M"}
      B --> C["sqrt(ln(t) / N_t(a)) -> infinito"]
      C --> D["A√ß√£o 'a' selecionada novamente"]
      D --> E["Contradi√ß√£o: N_t(a) n√£o cresce para infinito"]
      E --> F["N_t(a) -> infinito ou a√ß√£o 'a' √© selecionada infinitamente"]
      subgraph "Prova da Proposi√ß√£o 1"
        F
      end
  ```

Entretanto, o m√©todo UCB apresenta desafios quando aplicado a problemas n√£o estacion√°rios, nos quais a distribui√ß√£o de recompensas varia ao longo do tempo [^11]. Os m√©todos discutidos at√© agora assumem que as recompensas seguem distribui√ß√µes estacion√°rias. Em casos n√£o-estacion√°rios, torna-se necess√°rio dar maior peso √†s recompensas recentes, ao inv√©s de se levar em conta todos os retornos do passado. A implementa√ß√£o incremental da m√©dia das recompensas pode ser feita atrav√©s da seguinte equa√ß√£o [^7]:

$$
Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]
$$

onde $Q_{n+1}$ representa a nova estimativa do valor de uma a√ß√£o ap√≥s receber a recompensa $R_n$, $n$ √© o n√∫mero de vezes que a a√ß√£o foi selecionada, e $Q_n$ √© a estimativa anterior. A vers√£o com um par√¢metro constante de tamanho do passo para problemas n√£o estacion√°rios, √© definida como [^8]:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n]
$$

onde $\alpha \in (0,1]$ √© o par√¢metro constante de tamanho do passo, que d√° maior peso para recompensas mais recentes, conforme explicado em [^9]. Al√©m disso, o UCB tamb√©m enfrenta dificuldades ao lidar com grandes espa√ßos de estados, onde as estimativas de valor de a√ß√£o precisam ser generalizadas para estados n√£o observados anteriormente. As an√°lises e formula√ß√µes matem√°ticas do m√©todo UCB s√£o mais complexas do que os m√©todos $\epsilon$-greedy ou greedy, dificultando sua aplica√ß√£o quando h√° necessidade de generaliza√ß√£o, como em abordagens baseadas em *function approximation*, que s√£o exploradas mais a fundo na segunda parte do livro [^12].
  ```mermaid
  flowchart LR
      A["Q_n"] --> B["R_n"]
      B --> C["Q_{n+1} = Q_n + alpha * (R_n - Q_n)"]
      A --> C
    subgraph "Atualiza√ß√£o com Taxa de Aprendizagem Constante (alpha)"
      C
    end
  ```

> üí° **Exemplo Num√©rico:** Suponha que a a√ß√£o A, em um cen√°rio n√£o-estacion√°rio, tenha uma mudan√ßa na distribui√ß√£o de recompensas. Usando $\alpha = 0.1$, com $Q_n(A)=2$ e $R_n = 4$:
>
>  $Q_{n+1}(A) = 2 + 0.1 * (4 - 2) = 2 + 0.1 * 2 = 2.2$
>
>  Comparando com o m√©todo *sample-average*, que levaria o valor para:
>
>  $Q_{n+1}(A) = 2 + \frac{1}{7} (4 - 2) \approx 2 + 0.29 \approx 2.29$
>
>  Observe como o m√©todo com $\alpha$ d√° mais peso √† recompensa mais recente (4) e faz uma mudan√ßa mais r√°pida na estimativa do valor da a√ß√£o.

**Teorema 1** O uso de um par√¢metro de tamanho de passo constante $\alpha$ na atualiza√ß√£o da a√ß√£o-valor, como em $Q_{n+1} = Q_n + \alpha [R_n - Q_n]$, introduz um vi√©s nas estimativas em rela√ß√£o ao valor real da a√ß√£o em problemas estacion√°rios, ao contr√°rio do sample average.

*Proof:* Para o sample average, a estimativa $Q_n$ converge para o valor real da a√ß√£o (assumindo que a a√ß√£o √© selecionada infinitamente vezes e que as recompensas s√£o de uma distribui√ß√£o estacion√°ria). No caso do uso de um par√¢metro $\alpha$, a atualiza√ß√£o √© uma m√©dia ponderada exponencial das recompensas, onde as recompensas mais recentes recebem um peso maior. Esta m√©dia n√£o converge para a m√©dia verdadeira se o n√∫mero de recompensas for muito baixo ou se $\alpha$ for grande demais (pr√≥ximo de 1). Portanto, em cen√°rios estacion√°rios, a estimativa com $\alpha$ tem um vi√©s em rela√ß√£o √† estimativa baseada na m√©dia simples (sample average), pois n√£o considera todo o hist√≥rico de recompensas de forma igual.
  ```mermaid
  flowchart LR
      A["Sample-Average"] --> B["Q_n converge para valor real da a√ß√£o"]
      C["Taxa alpha"] --> D["M√©dia ponderada exponencial de recompensas"]
      D --> E["N√£o converge se alpha grande ou poucas recompensas"]
      B --> F["Sem vi√©s"]
      E --> G["Vi√©s em cen√°rios estacion√°rios"]
      subgraph "Teorema 1: Compara√ß√£o de M√©todos"
        F
        G
      end
  ```

### Conclus√£o

O m√©todo Upper-Confidence-Bound (UCB) oferece uma abordagem promissora para o equil√≠brio entre explora√ß√£o e explota√ß√£o em problemas do tipo multi-armed bandit, como visto no experimento com o *10-armed testbed* [^12]. No entanto, sua aplica√ß√£o √© limitada por sua dificuldade em lidar com problemas n√£o estacion√°rios e grandes espa√ßos de estados. A necessidade de m√©todos mais sofisticados para lidar com essas complexidades √© um ponto central a ser abordado em cap√≠tulos subsequentes deste livro, que aborda o problema do *reinforcement learning* em sua completude. A solu√ß√£o para tais problemas demanda m√©todos mais avan√ßados do que os apresentados neste cap√≠tulo introdut√≥rio, como o uso de *function approximation* ou m√©todos *policy-gradient*, que ser√£o explorados em cap√≠tulos posteriores.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions."
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected: q‚àó(a) = E[Rt | At=a]. "
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods."
[^4]: "The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section."
[^5]: "If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action's value."
[^6]: "The advantage of …õ-greedy over greedy methods depends on the task."
[^7]: "As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. Given Qn and the nth reward, Rn, the new average of all n rewards can be computed by"
[^8]: "One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule (2.3) for updating an average Qn of the n ‚Äì 1 past rewards is modified to be Qn+1 = Qn + a[Rn - Qn],"
[^9]: "where the step-size parameter a ‚àà (0, 1] is constant. This results in Qn+1 being a weighted average of past rewards and the initial estimate Q1:"
[^10]: "These are just asymptotic guarantees, however, and say little about the practical effectiveness of the methods."
[^11]:  "Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. The greedy actions are those that look best at present, but some of the other actions may actually be better. …õ-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates."
[^12]: "Results with UCB on the 10-armed testbed are shown in Figure 2.4. UCB often performs well, as shown here, but is more difficult than …õ-greedy to extend beyond bandits to the more general reinforcement learning settings considered in the rest of this book. One difficulty is in dealing with nonstationary problems; methods more complex than those presented in Section 2.5 would be needed. Another difficulty is dealing with large state spaces, particularly when using function approximation as developed in Part II of this book. In these more advanced settings the idea of UCB action selection is usually not practical."
