## Upper-Confidence-Bound Action Selection: Uma Abordagem para Explora√ß√£o em Bandidos Multiarmados

### Introdu√ß√£o
A necessidade de equilibrar **explora√ß√£o** e **explota√ß√£o** √© um desafio central no aprendizado por refor√ßo. Enquanto a explota√ß√£o foca em escolher a a√ß√£o que parece ser a melhor no momento, baseada no conhecimento atual, a explora√ß√£o √© necess√°ria para descobrir a√ß√µes potencialmente melhores que ainda n√£o foram suficientemente avaliadas. No contexto de **problemas de bandidos multiarmados**, essa dicotomia se manifesta na escolha entre usar as a√ß√µes que atualmente possuem as melhores estimativas de valor ou experimentar a√ß√µes menos conhecidas para melhorar essas estimativas. O **m√©todo de sele√ß√£o de a√ß√µes Upper-Confidence-Bound (UCB)** aborda essa quest√£o de forma sofisticada, fornecendo uma maneira eficaz de explorar a√ß√µes que podem ser otimamente ben√©ficas, mas que ainda possuem uma alta incerteza em suas estimativas de valor [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).
Este cap√≠tulo explora em detalhes o m√©todo UCB, elucidando sua l√≥gica, formula√ß√£o matem√°tica e como ele se relaciona com outras abordagens para a explora√ß√£o, como o m√©todo Œµ-greedy.

### Conceitos Fundamentais
A motiva√ß√£o para o m√©todo UCB surge da observa√ß√£o de que a explora√ß√£o √© necess√°ria devido √† incerteza nas estimativas de valor das a√ß√µes [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). As **a√ß√µes *greedy*** s√£o aquelas com as melhores estimativas no presente momento, mas pode haver outras a√ß√µes com potencial de serem superiores, cujas estimativas ainda n√£o foram refinadas por terem sido escolhidas com menos frequ√™ncia. O m√©todo Œµ-greedy for√ßa a explora√ß√£o de a√ß√µes n√£o-greedy, mas de maneira indiscriminada, sem favorecer a√ß√µes particularmente incertas. O m√©todo UCB, por outro lado, busca selecionar a√ß√µes n√£o-greedy que podem ser √≥timas, levando em conta tanto a sua proximidade do m√°ximo quanto as incertezas em suas estimativas [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).
```mermaid
graph LR
    A["Estimativa de Valor"] --> B("Incerteza da Estimativa");
    B --> C("Necessidade de Explora√ß√£o");
    C --> D{"A√ß√µes Greedy vs N√£o-Greedy"};
    D --> E("UCB: Explora√ß√£o Direcionada");
    E --> F("Equil√≠brio Explora√ß√£o/Explota√ß√£o");
```
A a√ß√£o selecionada pelo m√©todo UCB em um dado instante *$t$* √© dada por:
$$A_t = \underset{a}{\operatorname{argmax}} \left[Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}\right],$$
onde:
- **$Q_t(a)$** √© a estimativa de valor da a√ß√£o *$a$* no tempo *$t$*.
- **$N_t(a)$** √© o n√∫mero de vezes que a a√ß√£o *$a$* foi selecionada antes do tempo *$t$*.
- **$c > 0$** √© um par√¢metro que controla o grau de explora√ß√£o.
- **$\ln t$** √© o logaritmo natural de *$t$*. [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2)
```mermaid
graph LR
    A["A_t"] --> B{"argmax"};
    B --> C["Q_t(a)"];
    B --> D;
    D["+"] --> E;
    E --> F["c"];
    E --> G["sqrt(...)"];
     G-->H["ln t"];
     G-->I["N_t(a)"];
    H--> J["/"];
    I--> J
    F-->D
```
> üí° **Exemplo Num√©rico:** Vamos considerar um cen√°rio com 3 a√ß√µes (A, B, e C) em *$t$*=10, com *$c$*=2. As estimativas de valor e contagens de sele√ß√£o s√£o as seguintes:
> -   $Q_{10}(A) = 0.5$, $N_{10}(A) = 5$
> -   $Q_{10}(B) = 0.7$, $N_{10}(B) = 2$
> -   $Q_{10}(C) = 0.4$, $N_{10}(C) = 1$
>
> Calculamos os termos de UCB para cada a√ß√£o:
>
> -   $UCB(A) = 0.5 + 2\sqrt{\frac{\ln 10}{5}} \approx 0.5 + 2\sqrt{\frac{2.30}{5}} \approx 0.5 + 2\sqrt{0.46} \approx 0.5 + 2(0.678) \approx 0.5 + 1.356 \approx 1.856$
> -   $UCB(B) = 0.7 + 2\sqrt{\frac{\ln 10}{2}} \approx 0.7 + 2\sqrt{\frac{2.30}{2}} \approx 0.7 + 2\sqrt{1.15} \approx 0.7 + 2(1.072) \approx 0.7 + 2.144 \approx 2.844$
> -   $UCB(C) = 0.4 + 2\sqrt{\frac{\ln 10}{1}} \approx 0.4 + 2\sqrt{\frac{2.30}{1}} \approx 0.4 + 2\sqrt{2.30} \approx 0.4 + 2(1.516) \approx 0.4 + 3.032 \approx 3.432$
>
>  Neste caso, a a√ß√£o C √© selecionada pois possui o maior valor de UCB (3.432), apesar de ter uma estimativa de valor menor (0.4), porque foi menos explorada. Isso demonstra o equil√≠brio entre explora√ß√£o e explota√ß√£o, incentivando a escolha de a√ß√µes com maior incerteza.

O termo $\sqrt{\frac{\ln t}{N_t(a)}}$ √© uma medida da incerteza ou vari√¢ncia na estimativa do valor de *$a$* [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Ele atua como um fator de incentivo √† explora√ß√£o:
- Se $N_t(a)$ for pequeno (a a√ß√£o *$a$* foi selecionada poucas vezes), a incerteza √© alta e o valor da a√ß√£o √© inflacionado, incentivando sua sele√ß√£o.
- Se $N_t(a)$ for grande (a a√ß√£o *$a$* foi selecionada muitas vezes), a incerteza diminui, e a a√ß√£o √© selecionada mais frequentemente com base no seu valor estimado.
- O par√¢metro *$c$* controla a for√ßa desse incentivo √† explora√ß√£o [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).
```mermaid
graph LR
    A["N_t(a) pequeno"] --> B("Alta Incerteza");
    B --> C("Incentivo √† Explora√ß√£o");
    D["N_t(a) grande"] --> E("Baixa Incerteza");
    E --> F("Explota√ß√£o Baseada em Q_t(a)");
    G["c"] --> H("Controle da Explora√ß√£o")
    H --> C
    H --> F
```

> üí° **Exemplo Num√©rico:** Para ilustrar o efeito de $N_t(a)$ no termo de incerteza, considere duas a√ß√µes com o mesmo valor estimado de $Q_t(a) = 0.6$ e *$c$* = 1.5, no tempo *$t$*=100. A√ß√£o A foi escolhida 10 vezes, enquanto a a√ß√£o B foi escolhida 50 vezes.
>
> - Incerteza de A: $1.5\sqrt{\frac{\ln 100}{10}} \approx 1.5\sqrt{\frac{4.605}{10}} \approx 1.5\sqrt{0.4605} \approx 1.5 \times 0.6786 \approx 1.018$
> - Incerteza de B: $1.5\sqrt{\frac{\ln 100}{50}} \approx 1.5\sqrt{\frac{4.605}{50}} \approx 1.5\sqrt{0.0921} \approx 1.5 \times 0.3034 \approx 0.455$
>
>  Como vemos, a incerteza da a√ß√£o A √© muito maior do que a de B (1.018 vs 0.455), o que incentiva a explora√ß√£o de A, pois foi menos explorada.

Quando $N_t(a) = 0$, a a√ß√£o *$a$* √© considerada uma a√ß√£o maximizadora, assegurando que todas as a√ß√µes sejam exploradas no in√≠cio [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). O m√©todo UCB √© denominado "upper confidence bound" porque a express√£o $Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}$ pode ser interpretada como um limite superior da estimativa do valor verdadeiro da a√ß√£o *$a$*, com um n√≠vel de confian√ßa determinado por *$c$* [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).
```mermaid
graph LR
    A["Express√£o UCB"] --> B("Limite Superior da Estimativa");
    B --> C("N√≠vel de Confian√ßa: c");
```

O uso do logaritmo natural faz com que os aumentos na incerteza fiquem menores com o tempo, mas eles s√£o ilimitados, garantindo que todas as a√ß√µes sejam eventualmente selecionadas [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). No entanto, a√ß√µes com estimativas de valor mais baixas ou que j√° foram selecionadas frequentemente ser√£o selecionadas com menos frequ√™ncia ao longo do tempo [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).
```mermaid
graph LR
    A["ln t"] --> B("Aumentos de Incerteza Diminuem com o Tempo");
    B --> C("Todas as A√ß√µes Exploradas");
    D["A√ß√µes de Baixo Valor"] --> E("Selecionadas Menos Frequentemente");

```

**Proposi√ß√£o 1.** *O m√©todo UCB garante que todas as a√ß√µes sejam exploradas infinitamente em um ambiente estacion√°rio, desde que $c > 0$.*

*Proof Sketch*: A presen√ßa do termo $\ln t$ no numerador da express√£o de incerteza faz com que o incentivo √† explora√ß√£o cres√ßa lentamente com o tempo. Como o logaritmo natural tende a infinito, todas as a√ß√µes ser√£o eventualmente exploradas, mesmo que com uma frequ√™ncia cada vez menor. Uma a√ß√£o que inicialmente n√£o √© escolhida devido ao seu valor $Q_t(a)$ ser baixo ou por ter sido explorada v√°rias vezes, eventualmente ter√° sua incerteza aumentada o suficiente para ser selecionada novamente.
```mermaid
graph LR
    A["ln t"] --> B("Crescimento Lento da Explora√ß√£o");
    B --> C("Explora√ß√£o Infinita");
    C --> D("Todas A√ß√µes Selecionadas Eventualmente");

```

> üí° **Exemplo Num√©rico:** Para demonstrar a explora√ß√£o infinita, vamos simular o comportamento do termo $\sqrt{\frac{\ln t}{N_t(a)}}$ para uma a√ß√£o *$a$* com $N_t(a)$ inicialmente pequeno e que cresce lentamente ao longo do tempo. Considere que a a√ß√£o foi selecionada uma vez por cada 10 passos no inicio e depois mais lentamente. O par√¢metro *$c$* = 1.
>
> |  t  |   $N_t(a)$    |  $\sqrt{\frac{\ln t}{N_t(a)}}$ |
> |-----|---------------|--------------------------------|
> |  10 |       1      |   $\sqrt{\frac{\ln 10}{1}} \approx 1.517 $  |
> | 100 |       10    |  $\sqrt{\frac{\ln 100}{10}} \approx 0.678$    |
> | 1000|       20    |  $\sqrt{\frac{\ln 1000}{20}} \approx 0.591$     |
> |10000|      30     |   $\sqrt{\frac{\ln 10000}{30}} \approx 0.552 $   |
>
>  Note como o termo de incerteza diminui ao longo do tempo, mas mesmo assim, ele nunca atinge zero.  Com um *$c$* > 0, a explora√ß√£o continuar√°, garantindo que essa a√ß√£o ser√° selecionada novamente no futuro, se necess√°rio, por mais que seja explorada menos frequentemente.

### An√°lise Comparativa
O m√©todo UCB possui algumas vantagens not√°veis em rela√ß√£o a abordagens mais simples como o m√©todo Œµ-greedy [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2):
- **Explora√ß√£o Direcionada**: Enquanto o Œµ-greedy explora a√ß√µes n√£o-greedy aleatoriamente, o UCB direciona a explora√ß√£o para a√ß√µes com maior potencial de serem √≥timas, equilibrando o conhecimento atual com a incerteza.
- **Adaptabilidade**: O termo de incerteza em UCB se adapta naturalmente ao tempo, diminuindo a explora√ß√£o √† medida que as estimativas melhoram, o que √© mais eficiente do que a taxa constante de explora√ß√£o do Œµ-greedy.
- **Performance Superior**: Em muitos casos, o UCB demonstra melhor desempenho do que o Œµ-greedy, especialmente em tarefas onde a explora√ß√£o inicial √© crucial para descobrir a√ß√µes √≥timas.
```mermaid
graph LR
    A["Œµ-greedy"] --> B("Explora√ß√£o Aleat√≥ria");
    C["UCB"] --> D("Explora√ß√£o Direcionada");
    D --> E("Adaptabilidade Temporal");
    E --> F("Desempenho Superior");

```

> üí° **Exemplo Num√©rico:** Para comparar UCB com Œµ-greedy, vamos simular um cen√°rio com duas a√ß√µes (A e B) onde o valor verdadeiro de A √© 0.7 e de B √© 0.9, e Œµ=0.1.  Vamos comparar o n√∫mero de vezes que a a√ß√£o √≥tima (B) √© escolhida ao longo de 100 itera√ß√µes.
>
> Para o UCB, usaremos c = 2. Vamos considerar que a escolha inicial de cada a√ß√£o seja aleatoria, e em seguida o metodo de escolha das acoes de acordo com a formula do UCB.
>
> ```python
> import numpy as np
>
> def ucb_action(Q, N, t, c=2):
>     ucb_values = Q + c * np.sqrt(np.log(t) / (N + 1e-6)) #add a small value to avoid division by zero
>     return np.argmax(ucb_values)
>
> def epsilon_greedy_action(Q, epsilon):
>    if np.random.rand() < epsilon:
>        return np.random.randint(len(Q))  # Explore
>    else:
>        return np.argmax(Q) # Exploit
>
> num_actions = 2
> true_values = [0.7, 0.9]
>
> # UCB Simulation
> Q_ucb = np.zeros(num_actions)
> N_ucb = np.zeros(num_actions)
> ucb_optimal_selections = 0
>
> # Epsilon-Greedy Simulation
> Q_epsilon = np.zeros(num_actions)
> N_epsilon = np.zeros(num_actions)
> epsilon_greedy_optimal_selections = 0
>
>
> for t in range(1, 101):
>     # UCB
>     action_ucb = ucb_action(Q_ucb, N_ucb, t)
>     reward_ucb = np.random.normal(true_values[action_ucb], 0.1)
>     N_ucb[action_ucb] += 1
>     Q_ucb[action_ucb] = Q_ucb[action_ucb] + (reward_ucb - Q_ucb[action_ucb]) / N_ucb[action_ucb]
>     if action_ucb == 1:
>         ucb_optimal_selections += 1
>
>
>     # Epsilon-Greedy
>     action_epsilon = epsilon_greedy_action(Q_epsilon, 0.1)
>     reward_epsilon = np.random.normal(true_values[action_epsilon], 0.1)
>     N_epsilon[action_epsilon] += 1
>     Q_epsilon[action_epsilon] = Q_epsilon[action_epsilon] + (reward_epsilon - Q_epsilon[action_epsilon]) / N_epsilon[action_epsilon]
>     if action_epsilon == 1:
>         epsilon_greedy_optimal_selections += 1
>
> print(f"UCB Optimal Selections: {ucb_optimal_selections}")
> print(f"Epsilon-Greedy Optimal Selections: {epsilon_greedy_optimal_selections}")
> ```
> Os resultados de uma simula√ß√£o t√≠pica podem ser:
>
> ```text
> UCB Optimal Selections: 85
> Epsilon-Greedy Optimal Selections: 78
> ```
>
>  Como podemos observar, o m√©todo UCB, nesse caso, selecionou a a√ß√£o √≥tima (B) mais vezes do que o m√©todo Œµ-greedy, mostrando uma performance superior e uma explora√ß√£o mais direcionada.

**Lema 1.**  *A taxa de explora√ß√£o do UCB decresce com o tempo, garantindo uma transi√ß√£o mais suave da explora√ß√£o para a explota√ß√£o.*

*Proof Sketch:* A taxa de explora√ß√£o √© dada pelo termo $c\sqrt{\frac{\ln t}{N_t(a)}}$. Enquanto $\ln t$ cresce lentamente, o valor de $N_t(a)$ tamb√©m cresce com o tempo, conforme as a√ß√µes s√£o selecionadas. Assim, o denominador tende a crescer mais r√°pido que o numerador, diminuindo a taxa de explora√ß√£o.
```mermaid
graph LR
    A["Taxa de Explora√ß√£o"] --> B["c * sqrt(ln t / N_t(a))"];
    B --> C["N_t(a) Cresce"];
    C --> D["Redu√ß√£o da Taxa de Explora√ß√£o"];
    D --> E["Transi√ß√£o Explora√ß√£o/Explota√ß√£o"];

```

> üí° **Exemplo Num√©rico:** Vamos demonstrar a taxa decrescente da explora√ß√£o com o tempo. Consideramos novamente uma a√ß√£o com *$c$*=1 e $N_t(a)$ que aumenta a cada vez que a a√ß√£o √© escolhida. Vamos ver a taxa de explora√ß√£o da a√ß√£o para diferentes valores de *$t$*.
>
>  | t    |   $N_t(a)$  | $\sqrt{\frac{\ln t}{N_t(a)}}$  |
>  |------|-------------|-------------------------------|
>  | 10   |    2    | $\sqrt{\frac{\ln 10}{2}} \approx 1.072$ |
>  | 100  |    15    | $\sqrt{\frac{\ln 100}{15}} \approx 0.554$ |
>  | 1000 |    100  | $\sqrt{\frac{\ln 1000}{100}} \approx 0.263$|
>  |10000 |   500  | $\sqrt{\frac{\ln 10000}{500}} \approx 0.145$ |
>
>  A tabela mostra que, √† medida que o tempo *$t$* e o n√∫mero de vezes que a a√ß√£o √© selecionada, *$N_t(a)$*, aumentam, o termo de explora√ß√£o diminui, indicando uma transi√ß√£o da explora√ß√£o para a explota√ß√£o.

Apesar dessas vantagens, o UCB tamb√©m apresenta limita√ß√µes:
- **Complexidade**: O UCB √© mais complexo que o Œµ-greedy e pode ser menos intuitivo para implementar.
- **N√£o-Estacionariedade**: O UCB n√£o lida bem com problemas n√£o-estacion√°rios, onde as probabilidades de recompensa mudam ao longo do tempo [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).
- **Generaliza√ß√£o**: Em configura√ß√µes de aprendizado por refor√ßo mais gerais, como aquelas com grandes espa√ßos de estados e a√ß√µes, o m√©todo UCB n√£o √© facilmente generalizado [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).
```mermaid
graph LR
    A["UCB"] --> B("Complexidade");
    A --> C("Limita√ß√µes em N√£o-Estacionariedade");
        C-->D("Recompensas Variam");
    A --> E("Dificuldade de Generaliza√ß√£o");
     E-->F("Espa√ßos de A√ß√µes e Estados Grandes");
```
**Teorema 1.** *O UCB1, uma variante do UCB, tem uma garantia te√≥rica de arrependimento logar√≠tmico no contexto de bandidos multiarmados estoc√°sticos.*

*Proof Sketch:* O UCB1 (uma vers√£o espec√≠fica do UCB com escolha apropriada de *$c$*) possui uma demonstra√ß√£o formal que seu arrependimento acumulado (a diferen√ßa entre as recompensas obtidas e as recompensas que um agente que sempre escolhe a melhor a√ß√£o obteria) cresce no m√°ximo logaritmicamente com o tempo. O fator logar√≠tmico mostra que o UCB1 consegue alcan√ßar um bom equil√≠brio entre explora√ß√£o e explota√ß√£o. O resultado se baseia em t√©cnicas de an√°lise probabil√≠stica e desigualdades como a de Hoeffding.
```mermaid
graph LR
    A["UCB1"] --> B("Arrependimento Logar√≠tmico");
    B --> C("Arrependimento Cresce em ln(tempo)");
      C-->D("Equil√≠brio Explora√ß√£o/Explota√ß√£o");
```

> üí° **Exemplo Num√©rico:** Para ilustrar o conceito de arrependimento logar√≠tmico, vamos simular o UCB1 em um cen√°rio de 2 bandidos, onde o bandido 1 tem recompensa m√©dia de 0.3 e bandido 2 tem recompensa m√©dia de 0.7. A melhor a√ß√£o seria sempre escolher o bandido 2, e o arrependimento seria a diferen√ßa entre a recompensa acumulada obtida e a recompensa que seria obtida se sempre escolhessemos a melhor a√ß√£o.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def ucb1(Q, N, t):
>    ucb_values = Q + np.sqrt(2 * np.log(t) / (N + 1e-6))
>    return np.argmax(ucb_values)
>
> def simulate_bandit(num_actions, true_values, num_steps):
>    Q = np.zeros(num_actions)
>    N = np.zeros(num_actions)
>    rewards = []
>    cumulative_regret = [0]
>    for t in range(1, num_steps + 1):
>      action = ucb1(Q, N, t)
>      reward = np.random.normal(true_values[action], 0.1)
>      N[action] += 1
>      Q[action] = Q[action] + (reward - Q[action]) / N[action]
>      rewards.append(reward)
>      optimal_reward = max(true_values)
>      regret = optimal_reward - reward
>      cumulative_regret.append(cumulative_regret[-1]+regret)
>
>    return cumulative_regret
>
> num_actions = 2
> true_values = [0.3, 0.7]
> num_steps = 1000
>
> cumulative_regret = simulate_bandit(num_actions, true_values, num_steps)
>
> time_steps = list(range(num_steps + 1))
> plt.plot(time_steps, cumulative_regret)
> plt.xlabel("Time Step")
> plt.ylabel("Cumulative Regret")
> plt.title("Cumulative Regret over Time for UCB1")
> plt.grid(True)
> plt.show()
> ```
> Ao executar esse c√≥digo, vemos um gr√°fico de "Cumulative Regret" versus "Time Steps". O gr√°fico mostrar√° que, apesar do arrependimento aumentar, o crescimento √© sublinear, ou seja, o arrependimento cresce em propor√ß√£o ao logaritmo do tempo.
>
> ```mermaid
>  graph LR
>      A["Time Step"] --> B("Cumulative Regret")
> ```
>
> Isso demonstra que, no longo prazo, o algoritmo se aproxima da recompensa √≥tima.

**Corol√°rio 1.1** *A propriedade do arrependimento logar√≠tmico do UCB1 implica que, no longo prazo, a m√©dia das recompensas obtidas pelo algoritmo se aproxima da recompensa √≥tima.*
*Proof Sketch*: Este corol√°rio segue diretamente do Teorema 1. Se o arrependimento acumulado cresce logaritmicamente, ent√£o o arrependimento m√©dio (o arrependimento dividido pelo n√∫mero de passos de tempo) tende a zero quando o tempo vai para o infinito. Consequentemente, o desempenho do UCB1 se aproxima do desempenho do agente √≥timo no longo prazo.
```mermaid
graph LR
    A["Arrependimento Logar√≠tmico"] --> B("M√©dia das Recompensas -> √ìtima");
        B --> C("Desempenho UCB1 -> √ìtimo");

```
### Conclus√£o
O m√©todo Upper-Confidence-Bound (UCB) oferece uma abordagem sofisticada para a explora√ß√£o em problemas de bandidos multiarmados, equilibrando eficazmente a explota√ß√£o com a busca por a√ß√µes potencialmente melhores. Ao contr√°rio do Œµ-greedy, o UCB direciona a explora√ß√£o para a√ß√µes com maior incerteza em suas estimativas de valor, proporcionando uma adapta√ß√£o mais eficiente ao longo do tempo e frequentemente resultando em um desempenho superior. Apesar de suas limita√ß√µes em cen√°rios n√£o-estacion√°rios e configura√ß√µes de aprendizado por refor√ßo mais gerais, o m√©todo UCB permanece uma ferramenta valiosa para otimizar a explora√ß√£o em problemas de bandidos multiarmados, destacando a import√¢ncia de uma explora√ß√£o mais inteligente e estrat√©gica.

### Refer√™ncias
[^1]: "In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback differs from, and yet can be combined with, instructive feedback." *(Trecho de Multi-armed Bandits)*
[^2]: "Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. The greedy actions are those that look best at present, but some of the other actions may actually be better. …õ-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions according to $A_t = \underset{a}{\operatorname{argmax}} \left[Q_t(a) + c\sqrt{\frac{\ln t}{N_t(a)}}\right]$" *(Trecho de Multi-armed Bandits)*
