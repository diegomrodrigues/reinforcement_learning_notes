## Associative Search: Learning with Contextual Bandits

### Introdu√ß√£o
Em cap√≠tulos anteriores, exploramos cen√°rios de **aprendizado n√£o associativo**, onde o objetivo √© identificar uma √∫nica a√ß√£o √≥tima, seja em um ambiente *estacion√°rio* ou *n√£o estacion√°rio* [^41]. No entanto, muitas situa√ß√µes do mundo real exigem a capacidade de associar diferentes a√ß√µes a diferentes situa√ß√µes. Esta se√ß√£o introduz o conceito de **busca associativa**, tamb√©m conhecida como **bandidos contextuais**, que representa um passo importante em dire√ß√£o ao aprendizado por refor√ßo completo [^41].

### Conceitos Fundamentais
A busca associativa lida com cen√°rios onde o ambiente apresenta diferentes *contextos* ou *estados*, e a a√ß√£o √≥tima pode variar dependendo do contexto atual [^41]. O objetivo √© aprender uma **pol√≠tica**, ou seja, um mapeamento de estados para a√ß√µes, de modo a maximizar a recompensa esperada. Este cen√°rio serve como uma ponte entre o problema do bandido k-bra√ßos, que visa identificar uma √∫nica a√ß√£o √≥tima, e o problema completo de aprendizado por refor√ßo, que permite que as a√ß√µes influenciem os estados futuros [^41].

Considere o seguinte exemplo adaptado da se√ß√£o 2.9 [^41]: Imagine m√∫ltiplos problemas de bandido de *k*-bra√ßos, onde um deles √© selecionado aleatoriamente a cada passo. Sem informa√ß√£o adicional, este problema poderia ser tratado como um √∫nico problema de bandido de *k*-bra√ßos estacion√°rio [^41]. No entanto, suponha que voc√™ receba uma *pista* ou *contexto* sobre qual problema de bandido voc√™ est√° enfrentando. Por exemplo, voc√™ est√° diante de uma m√°quina ca√ßa-n√≠queis que muda a cor de seu display conforme seus valores de a√ß√£o mudam [^41].

Nesse cen√°rio, o problema se transforma em uma tarefa de aprendizado associativo. Seu objetivo √© aprender uma **pol√≠tica** que associe cada tarefa (indicada pela cor do display) com a a√ß√£o ideal para aquela tarefa [^41]. Por exemplo:
*   Se a tela estiver vermelha, selecione o bra√ßo 1.
*   Se a tela estiver verde, selecione o bra√ßo 2.

Com uma pol√≠tica bem definida, voc√™ pode alcan√ßar um desempenho significativamente superior do que conseguiria na aus√™ncia de qualquer informa√ß√£o que distinguisse um problema de bandido do outro [^41]. Essa abordagem permite adaptar o comportamento com base no contexto, levando a estrat√©gias de tomada de decis√£o mais eficientes.

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que temos dois contextos (vermelho e verde) e duas a√ß√µes (bra√ßo 1 e bra√ßo 2). As recompensas esperadas s√£o as seguintes:
>
> *   Contexto Vermelho:
>     *   Bra√ßo 1: Recompensa esperada = 10
>     *   Bra√ßo 2: Recompensa esperada = 2
> *   Contexto Verde:
>     *   Bra√ßo 1: Recompensa esperada = 2
>     *   Bra√ßo 2: Recompensa esperada = 10
>
> A pol√≠tica √≥tima seria:
>
> *   Se o contexto √© vermelho, selecionar o bra√ßo 1.
> *   Se o contexto √© verde, selecionar o bra√ßo 2.
>
> A recompensa esperada total seguindo a pol√≠tica √≥tima, se cada contexto ocorrer com probabilidade 0.5, √©:
>
> $$\text{Recompensa Esperada} = 0.5 \times 10 + 0.5 \times 10 = 10$$
>
> Sem informa√ß√£o de contexto, a melhor a√ß√£o seria selecionar aleatoriamente e aprender qual bra√ßo d√° mais recompensa *em m√©dia*, o que resultaria numa recompensa menor a longo prazo.

Para formalizar isso, denotamos:
*   $s \in \mathcal{S}$: o conjunto de poss√≠veis estados ou contextos
*   $a \in \mathcal{A}$: o conjunto de poss√≠veis a√ß√µes
*   $r(s, a)$: a recompensa esperada ao tomar a a√ß√£o $a$ no estado $s$

O objetivo √© aprender uma pol√≠tica $\pi: \mathcal{S} \rightarrow \mathcal{A}$ que maximize a recompensa esperada a longo prazo.

Diferentes algoritmos podem ser adaptados para resolver problemas de busca associativa. Por exemplo, podemos usar uma tabela de valores $Q(s, a)$ que estima o valor de tomar a a√ß√£o $a$ no estado $s$. Os algoritmos *Œµ*-greedy e UCB (Upper Confidence Bound) explorados anteriormente podem ser estendidos para usar esses valores $Q$ para a sele√ß√£o de a√ß√£o, resultando em uma busca associativa eficiente.

Para explicitar a adapta√ß√£o do algoritmo *Œµ*-greedy, podemos definir a seguinte abordagem: Com probabilidade $\epsilon$, selecionamos uma a√ß√£o aleat√≥ria de $\mathcal{A}$. Caso contr√°rio (com probabilidade $1 - \epsilon$), selecionamos a a√ß√£o $a$ que maximiza $Q(s, a)$ para o estado atual $s$. Formalmente,

$$a = \begin{cases}
\text{a√ß√£o aleat√≥ria de } \mathcal{A} & \text{com probabilidade } \epsilon \\
\underset{a \in \mathcal{A}}{\text{argmax }} Q(s, a) & \text{com probabilidade } 1 - \epsilon
\end{cases}$$

Ap√≥s observar a recompensa $r$ resultante da a√ß√£o $a$ no estado $s$, atualizamos a estimativa $Q(s, a)$ usando uma regra de atualiza√ß√£o como:

$$Q(s, a) \leftarrow Q(s, a) + \alpha [r - Q(s, a)]$$

onde $\alpha$ √© a taxa de aprendizado.

> üí° **Exemplo Num√©rico:**
>
> Suponha que $\epsilon = 0.1$, $\alpha = 0.2$, e temos dois estados ($s_1$ e $s_2$) e duas a√ß√µes ($a_1$ e $a_2$). Inicializamos $Q(s, a)$ para todos os pares estado-a√ß√£o como 0.
>
> 1.  **Estado $s_1$**:
>     *   No tempo $t=1$, exploramos (com probabilidade 0.1) e selecionamos a a√ß√£o $a_2$ aleatoriamente. Recebemos uma recompensa $r = 5$.
>     *   Atualizamos $Q(s_1, a_2)$: $Q(s_1, a_2) \leftarrow 0 + 0.2 * (5 - 0) = 1$.
>     *   No tempo $t=2$, exploramos novamente e selecionamos $a_1$. Recebemos $r=10$.
>     *   Atualizamos $Q(s_1, a_1)$: $Q(s_1, a_1) \leftarrow 0 + 0.2 * (10 - 0) = 2$.
>     *   No tempo $t=3$, exploramos e selecionamos $a_2$ novamente. Recebemos $r=5$.
>     *   Atualizamos $Q(s_1, a_2)$: $Q(s_1, a_2) \leftarrow 1 + 0.2 * (5 - 1) = 1.8$.
>     *   No tempo $t=4$, com probabilidade 0.9, escolhemos a a√ß√£o que maximiza $Q(s_1, a)$. Como $Q(s_1, a_1) = 2$ e $Q(s_1, a_2) = 1.8$, selecionamos $a_1$. Suponha que recebemos $r=10$.
>     *   Atualizamos $Q(s_1, a_1)$: $Q(s_1, a_1) \leftarrow 2 + 0.2 * (10 - 2) = 3.6$.
>
> 2.  **Estado $s_2$**:
>     *   No tempo $t=1$, exploramos e selecionamos $a_1$. Recebemos $r = 2$.
>     *   Atualizamos $Q(s_2, a_1)$: $Q(s_2, a_1) \leftarrow 0 + 0.2 * (2 - 0) = 0.4$.
>     *   No tempo $t=2$, exploramos e selecionamos $a_2$. Recebemos $r = 8$.
>     *   Atualizamos $Q(s_2, a_2)$: $Q(s_2, a_2) \leftarrow 0 + 0.2 * (8 - 0) = 1.6$.
>
> Ap√≥s algumas itera√ß√µes, os valores de $Q(s, a)$ convergir√£o para as recompensas esperadas reais para cada estado-a√ß√£o, permitindo que o agente tome decis√µes mais informadas.

**Teorema 1:** *O algoritmo $\epsilon$-greedy adaptado para busca associativa, com uma taxa de aprendizado $\alpha$ apropriada e explora√ß√£o $\epsilon > 0$, converge para a pol√≠tica √≥tima sob certas condi√ß√µes de regularidade no ambiente.*

*Prova (Esbo√ßo):* A prova segue uma l√≥gica similar √† converg√™ncia do $\epsilon$-greedy no cen√°rio n√£o associativo, mas agora considera a converg√™ncia de $Q(s, a)$ para cada estado $s \in \mathcal{S}$. A explora√ß√£o $\epsilon > 0$ garante que todas as a√ß√µes em todos os estados sejam visitadas infinitas vezes, enquanto a taxa de aprendizado $\alpha$ controla a velocidade e a estabilidade da converg√™ncia. Condi√ß√µes de regularidade, como recompensas limitadas, garantem que as m√©dias amostrais $Q(s, a)$ convirjam para os valores esperados $r(s, a)$.

Para tornar a prova mais expl√≠cita, podemos esbo√ßar um argumento mais detalhado:

**Prova:**

Queremos mostrar que $Q(s,a)$ converge para $r(s,a)$ para todo $s \in \mathcal{S}$ e $a \in \mathcal{A}$ sob as condi√ß√µes fornecidas.

I. **Defini√ß√£o:** Seja $Q_t(s,a)$ a estimativa de $Q(s,a)$ no tempo $t$. Queremos provar que $\lim_{t \to \infty} Q_t(s,a) = r(s,a)$.

II. **Atualiza√ß√£o:** A regra de atualiza√ß√£o √© $Q_{t+1}(s, a) = Q_t(s, a) + \alpha [r_t - Q_t(s, a)]$, onde $r_t$ √© a recompensa observada no tempo $t$ ao tomar a a√ß√£o $a$ no estado $s$.

III. **Reescrita:** Podemos reescrever a atualiza√ß√£o como $Q_{t+1}(s, a) = (1 - \alpha) Q_t(s, a) + \alpha r_t$.

IV. **Converg√™ncia:** Como $\epsilon > 0$, cada par $(s, a)$ √© visitado infinitas vezes. Portanto, podemos considerar uma sequ√™ncia de vezes $t_1, t_2, ...$ quando o par $(s, a)$ √© visitado.

V. **M√©dia:** $Q_t(s, a)$ √© uma m√©dia ponderada das recompensas observadas. Se $\alpha$ satisfaz as condi√ß√µes estoc√°sticas de Robbins-Monro (i.e., $\sum_{t=1}^{\infty} \alpha_t = \infty$ e $\sum_{t=1}^{\infty} \alpha_t^2 < \infty$), ent√£o $Q_t(s, a)$ converge para o valor esperado $r(s, a)$.

VI. **Regularidade:** As condi√ß√µes de regularidade (recompensas limitadas) garantem que a vari√¢ncia das recompensas seja finita, o que √© necess√°rio para a converg√™ncia.

VII. **Pol√≠tica √ìtima:** Quando $Q(s, a)$ converge para $r(s, a)$ para todo $s$ e $a$, ent√£o o algoritmo $\epsilon$-greedy converge para a pol√≠tica √≥tima, pois ele explora com probabilidade $\epsilon$ e explora o melhor valor estimado $Q(s, a)$ com probabilidade $1 - \epsilon$.

VIII. **Conclus√£o:** Portanto, o algoritmo $\epsilon$-greedy adaptado para busca associativa converge para a pol√≠tica √≥tima sob as condi√ß√µes de regularidade e a taxa de aprendizado $\alpha$ apropriada. ‚ñ†

#### Rela√ß√£o com o Aprendizado por Refor√ßo
√â importante notar que a busca associativa difere do problema completo de aprendizado por refor√ßo em um aspecto crucial: as a√ß√µes influenciam apenas a recompensa imediata e n√£o afetam o pr√≥ximo estado [^41]. Em outras palavras, o pr√≥ximo estado √© independente da a√ß√£o tomada. Se as a√ß√µes pudessem influenciar o pr√≥ximo estado, ter√≠amos um problema de aprendizado por refor√ßo completo, que ser√° abordado nos cap√≠tulos subsequentes [^41].

Antes de prosseguir, vale a pena formalizar a no√ß√£o de pol√≠tica √≥tima na busca associativa. Uma pol√≠tica $\pi^*$ √© considerada √≥tima se, para cada estado $s \in \mathcal{S}$, ela seleciona a a√ß√£o que maximiza a recompensa esperada naquele estado.

**Defini√ß√£o:** Uma pol√≠tica $\pi^*$ √© √≥tima se, para todo $s \in \mathcal{S}$:

$$\pi^*(s) = \underset{a \in \mathcal{A}}{\text{argmax }} r(s, a)$$

Al√©m disso, podemos definir o valor √≥timo $V^*(s)$ de um estado $s$ como a recompensa esperada ao seguir a pol√≠tica √≥tima $\pi^*$ naquele estado:

$$V^*(s) = r(s, \pi^*(s))$$

**Teorema 2:** Dada uma pol√≠tica √≥tima $\pi^*$, o valor √≥timo $V^*(s)$ satisfaz a equa√ß√£o de otimalidade de Bellman para busca associativa:

$$V^*(s) = \max_{a \in \mathcal{A}} r(s, a)$$

**Prova:**

I. **Defini√ß√£o de Valor √ìtimo:** O valor √≥timo $V^*(s)$ √© definido como a recompensa esperada ao seguir a pol√≠tica √≥tima $\pi^*$ no estado $s$: $V^*(s) = r(s, \pi^*(s))$.

II. **Pol√≠tica √ìtima:** A pol√≠tica √≥tima $\pi^*$ seleciona a a√ß√£o que maximiza a recompensa esperada no estado $s$: $\pi^*(s) = \underset{a \in \mathcal{A}}{\text{argmax }} r(s, a)$.

III. **Substitui√ß√£o:** Substituindo a defini√ß√£o de $\pi^*(s)$ na equa√ß√£o de $V^*(s)$, obtemos: $V^*(s) = r(s, \underset{a \in \mathcal{A}}{\text{argmax }} r(s, a))$.

IV. **Maximiza√ß√£o:** Como $r(s, \underset{a \in \mathcal{A}}{\text{argmax }} r(s, a))$ representa a recompensa m√°xima que pode ser obtida no estado $s$, podemos reescrever como: $V^*(s) = \max_{a \in \mathcal{A}} r(s, a)$.

V. **Conclus√£o:** Portanto, o valor √≥timo $V^*(s)$ satisfaz a equa√ß√£o de otimalidade de Bellman para busca associativa: $V^*(s) = \max_{a \in \mathcal{A}} r(s, a)$. ‚ñ†

#### Exerc√≠cio Ilustrativo
Revisitando o Exerc√≠cio 2.10 [^41], considere um problema de bandido de 2 bra√ßos, onde os valores verdadeiros das a√ß√µes mudam aleatoriamente a cada etapa de tempo. Especificamente, assuma que, para qualquer etapa de tempo, os valores verdadeiros das a√ß√µes 1 e 2 s√£o respectivamente 10 e 20 com probabilidade 0,5 (caso A), e 90 e 80 com probabilidade 0,5 (caso B). Se voc√™ n√£o for capaz de dizer qual caso voc√™ enfrenta em qualquer etapa, qual a melhor recompensa esperada que voc√™ pode alcan√ßar e como voc√™ deve se comportar para alcan√ß√°-la? Agora, suponha que em cada etapa voc√™ seja informado se voc√™ est√° enfrentando o caso A ou o caso B (embora voc√™ ainda n√£o conhe√ßa os valores verdadeiros da a√ß√£o). Esta √© uma tarefa de pesquisa associativa. Qual a melhor recompensa esperada que voc√™ pode alcan√ßar nesta tarefa e como voc√™ deve se comportar para alcan√ß√°-la?

> üí° **Exemplo Num√©rico:**
>
> **Sem Informa√ß√£o (Bandido k-bra√ßos):**
>
> *   Caso A: Bra√ßo 1 = 10, Bra√ßo 2 = 20
> *   Caso B: Bra√ßo 1 = 90, Bra√ßo 2 = 80
>
> A probabilidade de cada caso √© 0.5.
>
> Recompensa esperada do Bra√ßo 1: $0.5 \times 10 + 0.5 \times 90 = 50$
>
> Recompensa esperada do Bra√ßo 2: $0.5 \times 20 + 0.5 \times 80 = 50$
>
> Nesse caso, a melhor estrat√©gia √© selecionar qualquer um dos bra√ßos, j√° que ambos t√™m a mesma recompensa esperada de 50.
>
> **Com Informa√ß√£o (Busca Associativa):**
>
> *   Caso A (Estado A): Selecionar o Bra√ßo 2 (recompensa 20)
> *   Caso B (Estado B): Selecionar o Bra√ßo 1 (recompensa 90)
>
> Recompensa esperada total: $0.5 \times 20 + 0.5 \times 90 = 55$
>
> Isso demonstra que, com informa√ß√£o de contexto, podemos aumentar a recompensa esperada em compara√ß√£o com o cen√°rio sem informa√ß√£o.

### Conclus√£o
A busca associativa (bandidos contextuais) representa um passo fundamental em dire√ß√£o ao aprendizado por refor√ßo, introduzindo a capacidade de associar a√ß√µes a estados ou contextos espec√≠ficos [^41]. Embora as a√ß√µes ainda n√£o influenciem os estados futuros, a capacidade de tomar decis√µes informadas com base no contexto do estado representa um avan√ßo significativo em rela√ß√£o aos problemas de bandido n√£o associativos. Ao adaptar algoritmos como *Œµ*-greedy e UCB, podemos efetivamente aprender pol√≠ticas que maximizam a recompensa esperada em ambientes de busca associativa. O estudo de bandidos contextuais fornece insights valiosos e t√©cnicas que ser√£o fundamentais para abordar o problema completo de aprendizado por refor√ßo, que ser√° explorado nos pr√≥ximos cap√≠tulos.

### Refer√™ncias
[^41]: Cap√≠tulo 2, Se√ß√£o 2.9
<!-- END -->