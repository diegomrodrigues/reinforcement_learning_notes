## Associative Busca e Bandidos Contextuais

### Introdu√ß√£o
Este cap√≠tulo explora o conceito de **Associative Search**, tamb√©m conhecido como **Bandidos Contextuais**, como uma extens√£o do problema de *k-armed bandit* [^1]. Enquanto as tarefas *nonassociative* focam em encontrar uma √∫nica a√ß√£o ideal, seja de forma est√°tica ou rastreando suas mudan√ßas ao longo do tempo, a *associative search* introduz a complexidade de aprender uma *policy*, ou seja, um mapeamento de situa√ß√µes para as a√ß√µes √≥timas [^2].

### Conceitos Fundamentais
Em tarefas *nonassociative*, o objetivo √© simples: identificar a melhor a√ß√£o em um ambiente estacion√°rio ou rastrear a a√ß√£o ideal em um ambiente n√£o estacion√°rio [^2]. No entanto, problemas de *reinforcement learning* (RL) geralmente envolvem m√∫ltiplos contextos, exigindo que o agente aprenda uma *policy* que associa cada contexto √† a√ß√£o mais recompensadora. A *associative search* serve como uma ponte entre o problema de *k-armed bandit* e o problema completo de RL, mantendo a caracter√≠stica de que cada a√ß√£o afeta apenas a recompensa imediata [^2].

Para ilustrar, imagine m√∫ltiplos problemas de *k-armed bandit*, onde, a cada passo, um deles √© escolhido aleatoriamente [^2]. Se a probabilidade de cada problema ser selecionado permanecer constante ao longo do tempo, a tarefa se assemelha a um √∫nico problema de *k-armed bandit* estacion√°rio. No entanto, se o agente receber uma pista sobre a identidade do problema de *k-armed bandit* (sem conhecer os valores das a√ß√µes), ele pode aprender uma *policy* que associe essa pista (contexto) √† a√ß√£o ideal para aquele problema espec√≠fico. Por exemplo, se a cor da m√°quina ca√ßa-n√≠queis muda dependendo dos valores das a√ß√µes, o agente pode aprender a associar cada cor √† melhor alavanca a ser puxada [^2].

> üí° **Exemplo Num√©rico:** Suponha que tenhamos duas m√°quinas ca√ßa-n√≠queis. A m√°quina vermelha tem uma recompensa m√©dia de 1 quando puxada e a m√°quina azul tem uma recompensa m√©dia de 5. Se a probabilidade de cada m√°quina ser escolhida √© igual (0.5), ent√£o a recompensa esperada de escolher aleatoriamente seria (1+5)/2 = 3. Agora, se o agente puder identificar a cor da m√°quina antes de puxar a alavanca (o contexto), ele pode aprender a sempre escolher a m√°quina azul, obtendo uma recompensa esperada de 5. Isso demonstra como o conhecimento do contexto pode melhorar o desempenho.

Este cen√°rio √© um exemplo de *associative search*, pois envolve tanto a aprendizagem por tentativa e erro para encontrar as melhores a√ß√µes quanto a associa√ß√£o dessas a√ß√µes aos contextos em que s√£o mais eficazes [^2]. Na literatura moderna, essas tarefas s√£o frequentemente chamadas de *contextual bandits* [^2]. √â importante notar que, se as a√ß√µes pudessem influenciar o pr√≥ximo estado (al√©m da recompensa imediata), ter√≠amos o problema completo de *reinforcement learning* [^2].

Antes de prosseguirmos, √© √∫til formalizar a estrutura de um problema de *contextual bandit*. Um problema de *contextual bandit* pode ser definido como uma tupla $(\mathcal{S}, \mathcal{A}, r)$, onde $\mathcal{S}$ √© o espa√ßo de contextos (ou estados), $\mathcal{A}$ √© o espa√ßo de a√ß√µes e $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ √© a fun√ß√£o de recompensa, que denota a recompensa esperada ao tomar a a√ß√£o $a \in \mathcal{A}$ no contexto $s \in \mathcal{S}$. O objetivo do agente √© aprender uma *policy* $\pi: \mathcal{S} \rightarrow \mathcal{A}$ que maximize a recompensa esperada [^3].

**Defini√ß√£o 1** (Pol√≠tica √ìtima): Uma pol√≠tica $\pi^*$ √© considerada √≥tima se, para todo contexto $s \in \mathcal{S}$, ela seleciona a a√ß√£o que maximiza a recompensa esperada:
$$ \pi^*(s) = \arg\max_{a \in \mathcal{A}} r(s, a) $$

> üí° **Exemplo Num√©rico:** Suponha que $\mathcal{S} = \{s_1, s_2\}$ (dois contextos) e $\mathcal{A} = \{a_1, a_2\}$ (duas a√ß√µes). Se $r(s_1, a_1) = 1$, $r(s_1, a_2) = 2$, $r(s_2, a_1) = 3$, e $r(s_2, a_2) = 0$, ent√£o a pol√≠tica √≥tima $\pi^*$ seria definida como $\pi^*(s_1) = a_2$ e $\pi^*(s_2) = a_1$, pois essas a√ß√µes maximizam a recompensa em cada contexto.

Agora, podemos introduzir um lema que estabelece um limite superior para a recompensa que pode ser obtida em um problema de *contextual bandit*, dada a pol√≠tica √≥tima.

**Lema 1** (Limite Superior da Recompensa): A recompensa esperada m√°xima que um agente pode obter em um problema de *contextual bandit* com pol√≠tica √≥tima $\pi^*$ √© dada por:
$$ V^* = \mathbb{E}_{s \sim \mathcal{D}} [r(s, \pi^*(s))] $$
onde $\mathcal{D}$ representa a distribui√ß√£o de probabilidade sobre o espa√ßo de contextos $\mathcal{S}$.

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, suponha que a distribui√ß√£o de probabilidade sobre os contextos seja uniforme, ou seja, $\mathcal{D}(s_1) = 0.5$ e $\mathcal{D}(s_2) = 0.5$.  A recompensa esperada m√°xima seria $V^* = 0.5 * r(s_1, \pi^*(s_1)) + 0.5 * r(s_2, \pi^*(s_2)) = 0.5 * 2 + 0.5 * 3 = 2.5$.

*Proof.* Este resultado segue diretamente da defini√ß√£o de pol√≠tica √≥tima, onde para cada estado $s$, a a√ß√£o selecionada $\pi^*(s)$ maximiza a recompensa esperada. A recompensa esperada geral √© ent√£o a m√©dia das recompensas m√°ximas em cada estado, ponderada pela distribui√ß√£o dos estados. $\blacksquare$

### Um Exemplo Detalhado

Considere um problema de *2-armed bandit* em que os valores verdadeiros das a√ß√µes variam aleatoriamente ao longo do tempo [^17]. Em um cen√°rio, os valores das a√ß√µes 1 e 2 s√£o 10 e 20, respectivamente, com probabilidade 0.5 (caso A). No outro cen√°rio, os valores s√£o 90 e 80, com probabilidade 0.5 (caso B). Inicialmente, suponha que o agente n√£o consegue distinguir entre os casos A e B. Nesse caso, a melhor estrat√©gia √© calcular a recompensa esperada para cada a√ß√£o, ponderada pela probabilidade de cada caso, e sempre escolher a a√ß√£o com a maior recompensa esperada [^17].

Agora, suponha que o agente seja informado sobre qual caso (A ou B) est√° enfrentando antes de escolher uma a√ß√£o [^17]. Este √© um problema de *associative search*. Para maximizar a recompensa esperada, o agente deve aprender a *policy* ideal: escolher a a√ß√£o que maximiza a recompensa esperada para cada caso. Por exemplo, se o agente aprender que, no caso A, a a√ß√£o 2 (com valor 20) √© melhor e, no caso B, a a√ß√£o 1 (com valor 90) √© melhor, ele pode alcan√ßar uma recompensa esperada muito maior do que no cen√°rio sem contexto [^17].

Podemos quantificar o ganho em recompensa ao introduzir o contexto. Sem contexto, a melhor a√ß√£o seria sempre escolher a a√ß√£o com maior recompensa m√©dia:

$Q(a_1) = 0.5 * 10 + 0.5 * 90 = 50$
$Q(a_2) = 0.5 * 20 + 0.5 * 80 = 50$

Nesse caso, as a√ß√µes s√£o equivalentes, e a recompensa esperada seria 50. Com contexto, a *policy* √≥tima √© escolher a a√ß√£o 2 no caso A e a a√ß√£o 1 no caso B, resultando em uma recompensa esperada de:

$V^* = 0.5 * 20 + 0.5 * 90 = 55$

O ganho em recompensa devido ao contexto √©, portanto, 5.

**Proposi√ß√£o 1:** (Ganho de Informa√ß√£o Contextual) O ganho de recompensa ao utilizar informa√ß√£o contextual √© dado pela diferen√ßa entre a recompensa esperada com a pol√≠tica √≥tima $\pi^*$ e a recompensa esperada sem contexto.

*Proof.*

Para provar que o ganho de recompensa √© dado pela diferen√ßa entre a recompensa esperada com a pol√≠tica √≥tima e a recompensa esperada sem contexto, podemos seguir os seguintes passos:

I. Defini√ß√£o da Recompensa Esperada sem Contexto:
   Seja $Q(a)$ a recompensa esperada da a√ß√£o $a$ sem contexto. Neste caso, $Q(a)$ √© a m√©dia ponderada das recompensas em todos os contextos poss√≠veis, onde os pesos s√£o as probabilidades dos contextos.

II. Defini√ß√£o da Recompensa Esperada com Contexto:
    Seja $V^*$ a recompensa esperada com a pol√≠tica √≥tima $\pi^*$, onde $\pi^*(s)$ √© a a√ß√£o escolhida no contexto $s$. Ent√£o $V^*$ √© a m√©dia ponderada das recompensas √≥timas em cada contexto, ponderada pela distribui√ß√£o dos contextos.

III. Express√£o para o Ganho de Recompensa:
     O ganho de recompensa, denotado por $G$, √© a diferen√ßa entre a recompensa esperada com contexto e a recompensa esperada sem contexto:
     $$G = V^* - \max_{a \in \mathcal{A}} Q(a)$$
     No caso em que as a√ß√µes s√£o equivalentes sem contexto, como demonstrado no exemplo, $\max_{a \in \mathcal{A}} Q(a) = Q(a)$ para qualquer $a$, e a express√£o se torna:
      $$G = V^* - Q(a)$$

IV. Aplicando ao Exemplo:
    No exemplo dado, $Q(a_1) = Q(a_2) = 50$ e $V^* = 55$. Portanto, o ganho de recompensa √©:
     $$G = 55 - 50 = 5$$

V. Generaliza√ß√£o:
   Em geral, se a melhor a√ß√£o sem contexto fornece uma recompensa esperada de $Q^* = \max_{a \in \mathcal{A}} Q(a)$, ent√£o o ganho de recompensa √© a diferen√ßa entre a recompensa esperada com a pol√≠tica √≥tima e $Q^*$. Portanto, a utiliza√ß√£o da informa√ß√£o contextual pode potencialmente aumentar a recompensa esperada.

Assim, provamos que o ganho de recompensa ao utilizar informa√ß√£o contextual √© dado pela diferen√ßa entre a recompensa esperada com a pol√≠tica √≥tima e a recompensa esperada sem contexto. $\blacksquare$

> üí° **Exemplo Num√©rico:** Para ilustrar o efeito da explora√ß√£o, considere um cen√°rio em que, inicialmente, o agente superestima o valor da a√ß√£o 1 no caso A como sendo 30 (em vez de 10). Se o agente explora suficientemente a a√ß√£o 2 no caso A e observa sua recompensa real de 20, ele ir√° atualizar sua estimativa e eventualmente mudar para a a√ß√£o √≥tima (a√ß√£o 2). A taxa de explora√ß√£o influencia diretamente a velocidade com que o agente aprende a pol√≠tica √≥tima. Se a explora√ß√£o for muito baixa, o agente pode ficar preso a uma pol√≠tica sub√≥tima.
```python
import numpy as np
import matplotlib.pyplot as plt

# Configura√ß√£o do problema
n_episodes = 100
rewards_A = [10, 20]  # Recompensas no caso A
rewards_B = [90, 80]  # Recompensas no caso B
probs = [0.5, 0.5]  # Probabilidade de cada caso

# Inicializa√ß√£o das estimativas e contadores
Q_A = [30, 0]  # Estimativa inicial (superestimada para a a√ß√£o 1)
N_A = [0, 0]  # Contador de vezes que cada a√ß√£o foi escolhida

# Loop de aprendizado
rewards_per_episode = []
for episode in range(n_episodes):
    # Escolhe o contexto (A ou B)
    context = np.random.choice([0, 1], p=probs)
    if context == 0:  # Caso A
        # Escolhe a a√ß√£o (explora√ß√£o/explota√ß√£o)
        if np.random.rand() < 0.1:  # 10% de chance de explorar
            action = np.random.choice([0, 1])
        else:
            action = np.argmax(Q_A)
        # Obt√©m a recompensa
        reward = rewards_A[action]
        # Atualiza as estimativas e contadores
        N_A[action] += 1
        Q_A[action] = Q_A[action] + (1/N_A[action]) * (reward - Q_A[action])
        rewards_per_episode.append(reward)

    else:  # Caso B (para simplificar, n√£o implementamos o aprendizado aqui)
        reward = max(rewards_B) # Sempre escolhe a melhor a√ß√£o em B
        rewards_per_episode.append(reward)


# Plotting
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(rewards_per_episode) / np.arange(1, n_episodes + 1))
plt.title("Recompensa M√©dia por Epis√≥dio (Caso A com Explora√ß√£o)")
plt.xlabel("Epis√≥dio")
plt.ylabel("Recompensa M√©dia")
plt.grid(True)
plt.show()
```

### Conclus√£o

A *associative search* ou *contextual bandits* representam um passo crucial em dire√ß√£o ao problema completo de *reinforcement learning*, introduzindo a necessidade de aprender uma *policy* que associa contextos a a√ß√µes [^2]. Embora cada a√ß√£o ainda afete apenas a recompensa imediata, a capacidade de adaptar o comportamento com base no contexto permite solu√ß√µes muito mais eficazes em ambientes complexos. Os m√©todos para resolver o problema de *k-armed bandit* discutidos nos cap√≠tulos anteriores podem ser adaptados para resolver problemas de *associative search*, mantendo o foco no *trade-off* entre explora√ß√£o e explora√ß√£o [^1]. O cap√≠tulo seguinte mergulhar√° no problema completo de *reinforcement learning*, onde as a√ß√µes podem influenciar tanto a recompensa imediata quanto o pr√≥ximo estado [^2].

### Refer√™ncias
[^1]: Chapter 2: Multi-armed Bandits
[^2]: Section 2.9: Associative Search (Contextual Bandits)
[^3]: Li, L., Chu, W., Langford, J., & Schapire, R. E. (2010). A contextual-bandit approach to personalized news article recommendation. *Proceedings of the 19th international conference on World wide web - WWW '10*.
[^17]: Exercise 2.10
<!-- END -->