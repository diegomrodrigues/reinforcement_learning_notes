## Associative Search as an Intermediate Step in Reinforcement Learning

### Introdu√ß√£o
O estudo de **multi-armed bandits** [^1] oferece um arcabou√ßo simplificado para entender os desafios de *exploration* e *exploitation* no aprendizado por refor√ßo (RL). Conforme explorado na se√ß√£o anterior, o problema cl√°ssico do k-armed bandit envolve a sele√ß√£o repetida entre $k$ a√ß√µes, cada uma fornecendo uma recompensa num√©rica baseada em uma distribui√ß√£o de probabilidade estacion√°ria [^1]. No entanto, essa formula√ß√£o assume um cen√°rio *n√£o associativo*, onde a melhor a√ß√£o a ser tomada √© independente do contexto ou estado do ambiente [^1].

Este cap√≠tulo se aprofunda na transi√ß√£o do problema do k-armed bandit para o cen√°rio mais complexo do aprendizado por refor√ßo completo, introduzindo o conceito de **associative search**, tamb√©m conhecido como **contextual bandits** [^17]. Esta formula√ß√£o intermedi√°ria introduz a no√ß√£o de *estado* ou *contexto*, permitindo que o agente aprenda uma *pol√≠tica* que mapeia situa√ß√µes para a√ß√µes √≥timas [^17]. Diferentemente do problema do k-armed bandit, onde o objetivo √© encontrar uma √∫nica a√ß√£o "melhor", no associative search o objetivo √© aprender qual a√ß√£o √© a melhor *em cada situa√ß√£o* [^17].

Para formalizar a transi√ß√£o, podemos definir o problema de k-armed bandit como um caso especial de associative search onde o n√∫mero de estados √© 1.

**Proposi√ß√£o 1** O problema de k-armed bandit √© um caso particular de associative search com um √∫nico estado.

*Proof:* No problema de k-armed bandit, o agente escolhe uma a√ß√£o $a$ de um conjunto de $k$ a√ß√µes sem observar nenhum estado. Isso √© equivalente a um associative search onde existe apenas um √∫nico estado $s$, e a pol√≠tica trivialmente mapeia esse estado para uma a√ß√£o $a \in \{1, ..., k\}$. Portanto, qualquer algoritmo para associative search pode ser usado para resolver o problema do k-armed bandit, considerando que todos os estados s√£o id√™nticos.

I. **Defini√ß√£o do problema k-armed bandit:** No problema k-armed bandit, o agente escolhe uma a√ß√£o $a$ de um conjunto de $k$ a√ß√µes, $\mathcal{A} = \{a_1, a_2, \ldots, a_k\}$. A recompensa $r_a$ para cada a√ß√£o $a$ √© independente de qualquer estado.

II. **Defini√ß√£o do problema associative search com um √∫nico estado:** Considere um problema de associative search com um √∫nico estado $s$. O agente escolhe uma a√ß√£o $a$ de um conjunto de a√ß√µes $\mathcal{A}$ e recebe uma recompensa $r(s, a)$ que depende do estado $s$ e da a√ß√£o $a$.

III. **Equival√™ncia:** Se existe apenas um estado $s$, ent√£o $r(s, a)$ se torna $r(a)$, pois a depend√™ncia do estado √© irrelevante. Isso √© id√™ntico ao problema k-armed bandit, onde a recompensa depende apenas da a√ß√£o escolhida.

IV. **Pol√≠tica:** No problema k-armed bandit, a pol√≠tica √© simplesmente selecionar a a√ß√£o que maximiza a recompensa esperada, ou seja, $\pi = \arg\max_{a \in \mathcal{A}} \mathbb{E}[r(a)]$. No associative search com um √∫nico estado, a pol√≠tica √© $\pi(s) = \arg\max_{a \in \mathcal{A}} \mathbb{E}[r(s, a)] = \arg\max_{a \in \mathcal{A}} \mathbb{E}[r(a)]$, que √© a mesma pol√≠tica.

V. **Conclus√£o:** Portanto, o problema k-armed bandit √© um caso particular do associative search com um √∫nico estado. ‚ñ†

Este resultado destaca a hierarquia entre os dois problemas, onde o k-armed bandit serve como um bloco de constru√ß√£o fundamental para o associative search.

### Conceitos Fundamentais
O conceito de **associative search** representa um passo crucial em dire√ß√£o ao problema completo de aprendizado por refor√ßo, preenchendo a lacuna entre as tarefas n√£o associativas, como o problema do k-armed bandit, e as tarefas mais complexas que envolvem aprendizado de pol√≠ticas dependentes do estado [^17].

No associative search:
*   O agente observa uma representa√ß√£o do **estado** do ambiente [^17].
*   Com base nesse estado, o agente seleciona uma **a√ß√£o** dentre um conjunto de op√ß√µes [^17].
*   O agente recebe uma **recompensa** num√©rica, que depende da a√ß√£o selecionada e do estado do ambiente [^1].
*   O objetivo do agente √© aprender uma **pol√≠tica** que mapeie cada estado para a a√ß√£o que maximize a recompensa esperada [^17].

A principal distin√ß√£o entre o associative search e o problema do k-armed bandit reside na **depend√™ncia do estado**. No k-armed bandit, a distribui√ß√£o de probabilidade da recompensa para cada a√ß√£o √© fixa e independente de qualquer fator externo [^1]. No associative search, a distribui√ß√£o da recompensa depende do estado do ambiente, tornando a tarefa de aprendizado mais desafiadora [^17].

Para formalizar essa depend√™ncia, podemos definir $r(s, a)$ como a recompensa esperada ao selecionar a a√ß√£o $a$ no estado $s$. O objetivo do agente √© encontrar uma pol√≠tica $\pi(s)$ que maximize a recompensa esperada para cada estado:

$$\pi^*(s) = \arg\max_{a} r(s, a)$$

Onde $\pi^*(s)$ denota a pol√≠tica √≥tima.

> üí° **Exemplo Num√©rico:** Suponha que temos dois estados, $s_1$ e $s_2$, e duas a√ß√µes, $a_1$ e $a_2$. As recompensas esperadas s√£o:
>
> *   $r(s_1, a_1) = 2$
> *   $r(s_1, a_2) = 5$
> *   $r(s_2, a_1) = 8$
> *   $r(s_2, a_2) = 1$
>
> Neste caso, a pol√≠tica √≥tima seria $\pi^*(s_1) = a_2$ e $\pi^*(s_2) = a_1$, pois $a_2$ maximiza a recompensa em $s_1$ (5 > 2) e $a_1$ maximiza a recompensa em $s_2$ (8 > 1). O agente deve aprender essa associa√ß√£o estado-a√ß√£o para otimizar suas recompensas.

Em contrapartida com o problema de Reinforcement Learning completo [^17]:

> Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward.

No RL completo, as a√ß√µes podem influenciar o pr√≥ximo estado e a recompensa, introduzindo a complexidade da tomada de decis√£o sequencial [^17]. Associative search restringe a a√ß√£o ao efeito imediato da recompensa.

Podemos definir formalmente a diferen√ßa entre associative search e reinforcement learning.

**Defini√ß√£o 1** (Associative Search) Um problema de associative search √© definido por uma tupla $(\mathcal{S}, \mathcal{A}, R)$, onde:
*   $\mathcal{S}$ √© o conjunto de estados.
*   $\mathcal{A}$ √© o conjunto de a√ß√µes.
*   $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ √© a fun√ß√£o de recompensa, que associa cada par estado-a√ß√£o a uma recompensa.

**Defini√ß√£o 2** (Reinforcement Learning) Um problema de reinforcement learning √© definido por uma tupla $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, onde:
*   $\mathcal{S}$ √© o conjunto de estados.
*   $\mathcal{A}$ √© o conjunto de a√ß√µes.
*   $P: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$ √© a fun√ß√£o de transi√ß√£o de estado, que associa cada par estado-a√ß√£o a uma distribui√ß√£o de probabilidade sobre o pr√≥ximo estado.
*   $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ √© a fun√ß√£o de recompensa, que associa cada par estado-a√ß√£o a uma recompensa.
*   $\gamma \in [0, 1]$ √© o fator de desconto, que pondera as recompensas futuras.

A principal diferen√ßa √© a fun√ß√£o de transi√ß√£o $P$ e o fator de desconto $\gamma$, que n√£o est√£o presentes no associative search. No associative search, o pr√≥ximo estado √© sempre o mesmo (implicitamente determinado pelo ambiente), ou n√£o tem influ√™ncia na recompensa.

### Exemplos e Aplica√ß√µes
Um exemplo ilustrativo de associative search √© o cen√°rio de um slot machine que muda a cor de sua tela, sinalizando diferentes valores de a√ß√£o [^17]. O agente deve aprender a associar cada cor (estado) com o bra√ßo (a√ß√£o) que proporciona a maior recompensa [^17]. Outro exemplo seria um sistema de recomenda√ß√£o que adapta as sugest√µes aos gostos do usu√°rio, e que n√£o consegue prever o efeito a longo prazo das suas a√ß√µes.

Outro exemplo importante √© a personaliza√ß√£o de an√∫ncios online. Um sistema de publicidade pode usar informa√ß√µes sobre o usu√°rio (idade, localiza√ß√£o, hist√≥rico de navega√ß√£o) como o estado e selecionar um an√∫ncio para exibir (a√ß√£o). A recompensa √© um clique no an√∫ncio ou uma convers√£o. O sistema deve aprender qual an√∫ncio exibir para cada tipo de usu√°rio para maximizar a taxa de cliques ou convers√µes. Este cen√°rio se encaixa no framework do associative search porque a escolha de um an√∫ncio n√£o influencia diretamente o estado futuro do usu√°rio.

> üí° **Exemplo Num√©rico:** Considere um sistema de recomenda√ß√£o de filmes.
>
> *   **Estado:** G√™nero de filme preferido pelo usu√°rio (e.g., A√ß√£o, Com√©dia, Drama).
> *   **A√ß√µes:** Recomendar um filme espec√≠fico (e.g., Filme A, Filme B, Filme C).
> *   **Recompensa:** O usu√°rio assiste o filme recomendado (recompensa = 1) ou n√£o (recompensa = 0).
>
> Suponha que, ap√≥s algumas intera√ß√µes, o sistema aprendeu as seguintes recompensas m√©dias:
>
> | Estado (G√™nero) | A√ß√£o (Filme) | Recompensa M√©dia |
> |-----------------|--------------|-------------------|
> | A√ß√£o            | Filme A      | 0.8               |
> | A√ß√£o            | Filme B      | 0.2               |
> | Com√©dia         | Filme B      | 0.7               |
> | Com√©dia         | Filme C      | 0.9               |
> | Drama           | Filme A      | 0.3               |
> | Drama           | Filme C      | 0.6               |
>
> A pol√≠tica aprendida pelo sistema seria:
>
> *   Se o usu√°rio gosta de filmes de A√ß√£o, recomendar Filme A.
> *   Se o usu√°rio gosta de filmes de Com√©dia, recomendar Filme C.
> *   Se o usu√°rio gosta de filmes de Drama, recomendar Filme C.
>
> Esse sistema est√° aprendendo a associar o estado (g√™nero preferido) com a a√ß√£o (filme recomendado) que maximiza a recompensa esperada (usu√°rio assiste o filme).

### Conclus√£o
O associative search representa uma extens√£o natural do problema do k-armed bandit, introduzindo a no√ß√£o de estado e a necessidade de aprender uma pol√≠tica que mapeie estados para a√ß√µes [^17]. Embora mais simples do que o problema completo de aprendizado por refor√ßo, o associative search apresenta desafios significativos em termos de explora√ß√£o e generaliza√ß√£o [^17].

Uma das principais dificuldades no associative search √© a necessidade de equilibrar a explora√ß√£o de a√ß√µes em estados desconhecidos com a explora√ß√£o de a√ß√µes que j√° se mostraram recompensadoras em estados conhecidos. Algoritmos como $\epsilon$-greedy e Upper Confidence Bound (UCB) podem ser adaptados para lidar com essa explora√ß√£o/explota√ß√£o, generalizando diretamente suas contrapartes do problema de k-armed bandit.

**Teorema 1** Algoritmos $\epsilon$-greedy e UCB, originalmente formulados para o problema de k-armed bandit, podem ser estendidos para o problema de associative search, mantendo propriedades de converg√™ncia assint√≥tica para a pol√≠tica √≥tima sob certas condi√ß√µes de estacionariedade e ergodicidade do ambiente.

*Proof (Outline):* A prova envolve mostrar que a explora√ß√£o aleat√≥ria (no caso do $\epsilon$-greedy) ou a quantifica√ß√£o da incerteza (no caso do UCB) em cada estado garante que todas as a√ß√µes em todos os estados sejam suficientemente amostradas ao longo do tempo. Sob a suposi√ß√£o de que as recompensas s√£o estacion√°rias e que o ambiente visita todos os estados com frequ√™ncia suficiente (ergodidade), a estimativa da recompensa m√©dia para cada a√ß√£o em cada estado converge para o valor real, levando √† converg√™ncia para a pol√≠tica √≥tima.

I. **Definir $\epsilon$-greedy para Associative Search:** Em cada estado $s$, o algoritmo $\epsilon$-greedy escolhe a a√ß√£o com a maior recompensa estimada com probabilidade $1 - \epsilon$ e escolhe uma a√ß√£o aleat√≥ria com probabilidade $\epsilon$.

II. **Definir UCB para Associative Search:** Em cada estado $s$, o algoritmo UCB escolhe a a√ß√£o $a$ que maximiza $Q(s, a) + U(s, a)$, onde $Q(s, a)$ √© a recompensa m√©dia estimada para a a√ß√£o $a$ no estado $s$, e $U(s, a)$ √© um termo de incerteza que diminui com o n√∫mero de vezes que a a√ß√£o $a$ foi tomada no estado $s$.

III. **Estacionariedade:** Assumir que a distribui√ß√£o de recompensas $r(s, a)$ √© estacion√°ria para todos os estados $s$ e a√ß√µes $a$. Isso significa que a m√©dia da recompensa $\mathbb{E}[r(s, a)]$ n√£o muda com o tempo.

IV. **Ergodicidade:** Assumir que o ambiente √© erg√≥dico. Isso significa que, com o tempo, o agente visita todos os estados $s \in \mathcal{S}$ com uma frequ√™ncia positiva. Mais formalmente, existe um $N$ tal que, para qualquer estado $s$, o agente visita o estado $s$ pelo menos uma vez em cada $N$ passos.

V. **Converg√™ncia do $\epsilon$-greedy:** Devido √† explora√ß√£o $\epsilon$, cada a√ß√£o em cada estado ser√° amostrada infinitas vezes. Pela lei dos grandes n√∫meros, a recompensa m√©dia estimada $Q(s, a)$ converge para a recompensa esperada real $\mathbb{E}[r(s, a)]$ para todas as a√ß√µes $a$ e estados $s$.  √Ä medida que $Q(s, a)$ converge para $\mathbb{E}[r(s, a)]$, o algoritmo $\epsilon$-greedy ir√°, com probabilidade $1 - \epsilon$, escolher a a√ß√£o √≥tima $\arg\max_{a} \mathbb{E}[r(s, a)]$ em cada estado $s$.

VI. **Converg√™ncia do UCB:** O termo de incerteza $U(s, a)$ garante que todas as a√ß√µes sejam exploradas no in√≠cio. √Ä medida que o agente explora, $U(s, a)$ diminui para as a√ß√µes frequentemente selecionadas. O termo UCB garante que as a√ß√µes subestimadas (devido a amostras iniciais baixas) ainda sejam exploradas.  Sob estacionariedade e ergodicidade, $Q(s, a)$ converge para $\mathbb{E}[r(s, a)]$ e $U(s, a)$ converge para 0 para todas as a√ß√µes e estados. Portanto, o algoritmo UCB converge para a a√ß√£o √≥tima em cada estado.

VII. **Conclus√£o:** Sob as condi√ß√µes de estacionariedade e ergodicidade, os algoritmos $\epsilon$-greedy e UCB convergem assintoticamente para a pol√≠tica √≥tima $\pi^*(s) = \arg\max_{a} r(s, a)$ para o problema de associative search. ‚ñ†

> üí° **Exemplo Num√©rico:** Implementa√ß√£o do $\epsilon$-greedy para associative search em Python com 2 estados e 3 a√ß√µes:

```python
import numpy as np
import random

# N√∫mero de estados e a√ß√µes
n_estados = 2
n_acoes = 3
epsilon = 0.1
n_episodes = 1000

# Inicializa Q-values com zeros
Q = np.zeros((n_estados, n_acoes))

# Recompensas verdadeiras (desconhecidas para o agente)
recompensas_verdadeiras = np.array([[1, 5, 2], [6, 2, 8]])  # Recompensas para cada estado-a√ß√£o

def escolha_acao_epsilon_greedy(estado, epsilon):
    if random.random() < epsilon:
        return random.randint(0, n_acoes - 1)  # Explora√ß√£o: Escolhe uma a√ß√£o aleat√≥ria
    else:
        return np.argmax(Q[estado])  # Explota√ß√£o: Escolhe a a√ß√£o com maior Q-value

# Loop de treinamento
for episodio in range(n_episodes):
    # Escolhe um estado aleat√≥rio (simula√ß√£o do ambiente)
    estado = random.randint(0, n_estados - 1)

    # Escolhe uma a√ß√£o usando epsilon-greedy
    acao = escolha_acao_epsilon_greedy(estado, epsilon)

    # Obt√©m a recompensa (simula√ß√£o do ambiente)
    recompensa = recompensas_verdadeiras[estado, acao] + np.random.normal(0, 1) # Adiciona ru√≠do

    # Atualiza o Q-value (aprendizagem)
    Q[estado, acao] = Q[estado, acao] + 0.1 * (recompensa - Q[estado, acao])

print("Q-values aprendidos:")
print(Q)

# Pol√≠tica √≥tima aprendida
politica_otima = np.argmax(Q, axis=1)
print("\nPol√≠tica √≥tima aprendida:")
print(politica_otima)
```

> Neste exemplo, o agente aprende a aproximar os Q-values verdadeiros e converge para uma pol√≠tica pr√≥xima da √≥tima. Os Q-values aprendidos representam a estimativa do agente sobre a recompensa esperada para cada par estado-a√ß√£o. A pol√≠tica √≥tima indica qual a√ß√£o o agente deve tomar em cada estado para maximizar a recompensa.

O estudo do associative search fornece uma base s√≥lida para a compreens√£o dos algoritmos e t√©cnicas utilizados no aprendizado por refor√ßo completo, que ser√° abordado em cap√≠tulos posteriores [^17]. Ao restringir a a√ß√£o ao efeito imediato da recompensa, o associative search permite focar no aprendizado das pol√≠ticas √≥timas.

### Refer√™ncias
[^1]: Cap√≠tulo 2, Multi-armed Bandits
[^17]: Se√ß√£o 2.9, Associative Search (Contextual Bandits)
<!-- END -->