## Associative Search as an Intermediate Step in Reinforcement Learning

### IntroduÃ§Ã£o
O estudo de **multi-armed bandits** [^1] oferece um arcabouÃ§o simplificado para entender os desafios de *exploration* e *exploitation* no aprendizado por reforÃ§o (RL). Conforme explorado na seÃ§Ã£o anterior, o problema clÃ¡ssico do k-armed bandit envolve a seleÃ§Ã£o repetida entre $k$ aÃ§Ãµes, cada uma fornecendo uma recompensa numÃ©rica baseada em uma distribuiÃ§Ã£o de probabilidade estacionÃ¡ria [^1]. No entanto, essa formulaÃ§Ã£o assume um cenÃ¡rio *nÃ£o associativo*, onde a melhor aÃ§Ã£o a ser tomada Ã© independente do contexto ou estado do ambiente [^1].

Este capÃ­tulo se aprofunda na transiÃ§Ã£o do problema do k-armed bandit para o cenÃ¡rio mais complexo do aprendizado por reforÃ§o completo, introduzindo o conceito de **associative search**, tambÃ©m conhecido como **contextual bandits** [^17]. Esta formulaÃ§Ã£o intermediÃ¡ria introduz a noÃ§Ã£o de *estado* ou *contexto*, permitindo que o agente aprenda uma *polÃ­tica* que mapeia situaÃ§Ãµes para aÃ§Ãµes Ã³timas [^17]. Diferentemente do problema do k-armed bandit, onde o objetivo Ã© encontrar uma Ãºnica aÃ§Ã£o "melhor", no associative search o objetivo Ã© aprender qual aÃ§Ã£o Ã© a melhor *em cada situaÃ§Ã£o* [^17].

Para formalizar a transiÃ§Ã£o, podemos definir o problema de k-armed bandit como um caso especial de associative search onde o nÃºmero de estados Ã© 1.

**ProposiÃ§Ã£o 1** O problema de k-armed bandit Ã© um caso particular de associative search com um Ãºnico estado.

*Proof:* No problema de k-armed bandit, o agente escolhe uma aÃ§Ã£o $a$ de um conjunto de $k$ aÃ§Ãµes sem observar nenhum estado. Isso Ã© equivalente a um associative search onde existe apenas um Ãºnico estado $s$, e a polÃ­tica trivialmente mapeia esse estado para uma aÃ§Ã£o $a \in \{1, ..., k\}$. Portanto, qualquer algoritmo para associative search pode ser usado para resolver o problema do k-armed bandit, considerando que todos os estados sÃ£o idÃªnticos.

I. **DefiniÃ§Ã£o do problema k-armed bandit:** No problema k-armed bandit, o agente escolhe uma aÃ§Ã£o $a$ de um conjunto de $k$ aÃ§Ãµes, $\mathcal{A} = \{a_1, a_2, \ldots, a_k\}$. A recompensa $r_a$ para cada aÃ§Ã£o $a$ Ã© independente de qualquer estado.

II. **DefiniÃ§Ã£o do problema associative search com um Ãºnico estado:** Considere um problema de associative search com um Ãºnico estado $s$. O agente escolhe uma aÃ§Ã£o $a$ de um conjunto de aÃ§Ãµes $\mathcal{A}$ e recebe uma recompensa $r(s, a)$ que depende do estado $s$ e da aÃ§Ã£o $a$.

III. **EquivalÃªncia:** Se existe apenas um estado $s$, entÃ£o $r(s, a)$ se torna $r(a)$, pois a dependÃªncia do estado Ã© irrelevante. Isso Ã© idÃªntico ao problema k-armed bandit, onde a recompensa depende apenas da aÃ§Ã£o escolhida.

IV. **PolÃ­tica:** No problema k-armed bandit, a polÃ­tica Ã© simplesmente selecionar a aÃ§Ã£o que maximiza a recompensa esperada, ou seja, $\pi = \arg\max_{a \in \mathcal{A}} \mathbb{E}[r(a)]$. No associative search com um Ãºnico estado, a polÃ­tica Ã© $\pi(s) = \arg\max_{a \in \mathcal{A}} \mathbb{E}[r(s, a)] = \arg\max_{a \in \mathcal{A}} \mathbb{E}[r(a)]$, que Ã© a mesma polÃ­tica.

V. **ConclusÃ£o:** Portanto, o problema k-armed bandit Ã© um caso particular do associative search com um Ãºnico estado. â– 

Este resultado destaca a hierarquia entre os dois problemas, onde o k-armed bandit serve como um bloco de construÃ§Ã£o fundamental para o associative search.

### Conceitos Fundamentais
O conceito de **associative search** representa um passo crucial em direÃ§Ã£o ao problema completo de aprendizado por reforÃ§o, preenchendo a lacuna entre as tarefas nÃ£o associativas, como o problema do k-armed bandit, e as tarefas mais complexas que envolvem aprendizado de polÃ­ticas dependentes do estado [^17].

No associative search:
*   O agente observa uma representaÃ§Ã£o do **estado** do ambiente [^17].
*   Com base nesse estado, o agente seleciona uma **aÃ§Ã£o** dentre um conjunto de opÃ§Ãµes [^17].
*   O agente recebe uma **recompensa** numÃ©rica, que depende da aÃ§Ã£o selecionada e do estado do ambiente [^1].
*   O objetivo do agente Ã© aprender uma **polÃ­tica** que mapeie cada estado para a aÃ§Ã£o que maximize a recompensa esperada [^17].

A principal distinÃ§Ã£o entre o associative search e o problema do k-armed bandit reside na **dependÃªncia do estado**. No k-armed bandit, a distribuiÃ§Ã£o de probabilidade da recompensa para cada aÃ§Ã£o Ã© fixa e independente de qualquer fator externo [^1]. No associative search, a distribuiÃ§Ã£o da recompensa depende do estado do ambiente, tornando a tarefa de aprendizado mais desafiadora [^17].

Para formalizar essa dependÃªncia, podemos definir $r(s, a)$ como a recompensa esperada ao selecionar a aÃ§Ã£o $a$ no estado $s$. O objetivo do agente Ã© encontrar uma polÃ­tica $\pi(s)$ que maximize a recompensa esperada para cada estado:

$$\pi^*(s) = \arg\max_{a} r(s, a)$$

Onde $\pi^*(s)$ denota a polÃ­tica Ã³tima.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos dois estados, $s_1$ e $s_2$, e duas aÃ§Ãµes, $a_1$ e $a_2$. As recompensas esperadas sÃ£o:
>
> *   $r(s_1, a_1) = 2$
> *   $r(s_1, a_2) = 5$
> *   $r(s_2, a_1) = 8$
> *   $r(s_2, a_2) = 1$
>
> Neste caso, a polÃ­tica Ã³tima seria $\pi^*(s_1) = a_2$ e $\pi^*(s_2) = a_1$, pois $a_2$ maximiza a recompensa em $s_1$ (5 > 2) e $a_1$ maximiza a recompensa em $s_2$ (8 > 1). O agente deve aprender essa associaÃ§Ã£o estado-aÃ§Ã£o para otimizar suas recompensas.

Em contrapartida com o problema de Reinforcement Learning completo [^17]:

> Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward.

No RL completo, as aÃ§Ãµes podem influenciar o prÃ³ximo estado e a recompensa, introduzindo a complexidade da tomada de decisÃ£o sequencial [^17]. Associative search restringe a aÃ§Ã£o ao efeito imediato da recompensa.

Podemos definir formalmente a diferenÃ§a entre associative search e reinforcement learning.

**DefiniÃ§Ã£o 1** (Associative Search) Um problema de associative search Ã© definido por uma tupla $(\mathcal{S}, \mathcal{A}, R)$, onde:
*   $\mathcal{S}$ Ã© o conjunto de estados.
*   $\mathcal{A}$ Ã© o conjunto de aÃ§Ãµes.
*   $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ Ã© a funÃ§Ã£o de recompensa, que associa cada par estado-aÃ§Ã£o a uma recompensa.

**DefiniÃ§Ã£o 2** (Reinforcement Learning) Um problema de reinforcement learning Ã© definido por uma tupla $(\mathcal{S}, \mathcal{A}, P, R, \gamma)$, onde:
*   $\mathcal{S}$ Ã© o conjunto de estados.
*   $\mathcal{A}$ Ã© o conjunto de aÃ§Ãµes.
*   $P: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})$ Ã© a funÃ§Ã£o de transiÃ§Ã£o de estado, que associa cada par estado-aÃ§Ã£o a uma distribuiÃ§Ã£o de probabilidade sobre o prÃ³ximo estado.
*   $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ Ã© a funÃ§Ã£o de recompensa, que associa cada par estado-aÃ§Ã£o a uma recompensa.
*   $\gamma \in [0, 1]$ Ã© o fator de desconto, que pondera as recompensas futuras.

A principal diferenÃ§a Ã© a funÃ§Ã£o de transiÃ§Ã£o $P$ e o fator de desconto $\gamma$, que nÃ£o estÃ£o presentes no associative search. No associative search, o prÃ³ximo estado Ã© sempre o mesmo (implicitamente determinado pelo ambiente), ou nÃ£o tem influÃªncia na recompensa.

### Exemplos e AplicaÃ§Ãµes
Um exemplo ilustrativo de associative search Ã© o cenÃ¡rio de um slot machine que muda a cor de sua tela, sinalizando diferentes valores de aÃ§Ã£o [^17]. O agente deve aprender a associar cada cor (estado) com o braÃ§o (aÃ§Ã£o) que proporciona a maior recompensa [^17]. Outro exemplo seria um sistema de recomendaÃ§Ã£o que adapta as sugestÃµes aos gostos do usuÃ¡rio, e que nÃ£o consegue prever o efeito a longo prazo das suas aÃ§Ãµes.

Outro exemplo importante Ã© a personalizaÃ§Ã£o de anÃºncios online. Um sistema de publicidade pode usar informaÃ§Ãµes sobre o usuÃ¡rio (idade, localizaÃ§Ã£o, histÃ³rico de navegaÃ§Ã£o) como o estado e selecionar um anÃºncio para exibir (aÃ§Ã£o). A recompensa Ã© um clique no anÃºncio ou uma conversÃ£o. O sistema deve aprender qual anÃºncio exibir para cada tipo de usuÃ¡rio para maximizar a taxa de cliques ou conversÃµes. Este cenÃ¡rio se encaixa no framework do associative search porque a escolha de um anÃºncio nÃ£o influencia diretamente o estado futuro do usuÃ¡rio.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um sistema de recomendaÃ§Ã£o de filmes.
>
> *   **Estado:** GÃªnero de filme preferido pelo usuÃ¡rio (e.g., AÃ§Ã£o, ComÃ©dia, Drama).
> *   **AÃ§Ãµes:** Recomendar um filme especÃ­fico (e.g., Filme A, Filme B, Filme C).
> *   **Recompensa:** O usuÃ¡rio assiste o filme recomendado (recompensa = 1) ou nÃ£o (recompensa = 0).
>
> Suponha que, apÃ³s algumas interaÃ§Ãµes, o sistema aprendeu as seguintes recompensas mÃ©dias:
>
> | Estado (GÃªnero) | AÃ§Ã£o (Filme) | Recompensa MÃ©dia |
> |-----------------|--------------|-------------------|
> | AÃ§Ã£o            | Filme A      | 0.8               |
> | AÃ§Ã£o            | Filme B      | 0.2               |
> | ComÃ©dia         | Filme B      | 0.7               |
> | ComÃ©dia         | Filme C      | 0.9               |
> | Drama           | Filme A      | 0.3               |
> | Drama           | Filme C      | 0.6               |
>
> A polÃ­tica aprendida pelo sistema seria:
>
> *   Se o usuÃ¡rio gosta de filmes de AÃ§Ã£o, recomendar Filme A.
> *   Se o usuÃ¡rio gosta de filmes de ComÃ©dia, recomendar Filme C.
> *   Se o usuÃ¡rio gosta de filmes de Drama, recomendar Filme C.
>
> Esse sistema estÃ¡ aprendendo a associar o estado (gÃªnero preferido) com a aÃ§Ã£o (filme recomendado) que maximiza a recompensa esperada (usuÃ¡rio assiste o filme).

### ConclusÃ£o
O associative search representa uma extensÃ£o natural do problema do k-armed bandit, introduzindo a noÃ§Ã£o de estado e a necessidade de aprender uma polÃ­tica que mapeie estados para aÃ§Ãµes [^17]. Embora mais simples do que o problema completo de aprendizado por reforÃ§o, o associative search apresenta desafios significativos em termos de exploraÃ§Ã£o e generalizaÃ§Ã£o [^17].

Uma das principais dificuldades no associative search Ã© a necessidade de equilibrar a exploraÃ§Ã£o de aÃ§Ãµes em estados desconhecidos com a exploraÃ§Ã£o de aÃ§Ãµes que jÃ¡ se mostraram recompensadoras em estados conhecidos. Algoritmos como $\epsilon$-greedy e Upper Confidence Bound (UCB) podem ser adaptados para lidar com essa exploraÃ§Ã£o/explotaÃ§Ã£o, generalizando diretamente suas contrapartes do problema de k-armed bandit.

**Teorema 1** Algoritmos $\epsilon$-greedy e UCB, originalmente formulados para o problema de k-armed bandit, podem ser estendidos para o problema de associative search, mantendo propriedades de convergÃªncia assintÃ³tica para a polÃ­tica Ã³tima sob certas condiÃ§Ãµes de estacionariedade e ergodicidade do ambiente.

*Proof (Outline):* A prova envolve mostrar que a exploraÃ§Ã£o aleatÃ³ria (no caso do $\epsilon$-greedy) ou a quantificaÃ§Ã£o da incerteza (no caso do UCB) em cada estado garante que todas as aÃ§Ãµes em todos os estados sejam suficientemente amostradas ao longo do tempo. Sob a suposiÃ§Ã£o de que as recompensas sÃ£o estacionÃ¡rias e que o ambiente visita todos os estados com frequÃªncia suficiente (ergodidade), a estimativa da recompensa mÃ©dia para cada aÃ§Ã£o em cada estado converge para o valor real, levando Ã  convergÃªncia para a polÃ­tica Ã³tima.

I. **Definir $\epsilon$-greedy para Associative Search:** Em cada estado $s$, o algoritmo $\epsilon$-greedy escolhe a aÃ§Ã£o com a maior recompensa estimada com probabilidade $1 - \epsilon$ e escolhe uma aÃ§Ã£o aleatÃ³ria com probabilidade $\epsilon$.

II. **Definir UCB para Associative Search:** Em cada estado $s$, o algoritmo UCB escolhe a aÃ§Ã£o $a$ que maximiza $Q(s, a) + U(s, a)$, onde $Q(s, a)$ Ã© a recompensa mÃ©dia estimada para a aÃ§Ã£o $a$ no estado $s$, e $U(s, a)$ Ã© um termo de incerteza que diminui com o nÃºmero de vezes que a aÃ§Ã£o $a$ foi tomada no estado $s$.

III. **Estacionariedade:** Assumir que a distribuiÃ§Ã£o de recompensas $r(s, a)$ Ã© estacionÃ¡ria para todos os estados $s$ e aÃ§Ãµes $a$. Isso significa que a mÃ©dia da recompensa $\mathbb{E}[r(s, a)]$ nÃ£o muda com o tempo.

IV. **Ergodicidade:** Assumir que o ambiente Ã© ergÃ³dico. Isso significa que, com o tempo, o agente visita todos os estados $s \in \mathcal{S}$ com uma frequÃªncia positiva. Mais formalmente, existe um $N$ tal que, para qualquer estado $s$, o agente visita o estado $s$ pelo menos uma vez em cada $N$ passos.

V. **ConvergÃªncia do $\epsilon$-greedy:** Devido Ã  exploraÃ§Ã£o $\epsilon$, cada aÃ§Ã£o em cada estado serÃ¡ amostrada infinitas vezes. Pela lei dos grandes nÃºmeros, a recompensa mÃ©dia estimada $Q(s, a)$ converge para a recompensa esperada real $\mathbb{E}[r(s, a)]$ para todas as aÃ§Ãµes $a$ e estados $s$.  Ã€ medida que $Q(s, a)$ converge para $\mathbb{E}[r(s, a)]$, o algoritmo $\epsilon$-greedy irÃ¡, com probabilidade $1 - \epsilon$, escolher a aÃ§Ã£o Ã³tima $\arg\max_{a} \mathbb{E}[r(s, a)]$ em cada estado $s$.

VI. **ConvergÃªncia do UCB:** O termo de incerteza $U(s, a)$ garante que todas as aÃ§Ãµes sejam exploradas no inÃ­cio. Ã€ medida que o agente explora, $U(s, a)$ diminui para as aÃ§Ãµes frequentemente selecionadas. O termo UCB garante que as aÃ§Ãµes subestimadas (devido a amostras iniciais baixas) ainda sejam exploradas.  Sob estacionariedade e ergodicidade, $Q(s, a)$ converge para $\mathbb{E}[r(s, a)]$ e $U(s, a)$ converge para 0 para todas as aÃ§Ãµes e estados. Portanto, o algoritmo UCB converge para a aÃ§Ã£o Ã³tima em cada estado.

VII. **ConclusÃ£o:** Sob as condiÃ§Ãµes de estacionariedade e ergodicidade, os algoritmos $\epsilon$-greedy e UCB convergem assintoticamente para a polÃ­tica Ã³tima $\pi^*(s) = \arg\max_{a} r(s, a)$ para o problema de associative search. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** ImplementaÃ§Ã£o do $\epsilon$-greedy para associative search em Python com 2 estados e 3 aÃ§Ãµes:

```python
import numpy as np
import random

# NÃºmero de estados e aÃ§Ãµes
n_estados = 2
n_acoes = 3
epsilon = 0.1
n_episodes = 1000

# Inicializa Q-values com zeros
Q = np.zeros((n_estados, n_acoes))

# Recompensas verdadeiras (desconhecidas para o agente)
recompensas_verdadeiras = np.array([[1, 5, 2], [6, 2, 8]])  # Recompensas para cada estado-aÃ§Ã£o

def escolha_acao_epsilon_greedy(estado, epsilon):
    if random.random() < epsilon:
        return random.randint(0, n_acoes - 1)  # ExploraÃ§Ã£o: Escolhe uma aÃ§Ã£o aleatÃ³ria
    else:
        return np.argmax(Q[estado])  # ExplotaÃ§Ã£o: Escolhe a aÃ§Ã£o com maior Q-value

# Loop de treinamento
for episodio in range(n_episodes):
    # Escolhe um estado aleatÃ³rio (simulaÃ§Ã£o do ambiente)
    estado = random.randint(0, n_estados - 1)

    # Escolhe uma aÃ§Ã£o usando epsilon-greedy
    acao = escolha_acao_epsilon_greedy(estado, epsilon)

    # ObtÃ©m a recompensa (simulaÃ§Ã£o do ambiente)
    recompensa = recompensas_verdadeiras[estado, acao] + np.random.normal(0, 1) # Adiciona ruÃ­do

    # Atualiza o Q-value (aprendizagem)
    Q[estado, acao] = Q[estado, acao] + 0.1 * (recompensa - Q[estado, acao])

print("Q-values aprendidos:")
print(Q)

# PolÃ­tica Ã³tima aprendida
politica_otima = np.argmax(Q, axis=1)
print("\nPolÃ­tica Ã³tima aprendida:")
print(politica_otima)
```

> Neste exemplo, o agente aprende a aproximar os Q-values verdadeiros e converge para uma polÃ­tica prÃ³xima da Ã³tima. Os Q-values aprendidos representam a estimativa do agente sobre a recompensa esperada para cada par estado-aÃ§Ã£o. A polÃ­tica Ã³tima indica qual aÃ§Ã£o o agente deve tomar em cada estado para maximizar a recompensa.

O estudo do associative search fornece uma base sÃ³lida para a compreensÃ£o dos algoritmos e tÃ©cnicas utilizados no aprendizado por reforÃ§o completo, que serÃ¡ abordado em capÃ­tulos posteriores [^17]. Ao restringir a aÃ§Ã£o ao efeito imediato da recompensa, o associative search permite focar no aprendizado das polÃ­ticas Ã³timas.

### ReferÃªncias
[^1]: CapÃ­tulo 2, Multi-armed Bandits
[^17]: SeÃ§Ã£o 2.9, Associative Search (Contextual Bandits)
<!-- END -->