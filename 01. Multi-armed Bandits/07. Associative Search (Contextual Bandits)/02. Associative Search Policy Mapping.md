## Associative Search: Contextual Bandits with Task Identification

### Introdu√ß√£o
Em continuidade aos m√©todos explorados para o problema de *k-armed bandit*, focaremos agora em tarefas que requerem **associa√ß√£o entre a√ß√µes e situa√ß√µes**, tamb√©m conhecidas como *contextual bandits* [^41]. Anteriormente, abordamos tarefas *n√£o associativas* onde o objetivo era encontrar uma √∫nica melhor a√ß√£o, seja em um ambiente estacion√°rio ou rastreando a melhor a√ß√£o em um ambiente n√£o estacion√°rio [^41]. Agora, exploramos como adaptar esses m√©todos para cen√°rios onde a escolha da melhor a√ß√£o depende do contexto, ou seja, da situa√ß√£o em que o agente se encontra. O objetivo √© aprender uma *policy*, que mapeia situa√ß√µes para as a√ß√µes mais apropriadas [^41].

### Conceitos Fundamentais
**1. Tarefas Associativas:**
Em uma tarefa associativa, o agente enfrenta m√∫ltiplos problemas de *k-armed bandit*, e em cada etapa, um desses problemas √© escolhido aleatoriamente [^41]. A diferen√ßa fundamental reside no fato de que o agente recebe uma *clue* ou sinal distintivo sobre a identidade do problema de *bandit* espec√≠fico que est√° enfrentando [^41]. Essa informa√ß√£o contextual permite que o agente aprenda uma pol√≠tica que associa cada contexto √† a√ß√£o √≥tima para aquele contexto.

**2. Exemplo Ilustrativo:**
Considere um cen√°rio onde o agente est√° diante de uma *slot machine* que muda a cor de seu display √† medida que seus valores de a√ß√£o mudam [^41]. O agente pode aprender a associar a cor do display (o contexto) com a melhor alavanca a ser puxada [^41]. Por exemplo, se a cor for vermelha, selecionar a alavanca 1; se a cor for verde, selecionar a alavanca 2 [^41].

> üí° **Exemplo Num√©rico:** Imagine que a m√°quina ca√ßa-n√≠queis mostre as cores vermelho, verde e azul. Ap√≥s algumas intera√ß√µes, o agente aprende que:
> *   Vermelho (contexto 1): Puxar a alavanca 1 tem recompensa m√©dia de 0.8.
> *   Verde (contexto 2): Puxar a alavanca 2 tem recompensa m√©dia de 0.9.
> *   Azul (contexto 3): Puxar a alavanca 3 tem recompensa m√©dia de 0.7.
>
> Assim, a pol√≠tica aprendida $\pi(a|s)$ seria:
> *   $\pi(\text{alavanca 1} | \text{vermelho}) = 1$
> *   $\pi(\text{alavanca 2} | \text{verde}) = 1$
> *   $\pi(\text{alavanca 3} | \text{azul}) = 1$
>
> Essa pol√≠tica garante que, ao ver a cor vermelha, o agente sempre escolha a alavanca 1, maximizando sua recompensa esperada naquele contexto.

**3. Pol√≠tica de A√ß√µes:**
Com a pol√≠tica correta, o agente pode ter um desempenho significativamente melhor do que em um cen√°rio onde n√£o h√° informa√ß√£o contextual dispon√≠vel [^41]. A pol√≠tica representa o aprendizado da associa√ß√£o entre o sinal distintivo e a a√ß√£o apropriada.

**4. Natureza das Tarefas Associativas:**
As tarefas associativas s√£o *intermedi√°rias* entre o problema do *k-armed bandit* e o problema completo de *reinforcement learning* [^41]. Elas compartilham caracter√≠sticas de ambos:
*   Como no problema de *reinforcement learning*, o agente precisa aprender uma pol√≠tica.
*   Como no problema do *k-armed bandit*, cada a√ß√£o afeta apenas a recompensa imediata, sem influenciar o pr√≥ximo estado ou situa√ß√£o.

**5. Diferen√ßa para o Reinforcement Learning Completo:**
A distin√ß√£o fundamental entre tarefas associativas e o problema completo de *reinforcement learning* reside na influ√™ncia das a√ß√µes sobre o pr√≥ximo estado [^41]. Em tarefas associativas, as a√ß√µes n√£o afetam a pr√≥xima situa√ß√£o, enquanto no *reinforcement learning* completo, as a√ß√µes podem influenciar tanto a recompensa imediata quanto o estado futuro.

**6. Formula√ß√£o Matem√°tica:**
Embora o texto n√£o forne√ßa uma formula√ß√£o matem√°tica expl√≠cita para o *contextual bandit* com task identification, podemos inferir que a pol√≠tica aprendida √© uma fun√ß√£o $\pi(a|s)$, onde $a$ √© a a√ß√£o e $s$ √© o sinal distintivo ou contexto. O objetivo √© maximizar a recompensa esperada:
$$
\mathbb{E}[R] = \sum_{s} p(s) \sum_{a} \pi(a|s) q_{*}(a, s)
$$
Onde $p(s)$ √© a probabilidade de encontrar o contexto $s$, e $q_{*}(a, s)$ √© o valor verdadeiro da a√ß√£o $a$ no contexto $s$.

> üí° **Exemplo Num√©rico:** Suponha que temos dois contextos, $s_1$ e $s_2$, com probabilidades $p(s_1) = 0.6$ e $p(s_2) = 0.4$, respectivamente. Cada contexto tem duas a√ß√µes, $a_1$ e $a_2$. Os valores verdadeiros das a√ß√µes s√£o:
> *   $q_{*}(a_1, s_1) = 0.7$
> *   $q_{*}(a_2, s_1) = 0.3$
> *   $q_{*}(a_1, s_2) = 0.2$
> *   $q_{*}(a_2, s_2) = 0.8$
>
> Uma pol√≠tica √≥tima seria $\pi(a_1|s_1) = 1$ e $\pi(a_2|s_2) = 1$.  A recompensa esperada seria:
>
> $\mathbb{E}[R] = p(s_1) \cdot q_{*}(a_1, s_1) + p(s_2) \cdot q_{*}(a_2, s_2) = 0.6 \cdot 0.7 + 0.4 \cdot 0.8 = 0.42 + 0.32 = 0.74$
>
> Se a pol√≠tica fosse aleat√≥ria, por exemplo, $\pi(a_1|s_1) = 0.5$, $\pi(a_2|s_1) = 0.5$, $\pi(a_1|s_2) = 0.5$, $\pi(a_2|s_2) = 0.5$, a recompensa esperada seria menor:
>
> $\mathbb{E}[R] = 0.6 \cdot (0.5 \cdot 0.7 + 0.5 \cdot 0.3) + 0.4 \cdot (0.5 \cdot 0.2 + 0.5 \cdot 0.8) = 0.6 \cdot 0.5 + 0.4 \cdot 0.5 = 0.3 + 0.2 = 0.5$
>
> Isso demonstra como uma pol√≠tica bem definida pode maximizar a recompensa esperada em tarefas associativas.

Para garantir a converg√™ncia e otimiza√ß√£o da pol√≠tica, √© crucial empregar m√©todos eficientes de estima√ß√£o de $q_{*}(a, s)$ e explora√ß√£o-explota√ß√£o.

**Teorema 1** (Converg√™ncia em Contextual Bandits): Sob certas condi√ß√µes de regularidade e utilizando algoritmos como Upper Confidence Bound (UCB) ou Thompson Sampling adaptados para o contexto, a pol√≠tica aprendida $\pi(a|s)$ converge para a pol√≠tica √≥tima $\pi^{*}(a|s)$ no limite, ou seja, $\lim_{t \to \infty} \pi_t(a|s) = \pi^{*}(a|s)$.

*Proof Sketch:* A prova tipicamente envolve mostrar que a estimativa de $q_{*}(a, s)$ converge para o valor verdadeiro com alta probabilidade e que a estrat√©gia de explora√ß√£o garante que todas as a√ß√µes em todos os contextos sejam suficientemente amostradas.  Isso pode ser feito atrav√©s de desigualdades de concentra√ß√£o como Hoeffding ou Bernstein, combinadas com a an√°lise do algoritmo de explora√ß√£o escolhido.

Para ilustrar a converg√™ncia em contextual bandits usando o algoritmo UCB, podemos fornecer um exemplo simplificado e uma prova da sua converg√™ncia.

**Exemplo de prova de converg√™ncia para UCB em Contextual Bandits (Simplificado):**

Para simplificar, vamos considerar um cen√°rio com um n√∫mero finito de contextos $S$ e um n√∫mero finito de a√ß√µes $A$. O algoritmo UCB atualiza iterativamente as estimativas de valor das a√ß√µes em cada contexto e utiliza um limite de confian√ßa superior para guiar a explora√ß√£o.

I. **Defini√ß√µes:**
   - $q_*(s, a)$: Valor verdadeiro da a√ß√£o $a$ no contexto $s$.
   - $\hat{q}_t(s, a)$: Estimativa do valor da a√ß√£o $a$ no contexto $s$ no tempo $t$.
   - $N_t(s, a)$: N√∫mero de vezes que a a√ß√£o $a$ foi selecionada no contexto $s$ at√© o tempo $t$.
   - $UCB_t(s, a) = \hat{q}_t(s, a) + \sqrt{\frac{2\ln(t)}{N_t(s, a)}}$: Limite de confian√ßa superior para a a√ß√£o $a$ no contexto $s$ no tempo $t$.

II. **Algoritmo UCB:**
   No tempo $t$, dado o contexto $s_t$, o agente escolhe a a√ß√£o:
   $$a_t = \arg\max_{a \in A} UCB_t(s_t, a)$$
   Ap√≥s observar a recompensa $r_t$, atualiza a estimativa de valor:
   $$\hat{q}_{t+1}(s_t, a_t) = \hat{q}_t(s_t, a_t) + \frac{1}{N_t(s_t, a_t)}(r_t - \hat{q}_t(s_t, a_t))$$
   E incrementa o contador:
   $$N_{t+1}(s_t, a_t) = N_t(s_t, a_t) + 1$$

III. **An√°lise da Converg√™ncia:**
   O objetivo √© mostrar que $\hat{q}_t(s, a)$ converge para $q_*(s, a)$ e que a explora√ß√£o √© suficiente para garantir que todas as a√ß√µes sub√≥timas sejam eventualmente identificadas.

IV. **Limite de Confian√ßa:**
   Pela desigualdade de Hoeffding, temos que para cada a√ß√£o $a$ e contexto $s$:
   $$P(|\hat{q}_t(s, a) - q_*(s, a)| > \epsilon) \leq 2e^{-2N_t(s, a)\epsilon^2}$$
   Escolhendo $\epsilon = \sqrt{\frac{2\ln(t)}{N_t(s, a)}}$, obtemos:
   $$P(|\hat{q}_t(s, a) - q_*(s, a)| > \sqrt{\frac{2\ln(t)}{N_t(s, a)}}) \leq \frac{2}{t^4}$$
   Isto significa que, com alta probabilidade, $q_*(s, a)$ est√° dentro do intervalo de confian√ßa definido por $UCB_t(s, a)$.

V. **Explora√ß√£o Suficiente:**
   Para qualquer a√ß√£o sub√≥tima $a'$ no contexto $s$, eventualmente $N_t(s, a')$ ser√° grande o suficiente para que o limite de confian√ßa superior $UCB_t(s, a')$ seja menor que o valor verdadeiro da a√ß√£o √≥tima $a^*$ no contexto $s$.  Quando isso acontecer, a a√ß√£o √≥tima $a^*$ ser√° sempre escolhida em vez de $a'$.

VI. **Converg√™ncia da Pol√≠tica:**
   √Ä medida que $t \to \infty$, a probabilidade de escolher uma a√ß√£o sub√≥tima diminui exponencialmente. Portanto, a pol√≠tica $\pi_t(a|s)$ converge para a pol√≠tica √≥tima $\pi^*(a|s)$, que sempre escolhe a a√ß√£o com o maior valor esperado no contexto $s$.

VII. **Conclus√£o:**
    Portanto, sob as condi√ß√µes de regularidade e usando o algoritmo UCB, a pol√≠tica aprendida $\pi_t(a|s)$ converge para a pol√≠tica √≥tima $\pi^{*}(a|s)$ no limite.  Essa converg√™ncia √© garantida pela combina√ß√£o da estima√ß√£o precisa dos valores das a√ß√µes e da estrat√©gia de explora√ß√£o que assegura que todas as a√ß√µes sejam suficientemente amostradas. ‚ñ†

Dado que temos a formula√ß√£o matem√°tica, podemos extender a no√ß√£o de *regret* para o caso de *contextual bandits*.

**7. Regret em Contextual Bandits:**

O *regret* em um *contextual bandit* mede a diferen√ßa entre a recompensa acumulada obtida pela pol√≠tica √≥tima e a recompensa acumulada obtida pela pol√≠tica aprendida pelo agente ao longo do tempo. Formalmente, o *regret* em um horizonte de tempo $T$ √© definido como:

$$
Regret(T) = \mathbb{E}\left[\sum_{t=1}^{T} q_{*}(s_t, a^{*}_t) - q_{*}(s_t, a_t)\right]
$$

Onde:
* $s_t$ √© o contexto no tempo $t$.
* $a^{*}_t = \arg\max_{a} q_{*}(s_t, a)$ √© a a√ß√£o √≥tima no contexto $s_t$.
* $a_t$ √© a a√ß√£o selecionada pelo agente no tempo $t$.

Minimizar o *regret* √© um objetivo central no design de algoritmos para *contextual bandits*. Algoritmos eficientes buscam alcan√ßar um *regret* sublinear em rela√ß√£o a $T$, indicando que o agente aprende a se comportar quase otimamente com o tempo.

> üí° **Exemplo Num√©rico:**
>
> Considere um *contextual bandit* com dois contextos ($s_1$, $s_2$) e duas a√ß√µes ($a_1$, $a_2$). Suponha que os valores verdadeiros das a√ß√µes em cada contexto sejam:
>
> *   $q_{*}(s_1, a_1) = 0.9$
> *   $q_{*}(s_1, a_2) = 0.2$
> *   $q_{*}(s_2, a_1) = 0.1$
> *   $q_{*}(s_2, a_2) = 0.7$
>
> A pol√≠tica √≥tima seria:
>
> *   $\pi^{*}(a_1|s_1) = 1$ (escolher $a_1$ no contexto $s_1$)
> *   $\pi^{*}(a_2|s_2) = 1$ (escolher $a_2$ no contexto $s_2$)
>
> Suponha que o agente execute o algoritmo por $T = 100$ passos. Em cada passo, um contexto √© selecionado aleatoriamente com $p(s_1) = 0.5$ e $p(s_2) = 0.5$.
>
> Vamos simular que o agente inicialmente age de forma sub√≥tima, escolhendo $a_2$ no contexto $s_1$ por 20 vezes e $a_1$ no contexto $s_2$ por 15 vezes antes de aprender a pol√≠tica √≥tima.
>
> O *regret* pode ser calculado da seguinte forma:
>
> *   *Regret* devido a escolhas sub√≥timas em $s_1$: $20 \cdot (q_{*}(s_1, a_1) - q_{*}(s_1, a_2)) = 20 \cdot (0.9 - 0.2) = 20 \cdot 0.7 = 14$
> *   *Regret* devido a escolhas sub√≥timas em $s_2$: $15 \cdot (q_{*}(s_2, a_2) - q_{*}(s_2, a_1)) = 15 \cdot (0.7 - 0.1) = 15 \cdot 0.6 = 9$
>
> O *regret* total √© $14 + 9 = 23$.
>
> Agora, vamos calcular o *regret* m√©dio por passo: $\frac{23}{100} = 0.23$. Isso significa que, em m√©dia, o agente perde 0.23 unidades de recompensa por passo devido a suas escolhas sub√≥timas iniciais.
>
> Este exemplo ilustra como o *regret* quantifica a perda de desempenho devido √† explora√ß√£o e aprendizagem. Algoritmos eficazes visam minimizar esse *regret* ao longo do tempo.

**Prova de Limite Superior para Regret em Contextual Bandit com UCB (Esbo√ßo):**

Aqui, fornecemos um esbo√ßo de como derivar um limite superior para o *regret* acumulado no algoritmo UCB para *contextual bandits*.

I. **Defini√ß√µes:**
   - $q_*(s, a)$: Valor verdadeiro da a√ß√£o $a$ no contexto $s$.
   - $a_t^*$: A√ß√£o √≥tima no contexto $s_t$ no tempo $t$, i.e., $a_t^* = \arg\max_{a} q_{*}(s_t, a)$.
   - $a_t$: A√ß√£o selecionada pelo algoritmo UCB no tempo $t$.
   - $\Delta_t(a) = q_*(s_t, a_t^*) - q_*(s_t, a)$: A diferen√ßa de valor entre a a√ß√£o √≥tima e a a√ß√£o $a$ no contexto $s_t$.

II. **Decomposi√ß√£o do Regret:**
   O *regret* no tempo $t$ √© dado por:
   $$r_t = q_*(s_t, a_t^*) - q_*(s_t, a_t)$$
   O *regret* total ao longo de $T$ passos √©:
   $$Regret(T) = \sum_{t=1}^{T} r_t = \sum_{t=1}^{T} (q_*(s_t, a_t^*) - q_*(s_t, a_t))$$

III. **Limite no Regret Instant√¢neo:**
   Seja $A_t$ o conjunto de a√ß√µes sub√≥timas no tempo $t$, i.e., $A_t = \{a \in A : q_*(s_t, a) < q_*(s_t, a_t^*)\}$.  O *regret* instant√¢neo pode ser limitado considerando quando uma a√ß√£o sub√≥tima √© selecionada:

   $$Regret(T) = \sum_{t=1}^{T} \sum_{a \in A_t} \mathbb{I}\{a_t = a\} (q_*(s_t, a_t^*) - q_*(s_t, a))$$
   Onde $\mathbb{I}\{a_t = a\}$ √© uma fun√ß√£o indicadora que vale 1 se $a_t = a$ e 0 caso contr√°rio.

IV. **UCB e Sele√ß√£o de A√ß√µes Sub√≥timas:**
   Se uma a√ß√£o sub√≥tima $a$ √© selecionada no tempo $t$, ent√£o seu limite de confian√ßa superior deve ser maior ou igual ao limite de confian√ßa superior da a√ß√£o √≥tima:
   $$\hat{q}_t(s_t, a) + \sqrt{\frac{2\ln(t)}{N_t(s_t, a)}} \geq \hat{q}_t(s_t, a_t^*) + \sqrt{\frac{2\ln(t)}{N_t(s_t, a_t^*)}}$$

V. **Limite no N√∫mero de Sele√ß√µes de A√ß√µes Sub√≥timas:**
   Usando a desigualdade acima e a defini√ß√£o de $\Delta_t(a)$, pode-se mostrar que o n√∫mero de vezes que uma a√ß√£o sub√≥tima $a$ √© selecionada √© limitado.  Este limite geralmente depende de $\ln(T)$ e de $\Delta_t(a)$.

VI. **Limite Superior para o Regret:**
   Combinando os limites no n√∫mero de sele√ß√µes de a√ß√µes sub√≥timas com a defini√ß√£o de *regret*, podemos obter um limite superior para o *regret* total.  Em geral, para o algoritmo UCB em *contextual bandits*, o *regret* acumulado √© da ordem de:
   $$Regret(T) = O(\sqrt{T \ln(T)})$$
   Este resultado indica que o *regret* cresce sublinearmente com o tempo, o que significa que o algoritmo aprende a se comportar quase otimamente √† medida que o tempo aumenta.

VII. **Conclus√£o:**
    O limite superior para o *regret* no algoritmo UCB para *contextual bandits* demonstra que o algoritmo consegue explorar e explotar de forma eficiente, garantindo um desempenho quase √≥timo ao longo do tempo. A prova detalhada envolve t√©cnicas de an√°lise de desigualdades de concentra√ß√£o e otimiza√ß√£o do processo de aprendizagem. ‚ñ†

### Conclus√£o
O conceito de tarefas associativas, ou *contextual bandits*, representa um passo importante em dire√ß√£o ao problema completo de *reinforcement learning* [^41]. Ao introduzir a necessidade de associar a√ß√µes a diferentes situa√ß√µes, preparamos o terreno para a explora√ß√£o de ambientes mais complexos onde as a√ß√µes podem influenciar tanto a recompensa imediata quanto o estado futuro. No pr√≥ximo cap√≠tulo, o problema completo de *reinforcement learning* ser√° apresentado, e suas ramifica√ß√µes ser√£o consideradas ao longo do restante deste livro [^41].

### Refer√™ncias
[^41]: Cap√≠tulo 2, Se√ß√£o 2.9
<!-- END -->