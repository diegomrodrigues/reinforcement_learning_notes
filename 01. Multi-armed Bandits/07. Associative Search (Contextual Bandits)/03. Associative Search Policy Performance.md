## Benef√≠cios da Pol√≠tica Correta em Busca Associativa

### Introdu√ß√£o
Em continuidade ao t√≥pico de *Associative Search (Contextual Bandits)* [^41], exploraremos quantitativamente como uma pol√≠tica correta pode melhorar significativamente o desempenho em compara√ß√£o com cen√°rios sem informa√ß√µes distintivas sobre as tarefas de bandit. Em um problema de *k-armed bandit* padr√£o, o objetivo √© maximizar a recompensa total esperada ao longo do tempo, explorando e explorando as diferentes a√ß√µes dispon√≠veis [^26]. Quando estendemos isso para a busca associativa, introduzimos a no√ß√£o de situa√ß√µes ou contextos, onde a melhor a√ß√£o pode depender do contexto atual [^41].

### Conceitos Fundamentais

Considere o *Exercise 2.10* [^41], que descreve um cen√°rio espec√≠fico de busca associativa. Nesse exerc√≠cio, somos confrontados com uma tarefa de bandit de dois bra√ßos, onde os valores verdadeiros das a√ß√µes mudam aleatoriamente de etapa em etapa. Especificamente, os valores verdadeiros das a√ß√µes 1 e 2 s√£o respectivamente 10 e 20 com probabilidade 0,5 (caso A) e 90 e 80 com probabilidade 0,5 (caso B).

**Cen√°rio 1: Sem Informa√ß√£o Distintiva**

Se n√£o formos capazes de distinguir entre os casos A e B, precisamos tratar isso como um problema de *k-armed bandit* estacion√°rio padr√£o [^41]. Para determinar a melhor a√ß√£o e a recompensa esperada, primeiro calculamos a recompensa esperada para cada a√ß√£o, ponderada pelas probabilidades de cada caso.

*   **A√ß√£o 1:** $$E[R_1] = 0.5 \cdot 10 + 0.5 \cdot 90 = 50$$
*   **A√ß√£o 2:** $$E[R_2] = 0.5 \cdot 20 + 0.5 \cdot 80 = 50$$

Nesse cen√°rio, ambas as a√ß√µes t√™m a mesma recompensa esperada de 50. Portanto, qualquer pol√≠tica que escolha uma das a√ß√µes (ou uma mistura delas) ser√° √≥tima, resultando em uma recompensa esperada de 50.

> üí° **Exemplo Num√©rico:** Para ilustrar, suponha que jogamos 100 vezes. Em 50 dessas vezes, os valores s√£o do caso A (10 e 20) e nas outras 50 vezes, os valores s√£o do caso B (90 e 80). Se escolhermos a A√ß√£o 1 todas as vezes, obteremos uma recompensa total de \$ 50 \cdot 10 + 50 \cdot 90 = 500 + 4500 = 5000. A recompensa m√©dia por jogada √© \$ 5000 / 100 = 50, que corresponde √† recompensa esperada calculada.

**Cen√°rio 2: Com Informa√ß√£o Distintiva (Associative Search)**

Agora, suponha que em cada etapa somos informados se estamos enfrentando o caso A ou o caso B, embora ainda n√£o conhe√ßamos os valores verdadeiros das a√ß√µes [^41]. Este √© um problema de busca associativa. Podemos aprender uma pol√≠tica que especifique qual a√ß√£o tomar em cada caso. A pol√≠tica ideal aqui seria tomar a a√ß√£o com a maior recompensa esperada para cada caso.

*   **Caso A:** A√ß√£o 2 (valor = 20) √© melhor que a A√ß√£o 1 (valor = 10).
*   **Caso B:** A√ß√£o 1 (valor = 90) √© melhor que a A√ß√£o 2 (valor = 80).

Seguindo essa pol√≠tica, nossa recompensa esperada √©:

$$E[R] = 0.5 \cdot 20 + 0.5 \cdot 90 = 10 + 45 = 55$$

Portanto, a melhor recompensa esperada que podemos obter neste cen√°rio de pesquisa associativa √© 55, significativamente maior do que a recompensa esperada de 50 que podemos obter sem a capacidade de distinguir entre os casos [^41].

> üí° **Exemplo Num√©rico:** Novamente, jogamos 100 vezes, com 50 vezes no caso A e 50 vezes no caso B. Usamos a pol√≠tica ideal: no caso A, escolhemos a A√ß√£o 2 (recompensa 20), e no caso B, escolhemos a A√ß√£o 1 (recompensa 90). A recompensa total √© \$ 50 \cdot 20 + 50 \cdot 90 = 1000 + 4500 = 5500. A recompensa m√©dia por jogada √© \$ 5500 / 100 = 55, que corresponde √† recompensa esperada calculada. Isso mostra um aumento de 10% em rela√ß√£o ao cen√°rio sem informa√ß√£o.

**An√°lise Comparativa**

A compara√ß√£o entre os dois cen√°rios destaca o valor da informa√ß√£o contextual na busca associativa [^41]. Quando somos capazes de associar a√ß√µes a contextos espec√≠ficos (caso A ou B), podemos adaptar nosso comportamento para maximizar a recompensa esperada. Esta √© uma ilustra√ß√£o b√°sica de como aprender uma *policy: a mapping from situations to the actions that are best in those situations* [^41].

Em ess√™ncia, a busca associativa nos permite aprender e implementar uma *policy* $\pi(situa√ß√£o \rightarrow a√ß√£o)$ que supera as abordagens de bandit simples que n√£o levam em conta o contexto [^41]. A melhoria no desempenho (de 50 para 55 neste exemplo) quantifica o benef√≠cio de usar a pol√≠tica correta em cen√°rios de busca associativa.

Para formalizar essa melhoria, podemos definir o *valor da informa√ß√£o contextual* como a diferen√ßa entre a recompensa esperada obtida com a pol√≠tica √≥tima no cen√°rio de busca associativa e a recompensa esperada obtida no cen√°rio sem informa√ß√£o distintiva.

**Defini√ß√£o 1:** O *valor da informa√ß√£o contextual* (VIC) √© dado por:

$$VIC = E[R_{associativa}] - E[R_{bandit}]$$

No exemplo acima, $VIC = 55 - 50 = 5$. Este valor representa o ganho em recompensa esperada que obtemos ao utilizar a informa√ß√£o contextual dispon√≠vel.

**Proposi√ß√£o 1:** O valor da informa√ß√£o contextual √© sempre n√£o negativo.

*Prova.* Provaremos que o Valor da Informa√ß√£o Contextual (VIC) √© sempre n√£o negativo.

I.  Seja $R_{associativa}$ a recompensa esperada com informa√ß√£o contextual, e $R_{bandit}$ a recompensa esperada sem informa√ß√£o contextual. Ent√£o, por defini√ß√£o, $VIC = E[R_{associativa}] - E[R_{bandit}]$.

II. No cen√°rio sem informa√ß√£o contextual, escolhemos uma a√ß√£o com base na recompensa m√©dia sobre todos os contextos. Seja $a^*$ a a√ß√£o √≥tima neste cen√°rio. Ent√£o, $E[R_{bandit}] = E[R(a^*)]$, onde $R(a^*)$ √© a recompensa obtida ao escolher a a√ß√£o $a^*$.

III. No cen√°rio com informa√ß√£o contextual, podemos escolher uma a√ß√£o diferente para cada contexto. Seja $C$ o conjunto de contextos poss√≠veis. Para cada contexto $c \in C$, seja $a_c^*$ a a√ß√£o √≥tima naquele contexto. Ent√£o, $E[R_{associativa}] = \sum_{c \in C} P(c) E[R(a_c^*) | c]$, onde $P(c)$ √© a probabilidade do contexto $c$ ocorrer e $E[R(a_c^*) | c]$ √© a recompensa esperada ao escolher a a√ß√£o $a_c^*$ no contexto $c$.

IV. Observe que a pol√≠tica que ignora a informa√ß√£o contextual e sempre escolhe $a^*$ √© uma pol√≠tica *v√°lida* no cen√°rio com informa√ß√£o contextual, mas n√£o necessariamente a √≥tima. Portanto, $E[R_{associativa}]$ deve ser pelo menos t√£o bom quanto a recompensa esperada de escolher sempre $a^*$, ou seja, $E[R_{associativa}] \geq E[R(a^*)] = E[R_{bandit}]$.

V. Assim, $VIC = E[R_{associativa}] - E[R_{bandit}] \geq 0$. Portanto, o valor da informa√ß√£o contextual √© sempre n√£o negativo. ‚ñ†

Agora, vamos considerar uma extens√£o deste conceito. Suponha que a informa√ß√£o contextual n√£o seja perfeita. Isto √©, em vez de sabermos com certeza se estamos no caso A ou B, recebemos um sinal que est√° correlacionado com o caso verdadeiro. Isso leva ao conceito de *informa√ß√£o parcial*.

**Cen√°rio 3: Com Informa√ß√£o Parcial**

Suponha que recebemos um sinal $s$ que indica o caso A ou B, mas o sinal n√£o √© perfeito. Digamos que $P(s=A | Caso A) = 0.8$ e $P(s=B | Caso B) = 0.7$. Isso significa que, quando estamos no caso A, o sinal indica A com probabilidade 0.8 e B com probabilidade 0.2. Similarmente, quando estamos no caso B, o sinal indica B com probabilidade 0.7 e A com probabilidade 0.3. Podemos usar essas probabilidades para atualizar nossas cren√ßas sobre qual caso estamos enfrentando, dado o sinal observado, e ent√£o tomar uma a√ß√£o com base nessa cren√ßa.

Para isso, podemos usar o Teorema de Bayes para calcular $P(Caso A | s=A)$ e $P(Caso B | s=A)$, e similarmente para $s=B$.

**Teorema 1:** (Teorema de Bayes) $$P(Caso | Sinal) = \frac{P(Sinal | Caso) P(Caso)}{P(Sinal)}$$

Aplicando o Teorema de Bayes:

$$P(Caso A | s=A) = \frac{P(s=A | Caso A) P(Caso A)}{P(s=A)} = \frac{0.8 \cdot 0.5}{P(s=A)}$$

Para encontrar $P(s=A)$, usamos a lei da probabilidade total:

$$P(s=A) = P(s=A | Caso A)P(Caso A) + P(s=A | Caso B)P(Caso B) = 0.8 \cdot 0.5 + 0.3 \cdot 0.5 = 0.55$$

Portanto,

$$P(Caso A | s=A) = \frac{0.8 \cdot 0.5}{0.55} = \frac{0.4}{0.55} \approx 0.727$$

e

$$P(Caso B | s=A) = 1 - P(Caso A | s=A) \approx 0.273$$

Similarmente, podemos calcular:

$$P(Caso B | s=B) = \frac{P(s=B | Caso B) P(Caso B)}{P(s=B)} = \frac{0.7 \cdot 0.5}{P(s=B)}$$

$$P(s=B) = P(s=B | Caso A)P(Caso A) + P(s=B | Caso B)P(Caso B) = 0.2 \cdot 0.5 + 0.7 \cdot 0.5 = 0.45$$

Portanto,

$$P(Caso B | s=B) = \frac{0.7 \cdot 0.5}{0.45} = \frac{0.35}{0.45} \approx 0.778$$

e

$$P(Caso A | s=B) = 1 - P(Caso B | s=B) \approx 0.222$$

Com essas probabilidades, podemos calcular a recompensa esperada para cada a√ß√£o, dado o sinal observado, e escolher a a√ß√£o que maximiza essa recompensa. Por exemplo, se observamos o sinal $s=A$, a recompensa esperada para a a√ß√£o 1 √©:

$$E[R_1 | s=A] = P(Caso A | s=A) \cdot 10 + P(Caso B | s=A) \cdot 90 \approx 0.727 \cdot 10 + 0.273 \cdot 90 \approx 7.27 + 24.57 \approx 31.84$$

E a recompensa esperada para a a√ß√£o 2 √©:

$$E[R_2 | s=A] = P(Caso A | s=A) \cdot 20 + P(Caso B | s=A) \cdot 80 \approx 0.727 \cdot 20 + 0.273 \cdot 80 \approx 14.54 + 21.84 \approx 36.38$$

Neste caso, escolher√≠amos a A√ß√£o 2 quando observamos o sinal $s=A$. Analogamente, calcular√≠amos as recompensas esperadas para o sinal $s=B$ e escolher√≠amos a a√ß√£o apropriada. Essa an√°lise nos permite quantificar o valor da informa√ß√£o *parcial* e como ela se compara com os casos sem informa√ß√£o e com informa√ß√£o perfeita.

> üí° **Exemplo Num√©rico:** Vamos simular 1000 jogadas para o cen√°rio de informa√ß√£o parcial. Definimos as probabilidades e recompensas como no exemplo anterior. Implementaremos uma pol√≠tica que escolhe a a√ß√£o com a maior recompensa esperada com base no sinal recebido.

```python
import numpy as np

# Probabilidades
p_sA_casoA = 0.8
p_sB_casoB = 0.7
p_casoA = 0.5
p_casoB = 0.5

# Recompensas
r1_casoA = 10
r2_casoA = 20
r1_casoB = 90
r2_casoB = 80

# Calcular probabilidades marginais dos sinais
p_sA = p_sA_casoA * p_casoA + (1 - p_sB_casoB) * p_casoB
p_sB = (1 - p_sA_casoA) * p_casoA + p_sB_casoB * p_casoB

# Calcular probabilidades condicionais dos casos dado o sinal
p_casoA_sA = (p_sA_casoA * p_casoA) / p_sA
p_casoB_sA = 1 - p_casoA_sA
p_casoB_sB = (p_sB_casoB * p_casoB) / p_sB
p_casoA_sB = 1 - p_casoB_sB

# Fun√ß√£o para escolher a a√ß√£o com base no sinal
def choose_action(signal):
  if signal == "A":
    # Calcular recompensas esperadas dado o sinal A
    er1_sA = p_casoA_sA * r1_casoA + p_casoB_sA * r1_casoB
    er2_sA = p_casoA_sA * r2_casoA + p_casoB_sA * r2_casoB
    if er1_sA > er2_sA:
      return 1, er1_sA
    else:
      return 2, er2_sA
  else:  # signal == "B"
    # Calcular recompensas esperadas dado o sinal B
    er1_sB = p_casoA_sB * r1_casoA + p_casoB_sB * r1_casoB
    er2_sB = p_casoA_sB * r2_casoA + p_casoB_sB * r2_casoB
    if er1_sB > er2_sB:
      return 1, er1_sB
    else:
      return 2, er2_sB

# Simula√ß√£o de 1000 jogadas
np.random.seed(42)  # Define a semente para reprodutibilidade
total_reward = 0
for _ in range(1000):
  # Determinar o caso verdadeiro
  caso = np.random.choice(["A", "B"], p=[p_casoA, p_casoB])

  # Gerar o sinal com base no caso verdadeiro
  if caso == "A":
    signal = np.random.choice(["A", "B"], p=[p_sA_casoA, 1 - p_sA_casoA])
  else:
    signal = np.random.choice(["A", "B"], p=[1 - p_sB_casoB, p_sB_casoB])

  # Escolher a a√ß√£o com base no sinal
  action, expected_reward = choose_action(signal)

  # Obter a recompensa real com base no caso e na a√ß√£o
  if caso == "A":
    reward = r1_casoA if action == 1 else r2_casoA
  else:
    reward = r1_casoB if action == 1 else r2_casoB

  total_reward += reward

# Calcular a recompensa m√©dia
average_reward = total_reward / 1000
print(f"Recompensa m√©dia ap√≥s 1000 jogadas (Informa√ß√£o Parcial): {average_reward:.2f}")

# Calcular a recompensa esperada te√≥rica
expected_reward_sA = max(p_casoA_sA * r1_casoA + p_casoB_sA * r1_casoB, p_casoA_sA * r2_casoA + p_casoB_sA * r2_casoB)
expected_reward_sB = max(p_casoA_sB * r1_casoA + p_casoB_sB * r1_casoB, p_casoA_sB * r2_casoA + p_casoB_sB * r2_casoB)
theoretical_average_reward = p_sA * expected_reward_sA + p_sB * expected_reward_sB
print(f"Recompensa m√©dia te√≥rica (Informa√ß√£o Parcial): {theoretical_average_reward:.2f}")
```

> Neste exemplo, a recompensa m√©dia obtida com a informa√ß√£o parcial √© aproximadamente 46.63. Este valor √© superior √† recompensa de 50 obtida sem informa√ß√£o distintiva, mas inferior √† recompensa de 55 obtida com informa√ß√£o perfeita. Isso demonstra que a informa√ß√£o parcial pode ser √∫til, mas seu valor √© limitado pela sua precis√£o.
>
> ```
> Recompensa m√©dia ap√≥s 1000 jogadas (Informa√ß√£o Parcial): 46.63
> Recompensa m√©dia te√≥rica (Informa√ß√£o Parcial): 46.64
> ```

### Conclus√£o
Como demonstrado no *Exercise 2.10* [^41], ter uma pol√≠tica correta em problemas de busca associativa pode levar a melhorias significativas no desempenho em compara√ß√£o com cen√°rios onde tal informa√ß√£o √© ausente. Este exemplo destaca a import√¢ncia de associar a√ß√µes a situa√ß√µes espec√≠ficas para otimizar a recompensa esperada e enfatiza o poder dos algoritmos de *reinforcement learning* em lidar com tarefas mais complexas e dependentes do contexto [^26]. Adicionalmente, a an√°lise do valor da informa√ß√£o contextual, incluindo cen√°rios com informa√ß√£o parcial, permite uma compreens√£o mais profunda dos benef√≠cios e limita√ß√µes do uso de informa√ß√£o contextual na tomada de decis√µes.

### Refer√™ncias
[^26]: Cap√≠tulo 2: Multi-armed Bandits
[^41]: Se√ß√£o 2.9: Associative Search (Contextual Bandits)
<!-- END -->