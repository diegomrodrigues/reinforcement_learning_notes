## O Testbed de 10 Bra√ßos: Uma An√°lise da Explora√ß√£o Œµ-Gananciosa

### Introdu√ß√£o
O conceito de **aprendizado por refor√ßo** (RL) se distingue de outras formas de aprendizado por utilizar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de instruir diretamente quais a√ß√µes s√£o corretas. Essa distin√ß√£o gera a necessidade de explora√ß√£o ativa para encontrar comportamentos otimizados. O feedback avaliativo indica a qualidade de uma a√ß√£o, mas n√£o se √© a melhor poss√≠vel, enquanto o feedback instrutivo informa qual a√ß√£o tomar, independentemente da a√ß√£o realizada. Este cap√≠tulo foca no aspecto avaliativo do aprendizado por refor√ßo em um ambiente simplificado, onde n√£o h√° necessidade de aprender a agir em diversas situa√ß√µes, evitando a complexidade do problema completo de RL [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). O problema do *k-armed bandit* √© utilizado para introduzir m√©todos b√°sicos de aprendizado, que posteriormente s√£o estendidos para aplicar ao problema de aprendizado por refor√ßo em sua totalidade. Ao final deste cap√≠tulo, o texto avan√ßa em dire√ß√£o ao problema completo de RL, abordando o que acontece quando o problema do *bandit* se torna associativo, ou seja, quando a melhor a√ß√£o depende da situa√ß√£o [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). O presente cap√≠tulo, ent√£o, foca no **testbed de 10 bra√ßos**, uma metodologia para avaliar a efic√°cia de diferentes m√©todos de aprendizagem, com foco na explora√ß√£o e explota√ß√£o, utilizando os m√©todos *Œµ-greedy*.

### Conceitos Fundamentais
O problema do *k-armed bandit* envolve escolhas repetidas entre *k* op√ß√µes, cada uma oferecendo uma recompensa num√©rica amostrada de uma distribui√ß√£o de probabilidade estacion√°ria dependente da a√ß√£o selecionada. O objetivo √© maximizar a recompensa total esperada ao longo de um per√≠odo [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Cada a√ß√£o possui um valor esperado, denotado por $q_*(a)$, que representa a recompensa m√©dia obtida ao selecionar a a√ß√£o *a* [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2):
$$q_*(a) = E[R_t | A_t=a].$$
O valor estimado da a√ß√£o *a* no passo de tempo *t* √© representado por $Q_t(a)$, e o objetivo √© que $Q_t(a)$ se aproxime de $q_*(a)$. A **explora√ß√£o** √© o ato de selecionar a√ß√µes n√£o-gananciosas para melhorar as estimativas de valor, enquanto a **explota√ß√£o** √© o ato de escolher as a√ß√µes que, segundo as estimativas atuais, proporcionam a maior recompensa. Existe um conflito entre explora√ß√£o e explota√ß√£o, pois maximizar a recompensa imediata (explota√ß√£o) pode impedir a descoberta de a√ß√µes melhores a longo prazo (explora√ß√£o) [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). O m√©todo de m√©dia amostral (**sample-average method**) √© uma maneira natural de estimar os valores das a√ß√µes, calculando a m√©dia das recompensas recebidas ao selecionar cada a√ß√£o [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3):
$$Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i 1_{A_i=a}}{\sum_{i=1}^{t-1} 1_{A_i=a}},$$
onde $1_{predicate}$ √© 1 se o predicado √© verdadeiro e 0 caso contr√°rio. As a√ß√µes gananciosas s√£o aquelas com a maior estimativa de valor. A sele√ß√£o gananciosa sempre explora o conhecimento atual para maximizar a recompensa imediata, sem explorar a√ß√µes aparentemente inferiores. Uma alternativa √© o m√©todo **Œµ-greedy**, que age gananciosamente a maior parte do tempo, mas com uma pequena probabilidade *Œµ* seleciona uma a√ß√£o aleatoriamente, promovendo a explora√ß√£o [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). Este m√©todo de sele√ß√£o de a√ß√£o pode ser formalizado como:
$$ A_t = \begin{cases} \underset{a}{\text{argmax}} \, Q_t(a), & \text{com probabilidade } 1 - \epsilon \\  \text{a√ß√£o aleat√≥ria}, & \text{com probabilidade } \epsilon \end{cases} $$
O m√©todo Œµ-ganancioso garante que, no limite, todas as a√ß√µes ser√£o amostradas infinitamente, assegurando que os valores estimados convirjam para os valores reais [4](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-4).

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de *3-armed bandit*. Ap√≥s algumas itera√ß√µes, as estimativas de valor das a√ß√µes s√£o: $Q_t(1) = 2.5$, $Q_t(2) = 1.8$, e $Q_t(3) = 3.1$.
>
> *   **A√ß√£o Gananciosa:** O m√©todo ganancioso escolheria a a√ß√£o 3, pois tem o maior valor estimado (3.1).
> *   **A√ß√£o Œµ-Gananciosa (com Œµ=0.2):** H√° uma probabilidade de 80% de escolher a a√ß√£o 3 (explota√ß√£o) e 20% de escolher aleatoriamente uma das tr√™s a√ß√µes (explora√ß√£o). Se um n√∫mero aleat√≥rio entre 0 e 1 for menor que 0.2, uma a√ß√£o aleat√≥ria ser√° escolhida. Caso contr√°rio, a a√ß√£o 3 ser√° selecionada.
>
> Este exemplo ilustra como o m√©todo Œµ-ganancioso equilibra explora√ß√£o e explota√ß√£o ao introduzir uma probabilidade de escolha aleat√≥ria.

**Lema 1** *Converg√™ncia da M√©dia Amostral*: Se a recompensa para cada a√ß√£o *a* √© limitada, ou seja, $|R_t| \leq M$ para algum *M* > 0, ent√£o $Q_t(a)$ converge para $q_*(a)$ quando o n√∫mero de vezes que *a* foi selecionada tende ao infinito.

*Prova:* A converg√™ncia da m√©dia amostral segue diretamente da Lei Forte dos Grandes N√∫meros. Como as recompensas s√£o limitadas, a m√©dia amostral converge quase certamente para o valor esperado da recompensa.

> üí° **Exemplo Num√©rico:** Considere uma a√ß√£o com valor verdadeiro $q_*(a) = 4$. As recompensas obtidas ao longo de algumas intera√ß√µes foram: 3, 5, 4, 2, 6.
>
> *   $Q_1(a) = 3$
> *   $Q_2(a) = (3+5)/2 = 4$
> *   $Q_3(a) = (3+5+4)/3 = 4$
> *   $Q_4(a) = (3+5+4+2)/4 = 3.5$
> *   $Q_5(a) = (3+5+4+2+6)/5 = 4$
>
> √Ä medida que o n√∫mero de intera√ß√µes aumenta, $Q_t(a)$ se aproxima de $q_*(a) = 4$.
>
> ```mermaid
> graph LR
>     subgraph "M√©dia Amostral"
>     A["Q1(a) = 3"] --> B["Q2(a) = 4"];
>     B --> C["Q3(a) = 4"];
>     C --> D["Q4(a) = 3.5"];
>     D --> E["Q5(a) = 4"];
>     E --> F["Q_t(a) -> q*(a) = 4"];
>     end
> style F fill:#ccf,stroke:#333,stroke-width:2px
> ```
>
> Este exemplo visualiza como as m√©dias amostrais convergem para o valor verdadeiro da a√ß√£o.

Al√©m disso, podemos considerar uma formula√ß√£o alternativa para a atualiza√ß√£o da estimativa de valor usando uma m√©dia incremental, que computacionalmente √© mais eficiente. Esta formula√ß√£o evita o c√°lculo da soma e do n√∫mero de sele√ß√µes da a√ß√£o *a* a cada passo de tempo.
**Lema 1.1** *Atualiza√ß√£o Incremental da M√©dia Amostral*: O c√°lculo de $Q_t(a)$ usando a m√©dia amostral pode ser realizado incrementalmente como:
$$Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)}[R_t - Q_t(a)],$$
onde $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o instante *t*.

*Prova:* Seja $N_t(a)$ o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o tempo *t*. Ent√£o,
$$Q_{t+1}(a) = \frac{\sum_{i=1}^{t} R_i 1_{A_i=a}}{N_{t+1}(a)}.$$
Se $A_t = a$, ent√£o $N_{t+1}(a) = N_t(a) + 1$. Portanto,
\begin{align*}
Q_{t+1}(a) &= \frac{\sum_{i=1}^{t-1} R_i 1_{A_i=a} + R_t}{N_t(a) + 1} \\
&= \frac{N_t(a) Q_t(a) + R_t}{N_t(a) + 1} \\
&= Q_t(a) + \frac{R_t - Q_t(a)}{N_t(a) + 1}.
\end{align*}
Se $A_t \neq a$, ent√£o $N_{t+1}(a) = N_t(a)$, e $Q_{t+1}(a) = Q_t(a)$.

Este resultado mostra que a m√©dia amostral pode ser atualizada incrementalmente, o que √© computacionalmente eficiente para implementa√ß√µes.

> üí° **Exemplo Num√©rico:**  Suponha que, no instante *t*, $Q_t(a) = 2$ e $N_t(a) = 5$. Na pr√≥xima itera√ß√£o, selecionamos a a√ß√£o *a* e recebemos uma recompensa $R_t = 6$. Usando a atualiza√ß√£o incremental:
>
> $Q_{t+1}(a) = 2 + \frac{1}{5+1}(6 - 2) = 2 + \frac{4}{6} = 2 + \frac{2}{3} \approx 2.67$
>
> Este exemplo mostra como a estimativa de valor √© atualizada incrementalmente, sem precisar recalcular a m√©dia a cada passo.
>
> ```mermaid
> graph LR
>     subgraph "Atualiza√ß√£o Incremental"
>       A["Q_t(a) = 2"] --> B["N_t(a) = 5"];
>       B --> C["R_t = 6"];
>       C --> D["Q_{t+1}(a) = Q_t(a) + (1/N_t(a))(R_t - Q_t(a))"];
>       D --> E["Q_{t+1}(a) ‚âà 2.67"];
>     end
>     style E fill:#ccf,stroke:#333,stroke-width:2px
> ```

### O Testbed de 10 Bra√ßos e a Efic√°cia da Explora√ß√£o Œµ-Gananciosa
Para avaliar a efic√°cia dos m√©todos gananciosos e Œµ-gananciosos, foi usado um *suite* de testes com 2000 problemas de *k-armed bandit* gerados aleatoriamente, com *k = 10*. O testbed, denominado **10-armed testbed**, √© um ambiente onde as a√ß√µes t√™m valores reais $q_*(a)$ sorteados de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1. As recompensas reais $R_t$ s√£o, ent√£o, amostradas de uma distribui√ß√£o normal com m√©dia $q_*(A_t)$ e vari√¢ncia 1 [5](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-5). Cada rodada do teste dura 1000 passos de tempo, e o desempenho √© medido em rela√ß√£o √† melhoria da recompensa ao longo do tempo. A m√©dia do comportamento do algoritmo de aprendizado √© obtida ap√≥s 2000 rodadas independentes com diferentes problemas de *bandit* [5](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-5).

O m√©todo ganancioso, que sempre escolhe a a√ß√£o com maior estimativa de valor, inicialmente apresenta um desempenho ligeiramente melhor, mas rapidamente se estabiliza em um n√≠vel inferior devido √† falta de explora√ß√£o. Os m√©todos Œµ-gananciosos, por outro lado, continuam a explorar, melhorando suas estimativas e, consequentemente, seu desempenho ao longo do tempo. A escolha do valor de *Œµ* influencia o desempenho: um valor maior leva a uma maior explora√ß√£o e pode encontrar mais rapidamente a a√ß√£o √≥tima, mas pode n√£o convergir para uma pol√≠tica √≥tima t√£o rapidamente quanto valores menores de *Œµ*. Especificamente, um m√©todo Œµ-ganancioso com Œµ = 0.1 explora mais e geralmente encontra a a√ß√£o √≥tima mais cedo, mas nunca seleciona essa a√ß√£o mais de 91% do tempo. Enquanto isso, um m√©todo Œµ-ganancioso com Œµ = 0.01 melhora mais lentamente, mas, em √∫ltima an√°lise, apresentaria melhor desempenho [6](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-6). A efic√°cia do m√©todo Œµ-ganancioso em rela√ß√£o ao m√©todo puramente ganancioso depende da natureza da tarefa. Por exemplo, se as recompensas fossem mais ruidosas, a explora√ß√£o se tornaria ainda mais crucial, e os m√©todos Œµ-gananciosos superariam os m√©todos gananciosos por uma margem maior. Al√©m disso, em problemas n√£o estacion√°rios, onde os valores reais das a√ß√µes mudam com o tempo, a explora√ß√£o √© essencial para adaptar a pol√≠tica [6](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-6).

> üí° **Exemplo Num√©rico:** Considere o testbed de 10 bra√ßos. As recompensas verdadeiras $q_*(a)$ para as a√ß√µes podem variar, digamos de -2 a 2.
>
> *   **M√©todo Ganancioso:** Inicialmente, o agente pode selecionar uma a√ß√£o com recompensa estimada $Q_1(a) = 0$. Se a primeira recompensa for -1, ele ficar√° preso explorando a√ß√µes com recompensas similares, sem perceber que existem a√ß√µes com $q_*(a)$ maiores.
> *   **M√©todo Œµ-Ganancioso (Œµ=0.1):** O agente, com probabilidade 0.1, explora outras a√ß√µes, mesmo que tenha uma a√ß√£o com recompensa estimada maior no momento.
>
> ```mermaid
>   graph LR
>   subgraph "Compara√ß√£o de Estrat√©gias"
>     A["Greedy: Initial Reward -1"] --> B{"Exploration?"};
>     B -- "No" --> C["Stuck"];
>     B -- "Yes" --> D["Œµ-Greedy: Explores"];
>     D --> E["Finds Better Action"];
>     C --> F["Low Performance"];
>     E --> G["High Performance"];
>   end
>   style C fill:#f9f,stroke:#333,stroke-width:2px
>   style G fill:#ccf,stroke:#333,stroke-width:2px
> ```
>
>  Este diagrama ilustra como o m√©todo Œµ-ganancioso pode escapar de m√≠nimos locais ao contr√°rio do m√©todo ganancioso.
>

**Proposi√ß√£o 2** *Impacto da Varia√ß√£o de Œµ*: Em tarefas estacion√°rias, o desempenho a longo prazo de um m√©todo Œµ-ganancioso √© geralmente melhor com valores menores de Œµ, enquanto valores maiores de Œµ podem levar a um aprendizado mais r√°pido inicialmente. Contudo, em tarefas n√£o estacion√°rias, valores maiores de Œµ podem ser mais ben√©ficos a longo prazo devido √† necessidade de adapta√ß√£o √†s mudan√ßas.

*Prova:* (Argumenta√ß√£o) A prova segue diretamente da discuss√£o no texto. Em tarefas estacion√°rias, a converg√™ncia para o valor √≥timo √© o objetivo principal, e a explora√ß√£o excessiva pode prejudicar o desempenho a longo prazo. Em tarefas n√£o estacion√°rias, a explora√ß√£o constante √© crucial para acompanhar as mudan√ßas nos valores das a√ß√µes, e um Œµ maior pode acelerar o processo de adapta√ß√£o.

> üí° **Exemplo Num√©rico:** Compara√ß√£o de desempenho em uma tarefa estacion√°ria vs. n√£o-estacion√°ria com diferentes valores de $\epsilon$.
>
> | Œµ     | Tarefa Estacion√°ria (Recompensa M√©dia) | Tarefa N√£o-Estacion√°ria (Recompensa M√©dia) |
> |-------|---------------------------------------|-------------------------------------------|
> | 0.1   | 0.85                                  | 0.60                                      |
> | 0.01  | 0.90                                  | 0.50                                      |
> | 0.2   | 0.80                                  | 0.70                                      |
>
> Em tarefas estacion√°rias, um Œµ menor (0.01) resulta em maior recompensa m√©dia a longo prazo. Em tarefas n√£o-estacion√°rias, um valor de Œµ maior (0.2) permite melhor adapta√ß√£o √†s mudan√ßas. Este exemplo evidencia o impacto de $\epsilon$ no desempenho do aprendizado por refor√ßo.

**Corol√°rio 2.1** *Agendamento de Œµ*: Para otimizar o desempenho em cen√°rios onde a explora√ß√£o inicial √© desej√°vel e a explota√ß√£o √© prefer√≠vel a longo prazo, pode-se usar um agendamento de Œµ, no qual o valor de Œµ diminui ao longo do tempo.

A abordagem de agendamento de Œµ permite que o agente explore mais no in√≠cio do aprendizado, quando a incerteza √© maior, e explore mais no final, quando as estimativas de valor est√£o mais precisas. Essa t√©cnica permite ajustar o equil√≠brio entre explora√ß√£o e explota√ß√£o ao longo do tempo.

> üí° **Exemplo Num√©rico:** Um agendamento comum para Œµ pode ser:
>
> $\epsilon_t = \epsilon_0 * e^{-kt}$,
>
> onde $\epsilon_0$ √© o valor inicial de epsilon, *k* √© uma taxa de decaimento e *t* √© o tempo. Se $\epsilon_0 = 0.5$ e $k = 0.01$, o valor de $\epsilon$ diminui com o tempo.
>
> *   Em *t* = 0, $\epsilon$ = 0.5
> *   Em *t* = 10, $\epsilon \approx 0.45$
> *   Em *t* = 100, $\epsilon \approx 0.18$
>
> Isso ilustra como a explora√ß√£o diminui com o tempo, incentivando mais explota√ß√£o √† medida que o aprendizado avan√ßa.
> ```mermaid
>   graph LR
>     subgraph "Agendamento de Œµ"
>       A["Œµ_0 = 0.5"] --> B["k = 0.01"];
>       B --> C["Œµ_t = Œµ_0 * e^(-kt)"];
>       C --> D["t = 0, Œµ = 0.5"];
>       D --> E["t = 10, Œµ ‚âà 0.45"];
>       E --> F["t = 100, Œµ ‚âà 0.18"];
>     end
>       style F fill:#ccf,stroke:#333,stroke-width:2px
> ```

### Conclus√£o
Em resumo, o **testbed de 10 bra√ßos** demonstra que m√©todos Œµ-gananciosos s√£o mais eficazes a longo prazo em compara√ß√£o com m√©todos puramente gananciosos devido √† sua capacidade de equilibrar explora√ß√£o e explota√ß√£o. A explora√ß√£o cont√≠nua permite que os m√©todos Œµ-gananciosos refinem suas estimativas e descubram melhores a√ß√µes, mesmo que inicialmente apresentem um desempenho inferior. O valor de Œµ √© crucial para o desempenho do algoritmo, sendo importante ajustar de forma correta para cada problema a fim de atingir os resultados desejados. Essa abordagem destaca a necessidade de uma estrat√©gia que combine explora√ß√£o e explota√ß√£o para alcan√ßar um aprendizado eficaz em ambientes de recompensa incerta e/ou n√£o estacion√°ria.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken. In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback differs from, and yet can be combined with, instructive feedback. The particular nonassociative, evaluative feedback problem that we explore is a simple version of the k-armed bandit problem. We use this problem to introduce a number of basic learning methods which we extend in later chapters to apply to the full reinforcement learning problem. At the end of this chapter, we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when the best action depends on the situation." *(Trecho de Multi-armed Bandits)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected: $q*(a) = E[R_t | A_t=a]$. If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time stept as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q‚àó(a)$. If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action‚Äôs value. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. For example, suppose a greedy action‚Äôs value is known with certainty, while several other actions are estimated to be nearly as good but with substantial uncertainty. The uncertainty is such that at least one of these other actions probably is actually better than the greedy action, but you don‚Äôt know which one. If you have many time steps ahead on which to make action selections, then it may be better to explore the nongreedy actions and discover which of them are better than the greedy action. Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the ‚Äúconflict‚Äù between exploration and exploitation." *(Trecho de Multi-armed Bandits)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: $Q_t(a) = (\text{sum of rewards when a taken prior to t}) / (\text{number of times a taken prior to t}) = (\sum_{i=1}^{t-1} R_i 1_{A_i=a}) / (\sum_{i=1}^{t-1} 1_{A_i=a})$ where $1_{predicate}$ denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as 0. As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q‚àó(a)$. We call this the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards. Of course this is just one way to estimate action values, and not necessarily the best one. Nevertheless, for now let us stay with this simple estimation method and turn to the question of how the estimates might be used to select actions. The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as $A_t = \text{argmax}_a \, Q_t(a)$, where argmaxa denotes the action a for which the expression that follows is maximized (with ties broken arbitrarily). Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability Œµ, instead select randomly" *(Trecho de Multi-armed Bandits)*
[^4]: "from among all the actions with equal probability, independently of the action-value estimates. We call methods using this near-greedy action selection rule Œµ-greedy methods. An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the $Q_t(a)$ converge to their respective $q‚àó(a)$. This of course implies that the probability of selecting the optimal action converges to greater than 1 ‚Äì Œµ, that is, to near certainty. These are just asymptotic guarantees, however, and say little about the practical effectiveness of the methods." *(Trecho de Multi-armed Bandits)*
[^5]: "To roughly assess the relative effectiveness of the greedy and …õ-greedy action-value methods, we compared them numerically on a suite of test problems. This was a set of 2000 randomly generated k-armed bandit problems with k = 10. For each bandit problem, such as the one shown in Figure 2.1, the action values, q‚àó(a), a = 1, ..., 10, were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. Then, when a learning method applied to that problem selected action At at time step t, the actual reward, Rt, was selected from a normal distribution with mean $q‚àó(A_t)$ and variance 1. These distributions are shown in gray in Figure 2.1. We call this suite of test tasks the 10-armed testbed. For any learning method, we can measure its performance and behavior as it improves with experience over 1000 time steps when applied to one of the bandit problems. This makes up one run. Repeating this for 2000 independent runs, each with a different bandit problem, we obtained measures of the learning algorithm‚Äôs average behavior." *(Trecho de Multi-armed Bandits)*
[^6]: "shows that the greedy method found the optimal action in only approximately one-third of the tasks. In the other two-thirds, its initial samples of the optimal action were disappointing, and it never returned to it. The …õ-greedy methods eventually performed better because they continued to explore and to improve their chances of recognizing the optimal action. The …õ = 0.1 method explored more, and usually found the optimal action earlier, but it never selected that action more than 91% of the time. The …õ = 0.01 method improved more slowly, but eventually would perform better than the …õ = 0.1 method on both performance measures shown in the figure. It is also possible to reduce Œµ over time to try to get the best of both high and low values. The advantage of …õ-greedy over greedy methods depends on the task. For example, suppose the reward variance had been larger, say 10 instead of 1. With noisier rewards it takes more exploration to find the optimal action, and …õ-greedy methods should fare even better relative to the greedy method. On the other hand, if the reward variances were zero, then the greedy method would know the true value of each action after trying it once. In this case the greedy method might actually perform best because it would soon find the optimal action and then never explore. But even in the deterministic case there is a large advantage to exploring if we weaken some of the other assumptions. For example, suppose the bandit task were nonstationary, that is, the true values of the actions changed over time. In this case exploration is needed even in the deterministic case to make sure one of the nongreedy actions has not changed to become better than the greedy one. As we shall see in the next few chapters, nonstationarity is the case most commonly encountered in reinforcement learning. Even if the underlying task is stationary and deterministic, the learner faces a set of banditlike decision tasks each of which changes over time as learning proceeds and the agent‚Äôs decision-making policy changes. Reinforcement learning requires a balance between exploration and exploitation." *(Trecho de Multi-armed Bandits)*
