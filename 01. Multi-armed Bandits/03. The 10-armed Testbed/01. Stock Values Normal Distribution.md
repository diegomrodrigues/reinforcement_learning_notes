## O Testbed de 10 Bra√ßos: Gera√ß√£o dos Valores de A√ß√£o
### Introdu√ß√£o
Este cap√≠tulo foca no estudo do *k-armed bandit problem* utilizando um ambiente de teste espec√≠fico, denominado **10-armed testbed**. O objetivo principal √© avaliar e comparar diferentes m√©todos de aprendizado por refor√ßo, particularmente aqueles que equilibram explora√ß√£o e explota√ß√£o, usando um conjunto de problemas bem definidos. O *10-armed testbed* oferece um ambiente controlado para esse tipo de avalia√ß√£o, permitindo analisar o comportamento dos algoritmos em diversas condi√ß√µes [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

### Conceitos Fundamentais
O *k-armed bandit problem* envolve a tomada de decis√µes repetidas entre *k* diferentes op√ß√µes ou a√ß√µes. Ap√≥s cada escolha, um agente recebe uma recompensa num√©rica, que √© amostrada de uma distribui√ß√£o de probabilidade estacion√°ria dependente da a√ß√£o selecionada. O objetivo do agente √© maximizar a recompensa total esperada ao longo de um per√≠odo de tempo espec√≠fico [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). No contexto do *10-armed testbed*, especificamente, *k* √© igual a 10, o que significa que o agente precisa escolher entre 10 a√ß√µes diferentes em cada passo.

Para criar este ambiente de teste, os valores verdadeiros de cada a√ß√£o, denotados como **q*(a)**, s√£o gerados aleatoriamente para cada problema. Esses valores s√£o selecionados de acordo com uma **distribui√ß√£o normal (Gaussiana) com m√©dia 0 e vari√¢ncia 1**. Essa distribui√ß√£o √© utilizada para garantir que os valores verdadeiros das a√ß√µes sejam inicialmente desconhecidos e que variem entre os diferentes problemas do testbed. Cada a√ß√£o *a*, para *a = 1,...,10*, tem um valor base **q*(a)** sorteado dessa distribui√ß√£o normal. √â importante notar que essa distribui√ß√£o √© usada apenas para gerar o valor verdadeiro subjacente de cada a√ß√£o para um dado problema; as recompensas reais que o agente receber√° ao selecionar uma a√ß√£o ser√£o amostradas de uma distribui√ß√£o diferente.

> üí° **Exemplo Num√©rico:** Vamos simular a gera√ß√£o dos valores `q*(a)` para um √∫nico problema do *10-armed testbed*. Usaremos Python com `numpy` para isso.
> ```python
> import numpy as np
>
> # Define o n√∫mero de a√ß√µes
> num_actions = 10
>
> # Sorteia os valores q*(a) de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1
> np.random.seed(42) # Para reprodutibilidade
> q_star_values = np.random.normal(0, 1, num_actions)
>
> print("Valores q*(a) gerados para as 10 a√ß√µes:")
> for i, q_star in enumerate(q_star_values):
>     print(f"A√ß√£o {i+1}: q*(a) = {q_star:.3f}")
> ```
> Este c√≥digo ir√° gerar 10 valores, cada um representando o valor verdadeiro `q*(a)` de uma a√ß√£o espec√≠fica. Por exemplo, a sa√≠da pode ser algo como:
> ```
> Valores q*(a) gerados para as 10 a√ß√µes:
> A√ß√£o 1: q*(a) = 0.497
> A√ß√£o 2: q*(a) = -0.139
> A√ß√£o 3: q*(a) = 0.647
> A√ß√£o 4: q*(a) = 1.523
> A√ß√£o 5: q*(a) = -0.234
> A√ß√£o 6: q*(a) = -0.137
> A√ß√£o 7: q*(a) = 1.579
> A√ß√£o 8: q*(a) = 1.406
> A√ß√£o 9: q*(a) = -0.007
> A√ß√£o 10: q*(a) = 0.277
> ```
> Observe que esses valores s√£o aleat√≥rios e variam em torno de 0, com alguns sendo positivos e outros negativos, como esperado de uma distribui√ß√£o normal com m√©dia 0.

**Proposi√ß√£o 1 (Independ√™ncia dos Valores de A√ß√£o):** Os valores **q*(a)** s√£o gerados independentemente para cada a√ß√£o *a* dentro de um dado problema, e para cada problema dentro do *testbed*. Esta independ√™ncia √© crucial para garantir a diversidade dos desafios colocados aos algoritmos de aprendizado por refor√ßo.

*Proof Strategy:* A independ√™ncia √© uma consequ√™ncia direta do processo de gera√ß√£o: para cada a√ß√£o *a* em cada problema, o valor **q*(a)** √© amostrado independentemente da distribui√ß√£o normal $\mathcal{N}(0,1)$. N√£o h√° nenhuma correla√ß√£o introduzida no processo de amostragem entre as a√ß√µes nem entre os problemas.

  ```mermaid
  graph LR
      subgraph "Problema i"
          A1["A√ß√£o 1"] -->|q*(1) ~ N(0, 1)| B1("Valor q*(1)");
          A2["A√ß√£o 2"] -->|q*(2) ~ N(0, 1)| B2("Valor q*(2)");
          ...
          A10["A√ß√£o 10"] -->|q*(10) ~ N(0, 1)| B10("Valor q*(10)");
      end
  ```

Para cada problema do testbed, portanto, os dez valores **q*(a)** s√£o sorteados. Este procedimento garante que cada problema represente um cen√°rio ligeiramente diferente para o agente. Depois de selecionado um valor **q*(a)** para cada a√ß√£o, as recompensas reais s√£o amostradas para a a√ß√£o selecionada. Mais especificamente, quando o agente seleciona a a√ß√£o *A_t* no instante *t*, ele recebe uma recompensa *R_t* amostrada de uma distribui√ß√£o normal com m√©dia igual a **q*(A_t)** e vari√¢ncia 1. Assim, a distribui√ß√£o das recompensas √© centrada no valor verdadeiro da a√ß√£o selecionada, introduzindo um certo grau de aleatoriedade em torno do valor esperado. √â importante ressaltar que, enquanto o valor verdadeiro **q*(a)** √© fixo para um dado problema do testbed, as recompensas *R_t* variam em torno de **q*(a)**.

> üí° **Exemplo Num√©rico:** Vamos supor que o agente escolheu a a√ß√£o 7, que no nosso exemplo num√©rico anterior tem um valor verdadeiro `q*(7) = 1.579`. A recompensa `R_t` ser√° amostrada de uma distribui√ß√£o normal com m√©dia 1.579 e vari√¢ncia 1. Podemos simular isso usando o seguinte c√≥digo:
> ```python
> # Valor verdadeiro da a√ß√£o selecionada (A√ß√£o 7)
> q_star_selected = q_star_values[6] # q*(7) que est√° no √≠ndice 6
>
> # Simula a recompensa R_t
> reward = np.random.normal(q_star_selected, 1)
>
> print(f"Valor verdadeiro q*(A_t) da a√ß√£o 7: {q_star_selected:.3f}")
> print(f"Recompensa R_t obtida: {reward:.3f}")
> ```
> Executando este c√≥digo, podemos obter uma recompensa como, por exemplo, `1.950`, que est√° pr√≥xima de `1.579`, mas n√£o exatamente igual, devido √† aleatoriedade da amostragem. Se executarmos novamente, obteremos outro valor diferente. Isso demonstra a variabilidade das recompensas em torno do valor verdadeiro.
>
>  ```mermaid
>  graph LR
>      A[A√ß√£o A_t Escolhida] -->|q*(A_t) = 1.579| B(Distribui√ß√£o Normal N(1.579, 1));
>      B -->|Amostragem| C[Recompensa R_t = 1.950];
>  ```

**Lemma 1 (Propriedades da Distribui√ß√£o Normal):** Seja X uma vari√°vel aleat√≥ria seguindo uma distribui√ß√£o normal com m√©dia $\mu$ e vari√¢ncia $\sigma^2$, denotada como $X \sim \mathcal{N}(\mu, \sigma^2)$. Ent√£o:
  1. O valor esperado de $X$ √© $E[X] = \mu$.
  2. A vari√¢ncia de $X$ √© $Var[X] = \sigma^2$.
  3. A densidade de probabilidade de $X$ √© dada por $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$.

**Lemma 1.1 (Distribui√ß√£o da Recompensa):** Seja $R_t$ a recompensa obtida ao selecionar a a√ß√£o $A_t$ no instante *t*. Ent√£o $R_t$ segue uma distribui√ß√£o normal com m√©dia **q*(A_t)** e vari√¢ncia 1, isto √©, $R_t \sim \mathcal{N}(q^*(A_t), 1)$. Al√©m disso, a distribui√ß√£o de $R_t$ √© condicional √† escolha da a√ß√£o $A_t$, pois a m√©dia da distribui√ß√£o √© dada pelo valor verdadeiro dessa a√ß√£o, **q*(A_t)**.

*Proof Strategy:*  Este resultado segue diretamente da descri√ß√£o do *10-armed testbed*, onde as recompensas s√£o amostradas de uma distribui√ß√£o normal centrada no valor verdadeiro da a√ß√£o selecionada.

  ```mermaid
  graph LR
      A["A√ß√£o A_t"] --> B("q*(A_t)");
      B -->|M√©dia| C["Distribui√ß√£o de R_t: N(q*(A_t), 1)"];
      C -->|Amostra| D["Recompensa R_t"];
  ```

No contexto do *10-armed testbed*, a aplica√ß√£o do Lemma 1 √© direta. Ao gerar os valores **q*(a)** para cada a√ß√£o *a* num dado problema, estes s√£o retirados da distribui√ß√£o normal $\mathcal{N}(0, 1)$. Assim, o valor esperado de **q*(a)** √© 0 e a sua vari√¢ncia √© 1. Ao receber uma recompensa *R_t* para uma a√ß√£o *A_t*, esta √© obtida da distribui√ß√£o normal $\mathcal{N}(q^*(A_t), 1)$. Assim, o valor esperado de *R_t* √© **q*(A_t)** e a vari√¢ncia √© 1. Essas propriedades da distribui√ß√£o normal s√£o essenciais para entender como o *10-armed testbed* foi constru√≠do e como ele influencia o comportamento dos algoritmos de aprendizado por refor√ßo.

**Teorema 1 (Otimalidade em Esperan√ßa):** Em um dado problema do *10-armed testbed*, a a√ß√£o que maximiza o valor esperado da recompensa √© a a√ß√£o *a* com o maior valor **q*(a)**.

*Proof Strategy:* O valor esperado da recompensa ao selecionar a a√ß√£o *a* √© dado por $E[R_t \,|\, A_t = a] = $**q*(a)**. Portanto, a a√ß√£o que maximiza este valor esperado ser√° a a√ß√£o *a* que tem o maior valor **q*(a)**.

> üí° **Exemplo Num√©rico:** Retomando o exemplo anterior, a a√ß√£o com maior valor `q*(a)` gerado foi a a√ß√£o 7 com  `q*(7) = 1.579`. Pelo Teorema 1, essa a√ß√£o representa a a√ß√£o √≥tima em termos de valor esperado da recompensa para este problema espec√≠fico. Note que, na pr√°tica, o agente n√£o conhece os valores de `q*(a)` e, portanto, precisa estim√°-los atrav√©s de explora√ß√£o e explota√ß√£o.

  ```mermaid
  graph LR
      subgraph "A√ß√µes"
          A1["A√ß√£o 1"] --> B1("q*(1)");
          A2["A√ß√£o 2"] --> B2("q*(2)");
          ...
          A10["A√ß√£o 10"] --> B10("q*(10)");
      end
      B1 --> C("Compara√ß√£o de q*(a)");
      B2 --> C
      B10 --> C
      C --> D["A√ß√£o √ìtima a*: max q*(a)"]
  ```

O processo de gera√ß√£o dos valores **q*(a)** e a amostragem das recompensas podem ser visualizados na Figura 2.1 [4](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-4), onde √© mostrado um exemplo de um problema t√≠pico do *10-armed testbed*. As distribui√ß√µes em cinza representam as distribui√ß√µes normais das quais as recompensas *R_t* s√£o amostradas, centradas em torno de cada **q*(a)**, e ilustram a aleatoriedade das recompensas. √â importante lembrar que cada problema no *testbed* √© gerado de forma independente, garantindo a diversidade dos desafios enfrentados pelos algoritmos.

**Observa√ß√£o 1:** Embora o valor esperado da recompensa seja igual a **q*(a)**, cada recompensa individual *R_t* √© aleat√≥ria e segue uma distribui√ß√£o com desvio padr√£o igual a 1. Isto significa que, mesmo para a a√ß√£o √≥tima, o agente ir√° ocasionalmente receber recompensas menores que a de outras a√ß√µes com menor **q*(a)**.

### Conclus√£o
A cria√ß√£o dos valores de a√ß√£o, **q*(a)**, atrav√©s de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1 no *10-armed testbed* √© uma etapa crucial para simular um cen√°rio realista onde os valores verdadeiros das a√ß√µes s√£o desconhecidos. Essa abordagem garante que os algoritmos de aprendizado por refor√ßo tenham que lidar com incerteza e, consequentemente, equilibrarem de forma eficaz explora√ß√£o e explota√ß√£o. A gera√ß√£o aleat√≥ria dos valores de a√ß√£o, baseada na distribui√ß√£o normal, combinada com as recompensas aleat√≥rias que dependem dessas mesmas a√ß√µes, permite que se possa avaliar a efic√°cia de diferentes algoritmos em um ambiente controlado e bem definido.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Cap√≠tulo 2: Multi-armed Bandits)*
[^2]: "For each bandit problem, such as the one shown in Figure 2.1, the action values, q*(a), a = 1, ..., 10, were selected according to a normal (Gaussian) distribution with mean 0 and variance 1." *(Trecho de Cap√≠tulo 2: Multi-armed Bandits)*
[^3]: "Then, when a learning method applied to that problem selected action At at time step t, the actual reward, Rt, was selected from a normal distribution with mean q*(At) and variance 1." *(Trecho de Cap√≠tulo 2: Multi-armed Bandits)*
[^4]: "Figure 2.1: An example bandit problem from the 10-armed testbed. The true value q‚àó(a) of each of the ten actions was selected according to a normal distribution with mean zero and unit variance, and then the actual rewards were selected according to a mean q‚àó(a), unit-variance normal distribution, as suggested by these gray distributions." *(Trecho de Cap√≠tulo 2: Multi-armed Bandits)*
