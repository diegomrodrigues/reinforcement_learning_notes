### O Testbed de 10 Bra√ßos: Uma An√°lise Comparativa de M√©todos Gananciosos e Œµ-Gananciosos

### Introdu√ß√£o

Este cap√≠tulo explora o conceito de aprendizado por refor√ßo, focando no aspecto avaliativo e explorando o problema do bandido multi-armado (Multi-armed Bandit) em um ambiente n√£o associativo. O objetivo central √© entender como o feedback avaliativo, que indica a qualidade de uma a√ß√£o tomada sem necessariamente instruir sobre a melhor a√ß√£o, pode ser utilizado para o aprendizado [^1]. O problema do bandido k-armado √© apresentado como um modelo simplificado para estudar esses conceitos, onde um agente deve escolher repetidamente entre k op√ß√µes, recebendo recompensas num√©ricas probabil√≠sticas associadas a cada escolha [^1]. Este cap√≠tulo usa o problema do k-armed bandit para introduzir m√©todos b√°sicos de aprendizado que posteriormente ser√£o estendidos para problemas mais complexos. Uma das principais quest√µes abordadas √© o dilema entre explora√ß√£o e explota√ß√£o, ou seja, a decis√£o entre explorar novas a√ß√µes com potencial de recompensa ou explorar as a√ß√µes que j√° se mostraram promissoras [^1, ^2]. O objetivo final √© maximizar a recompensa total esperada ao longo do tempo [^2]. O presente cap√≠tulo aprofundar√° na compara√ß√£o de m√©todos para balancear explora√ß√£o e explota√ß√£o usando o *10-armed testbed*.

### Conceitos Fundamentais

O problema do bandido k-armado envolve um agente que deve escolher entre *k* a√ß√µes diferentes, onde cada a√ß√£o *a* tem um valor esperado $q^*(a)$ que representa a recompensa m√©dia obtida ao executar essa a√ß√£o [^2]. O objetivo do agente √© maximizar a recompensa total esperada ao longo do tempo. O desafio reside no fato de que o agente n√£o conhece $q^*(a)$ com certeza, tendo apenas estimativas $Q_t(a)$ que s√£o atualizadas com base nas recompensas recebidas. A explora√ß√£o refere-se √† escolha de a√ß√µes que podem n√£o parecer √≥timas no momento, mas podem levar √† descoberta de a√ß√µes com valores mais elevados no futuro. J√° a explota√ß√£o, √© a escolha de a√ß√µes com base no conhecimento atual para maximizar a recompensa imediata [^2].

Um m√©todo b√°sico para estimar $q^*(a)$ √© a m√©dia amostral, onde a estimativa $Q_t(a)$ √© dada pela m√©dia das recompensas recebidas para a a√ß√£o *a* at√© o instante *t* [^3]:

$$
Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$

onde $\mathbb{1}_{A_i=a}$ √© uma fun√ß√£o indicadora que retorna 1 se a a√ß√£o *a* foi selecionada no instante *i* e 0 caso contr√°rio. Caso o denominador seja 0, $Q_t(a)$ √© definido como um valor default, como 0 [^3].

> üí° **Exemplo Num√©rico:** Imagine que temos um bandido de 3 bra√ßos (k=3). Vamos focar no bra√ßo 1. Suponha que nas primeiras 5 tentativas, o bra√ßo 1 foi escolhido 3 vezes e gerou as recompensas: 0.5, 1.2 e 0.8. As outras duas tentativas foram para outros bra√ßos. Ent√£o, a estimativa $Q_5(1)$ seria calculada da seguinte forma:
>
>   $Q_5(1) = \frac{0.5 + 1.2 + 0.8}{3} = \frac{2.5}{3} \approx 0.83$
>
>   Esta estimativa de 0.83 seria usada para decis√µes futuras at√© que mais recompensas sejam obtidas para o bra√ßo 1. Se em *t=6*, o bra√ßo 1 √© escolhido novamente com recompensa 0.9, ent√£o:
>
>   $Q_6(1) = \frac{0.5 + 1.2 + 0.8 + 0.9}{4} = \frac{3.4}{4} = 0.85$
>
>   A estimativa √© atualizada com a nova recompensa.

**Observa√ß√£o 1:** A m√©dia amostral √© um estimador n√£o-viesado para o valor esperado $q^*(a)$. Isso significa que, se coletarmos um n√∫mero suficientemente grande de amostras, a m√©dia amostral $Q_t(a)$ convergir√° para $q^*(a)$. No entanto, em cen√°rios pr√°ticos, geralmente n√£o temos um n√∫mero infinito de amostras, e a estimativa $Q_t(a)$ √©, portanto, uma aproxima√ß√£o.

A *a√ß√£o gananciosa* √© aquela com a maior estimativa de valor $Q_t(a)$ [^2]. A escolha gananciosa sempre prioriza a explora√ß√£o, podendo levar a resultados sub√≥timos no longo prazo, caso as estimativas das demais a√ß√µes sejam inicialmente subestimadas [^3].

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, suponha que ap√≥s 10 tentativas, temos as seguintes estimativas para os 3 bra√ßos:  $Q_{10}(1) = 0.9$, $Q_{10}(2) = 0.5$ e $Q_{10}(3) = 0.7$. O m√©todo ganancioso escolheria o bra√ßo 1, pois tem a maior estimativa de valor. Se a recompensa subsequente do bra√ßo 1 for baixa (ex: 0.1), a estimativa de $Q_{11}(1)$ ir√° diminuir, mas ainda pode ser a maior se as recompensas dos outros bra√ßos tamb√©m forem baixas. No entanto, se o bra√ßo 2 ou 3 t√™m um valor esperado maior, mas ainda n√£o foram suficientemente explorados, o m√©todo ganancioso pode ficar preso no bra√ßo 1, perdendo uma recompensa maior no longo prazo.

```mermaid
graph LR
    A[ "In√≠cio" ] --> B{ "Calcular Q_t(a) para cada a√ß√£o" };
    B --> C{ "Selecionar a√ß√£o com maior Q_t(a)" };
    C --> D[ "Executar a√ß√£o" ];
    D --> E{ "Obter recompensa R_t" };
    E --> F{ "Atualizar Q_t(a)" };
    F --> B;
    F --> G[ "Fim" ];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

Um m√©todo mais sofisticado √© o *m√©todo Œµ-ganancioso*, no qual a maior parte do tempo uma a√ß√£o gananciosa √© selecionada, por√©m, com uma pequena probabilidade *Œµ*, uma a√ß√£o √© selecionada aleatoriamente [^3]. Este m√©todo garante que todas as a√ß√µes ser√£o exploradas ao longo do tempo e, portanto, as estimativas $Q_t(a)$ convergir√£o para os verdadeiros valores $q^*(a)$.

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo anterior, com $Q_{10}(1) = 0.9$, $Q_{10}(2) = 0.5$ e $Q_{10}(3) = 0.7$. Se usarmos um m√©todo Œµ-ganancioso com Œµ=0.1, em cada passo, haver√° 90% de chance de escolher a a√ß√£o gananciosa (bra√ßo 1) e 10% de chance de escolher um bra√ßo aleatoriamente (bra√ßo 1, 2 ou 3 com 10%/3 de chance cada). Isso permite explorar outros bra√ßos, mesmo que pare√ßam menos promissores no momento, dando-lhes a oportunidade de melhorar suas estimativas.

```mermaid
graph LR
    A[ "In√≠cio" ] --> B{ "Gerar n√∫mero aleat√≥rio 'p' entre 0 e 1" };
    B -- "p > Œµ" --> C{ "Selecionar a√ß√£o com maior Q_t(a) (A√ß√£o Gananciosa)" };
    B -- "p <= Œµ" --> D{ "Selecionar a√ß√£o aleatoriamente" };
    C --> E[ "Executar a√ß√£o selecionada" ];
    D --> E
    E --> F{ "Obter recompensa R_t" };
    F --> G{ "Atualizar Q_t(a)" };
    G --> A;
    G --> H[ "Fim" ];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style H fill:#ccf,stroke:#333,stroke-width:2px
```

**Lema 1:** *Sob condi√ß√µes estacion√°rias (isto √©, quando os valores esperados $q^*(a)$ n√£o mudam ao longo do tempo), o m√©todo Œµ-ganancioso garante que cada a√ß√£o ser√° selecionada infinitas vezes conforme t tende ao infinito, desde que Œµ > 0. Em outras palavras, todas as a√ß√µes ser√£o exploradas eventualmente.*

*Prova:* A probabilidade de selecionar qualquer a√ß√£o espec√≠fica *a* no tempo *t* com o m√©todo Œµ-ganancioso √© pelo menos $\frac{\epsilon}{k}$, onde *k* √© o n√∫mero total de a√ß√µes. Como *Œµ > 0* e *k* √© finito, esta probabilidade √© estritamente positiva. Assim, ao longo de um n√∫mero infinito de passos, qualquer a√ß√£o ser√° selecionada uma quantidade infinita de vezes.

**Lema 1.1:** *Sob as mesmas condi√ß√µes de Lema 1, as estimativas $Q_t(a)$ convergem para $q^*(a)$ conforme t tende ao infinito, para todo a.*

*Prova:* Como demonstrado no Lema 1, cada a√ß√£o *a* ser√° selecionada um n√∫mero infinito de vezes. A estimativa $Q_t(a)$ √© dada pela m√©dia amostral das recompensas obtidas quando a a√ß√£o *a* √© selecionada. Pela lei dos grandes n√∫meros, a m√©dia amostral converge para o valor esperado, portanto, $Q_t(a)$ converge para $q^*(a)$ quando t tende ao infinito.

**Corol√°rio 1:** O m√©todo $\epsilon$-ganancioso garante a converg√™ncia das estimativas para os verdadeiros valores das a√ß√µes, a longo prazo, o que implica que ele pode identificar a a√ß√£o √≥tima e, portanto, maximizar a recompensa total, desde que haja um tempo de explora√ß√£o suficiente.

### Compara√ß√£o de M√©todos Gananciosos e Œµ-Gananciosos no Testbed de 10 Bra√ßos

Para avaliar a efic√°cia dos m√©todos gananciosos e Œµ-gananciosos, foi conduzido um experimento utilizando um conjunto de problemas de bandidos k-armados chamado de *10-armed testbed* [^4]. Este testbed consiste em 2000 problemas de bandidos k-armados gerados aleatoriamente, onde *k = 10*. Para cada problema, o valor verdadeiro $q^*(a)$ de cada a√ß√£o √© sorteado de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1. Em seguida, as recompensas s√£o selecionadas a partir de uma distribui√ß√£o normal com m√©dia $q^*(a)$ e vari√¢ncia 1 [^4].

```mermaid
graph LR
subgraph "10-armed Testbed"
    A["Gerar 2000 problemas k-armados (k=10)"]
    B["Para cada a√ß√£o 'a' (1 a 10)"]
    C["Sortear q*(a) de N(0,1)"]
    D["Recompensa R_t ~ N(q*(a), 1)"]
    A --> B
    B --> C
    C --> D
end
```

> üí° **Exemplo Num√©rico:** Para um dos 2000 problemas do testbed, os valores verdadeiros $q^*(a)$ para cada um dos 10 bra√ßos podem ser algo como:
>
> | Bra√ßo (a) | q\*(a) |
> |-----------|--------|
> | 1         | -0.5   |
> | 2         | 0.3    |
> | 3         | 1.2    |
> | 4         | -0.1   |
> | 5         | 0.8    |
> | 6         | -1.1   |
> | 7         | 0.6    |
> | 8         | -0.2   |
> | 9         | 0.4    |
> | 10        | -0.9   |
>
> Note que o bra√ßo 3 tem o maior valor esperado, $q^*(3) = 1.2$, que seria o √≥timo para este problema.
> As recompensas obtidas em cada tentativa, como descrito, vir√£o de uma distribui√ß√£o normal com m√©dia $q^*(a)$ e vari√¢ncia 1, por exemplo se o bra√ßo 3 for selecionado, uma recompensa poderia ser 1.1, outra poderia ser 2.3, outra 0.5, sempre distribu√≠das em torno de 1.2.

O experimento compara o desempenho do m√©todo ganancioso, onde a a√ß√£o com maior valor estimado √© sempre selecionada, e dois m√©todos Œµ-gananciosos, com Œµ=0.01 e Œµ=0.1 [^5]. As estimativas $Q_t(a)$ s√£o atualizadas usando a m√©dia amostral, com um valor inicial de 0. Os resultados s√£o ent√£o avaliados ao longo de 1000 passos temporais por problema, repetido por 2000 rodadas [^5].

Os resultados mostram que o m√©todo ganancioso melhora mais rapidamente no in√≠cio, mas depois se estabiliza em um n√≠vel de recompensa inferior, alcan√ßando cerca de 1, enquanto o melhor desempenho poss√≠vel nesse testbed √© aproximadamente 1.54 [^5]. Isso ocorre porque o m√©todo ganancioso explora pouco e pode ficar preso em a√ß√µes sub√≥timas [^5].

> üí° **Exemplo Num√©rico:** Ap√≥s algumas rodadas de intera√ß√£o com um dos problemas, imagine que o m√©todo ganancioso acaba com as seguintes estimativas:
>
> | Bra√ßo (a) | Q_t(a) |
> |-----------|--------|
> | 1         | -0.4   |
> | 2         | 0.2    |
> | 3         | 0.7    |
> | 4         | -0.2   |
> | 5         | 0.6    |
> | 6         | -1.0   |
> | 7         | 0.5    |
> | 8         | -0.3   |
> | 9         | 0.3    |
> | 10        | -0.8   |
>
> O m√©todo ganancioso ficaria preso no bra√ßo 3 com a estimativa de 0.7, mesmo que o valor real do bra√ßo 3 seja 1.2 e do bra√ßo 5 seja 0.8 (como definido no exemplo num√©rico anterior).

Por outro lado, os m√©todos Œµ-gananciosos apresentam um desempenho inferior no in√≠cio, mas eventualmente superam o m√©todo ganancioso no longo prazo devido √† sua capacidade de explorar [^5]. O m√©todo com Œµ=0.1 explora mais e inicialmente atinge um desempenho melhor que o m√©todo com Œµ=0.01, mas a longo prazo, o m√©todo com Œµ=0.01 acaba por alcan√ßar um resultado melhor que o m√©todo com Œµ=0.1, com uma maior probabilidade de escolha das a√ß√µes √≥timas [^5, ^6]. √â importante notar que o equil√≠brio entre explora√ß√£o e explota√ß√£o √© fundamental para um bom desempenho em problemas de bandidos multi-armados.

> üí° **Exemplo Num√©rico:** O m√©todo Œµ-ganancioso com Œµ=0.1 exploraria o espa√ßo de a√ß√µes mais ativamente e, portanto, com o tempo, se aproximaria das verdadeiras m√©dias, por exemplo, com a seguinte estimativa ap√≥s v√°rias rodadas:
>
> | Bra√ßo (a) | Q_t(a) |
> |-----------|--------|
> | 1         | -0.52   |
> | 2         | 0.31    |
> | 3         | 1.18    |
> | 4         | -0.15   |
> | 5         | 0.79    |
> | 6         | -1.05   |
> | 7         | 0.61    |
> | 8         | -0.18   |
> | 9         | 0.42    |
> | 10        | -0.88   |
>
> A estimativa do bra√ßo 3, $Q_t(3)=1.18$ est√° muito pr√≥xima do valor real $q^*(3)=1.2$ e o m√©todo ir√° convergir para a escolha do bra√ßo 3. Em contraste, o m√©todo com Œµ=0.01 exploraria menos no in√≠cio, mas, eventualmente, tamb√©m se aproximaria da estimativa correta.

A figura 2.2 do texto original ilustra que o m√©todo ganancioso encontrou a a√ß√£o √≥tima em apenas um ter√ßo das rodadas. Nos outros dois ter√ßos, as amostras iniciais da a√ß√£o √≥tima foram desapontadoras, e ele nunca mais voltou a essa a√ß√£o [^6]. Os m√©todos Œµ-gananciosos tiveram um desempenho melhor porque continuaram a explorar e melhoraram as chances de reconhecer a a√ß√£o √≥tima [^6]. O m√©todo com Œµ=0.1 explorou mais e encontrou a a√ß√£o √≥tima mais cedo, mas nunca a selecionou em mais de 91% das vezes. J√° o m√©todo com Œµ=0.01 melhorou mais lentamente, mas eventualmente superaria o m√©todo Œµ=0.1 nas duas medidas de desempenho [^6]. √â poss√≠vel tamb√©m reduzir Œµ ao longo do tempo, para que se obtenha o melhor de ambos os mundos: a explora√ß√£o inicial e a explora√ß√£o posterior [^6].

```mermaid
graph LR
subgraph "Compara√ß√£o dos M√©todos"
    A[ "M√©todo Ganancioso" ]
    B[ "M√©todo Œµ-Ganancioso (Œµ=0.1)" ]
    C[ "M√©todo Œµ-Ganancioso (Œµ=0.01)" ]

    A -- "Melhora R√°pida Inicial" --> D("Estabiliza em Recompensa Menor (~1)")
    B -- "Explora Mais Inicialmente" --> E("Atinge Melhor Desempenho Inicial")
    C -- "Explora Menos Inicialmente" --> F("Melhora Lentamente")
    E --> G("Desempenho Inferior a C no Longo Prazo")
    F --> H("Supera B no Longo Prazo")
    H --> I("Maior Probabilidade de Escolher A√ß√µes √ìtimas")
end
```

**Teorema 1:** *Existe um valor √≥timo de Œµ, que equilibra a explora√ß√£o e a explota√ß√£o, para atingir o melhor desempenho no longo prazo em problemas de bandido k-armado. Este valor √≥timo depende das caracter√≠sticas do problema, como a vari√¢ncia das recompensas e o n√∫mero de a√ß√µes.*

*Prova:* A prova deste teorema √© emp√≠rica e n√£o anal√≠tica. Como visto no experimento do 10-armed testbed, valores muito altos ou muito baixos de Œµ levam a resultados sub√≥timos. Valores muito altos exploram demais, dificultando a converg√™ncia para a√ß√µes √≥timas. Valores muito baixos exploram pouco, correndo o risco de se fixar em solu√ß√µes sub√≥timas.  A exist√™ncia de um √≥timo √© sugerida pelos resultados experimentais e estudos de par√¢metro, onde o desempenho melhora com o aumento de *Œµ* at√© um certo ponto e depois piora. A determina√ß√£o exata do valor √≥timo de *Œµ* depende das caracter√≠sticas espec√≠ficas do problema e pode ser encontrada atrav√©s da avalia√ß√£o de diferentes par√¢metros, como visto na figura 2.6 mencionada no texto original.

**Proposi√ß√£o 1:** Uma estrat√©gia para melhorar o desempenho do m√©todo $\epsilon$-ganancioso √© usar um decaimento din√¢mico de $\epsilon$ ao longo do tempo, come√ßando com um valor mais alto para priorizar a explora√ß√£o inicial e reduzindo-o gradativamente para privilegiar a explota√ß√£o quando as estimativas de a√ß√£o se tornam mais precisas.

*Justificativa:* No in√≠cio do aprendizado, as estimativas das a√ß√µes s√£o bastante incertas e o agente precisa explorar para descobrir qual a√ß√£o tem o maior valor esperado. Conforme o tempo passa, as estimativas se tornam mais confi√°veis, e √© mais vantajoso escolher as a√ß√µes que parecem melhores. A estrat√©gia de decaimento de $\epsilon$ permite um equil√≠brio adaptativo entre explora√ß√£o e explota√ß√£o ao longo do tempo.

```mermaid
graph LR
subgraph "Estrat√©gia de Decaimento de Œµ"
    A[ "In√≠cio" ] --> B{ "Definir Œµ inicial alto" };
    B --> C{ "Priorizar Explora√ß√£o Inicial" };
    C --> D{ "Conforme o tempo passa" };
    D --> E{ "Diminuir Œµ gradualmente" };
    E --> F{ "Priorizar Explota√ß√£o com Estimativas Confi√°veis" };
    F --> G[ "Fim" ];
  end
```
### Conclus√£o

A an√°lise do *10-armed testbed* demonstra a import√¢ncia do equil√≠brio entre explora√ß√£o e explota√ß√£o no aprendizado por refor√ßo. M√©todos puramente gananciosos, embora inicialmente mais r√°pidos, tendem a ficar presos em solu√ß√µes sub√≥timas devido √† falta de explora√ß√£o. J√° os m√©todos Œµ-gananciosos garantem uma explora√ß√£o cont√≠nua e, em geral, apresentam resultados melhores no longo prazo. A escolha do valor de Œµ, entretanto, afeta o desempenho, e uma explora√ß√£o excessiva pode ser t√£o prejudicial quanto a falta dela. A figura 2.6 mostra um estudo de par√¢metro no qual o desempenho de cada algoritmo √© apresentado em fun√ß√£o de seu pr√≥prio par√¢metro (Œµ para os m√©todos Œµ-gananciosos). Os resultados mostram que o desempenho de todos os algoritmos √© melhor em um valor intermedi√°rio de seus par√¢metros, sem ser muito alto nem muito baixo. Portanto, √© essencial ajustar os par√¢metros de cada m√©todo para obter o melhor desempenho em cada aplica√ß√£o espec√≠fica.

### Refer√™ncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior." *(Trecho de Multi-armed Bandits)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. [...] If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates." *(Trecho de Multi-armed Bandits)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: [...] The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. [...] A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability …õ, instead select randomly" *(Trecho de Multi-armed Bandits)*
[^4]: "To roughly assess the relative effectiveness of the greedy and …õ-greedy action-value methods, we compared them numerically on a suite of test problems. This was a set of 2000 randomly generated k-armed bandit problems with k = 10. For each bandit problem, such as the one shown in Figure 2.1, the action values, q*(a), a = 1, ..., 10, were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. Then, when a learning method applied to that problem selected action At at time step t, the actual reward, Rt, was selected from a normal distribution with mean q*(At) and variance 1. These distributions are shown in gray in Figure 2.1. We call this suite of test tasks the 10-armed testbed." *(Trecho de Multi-armed Bandits)*
[^5]: "Figure 2.2 compares a greedy method with two …õ-greedy methods (Œµ=0.01 and Œµ=0.1), as described above, on the 10-armed testbed. All the methods formed their action-value estimates using the sample-average technique (with an initial estimate of 0). The upper graph shows the increase in expected reward with experience. The greedy method improved slightly faster than the other methods at the very beginning, but then leveled off at a lower level. It achieved a reward-per-step of only about 1, compared with the best possible of about 1.54 on this testbed. The greedy method performed significantly worse in the long run because it often got stuck performing suboptimal actions. The lower graph" *(Trecho de Multi-armed Bandits)*
[^6]: "shows that the greedy method found the optimal action in only approximately one-third of the tasks. In the other two-thirds, its initial samples of the optimal action were disappointing, and it never returned to it. The …õ-greedy methods eventually performed better because they continued to explore and to improve their chances of recognizing the optimal action. The …õ = 0.1 method explored more, and usually found the optimal action earlier, but it never selected that action more than 91% of the time. The …õ = 0.01 method improved more slowly, but eventually would perform better than the …õ = 0.1 method on both performance measures shown in the figure. It is also possible to reduce Œµ over time to try to get the best of both high and low values." *(Trecho de Multi-armed Bandits)*
