### An√°lise Detalhada da Vantagem de M√©todos Œµ-Gananciosos no Testbed de 10 Bra√ßos

### Introdu√ß√£o
O aprendizado por refor√ßo, diferentemente de outras abordagens de aprendizado, utiliza informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas em vez de instruir sobre as a√ß√µes corretas [^1]. Essa distin√ß√£o fundamental leva √† necessidade de **explora√ß√£o ativa**, um processo de busca expl√≠cita por comportamentos √≥timos [^1]. Este cap√≠tulo aborda o aprendizado por refor√ßo no contexto simplificado de *multi-armed bandits*, um problema onde a complexidade do aprendizado para agir em m√∫ltiplas situa√ß√µes √© evitada, focando na avalia√ß√£o das a√ß√µes tomadas [^1]. O **problema do *k-armed bandit*** √© introduzido como um exemplo para demonstrar diversos m√©todos de aprendizado que podem ser estendidos para o problema completo de aprendizado por refor√ßo [^1].

### Conceitos Fundamentais
No problema do *k-armed bandit*, o agente deve escolher repetidamente entre *k* op√ß√µes, recebendo uma recompensa num√©rica a cada escolha, amostrada a partir de uma distribui√ß√£o de probabilidade estacion√°ria dependente da a√ß√£o selecionada [^1]. O objetivo √© maximizar a recompensa total esperada ao longo do tempo [^2]. Cada a√ß√£o tem um **valor esperado** ou recompensa m√©dia associada, denotada por  $q_*(a)$, que √© o valor da a√ß√£o $a$ [^2]. A **estimativa do valor da a√ß√£o** $a$ no passo de tempo $t$ √© denotada por $Q_t(a)$ [^2]. Se os valores de cada a√ß√£o fossem conhecidos, o problema se tornaria trivial, com o agente sempre selecionando a a√ß√£o de maior valor [^2]. Contudo, em geral, os valores das a√ß√µes s√£o desconhecidos, requerendo que o agente explore o ambiente e use essa experi√™ncia para estimar os valores das a√ß√µes [^2].

**A√ß√µes gananciosas**, tamb√©m chamadas de *greedy actions*, s√£o aquelas com os maiores valores estimados, enquanto **a√ß√µes n√£o-gananciosas** s√£o as outras a√ß√µes [^2]. A **explora√ß√£o** permite melhorar as estimativas de valores de a√ß√µes n√£o-gananciosas, o que pode levar a recompensas maiores no longo prazo [^2]. Existe um conflito entre **explora√ß√£o** e **explota√ß√£o**, uma vez que escolher uma a√ß√£o gananciosa explora o conhecimento atual, enquanto escolher uma n√£o-gananciosa pode levar a descobrir a√ß√µes melhores [^2].

**M√©todos de valor de a√ß√£o** s√£o usados para estimar os valores das a√ß√µes e fazer decis√µes de sele√ß√£o de a√ß√µes. Um m√©todo natural √© a **m√©dia amostral**, dada por:

$$
Q_t(a) = \frac{\text{soma das recompensas quando a √© tomada antes de t}}{\text{n√∫mero de vezes que a foi tomada antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$

onde $\mathbb{1}_{\text{predicado}}$ √© uma vari√°vel aleat√≥ria que √© 1 se o predicado for verdadeiro e 0 caso contr√°rio [^3]. Quando o denominador for zero, $Q_t(a)$ √© definido como um valor padr√£o, como 0 [^3]. Pela lei dos grandes n√∫meros, $Q_t(a)$ converge para $q_*(a)$ conforme o denominador tende ao infinito [^3].

> üí° **Exemplo Num√©rico:** Suponha que temos um *k-armed bandit* com 3 bra√ßos. Vamos focar no bra√ßo 1 (a=1). No tempo t=1, ele √© selecionado e retorna uma recompensa de 2. No tempo t=2, o bra√ßo 2 √© selecionado. No tempo t=3, o bra√ßo 1 √© selecionado novamente e retorna uma recompensa de 4. No tempo t=4, o bra√ßo 3 √© selecionado. Agora, para o bra√ßo 1 (a=1) no tempo t=4, o valor estimado √©:
>
> $$Q_4(1) = \frac{2 + 4}{2} = 3$$
>
> Se em t=5, o bra√ßo 1 fosse selecionado novamente e retornasse uma recompensa de 5, a estimativa seria:
>
> $$Q_5(1) = \frac{2 + 4 + 5}{3} = 3.667$$
>
> Este exemplo ilustra como a m√©dia amostral √© calculada ao longo do tempo.

**Lema 1**
A m√©dia amostral $Q_t(a)$ √© um estimador n√£o viesado do valor esperado $q_*(a)$, dado que todas as recompensas s√£o amostradas de uma mesma distribui√ß√£o.
*Prova*: Isso segue da linearidade da esperan√ßa e do fato que cada recompensa √© uma amostra da mesma distribui√ß√£o, resultando em $E[Q_t(a)] = q_*(a)$

A **sele√ß√£o de a√ß√£o gananciosa** escolhe a a√ß√£o com maior valor estimado:

$$
A_t = \underset{a}{\text{argmax}} Q_t(a)
$$

Uma alternativa √© o **m√©todo Œµ-ganancioso**, que seleciona uma a√ß√£o gananciosa com probabilidade $1-\epsilon$ e uma a√ß√£o aleat√≥ria com probabilidade $\epsilon$ [^3]. M√©todos Œµ-gananciosos garantem que todas as a√ß√µes sejam amostradas infinitas vezes no limite, assegurando que os valores estimados convirjam para os valores reais, e que a probabilidade de escolher a a√ß√£o √≥tima convirja para um valor maior que $1-\epsilon$ [^4].

```mermaid
flowchart TD
    A[In√≠cio] --> B{"Gerar N√∫mero Aleat√≥rio 'rand' entre 0 e 1"};
    B --> C{"rand < Œµ ?"};
    C -- "Sim" --> D["Escolher A√ß√£o Aleat√≥ria"];
    C -- "N√£o" --> E["Escolher A√ß√£o Gananciosa"];
    D --> F[Fim];
    E --> F;
    style C fill:#f9f,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Suponha que temos 3 bra√ßos com as seguintes estimativas $Q_t(1) = 2$, $Q_t(2) = 4$, e $Q_t(3) = 3$. Usando o m√©todo Œµ-ganancioso com $\epsilon = 0.1$, escolhemos a a√ß√£o com maior valor estimado (bra√ßo 2) com probabilidade $1 - \epsilon = 0.9$. Com probabilidade $\epsilon = 0.1$, escolhemos um dos bra√ßos aleatoriamente, sendo cada bra√ßo escolhido com probabilidade $\frac{\epsilon}{k} = \frac{0.1}{3} \approx 0.033$. Portanto, a probabilidade de escolher o bra√ßo 2 √© de 0.9 + 0.033 = 0.933 (aproximadamente), enquanto cada um dos bra√ßos 1 e 3 tem probabilidade 0.033.

**Lema 1.1**
No m√©todo $\epsilon$-ganancioso, a probabilidade de escolher a a√ß√£o √≥tima no limite √© maior ou igual a $1 - \epsilon + \frac{\epsilon}{k}$.
*Prova:* Com probabilidade $1-\epsilon$, a a√ß√£o gananciosa √© escolhida. Se o valor estimado da a√ß√£o √≥tima for o maior, ent√£o a a√ß√£o √≥tima √© escolhida. Com probabilidade $\epsilon$, uma a√ß√£o √© escolhida aleatoriamente, e a probabilidade de se escolher a a√ß√£o √≥tima √© $\frac{1}{k}$. Assim, a probabilidade total √© $1-\epsilon + \frac{\epsilon}{k}$ quando a a√ß√£o gananciosa √© a a√ß√£o √≥tima, e $1 - \epsilon$ quando a a√ß√£o gananciosa n√£o √© a √≥tima. Portanto, no limite quando $Q_t(a)$ converge para $q_*(a)$, a probabilidade de escolher a a√ß√£o √≥tima √© pelo menos $1 - \epsilon + \frac{\epsilon}{k}$.

> üí° **Exemplo Num√©rico:**  Com $\epsilon = 0.1$ e $k = 3$, a probabilidade de escolher a a√ß√£o √≥tima √© pelo menos $1 - 0.1 + \frac{0.1}{3} \approx 0.933$.  Isso significa que mesmo com a explora√ß√£o, o m√©todo tende a escolher a a√ß√£o √≥tima com alta probabilidade no longo prazo. Se $\epsilon = 0$, a probabilidade seria 1 (se a estimativa da a√ß√£o √≥tima estiver correta) ou 0 (se n√£o estiver). Se $\epsilon = 1$, a probabilidade seria 1/3, a mesma de qualquer outra a√ß√£o. Isso demonstra o compromisso do Œµ-ganancioso.

### Vantagem Dependente da Tarefa: Varia√ß√£o na Recompensa

A **vantagem dos m√©todos Œµ-gananciosos sobre os m√©todos puramente gananciosos depende da natureza da tarefa** [^6]. Especificamente, quando a vari√¢ncia da recompensa √© alta, os m√©todos Œµ-gananciosos tendem a apresentar um melhor desempenho [^6]. Isso ocorre porque a alta varia√ß√£o na recompensa torna mais dif√≠cil identificar com precis√£o qual a√ß√£o √© √≥tima com base em amostras limitadas [^6]. Assim, a explora√ß√£o, incentivada pelos m√©todos Œµ-gananciosos, √© crucial para descobrir a a√ß√£o ideal em meio a essa incerteza [^6]. Por exemplo, se a vari√¢ncia da recompensa fosse 10 em vez de 1, os m√©todos Œµ-gananciosos se beneficiariam mais ao explorar o espa√ßo de a√ß√µes, em compara√ß√£o ao caso de vari√¢ncia 1, em que uma pequena amostragem das recompensas j√° revela a a√ß√£o √≥tima com precis√£o [^6].

> üí° **Exemplo Num√©rico:** Considere dois cen√°rios com dois bra√ßos. No cen√°rio 1, o bra√ßo 1 tem recompensa m√©dia 1 com vari√¢ncia 1, e o bra√ßo 2 tem recompensa m√©dia 2 com vari√¢ncia 1. No cen√°rio 2, o bra√ßo 1 tem recompensa m√©dia 1 com vari√¢ncia 10, e o bra√ßo 2 tem recompensa m√©dia 2 com vari√¢ncia 10. Um m√©todo ganancioso puro poderia rapidamente convergir para o bra√ßo 2 no cen√°rio 1, porque as amostras das recompensas ser√£o consistentes e o bra√ßo 2 sempre parecer√° melhor. No cen√°rio 2, a alta vari√¢ncia leva a grandes flutua√ß√µes nas recompensas, fazendo com que o m√©todo ganancioso escolha o bra√ßo 1 em v√°rias ocasi√µes devido ao ru√≠do, mesmo que o bra√ßo 2 seja o √≥timo. O m√©todo Œµ-ganancioso, com explora√ß√£o, ter√° uma chance maior de encontrar o bra√ßo 2 no cen√°rio 2.

**Proposi√ß√£o 2**
A diferen√ßa no desempenho entre m√©todos $\epsilon$-gananciosos e puramente gananciosos aumenta com a vari√¢ncia da recompensa, mantendo-se os demais par√¢metros constantes.
*Prova (Esbo√ßo):* Uma alta vari√¢ncia da recompensa significa que as estimativas $Q_t(a)$ s√£o mais ruidosas, o que torna mais dif√≠cil para um m√©todo ganancioso convergir para a a√ß√£o √≥tima rapidamente. M√©todos $\epsilon$-gananciosos, explorando outras a√ß√µes, mitigam os efeitos desse ru√≠do e acabam descobrindo a a√ß√£o √≥tima com maior probabilidade ao longo do tempo, compensando o custo inicial da explora√ß√£o.

```mermaid
graph LR
    A["Baixa Vari√¢ncia"] --> B("M√©todo Ganancioso: Converg√™ncia R√°pida para o √ìtimo");
    A --> C("M√©todo Œµ-Ganancioso: Explora√ß√£o Menos Necess√°ria");
    D["Alta Vari√¢ncia"] --> E("M√©todo Ganancioso: Converg√™ncia Lenta/Sub√≥tima");
    D --> F("M√©todo Œµ-Ganancioso: Explora√ß√£o Crucial para Encontrar o √ìtimo");
    B --> G("Bom Desempenho");
    C --> G;
    F --> H("Melhor Desempenho");
    E --> I("Desempenho Inferior");
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style A fill:#aaf,stroke:#333,stroke-width:2px
```

Em contraste, quando as recompensas t√™m vari√¢ncia zero, um m√©todo ganancioso puro poderia ter um desempenho melhor [^6]. Nessa situa√ß√£o, a explora√ß√£o se torna desnecess√°ria porque a recompensa de cada a√ß√£o √© imediatamente conhecida ap√≥s uma √∫nica tentativa. O m√©todo ganancioso rapidamente descobriria a a√ß√£o √≥tima e pararia de explorar, sem perder tempo com explora√ß√£o in√∫til. No entanto, mesmo em casos determin√≠sticos, a explora√ß√£o √© necess√°ria se o problema se tornar n√£o estacion√°rio, onde os verdadeiros valores das a√ß√µes mudam ao longo do tempo [^6].

> üí° **Exemplo Num√©rico:** Suponha que temos um cen√°rio onde cada bra√ßo de um *k-armed bandit* tem um valor fixo, sem nenhuma varia√ß√£o (vari√¢ncia 0). Por exemplo, bra√ßo 1 tem recompensa sempre 1, bra√ßo 2 tem recompensa sempre 2, e bra√ßo 3 tem recompensa sempre 0. O m√©todo puramente ganancioso selecionar√° o bra√ßo 2 ap√≥s a primeira itera√ß√£o (se o inicializar adequadamente). A explora√ß√£o seria in√∫til porque n√£o h√° necessidade de refinar as estimativas, pois n√£o h√° ru√≠do nas recompensas.

**Teorema 3**
Para um problema *k-armed bandit* com recompensas estacion√°rias e uma vari√¢ncia de recompensa $\sigma^2$, existe um valor √≥timo $\epsilon^*$ para o m√©todo $\epsilon$-ganancioso que maximiza a recompensa acumulada ao longo do tempo.
*Prova (Esbo√ßo):* Para um valor de $\epsilon$ muito pequeno, a explora√ß√£o √© insuficiente, e o agente pode ficar preso em a√ß√µes sub√≥timas. Para um valor de $\epsilon$ muito grande, a explora√ß√£o excessiva leva a uma perda de recompensa. Portanto, existe um valor intermedi√°rio $\epsilon^*$ que balanceia explora√ß√£o e explota√ß√£o de forma ideal, e esse valor depende da vari√¢ncia $\sigma^2$.

> üí° **Exemplo Num√©rico:**  Em um problema de *k-armed bandit* com 10 bra√ßos, com recompensas com vari√¢ncia $\sigma^2 = 1$, um valor de $\epsilon^* = 0.1$ pode ser razo√°vel. Isso significa que 10% do tempo √© gasto explorando. Se a vari√¢ncia fosse maior, digamos $\sigma^2 = 10$, ent√£o $\epsilon^*$ poderia ser maior, talvez 0.2 ou 0.3, para incentivar mais explora√ß√£o. O valor exato de $\epsilon^*$ √© tipicamente encontrado atrav√©s de experimenta√ß√£o, pois depende da distribui√ß√£o das recompensas e de outros fatores espec√≠ficos do problema.

No cen√°rio n√£o-estacion√°rio, um m√©todo puramente ganancioso n√£o consegue se adaptar √†s mudan√ßas nos valores das a√ß√µes, uma vez que ele sempre escolhe a a√ß√£o que tem a maior estimativa atual [^6]. Por outro lado, os m√©todos Œµ-gananciosos continuam a explorar mesmo quando a tarefa muda, sendo capazes de se ajustar √†s novas din√¢micas do ambiente. A capacidade de equilibrar explora√ß√£o e explota√ß√£o √© um aspecto central do aprendizado por refor√ßo [^6].

> üí° **Exemplo Num√©rico:** Suponha que, em um dado momento, as recompensas do bra√ßo 2 se tornem subitamente muito piores, e que o bra√ßo 3 se torne o √≥timo. Um agente ganancioso que escolhe sempre o bra√ßo 2 n√£o perceber√° a mudan√ßa. Mas um agente Œµ-ganancioso com $\epsilon$ n√£o nulo eventualmente explorar√° o bra√ßo 3 e perceber√° que este se tornou o √≥timo. No entanto, se a mudan√ßa for muito r√°pida e $\epsilon$ for muito pequeno, o agente ainda demorar√° para se ajustar √† nova din√¢mica.

**Corol√°rio 3.1**
Em um cen√°rio n√£o-estacion√°rio, o valor √≥timo $\epsilon^*$ para o m√©todo $\epsilon$-ganancioso pode mudar ao longo do tempo, dependendo do qu√£o r√°pido e drasticamente os valores das a√ß√µes mudam.
*Prova (Esbo√ßo):* Se a din√¢mica do problema muda rapidamente, o agente necessita de mais explora√ß√£o para acompanhar essas mudan√ßas. Isso implica que $\epsilon^*$ dever√° ser maior do que no cen√°rio estacion√°rio.

> üí° **Exemplo Num√©rico:** Imagine um problema onde, a cada 100 passos, os valores √≥timos das a√ß√µes mudam aleatoriamente. Inicialmente, um $\epsilon^* = 0.1$ pode ser bom. Mas, ap√≥s 100 passos, um $\epsilon^* = 0.3$ pode ser necess√°rio para que o agente acompanhe a nova din√¢mica. Ajustar o valor de $\epsilon$ ao longo do tempo √© uma estrat√©gia avan√ßada que pode melhorar o desempenho em ambientes n√£o-estacion√°rios.

### Conclus√£o
Em resumo, a vantagem dos m√©todos Œµ-gananciosos sobre os m√©todos gananciosos √© intrinsecamente ligada √† complexidade e incerteza do problema do *k-armed bandit*. A vari√¢ncia das recompensas √© um fator cr√≠tico que determina qual abordagem ser√° mais eficaz [^6]. Enquanto em situa√ß√µes de baixa varia√ß√£o, onde a explora√ß√£o se torna menos essencial, m√©todos puramente gananciosos podem ser suficientes. Entretanto, em cen√°rios de alta varia√ß√£o, como os encontrados em problemas n√£o-estacion√°rios, a explora√ß√£o controlada pelos m√©todos Œµ-gananciosos demonstra sua efic√°cia em garantir que o agente n√£o fique preso em a√ß√µes sub√≥timas e, consequentemente, alcance um melhor desempenho no longo prazo.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
[^4]: "An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the Qt(a) converge to their respective q*(a)." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
[^6]: "The advantage of …õ-greedy over greedy methods depends on the task. For example, suppose the reward variance had been larger, say 10 instead of 1." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
