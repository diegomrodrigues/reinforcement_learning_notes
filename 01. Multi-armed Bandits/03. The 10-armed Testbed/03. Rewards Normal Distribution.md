## O Testbed de 10 Bra√ßos: Recompensas Gaussianas

### Introdu√ß√£o
Este cap√≠tulo explora o problema de *Multi-armed Bandits*, focando em m√©todos de aprendizado que equilibram explora√ß√£o e explota√ß√£o. Um aspecto crucial para a avalia√ß√£o desses m√©todos √© a cria√ß√£o de um ambiente de teste que permita compara√ß√µes significativas. Nesse contexto, o **testbed de 10 bra√ßos** surge como uma ferramenta fundamental, permitindo analisar o comportamento de diferentes algoritmos em cen√°rios controlados [^1]. O foco desta se√ß√£o √© detalhar a gera√ß√£o e o uso das recompensas dentro desse testbed, especificamente como elas s√£o amostradas de distribui√ß√µes normais, e como isso impacta na avalia√ß√£o dos m√©todos de aprendizado.

### Conceitos Fundamentais

O testbed de 10 bra√ßos √© um ambiente de teste composto por *k* = 10 a√ß√µes diferentes, onde cada a√ß√£o corresponde a um bra√ßo de uma m√°quina ca√ßa-n√≠queis [^1]. O objetivo do aprendiz √© maximizar a recompensa total esperada ao longo do tempo, explorando as diferentes a√ß√µes e identificando aquelas que produzem os melhores resultados. As recompensas associadas a cada a√ß√£o n√£o s√£o conhecidas inicialmente, e o aprendiz deve estim√°-las com base nas observa√ß√µes de recompensas passadas [^1].

Para tornar o problema mais concreto e desafiador, as recompensas s√£o geradas aleatoriamente, de acordo com uma distribui√ß√£o de probabilidade estacion√°ria. Dentro do contexto do testbed de 10 bra√ßos, especificamente, o valor verdadeiro de cada a√ß√£o, denotado por **q*(a)**, √© selecionado a partir de uma distribui√ß√£o normal (Gaussiana) com m√©dia 0 e vari√¢ncia 1 [^2]. Esse passo inicial estabelece as qualidades intr√≠nsecas de cada a√ß√£o no ambiente de teste. Posteriormente, a cada vez que uma a√ß√£o *a* √© selecionada em um passo de tempo *t*, uma recompensa *Rt* √© obtida. Esta recompensa *Rt* n√£o √© igual a **q*(a)**, mas sim amostrada de uma distribui√ß√£o normal com m√©dia **q*(a)** e vari√¢ncia 1 [^2].

**Defini√ß√£o formal:**

1.  **Valores das a√ß√µes:** Para cada a√ß√£o *a* no conjunto de 10 a√ß√µes (a = 1, 2, ..., 10), o valor verdadeiro **q*(a)** √© amostrado de:
$$ q^*(a) \sim \mathcal{N}(0, 1) $$
onde $\mathcal{N}(0, 1)$ representa uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1.

2.  **Recompensas:** Quando a a√ß√£o *a* √© selecionada em um passo de tempo *t*, a recompensa *Rt* √© amostrada de:
$$ R_t \sim \mathcal{N}(q^*(a), 1) $$
onde $\mathcal{N}(q^*(a), 1)$ representa uma distribui√ß√£o normal com m√©dia **q*(a)** e vari√¢ncia 1.
```mermaid
graph LR
    subgraph "Gera√ß√£o dos Valores das A√ß√µes"
        A["A√ß√µes 'a' = 1, 2, ..., 10"]
        B["Distribui√ß√£o Normal 'N(0, 1)'"]
        C["Valor Verdadeiro 'q*(a)'"]
        A --> B
        B --> C
     end
     subgraph "Gera√ß√£o das Recompensas"
        D["A√ß√£o Selecionada 'a' no Tempo 't'"]
        E["Distribui√ß√£o Normal 'N(q*(a), 1)'"]
        F["Recompensa 'Rt'"]
        D --> E
        E --> F
     end
     C --> E
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

Esta formula√ß√£o garante que as recompensas recebidas por uma a√ß√£o selecionada flutuem em torno do valor m√©dio dessa a√ß√£o, com a vari√¢ncia definida como 1, introduzindo uma componente de estocasticidade no processo de aprendizado [^2]. A vari√¢ncia unit√°ria representa um n√≠vel de ru√≠do moderado, tornando o problema de aprendizado desafiador.

> üí° **Exemplo Num√©rico:** Vamos gerar valores para algumas a√ß√µes usando Python.
> ```python
> import numpy as np
>
> # Semente para reprodutibilidade
> np.random.seed(42)
>
> # Amostrando os valores verdadeiros das a√ß√µes q*(a)
> q_star = np.random.normal(0, 1, 10)
>
> print("Valores verdadeiros das a√ß√µes q*(a):", q_star)
> ```
> Isso pode gerar valores como `[-0.138, -0.720, -0.040, 1.657, -0.381, 0.343, 0.721, -0.162, 1.059, -0.930]`. Note que esses valores s√£o amostrados aleatoriamente e cada vez que o c√≥digo √© executado com uma semente diferente, os resultados variam. O valor da a√ß√£o 4, por exemplo, √© 1.657, que √© relativamente alto, enquanto a a√ß√£o 10 tem um valor baixo de -0.930.
>
> Agora, vamos simular a obten√ß√£o de algumas recompensas para uma a√ß√£o espec√≠fica, por exemplo, a a√ß√£o 4, cujo valor verdadeiro √© q*(4) = 1.657.
> ```python
> # Simula recompensas para a a√ß√£o 4
> rewards = np.random.normal(q_star[3], 1, 5)
> print("Recompensas para a a√ß√£o 4:", rewards)
> ```
> As recompensas obtidas podem ser algo como `[1.463, 1.598, 2.458, 1.045, 2.266]`. Observe que cada recompensa √© diferente e flutua em torno do valor de q*(4) = 1.657 devido ao ru√≠do (vari√¢ncia 1) introduzido. Essas flutua√ß√µes tornam o problema mais desafiador, pois o algoritmo n√£o recebe diretamente o valor verdadeiro da a√ß√£o, mas sim amostras ruidosas.

#### Import√¢ncia da Distribui√ß√£o Normal

A escolha da distribui√ß√£o normal para gerar tanto os valores das a√ß√µes quanto as recompensas √© significativa por v√°rias raz√µes. Distribui√ß√µes normais s√£o comuns em modelagem estat√≠stica e frequentemente aparecem em fen√¥menos do mundo real. Al√©m disso, elas possuem a propriedade de estarem bem definidas por seus dois primeiros momentos (m√©dia e vari√¢ncia), o que facilita an√°lises matem√°ticas e compara√ß√µes. A vari√¢ncia unit√°ria garante que a dispers√£o das recompensas ao redor dos valores das a√ß√µes seja compar√°vel entre as diferentes a√ß√µes, permitindo uma avalia√ß√£o justa do desempenho dos algoritmos.

#### Lemma 1
*A amostra da recompensa $R_t$ tem uma m√©dia de $q^*(A_t)$ e vari√¢ncia 1.*

**Prova:**

Pela defini√ß√£o, a recompensa $R_t$ √© amostrada da distribui√ß√£o normal $\mathcal{N}(q^*(A_t), 1)$. A m√©dia de uma distribui√ß√£o normal $\mathcal{N}(\mu, \sigma^2)$ √© $\mu$, e a vari√¢ncia √© $\sigma^2$. Portanto, a m√©dia de $R_t$ √© $q^*(A_t)$ e sua vari√¢ncia √© 1.
$\blacksquare$

#### Corol√°rio 1
*A recompensa m√©dia esperada da a√ß√£o $A_t$, denotada por $E[R_t|A_t]$,  √© igual a $q^*(A_t)$.*

**Prova:**
Como demonstrado no Lemma 1, a m√©dia da recompensa $R_t$ dada a a√ß√£o $A_t$ √© $q^*(A_t)$. Portanto, a recompensa m√©dia esperada da a√ß√£o $A_t$ √© igual ao valor da a√ß√£o.
$\blacksquare$
```mermaid
graph LR
    A["Valor Verdadeiro da A√ß√£o 'q*(A_t)'"]
    B["Recompensa Amostrada 'R_t' ~ N(q*(A_t), 1)"]
    C["M√©dia Esperada 'E[R_t|A_t]'"]
    A --> B
    B --> C
    C -- "E[R_t|A_t] = q*(A_t)" --> A
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px

```

> üí° **Exemplo Num√©rico:**  Continuando com o exemplo num√©rico anterior, onde o valor verdadeiro da a√ß√£o 4 √© q*(4) = 1.657.  O Corol√°rio 1 nos diz que a m√©dia esperada das recompensas para a a√ß√£o 4, ou seja,  $E[R_t | A_t = 4]$, √© igual a 1.657. No exemplo anterior, algumas recompensas amostradas foram [1.463, 1.598, 2.458, 1.045, 2.266]. Se amostr√°ssemos um n√∫mero muito grande de recompensas para a a√ß√£o 4, a m√©dia dessas recompensas se aproximaria de 1.657.  Este √© um exemplo pr√°tico de como a recompensa m√©dia esperada corresponde ao valor da a√ß√£o, embora as amostras individuais flutuem ao redor desse valor.

**Proposi√ß√£o 1**
*As recompensas $R_t$ s√£o independentes dado o valor de $q^*(A_t)$.*

**Prova:**
As recompensas $R_t$ s√£o amostradas independentemente a partir da distribui√ß√£o $\mathcal{N}(q^*(A_t), 1)$ para cada passo de tempo *t*. Como a distribui√ß√£o de $R_t$ depende somente de $q^*(A_t)$, e n√£o de recompensas anteriores, e cada amostra √© independente, podemos concluir que as recompensas $R_t$ s√£o independentes dado $q^*(A_t)$.
$\blacksquare$
```mermaid
sequenceDiagram
    participant A√ß√£o_a
    participant Distribui√ß√£o_Normal
    participant Recompensa_Rt_1
    participant Recompensa_Rt_2
    
    A√ß√£o_a ->> Distribui√ß√£o_Normal:  "q*(A_t)"
    Distribui√ß√£o_Normal ->> Recompensa_Rt_1: "Amostra Rt_1 ~ N(q*(A_t), 1)"
    A√ß√£o_a ->> Distribui√ß√£o_Normal: "q*(A_t)"
    Distribui√ß√£o_Normal ->> Recompensa_Rt_2: "Amostra Rt_2 ~ N(q*(A_t), 1)"
    
    Note over Recompensa_Rt_1,Recompensa_Rt_2: "Rt_1 e Rt_2 s√£o independentes dados q*(A_t)"
```

> üí° **Exemplo Num√©rico:** Se em dois passos de tempo consecutivos, a a√ß√£o 4 for selecionada, as recompensas $R_{t1}$ e $R_{t2}$ ser√£o amostradas independentemente da mesma distribui√ß√£o normal $\mathcal{N}(1.657, 1)$.  Por exemplo,  $R_{t1}$ poderia ser 1.463 e  $R_{t2}$ poderia ser 2.266.  A Proposi√ß√£o 1 nos diz que o valor de $R_{t1}$ n√£o influencia o valor que $R_{t2}$ pode assumir. Cada recompensa √© uma amostra independente da mesma distribui√ß√£o.

**Lema 1.1**
*A vari√¢ncia da recompensa $R_t$, dado o valor $q^*(A_t)$,  √© $Var[R_t | A_t] = 1$.*

**Prova:**
Pela defini√ß√£o, $R_t$ √© amostrado de $\mathcal{N}(q^*(A_t), 1)$. A vari√¢ncia de uma distribui√ß√£o normal $\mathcal{N}(\mu, \sigma^2)$ √© $\sigma^2$. Portanto, $Var[R_t | A_t] = 1$.
$\blacksquare$
```mermaid
graph LR
    A["Distribui√ß√£o Normal 'N(q*(A_t), 1)'"]
    B["Recompensa Amostrada 'R_t'"]
    C["Vari√¢ncia 'Var[R_t | A_t]'"]
    A --> B
    B --> C
    C -- "Var[R_t | A_t] = 1" --> A
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**  Retomando a a√ß√£o 4 com q*(4) = 1.657, o Lema 1.1 nos diz que, independentemente do valor de $q^*(4)$, a vari√¢ncia da recompensa quando essa a√ß√£o √© selecionada √© sempre 1. Isso significa que as recompensas amostradas para a a√ß√£o 4 flutuar√£o em torno de 1.657, com uma dispers√£o que corresponde a uma vari√¢ncia de 1. Isto tamb√©m ocorre com todas as outras a√ß√µes. Ou seja, a dispers√£o das recompensas ao redor do valor verdadeiro √© sempre a mesma, independentemente da a√ß√£o.

**Observa√ß√£o 1:**
√â importante ressaltar que apesar de $E[R_t|A_t] = q^*(A_t)$, o valor observado de $R_t$ pode ser diferente de $q^*(A_t)$ em cada passo de tempo devido √† vari√¢ncia unit√°ria. Esta flutua√ß√£o estoc√°stica √© fundamental para o desafio do problema do multi-armed bandit, for√ßando os algoritmos a aprender os valores reais das a√ß√µes com base em dados ruidosos.

Essa abordagem assegura que cada a√ß√£o tenha um valor verdadeiro que o aprendiz deve descobrir por meio da explora√ß√£o, ao mesmo tempo em que as recompensas reais s√£o sujeitas a flutua√ß√µes aleat√≥rias, simulando um ambiente de aprendizado realista.

### Conclus√£o
A forma como as recompensas s√£o selecionadas no testbed de 10 bra√ßos, atrav√©s de distribui√ß√µes normais com m√©dia q*(a) e vari√¢ncia 1, desempenha um papel fundamental na an√°lise e compara√ß√£o de diferentes m√©todos de aprendizado por refor√ßo. A estocasticidade introduzida pelas amostras de recompensas garante um ambiente de aprendizado realista, permitindo que os algoritmos equilibrem a explora√ß√£o de novas a√ß√µes com a explota√ß√£o das a√ß√µes conhecidas que geram melhores recompensas. A utiliza√ß√£o da distribui√ß√£o normal facilita a an√°lise matem√°tica e estat√≠stica dos resultados, garantindo a validade e comparabilidade das avalia√ß√µes.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions."
[^2]: "The true value q*(a) of each of the ten actions was selected according to a normal distribution with mean zero and unit variance, and then the actual rewards were selected according to a mean q*(a), unit-variance normal distribution, as suggested by these gray distributions."
