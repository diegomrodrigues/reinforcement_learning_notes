## O Problema do Bandido de k BraÃ§os: A ConvergÃªncia SubÃ³tima do MÃ©todo Ganancioso

### IntroduÃ§Ã£o
O aprendizado por reforÃ§o (reinforcement learning) se distingue de outros tipos de aprendizado por utilizar informaÃ§Ãµes de treinamento que avaliam as aÃ§Ãµes tomadas, em vez de instruir por meio de aÃ§Ãµes corretas. Essa caracterÃ­stica cria a necessidade de exploraÃ§Ã£o ativa, uma busca explÃ­cita por um bom comportamento [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). No contexto do **problema do bandido de *k* braÃ§os**, a exploraÃ§Ã£o Ã© crucial, especialmente para algoritmos que inicialmente favorecem aÃ§Ãµes jÃ¡ conhecidas, como o mÃ©todo ganancioso (*greedy*). Este capÃ­tulo explora como mÃ©todos de aprendizado lidam com o balanÃ§o entre exploraÃ§Ã£o e aproveitamento (*exploitation*) e analisa a performance de diferentes mÃ©todos no *10-armed testbed*, com Ãªnfase na convergÃªncia subÃ³tima do mÃ©todo ganancioso [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1), [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

### Conceitos Fundamentais
O **problema do bandido de *k* braÃ§os** Ã© uma estrutura de aprendizado em que um agente deve escolher repetidamente entre *k* opÃ§Ãµes, ou aÃ§Ãµes. Cada aÃ§Ã£o resulta em uma recompensa numÃ©rica, extraÃ­da de uma distribuiÃ§Ã£o de probabilidade estacionÃ¡ria especÃ­fica da aÃ§Ã£o escolhida. O objetivo do agente Ã© maximizar a recompensa total esperada ao longo de um perÃ­odo de tempo [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1), [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). O valor de uma aÃ§Ã£o *a*, denotado por $q_*(a)$, Ã© a recompensa esperada quando essa aÃ§Ã£o Ã© selecionada. No entanto, esses valores nÃ£o sÃ£o conhecidos a priori, e o agente deve estimÃ¡-los. A estimativa do valor de uma aÃ§Ã£o *a* no passo *t* Ã© denotada por $Q_t(a)$ [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).
```mermaid
graph LR
    A("Agente") --> B("AÃ§Ãµes (k braÃ§os)");
    B --> C("Recompensa R_t");
    C --> A;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
```
A escolha entre exploraÃ§Ã£o e aproveitamento Ã© central neste problema. AÃ§Ãµes que maximizam a recompensa imediata com base no conhecimento atual sÃ£o chamadas **aÃ§Ãµes gananciosas**. Escolher uma aÃ§Ã£o gananciosa significa *aproveitar* o conhecimento disponÃ­vel. Por outro lado, a **exploraÃ§Ã£o** envolve selecionar aÃ§Ãµes nÃ£o-gananciosas para melhorar a estimativa dos valores das aÃ§Ãµes, o que pode levar a recompensas maiores no futuro [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). O mÃ©todo ganancioso, que sempre seleciona a aÃ§Ã£o com maior valor estimado, explora pouco ou nada, o que pode levar a um desempenho subÃ³timo.

**ProposiÃ§Ã£o 1** *O problema do bandido de k braÃ§os pode ser formalizado como um processo de decisÃ£o de Markov (MDP) com um Ãºnico estado*.

*Prova:* Um MDP Ã© definido por um conjunto de estados, aÃ§Ãµes, probabilidades de transiÃ§Ã£o e recompensas. No problema do bandido de *k* braÃ§os, o agente estÃ¡ sempre na mesma situaÃ§Ã£o (o mesmo estado), com *k* aÃ§Ãµes disponÃ­veis. As recompensas dependem apenas da aÃ§Ã£o escolhida e nÃ£o do estado anterior, e nÃ£o hÃ¡ transiÃ§Ã£o entre estados. Assim, podemos representar o problema como um MDP com um Ãºnico estado.

O *10-armed testbed* consiste em uma sÃ©rie de 2000 problemas de bandidos de *k* braÃ§os gerados aleatoriamente, cada um com *k* = 10 aÃ§Ãµes. Para cada problema, os valores das aÃ§Ãµes $q_*(a)$ sÃ£o selecionados de uma distribuiÃ§Ã£o normal com mÃ©dia 0 e variÃ¢ncia 1. Ao selecionar uma aÃ§Ã£o $A_t$ no passo $t$, a recompensa $R_t$ Ã© extraÃ­da de uma distribuiÃ§Ã£o normal com mÃ©dia $q_*(A_t)$ e variÃ¢ncia 1 [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3), [5](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-5). Este ambiente permite comparar diferentes algoritmos de aprendizado de forma controlada.

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine um problema de bandido de 3 braÃ§os. As recompensas verdadeiras ($q_*(a)$) para cada braÃ§o sÃ£o: $q_*(1) = 1$, $q_*(2) = 2$, e $q_*(3) = 0.5$. O agente nÃ£o conhece esses valores inicialmente e deve estimÃ¡-los. Ao escolher o braÃ§o 1 pela primeira vez, ele pode receber uma recompensa $R_1 = 0.8$ (extraÃ­da de uma distribuiÃ§Ã£o normal com mÃ©dia 1 e variÃ¢ncia 1). Ao escolher o braÃ§o 2 pela primeira vez, ele recebe $R_2 = 2.5$. Ao escolher o braÃ§o 3 pela primeira vez, ele recebe $R_3 = -0.1$. Essas recompensas iniciais sÃ£o aleatÃ³rias e podem nÃ£o refletir os valores esperados verdadeiros.

**Lema 1** *A recompensa mÃ©dia obtida por uma aÃ§Ã£o $a$ ao longo de um nÃºmero $n$ de tentativas, denotada por $\bar{R}_n(a)$, converge para o valor verdadeiro $q_*(a)$ quando $n$ tende a infinito, i.e., $\lim_{n\to\infty} \bar{R}_n(a) = q_*(a)$*.

*Prova:* Este Ã© um resultado direto da lei forte dos grandes nÃºmeros. A mÃ©dia amostral de uma sequÃªncia de variÃ¡veis aleatÃ³rias independentes e identicamente distribuÃ­das converge para a sua esperanÃ§a matemÃ¡tica quando o nÃºmero de amostras tende ao infinito. No contexto do problema do bandido, as recompensas de cada aÃ§Ã£o sÃ£o retiradas de uma distribuiÃ§Ã£o estacionÃ¡ria com mÃ©dia $q_*(a)$, portanto, a mÃ©dia amostral das recompensas converge para $q_*(a)$.
```mermaid
graph LR
    A["Recompensa $R_i$ (AÃ§Ã£o a)"] --> B["MÃ©dia Amostral $\\bar{R}_n(a)$"];
    B --> C("Valor Verdadeiro $q_*(a)$");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
    linkStyle 0,1 stroke:#333,stroke-width:2px
    B -- "n -> âˆž" --> C
```
O *sample-average method* Ã© um mÃ©todo de estimaÃ§Ã£o que calcula a mÃ©dia das recompensas recebidas para cada aÃ§Ã£o:

$$
Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$

onde $\mathbb{1}_{A_i=a}$ Ã© uma funÃ§Ã£o indicadora que retorna 1 se a aÃ§Ã£o *a* foi selecionada no passo *i* e 0 caso contrÃ¡rio. Se o denominador for zero, $Q_t(a)$ Ã© definido como um valor padrÃ£o, como 0 [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). A lei dos grandes nÃºmeros garante que $Q_t(a)$ converge para $q_*(a)$ quando o nÃºmero de amostras tende ao infinito.
```mermaid
graph LR
    A["AÃ§Ã£o a"] --> B["Recompensas $R_i$"];
    B --> C["Soma das Recompensas $\\sum R_i \\mathbb{1}_{A_i=a}$"];
    A --> D["Contador $\\mathbb{1}_{A_i=a}$"];
    D --> E["Soma dos Contadores $\\sum \\mathbb{1}_{A_i=a}$"];
    C --> F["EstimaÃ§Ã£o $Q_t(a)$"];
    E --> F;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#cfc,stroke:#333,stroke-width:2px
     linkStyle 0,1,2,3,4,5 stroke:#333,stroke-width:2px
```
> ðŸ’¡ **Exemplo NumÃ©rico:** Continuando o exemplo anterior, apÃ³s 5 iteraÃ§Ãµes, suponha que as seguintes aÃ§Ãµes foram tomadas e recompensas obtidas:
>
> | IteraÃ§Ã£o (i) | AÃ§Ã£o (A_i) | Recompensa (R_i) |
> |--------------|------------|------------------|
> | 1            | 1          | 0.8              |
> | 2            | 2          | 2.5              |
> | 3            | 3          | -0.1             |
> | 4            | 1          | 1.2              |
> | 5            | 2          | 1.8              |
>
> Usando o sample-average method, calculamos $Q_5(1)$, $Q_5(2)$, e $Q_5(3)$:
>
>  - $Q_5(1) = \frac{0.8 + 1.2}{2} = 1.0$
>  - $Q_5(2) = \frac{2.5 + 1.8}{2} = 2.15$
>  - $Q_5(3) = \frac{-0.1}{1} = -0.1$
>
> Note que essas estimativas estÃ£o comeÃ§ando a se aproximar dos valores verdadeiros, mas ainda podem ser bem diferentes.

**Lema 1.1** *O mÃ©todo sample-average Ã© uma estimativa nÃ£o-viesada de $q_*(a)$*.

*Prova:* Seja $N_t(a)$ o nÃºmero de vezes que a aÃ§Ã£o $a$ foi selecionada atÃ© o tempo $t$. EntÃ£o, $Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}$. Assumindo que $N_t(a) > 0$, a esperanÃ§a de $Q_t(a)$ Ã©:
$E[Q_t(a)] = E\left[\frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}\right] = \frac{1}{N_t(a)} \sum_{i=1}^{t-1} E[R_i \mathbb{1}_{A_i=a}] = \frac{1}{N_t(a)} N_t(a) q_*(a) = q_*(a)$. Portanto, o mÃ©todo sample-average Ã© nÃ£o viesado.

### A ConvergÃªncia SubÃ³tima do MÃ©todo Ganancioso
O mÃ©todo ganancioso seleciona sempre a aÃ§Ã£o com maior valor estimado:

$$
A_t = \underset{a}{\operatorname{argmax}} Q_t(a)
$$

Este mÃ©todo maximiza a recompensa imediata, mas nÃ£o explora outras aÃ§Ãµes que poderiam ter valores maiores. No *10-armed testbed*, o mÃ©todo ganancioso converge rapidamente para um desempenho subÃ³timo, como demonstrado na Figura 2.2 [5](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-5). Embora o mÃ©todo ganancioso melhore ligeiramente mais rÃ¡pido no inÃ­cio, ele acaba se estabilizando em um nÃ­vel de recompensa inferior. Este comportamento acontece porque o mÃ©todo ganancioso frequentemente fica preso em aÃ§Ãµes subÃ³timas [5](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-5). Em cerca de dois terÃ§os dos casos, as amostras iniciais da aÃ§Ã£o Ã³tima sÃ£o decepcionantes, e o algoritmo nunca mais a explora [6](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-6). O mÃ©todo ganancioso encontrou a aÃ§Ã£o Ã³tima em aproximadamente um terÃ§o dos testes [6](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-6).
```mermaid
graph LR
  subgraph "MÃ©todo Ganancioso"
    A["EstimaÃ§Ãµes Q_t(a)"] --> B{"argmax"};
    B --> C["AÃ§Ã£o A_t"];
  end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
     linkStyle 0,1 stroke:#333,stroke-width:2px
```

*   **ExploraÃ§Ã£o Insuficiente:** A principal razÃ£o para a convergÃªncia subÃ³tima do mÃ©todo ganancioso Ã© a sua falta de exploraÃ§Ã£o. Ele nÃ£o aloca tempo para amostrar outras aÃ§Ãµes que poderiam ter recompensas maiores a longo prazo. Ele se apega Ã  primeira aÃ§Ã£o aparentemente boa que encontra, mesmo que haja aÃ§Ãµes melhores.
*   **DependÃªncia das Amostras Iniciais:** O mÃ©todo ganancioso Ã© particularmente sensÃ­vel Ã s amostras iniciais de recompensas. Se uma aÃ§Ã£o subÃ³tima produz recompensas aparentemente altas no inÃ­cio, o algoritmo pode se concentrar nela e ignorar aÃ§Ãµes melhores, mas ainda nÃ£o exploradas.
```mermaid
graph LR
    A["AÃ§Ãµes"] --> B("AÃ§Ã£o 1 (SubÃ³tima)")
    A --> C("AÃ§Ã£o 2 (Ã“tima)")
    B -- "Recompensas Iniciais Altas" --> D["MÃ©todo Ganancioso Focado em AÃ§Ã£o 1"];
    C -- "Recompensas Iniciais Baixas" --> E["AÃ§Ã£o 2 NÃ£o Explorada"];
    D --> F["ConvergÃªncia SubÃ³tima"];
     linkStyle 0,1,2,3 stroke:#333,stroke-width:2px
     style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
      style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#cfc,stroke:#333,stroke-width:2px
      style F fill:#ccf,stroke:#333,stroke-width:2px
```
> ðŸ’¡ **Exemplo NumÃ©rico:** Voltando ao exemplo com 3 braÃ§os, apÃ³s as 5 primeiras iteraÃ§Ãµes, o mÃ©todo ganancioso selecionaria a aÃ§Ã£o 2 no passo 6 porque $Q_5(2) = 2.15$ Ã© o maior valor estimado atÃ© agora. Se a recompensa obtida nesse passo fosse $R_6 = 1.5$, $Q_6(2)$ seria atualizado para $\frac{2.5+1.8+1.5}{3}=1.93$. O mÃ©todo ganancioso continuaria a escolher o braÃ§o 2 (a menos que outra aÃ§Ã£o tivesse uma mÃ©dia maior), mesmo que o braÃ§o 1, com um valor verdadeiro de 1, seja na verdade melhor a longo prazo, porque ele foi menos explorado e suas recompensas iniciais foram menores.
>
> Considere que as verdadeiras recompensas mÃ©dias dos braÃ§os sÃ£o $q_*(1)=1$, $q_*(2)=2$, e $q_*(3)=0.5$. No entanto, o algoritmo ganancioso, por azar, obteve inicialmente recompensas $R_1=0.2$, $R_2=2.5$, e $R_3=0.1$. O algoritmo ganancioso escolherÃ¡ o braÃ§o 2 a partir de agora, mesmo que o braÃ§o 1 seja uma melhor opÃ§Ã£o em mÃ©dia, porque ele nÃ£o explora mais as opÃ§Ãµes. Isto Ã© a convergÃªncia subÃ³tima em aÃ§Ã£o.

**Lema 2** *O mÃ©todo ganancioso, no limite, nÃ£o garante a convergÃªncia para a aÃ§Ã£o Ã³tima no problema do bandido de k braÃ§os*.

*Prova:* Como mencionado no texto, o mÃ©todo ganancioso converge para aÃ§Ãµes subÃ³timas, em aproximadamente dois terÃ§os dos casos, devido Ã  falta de exploraÃ§Ã£o. Isso demonstra que, mesmo quando o nÃºmero de interaÃ§Ãµes tende ao infinito, o mÃ©todo ganancioso nÃ£o garante convergir para a aÃ§Ã£o com maior valor esperado.

**Teorema 1** *No 10-armed testbed, o mÃ©todo ganancioso tem probabilidade menor que um de convergir para a aÃ§Ã£o Ã³tima*.

*Prova:* Este resultado Ã© diretamente observado nos resultados empÃ­ricos do 10-armed testbed. O texto menciona que o mÃ©todo ganancioso encontrou a aÃ§Ã£o Ã³tima em aproximadamente um terÃ§o dos testes. Dado que o mÃ©todo nÃ£o explora adequadamente e se fixa nas amostras iniciais, a probabilidade de encontrar a aÃ§Ã£o Ã³tima Ã© estritamente menor que 1.

### ConclusÃ£o

O mÃ©todo ganancioso, embora simples de implementar, Ã© inadequado para problemas de aprendizado por reforÃ§o que exigem um equilÃ­brio entre exploraÃ§Ã£o e aproveitamento. Sua tendÃªncia a se concentrar nas aÃ§Ãµes que parecem melhores no inÃ­cio o leva a um desempenho subÃ³timo. O *10-armed testbed* ilustra claramente essa limitaÃ§Ã£o, mostrando que outros mÃ©todos que exploram mais, como os mÃ©todos *$\epsilon$-greedy*, podem atingir um desempenho superior a longo prazo. A necessidade de um balanÃ§o adequado entre exploraÃ§Ã£o e aproveitamento Ã© um desafio central em *reinforcement learning*, e Ã© fundamental usar mÃ©todos que nÃ£o se fixem em escolhas iniciais subÃ³timas.
```mermaid
graph LR
    A["MÃ©todo Ganancioso"] --> B["ExploraÃ§Ã£o Insuficiente"];
    B --> C["ConvergÃªncia SubÃ³tima"];
    D["MÃ©todos $\\epsilon$-Greedy"] --> E["ExploraÃ§Ã£o Melhorada"];
    E --> F["Desempenho Superior"];
    A --> G["Aproveitamento Exclusivo"];
     linkStyle 0,1,2,3,4,5 stroke:#333,stroke-width:2px
     style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
      style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#cfc,stroke:#333,stroke-width:2px
      style F fill:#ccf,stroke:#333,stroke-width:2px
       style G fill:#f9f,stroke:#333,stroke-width:2px
```
### ReferÃªncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Chapter 2 - Multi-armed Bandits)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted qâˆ—(a), is the expected reward given that a is selected: q*(a) = E[Rt | At=a]." *(Trecho de Chapter 2 - Multi-armed Bandits)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: Qt(a) = sum of rewards when a taken prior to t / number of times a taken prior to t = Î£tâˆ’1i=1 Ri1Ai=a / Î£tâˆ’1i=1 1Ai=a where 1predicate denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define Qt(a) as some default value, such as 0." *(Trecho de Chapter 2 - Multi-armed Bandits)*
[^4]: "The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as At = argmaxa Qt(a)." *(Trecho de Chapter 2 - Multi-armed Bandits)*
[^5]: "Figure 2.2 compares a greedy method with two É›-greedy methods (Îµ=0.01 and Îµ=0.1), as described above, on the 10-armed testbed. All the methods formed their action-value estimates using the sample-average technique (with an initial estimate of 0). The upper graph shows the increase in expected reward with experience. The greedy method improved slightly faster than the other methods at the very beginning, but then leveled off at a lower level. It achieved a reward-per-step of only about 1, compared with the best possible of about 1.54 on this testbed. The greedy method performed significantly worse in the long run because it often got stuck performing suboptimal actions. The lower graph shows that the greedy method found the optimal action in only approximately one-third of the tasks." *(Trecho de Chapter 2 - Multi-armed Bandits)*
[^6]: "In the other two-thirds, its initial samples of the optimal action were disappointing, and it never returned to it. The É›-greedy methods eventually performed better because they continued to explore and to improve their chances of recognizing the optimal action." *(Trecho de Chapter 2 - Multi-armed Bandits)*
