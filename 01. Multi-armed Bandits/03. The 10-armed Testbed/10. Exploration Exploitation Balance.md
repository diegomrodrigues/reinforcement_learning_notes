## Explorando o Equil√≠brio entre Explora√ß√£o e Explota√ß√£o em Problemas de Bandit Multi-armado

### Introdu√ß√£o

O aprendizado por refor√ßo se distingue de outras formas de aprendizado por utilizar informa√ß√µes de treinamento que *avaliam* as a√ß√µes tomadas, em vez de instruir a√ß√µes corretas [^1]. Essa caracter√≠stica fundamental introduz a necessidade de **explora√ß√£o ativa**, ou seja, uma busca expl√≠cita por comportamentos eficazes [^1]. Enquanto o feedback puramente avaliativo indica o qu√£o boa foi a a√ß√£o tomada, sem especificar se foi a melhor poss√≠vel, o feedback instrutivo aponta a a√ß√£o correta independentemente da a√ß√£o realizada [^1]. Este cap√≠tulo aborda o aspecto avaliativo do aprendizado por refor√ßo em um cen√°rio simplificado, o problema do **bandit multi-armado**, onde o agente aprende a agir em uma √∫nica situa√ß√£o [^1]. O estudo desse problema nos permite entender melhor como o feedback avaliativo difere e pode ser combinado com o feedback instrutivo [^1]. Uma vers√£o espec√≠fica deste problema, que exploramos, √© o **problema do k-armed bandit**, que introduz m√©todos b√°sicos de aprendizado que ser√£o estendidos em cap√≠tulos posteriores para o problema completo de aprendizado por refor√ßo [^1]. Ao final, discutimos o caso onde o problema do bandit se torna associativo, ou seja, quando a melhor a√ß√£o depende da situa√ß√£o [^1].

### Conceitos Fundamentais

No problema do **k-armed bandit**, o agente √© confrontado repetidamente com uma escolha entre *k* op√ß√µes diferentes, ou a√ß√µes [^1]. Ap√≥s cada escolha, o agente recebe uma recompensa num√©rica, selecionada de uma distribui√ß√£o de probabilidade estacion√°ria dependente da a√ß√£o escolhida [^1]. O objetivo √© maximizar a recompensa total esperada em um determinado per√≠odo, como em 1000 passos de tempo [^2]. Analogamente a uma slot machine com *k* alavancas, cada sele√ß√£o de a√ß√£o corresponde a puxar uma alavanca, e as recompensas s√£o os pagamentos por atingir o jackpot [^2]. Alternativamente, pode-se imaginar um m√©dico escolhendo entre tratamentos experimentais para uma s√©rie de pacientes graves, onde cada a√ß√£o √© a sele√ß√£o de um tratamento e a recompensa √© a sobreviv√™ncia ou bem-estar do paciente [^2]. Em nosso problema do k-armed bandit, cada a√ß√£o possui um valor esperado ou recompensa m√©dia, dada que essa a√ß√£o foi selecionada, que denotamos como $q_*(a)$ para uma a√ß√£o *a* [^2].

**Defini√ß√£o da a√ß√£o selecionada e recompensa:** Seja $A_t$ a a√ß√£o selecionada no instante *t* e $R_t$ a recompensa correspondente. O valor de uma a√ß√£o *a* √© dado por:

$$ q_*(a) = E[R_t | A_t = a] $$ [^2]
> üí° **Exemplo Num√©rico:** Suponha que temos um problema de 3-armado (k=3). As recompensas esperadas para cada a√ß√£o s√£o: $q_*(1) = 2$, $q_*(2) = 5$, e $q_*(3) = 1$. Isso significa que, em m√©dia, a a√ß√£o 1 retorna 2, a a√ß√£o 2 retorna 5, e a a√ß√£o 3 retorna 1. Se conhec√™ssemos esses valores, escolher√≠amos sempre a a√ß√£o 2 para maximizar a recompensa.

Se os valores de todas as a√ß√µes fossem conhecidos, a solu√ß√£o para o problema seria trivial: selecionar sempre a a√ß√£o com maior valor [^2]. Contudo, assumimos que os valores das a√ß√µes s√£o desconhecidos, embora possam ser estimados [^2]. Denotamos a estimativa do valor de a√ß√£o *a* no instante *t* como $Q_t(a)$ e desejamos que $Q_t(a)$ seja pr√≥xima de $q_*(a)$ [^2]. A manuten√ß√£o das estimativas dos valores das a√ß√µes define **a√ß√µes gulosas** como aquelas com a maior estimativa de valor. A sele√ß√£o de uma dessas a√ß√µes √© denominada **explota√ß√£o**, pois aproveita o conhecimento atual dos valores das a√ß√µes. A sele√ß√£o de uma a√ß√£o n√£o gulosa √© chamada de **explora√ß√£o**, permitindo que se melhore a estimativa do valor da a√ß√£o [^2]. Embora a explota√ß√£o maximize a recompensa esperada em um √∫nico passo, a explora√ß√£o pode resultar em uma recompensa total maior a longo prazo [^2]. O equil√≠brio entre explora√ß√£o e explota√ß√£o √© crucial e depende das estimativas, incertezas e n√∫mero de passos restantes [^2].
```mermaid
graph LR
    A("A√ß√µes ('a')") --> B("Sele√ß√£o de A√ß√£o (A_t)");
    B --> C{"Obter Recompensa (R_t)"};
    C --> D("Estimar Valor da A√ß√£o (Q_t(a))");
    D --> E{"Explora√ß√£o ou Explota√ß√£o"};
    E -- "Explora√ß√£o" --> F("A√ß√£o Aleat√≥ria");
    E -- "Explota√ß√£o" --> G("A√ß√£o com Maior Q_t(a)");
    F --> B;
    G --> B;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#cfc,stroke:#333,stroke-width:2px
```
**Lema 1: Converg√™ncia da M√©dia da Amostra**

A m√©dia da amostra das recompensas para uma a√ß√£o *a*, $Q_t(a)$, converge para o verdadeiro valor $q_*(a)$ quando o n√∫mero de vezes que *a* √© selecionado tende ao infinito.

*Prova:*
A estimativa do valor da a√ß√£o *a* no instante *t* √© dada por
$$Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}},$$
onde $\mathbb{1}_{A_i=a}$ √© 1 se a a√ß√£o $A_i$ for igual a *a* e 0 caso contr√°rio. Pela lei dos grandes n√∫meros, quando o n√∫mero de vezes que a a√ß√£o *a* √© selecionada tende ao infinito, a m√©dia das recompensas $Q_t(a)$ converge para o valor esperado $q_*(a)$ . $\blacksquare$

> üí° **Exemplo Num√©rico:**  Considere a a√ß√£o 1 do exemplo anterior, com $q_*(1) = 2$. Suponha que, nos primeiros 5 passos, escolhemos a a√ß√£o 1 tr√™s vezes e recebemos as seguintes recompensas: 1, 3, 2. A estimativa do valor da a√ß√£o 1 no passo 6 seria:
> $Q_6(1) = \frac{1+3+2}{3} = \frac{6}{3} = 2$.
>  Se continuarmos a selecionar a a√ß√£o 1, a m√©dia amostral $Q_t(1)$ se aproximar√° cada vez mais de 2, que √© o verdadeiro valor $q_*(1)$.

Essa converg√™ncia garante que, ao longo do tempo, nossas estimativas se tornar√£o mais precisas.

**Lema 1.1: Limite Superior da Varia√ß√£o da M√©dia da Amostra**

A vari√¢ncia da m√©dia amostral $Q_t(a)$ diminui √† medida que o n√∫mero de amostras de *a* aumenta. Mais precisamente, se as recompensas $R_i$ s√£o independentes e t√™m vari√¢ncia limitada por $\sigma^2$, ent√£o a vari√¢ncia de $Q_t(a)$ √© limitada por $\frac{\sigma^2}{N_t(a)}$, onde $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o instante *t*.
```mermaid
graph LR
    A("Recompensas 'R_i' (Independentes)") --> B("Vari√¢ncia(R_i) <= 'sigma^2'");
    B --> C("N√∫mero de vezes que 'a' foi selecionada (N_t(a))");
    C --> D("Vari√¢ncia(Q_t(a)) <= 'sigma^2' / N_t(a)");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```
*Prova:*
Seja $N_t(a) = \sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}$. Ent√£o, podemos reescrever $Q_t(a)$ como $Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}$.  Assumindo que as recompensas s√£o independentes, temos que
$$ Var(Q_t(a)) = Var \left( \frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a} \right) = \frac{1}{N_t(a)^2} Var\left( \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a} \right). $$
Dado que as recompensas $R_i$ s√£o independentes e com vari√¢ncia limitada por $\sigma^2$, temos
$$  Var\left( \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a} \right) = \sum_{i=1}^{t-1} Var(R_i \mathbb{1}_{A_i=a}) \leq \sum_{i=1}^{t-1} \sigma^2 \mathbb{1}_{A_i=a} = \sigma^2 N_t(a). $$
Portanto,
$$ Var(Q_t(a)) \leq \frac{1}{N_t(a)^2} \sigma^2 N_t(a) = \frac{\sigma^2}{N_t(a)}. $$
Este resultado mostra que a incerteza sobre o valor da a√ß√£o *a* diminui com o aumento de amostras. $\blacksquare$
> üí° **Exemplo Num√©rico:**  Assumindo que a vari√¢ncia das recompensas para a a√ß√£o 1 seja $\sigma^2 = 1$, e que no exemplo anterior $N_6(1) = 3$, a vari√¢ncia da m√©dia amostral da a√ß√£o 1 no passo 6 √© limitada por:
> $Var(Q_6(1)) \leq \frac{1}{3} \approx 0.33$.
> Se, ap√≥s mais algumas itera√ß√µes, o n√∫mero de vezes que a a√ß√£o 1 foi selecionada for $N_{10}(1) = 10$, a vari√¢ncia de $Q_{10}(1)$ ser√° menor:
> $Var(Q_{10}(1)) \leq \frac{1}{10} = 0.1$.
> Isso ilustra que a incerteza em torno da estimativa $Q_t(a)$ diminui com o aumento do n√∫mero de amostras.

Este resultado complementa o Lema 1, demonstrando n√£o apenas a converg√™ncia da m√©dia, mas tamb√©m a redu√ß√£o da sua variabilidade com o tempo.

**A√ß√µes $\epsilon$-Gulosas**
Uma estrat√©gia simples para equilibrar explora√ß√£o e explota√ß√£o √© selecionar a√ß√µes gulosas na maior parte do tempo, mas com uma pequena probabilidade $\epsilon$ selecionar uma a√ß√£o aleat√≥ria [^3]. Este m√©todo √© chamado de $\epsilon$-guloso e assegura que todas as a√ß√µes sejam amostradas um n√∫mero infinito de vezes no limite, garantindo que $Q_t(a)$ convirja para $q_*(a)$. Contudo, tais garantias s√£o assint√≥ticas e dizem pouco sobre a efic√°cia pr√°tica dos m√©todos [^3].

> üí° **Exemplo Num√©rico:**  Imagine que $\epsilon = 0.1$.  A cada passo, o algoritmo $\epsilon$-guloso decide se vai explorar ou explotar. Com probabilidade 0.9, ele escolhe a a√ß√£o com maior valor estimado no momento. Com probabilidade 0.1, ele escolhe uma a√ß√£o aleat√≥ria entre as 3 a√ß√µes dispon√≠veis. Isso garante que, mesmo que uma a√ß√£o inicialmente pare√ßa ruim, ela ainda ter√° a chance de ser explorada e seu valor ser reavaliado.
```mermaid
graph LR
    A("Estimativa de Valor Q_t(a)") --> B("Probabilidade 'epsilon'");
    B -- "1 - epsilon" --> C("Escolher A√ß√£o Gulosa");
    B -- "epsilon" --> D("Escolher A√ß√£o Aleat√≥ria");
    C --> E("Obter Recompensa");
    D --> E
     style A fill:#f9f,stroke:#333,stroke-width:2px
     style C fill:#ccf,stroke:#333,stroke-width:2px
      style D fill:#ccf,stroke:#333,stroke-width:2px
```
**Teorema 1: Converg√™ncia Assint√≥tica do $\epsilon$-Guloso**

Em um problema *k*-armado estacion√°rio, o m√©todo $\epsilon$-guloso garante que todas as a√ß√µes s√£o amostradas infinitamente, e as estimativas de valor $Q_t(a)$ convergem para o verdadeiro valor $q_*(a)$ com probabilidade 1 para todas as a√ß√µes.

*Prova:*
O m√©todo $\epsilon$-guloso garante que cada a√ß√£o √© escolhida com probabilidade pelo menos $\frac{\epsilon}{k}$, onde k √© o n√∫mero de a√ß√µes. Portanto, se considerarmos um n√∫mero infinito de passos, cada a√ß√£o ser√° selecionada um n√∫mero infinito de vezes. Pelo Lema 1, a m√©dia das recompensas $Q_t(a)$ converge para o valor esperado $q_*(a)$ quando o n√∫mero de vezes que a a√ß√£o *a* √© selecionada tende ao infinito. Assim, com probabilidade 1, $Q_t(a) \to q_*(a)$ para todas as a√ß√µes *a* quando $t \to \infty$.  $\blacksquare$

Este teorema formaliza a garantia de converg√™ncia do m√©todo $\epsilon$-guloso, demonstrando que, mesmo com uma pequena probabilidade de explora√ß√£o, o algoritmo eventualmente aprende os verdadeiros valores das a√ß√µes.

**Proposi√ß√£o 1: Sensibilidade a $\epsilon$**
A performance do m√©todo $\epsilon$-guloso √© sens√≠vel ao valor de $\epsilon$. Valores altos de $\epsilon$ promovem explora√ß√£o excessiva e podem levar a um aprendizado mais lento, enquanto valores baixos podem levar a uma explora√ß√£o insuficiente e √† converg√™ncia para uma a√ß√£o sub√≥tima.

*Prova:*
Valores elevados de $\epsilon$ levam a mais a√ß√µes aleat√≥rias, o que significa que o agente passa mais tempo explorando e menos tempo explorando o que j√° aprendeu. Isso pode levar a uma taxa de aprendizado mais lenta, pois o agente n√£o est√° se concentrando tanto em a√ß√µes que j√° mostraram ter boas recompensas. Por outro lado, valores pequenos de $\epsilon$ fazem com que o agente explore muito pouco e se apegue a a√ß√µes com estimativas altas no in√≠cio, mesmo que haja outras a√ß√µes melhores que ainda n√£o foram descobertas. A escolha de $\epsilon$ deve ser um equil√≠brio entre explora√ß√£o e explota√ß√£o, e o valor √≥timo depende do problema espec√≠fico. $\blacksquare$

> üí° **Exemplo Num√©rico:**  Comparando dois cen√°rios com diferentes valores de $\epsilon$, imagine que temos um problema de 10-armado.  
> - **Cen√°rio 1: $\epsilon = 0.3$** O agente explorar√° muito, selecionando uma a√ß√£o aleat√≥ria em 30% das vezes. Isso pode atrasar a converg√™ncia para a a√ß√£o √≥tima no in√≠cio, mas evita que ele fique preso em uma a√ß√£o sub√≥tima.
> - **Cen√°rio 2: $\epsilon = 0.01$** O agente explorar√° muito pouco, apenas em 1% das vezes. Isso o levar√° a explorar uma a√ß√£o que pare√ßa boa inicialmente, mas pode n√£o ser a melhor a longo prazo.
> A performance do algoritmo $\epsilon$-guloso ser√° sens√≠vel a essa escolha.

Esta proposi√ß√£o destaca a import√¢ncia da escolha do hiperpar√¢metro $\epsilon$ e a necessidade de ajustes para diferentes problemas.
```mermaid
graph LR
    subgraph "Valores Altos de 'epsilon'"
    A("epsilon Alto") --> B("Explora√ß√£o Excessiva");
        B --> C("Aprendizado Lento");
    end
   subgraph "Valores Baixos de 'epsilon'"
     D("epsilon Baixo")  --> E("Explora√ß√£o Insuficiente");
        E --> F("Converg√™ncia Sub√≥tima");
    end
     style A fill:#f9f,stroke:#333,stroke-width:2px
     style D fill:#f9f,stroke:#333,stroke-width:2px
     style C fill:#ccf,stroke:#333,stroke-width:2px
     style F fill:#ccf,stroke:#333,stroke-width:2px
```
### O Testbed de 10-armados

Para avaliar a efic√°cia dos m√©todos gulosos e $\epsilon$-gulosos, √© utilizado um conjunto de problemas de teste, o **10-armed testbed** [^3]. Este conjunto consiste em 2000 problemas k-armed bandit gerados aleatoriamente, com *k* = 10 [^3]. Para cada problema, os valores das a√ß√µes, $q_*(a)$, s√£o selecionados de uma distribui√ß√£o normal (gaussiana) com m√©dia 0 e vari√¢ncia 1 [^3]. Quando um m√©todo de aprendizado seleciona uma a√ß√£o $A_t$, a recompensa real $R_t$ √© selecionada de uma distribui√ß√£o normal com m√©dia $q_*(A_t)$ e vari√¢ncia 1 [^3]. Este procedimento define um "teste", onde o aprendizado do m√©todo pode ser medido ao longo de 1000 passos de tempo [^3]. A repeti√ß√£o deste processo por 2000 execu√ß√µes independentes, cada um com um problema de bandit diferente, permite obter medidas do comportamento m√©dio do algoritmo de aprendizado [^3]. Os resultados mostram que os m√©todos $\epsilon$-gulosos eventualmente superam os m√©todos gulosos, pois continuam a explorar e reconhecer a a√ß√£o √≥tima [^6]. A escolha do valor $\epsilon$ influencia a taxa de explora√ß√£o, com valores maiores levando a uma explora√ß√£o mais r√°pida e valores menores, a uma explora√ß√£o mais lenta [^6].
```mermaid
graph LR
    A("Problema de Teste (10-armado)") --> B("2000 problemas k-armed bandit (k=10)");
    B --> C("q_*(a) ~ N(0, 1)");
    C --> D("Sele√ß√£o de A√ß√£o A_t");
    D --> E("R_t ~ N(q_*(A_t), 1)");
    E --> F("Avaliar Desempenho em 1000 Passos");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```
> üí° **Exemplo Num√©rico:**  No testbed de 10-armados, vamos simular a escolha de a√ß√µes com m√©todos gulosos e $\epsilon$-gulosos. Suponha que rodamos o experimento por 1000 passos e repetimos 2000 vezes (2000 problemas diferentes de bandit). Abaixo um exemplo do que poder√≠amos observar em termos de recompensas m√©dias por passo de tempo:
>
> | Passo de Tempo | Recompensa M√©dia (Guloso) | Recompensa M√©dia ($\epsilon$-Guloso com $\epsilon=0.1$) |
> |---------------|---------------------------|--------------------------------------------------|
> | 10            | 0.5                       | 0.6                                              |
> | 100           | 0.8                       | 1.0                                              |
> | 500           | 1.0                       | 1.2                                              |
> | 1000          | 1.1                       | 1.3                                              |
>
>  Neste exemplo, observamos que no in√≠cio o m√©todo guloso obt√©m recompensas parecidas, mas conforme o tempo passa, o m√©todo $\epsilon$-guloso com $\epsilon=0.1$ apresenta melhores resultados. Isso acontece porque o m√©todo guloso explora as a√ß√µes que parecem promissoras no in√≠cio, enquanto o m√©todo $\epsilon$-guloso explora aleatoriamente algumas vezes, o que permite que o algoritmo descubra melhores a√ß√µes ao longo do tempo. Este exemplo ilustra que, apesar do m√©todo $\epsilon$-guloso explorar mais e obter recompensas um pouco menores no in√≠cio, ele performa melhor ao longo do tempo.

### Conclus√£o

A necessidade de equilibrar explora√ß√£o e explota√ß√£o √© um desafio distintivo do aprendizado por refor√ßo [^3]. O problema do bandit k-armado, com sua simplicidade, permite analisar claramente essa necessidade [^3]. No contexto da modelagem financeira avan√ßada, o balanceamento adequado entre explora√ß√£o e explota√ß√£o pode levar a estrat√©gias de investimento mais eficazes. A escolha de m√©todos para estimar os valores das a√ß√µes e para guiar a sele√ß√£o de a√ß√µes, s√£o cruciais para um bom desempenho. Os m√©todos $\epsilon$-gulosos provam ser uma ferramenta valiosa nesse contexto [^6]. A avalia√ß√£o atrav√©s do 10-armed testbed demonstra que m√©todos gulosos podem ficar presos em a√ß√µes sub√≥timas, enquanto m√©todos que exploram s√£o capazes de descobrir a√ß√µes √≥timas com maior frequ√™ncia [^6]. A escolha entre explorar e explotar tem um grande impacto no aprendizado e desempenho final do agente.

### Refer√™ncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Multi-armed Bandits)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action." *(Trecho de Multi-armed Bandits)*
[^3]: "In this book we do not worry about balancing exploration and exploitation in a sophisticated way; we worry only about balancing them at all." *(Trecho de Multi-armed Bandits)*
[^6]: "The advantage of …õ-greedy over greedy methods depends on the task." *(Trecho de Multi-armed Bandits)*
