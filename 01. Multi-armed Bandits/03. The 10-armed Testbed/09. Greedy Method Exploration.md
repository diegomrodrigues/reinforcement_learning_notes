## Explorando o Dilema da Explora√ß√£o-Explota√ß√£o no Testbed de 10 Bra√ßos
### Introdu√ß√£o
O conceito de **Multi-armed Bandits** √© central para entender a din√¢mica entre **explora√ß√£o** e **explota√ß√£o** em aprendizado por refor√ßo. Este cap√≠tulo foca no aspecto avaliativo do aprendizado por refor√ßo, ou seja, em como as a√ß√µes s√£o avaliadas com base no *feedback* recebido, e n√£o em instru√ß√µes sobre qual a√ß√£o tomar [^1]. O **k-armed bandit problem**, que ser√° explorado aqui, √© uma vers√£o simplificada, onde o agente precisa escolher entre *k* a√ß√µes, cada uma fornecendo recompensas com distribui√ß√µes de probabilidade estacion√°rias, a fim de maximizar a recompensa total esperada [^1]. O objetivo √© entender como o feedback avaliativo difere e se combina com o feedback instrutivo, evitando a complexidade do problema completo de aprendizado por refor√ßo [^1]. O presente texto ir√° se aprofundar na an√°lise do **10-armed testbed**, avaliando o desempenho do m√©todo ganancioso em compara√ß√£o com as abordagens *-greedy*, particularmente em cen√°rios determin√≠sticos e n√£o estacion√°rios [^2].

### Conceitos Fundamentais
#### O Problema k-armed Bandit
O problema *k-armed bandit* envolve um agente tomando decis√µes repetidamente entre *k* op√ß√µes, ou a√ß√µes. Ap√≥s cada escolha, uma recompensa num√©rica √© obtida, extra√≠da de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o selecionada. O objetivo √© maximizar a recompensa total esperada em um per√≠odo espec√≠fico, como 1000 sele√ß√µes de a√ß√£o ou passos de tempo [^2]. Cada a√ß√£o tem um valor esperado, *q*(a), denotado como $$ q_*(a) = E[R_t | A_t = a] $$ [^2]. Onde $A_t$ √© a a√ß√£o selecionada no tempo *t* e $R_t$ √© a recompensa correspondente. Em cen√°rios ideais, com conhecimento de todos os valores *q*(a), o problema se tornaria trivial, consistindo em selecionar sempre a a√ß√£o com o maior valor. No entanto, esses valores s√£o desconhecidos e precisam ser estimados [^2]. As estimativas s√£o denotadas como $Q_t(a)$, e o objetivo √© que $Q_t(a)$ se aproxime de $q_*(a)$ [^2].

> üí° **Exemplo Num√©rico:** Imagine um problema de 3 bra√ßos (k=3). As recompensas verdadeiras m√©dias s√£o $q_*(1) = 1.0$, $q_*(2) = 2.0$, e $q_*(3) = 1.5$. Inicialmente, n√£o sabemos esses valores, e nossas estimativas $Q_t(a)$ s√£o todas 0. Se selecionarmos o bra√ßo 1 na primeira itera√ß√£o ($A_1=1$) e recebermos uma recompensa $R_1=0.8$, ent√£o nossa estimativa para o bra√ßo 1, $Q_1(1)$, ser√° atualizada. Vamos usar a m√©dia amostral como estimador $Q_t(a)$. Depois de uma itera√ß√£o, $Q_1(1) = 0.8$, $Q_1(2) = 0$, e $Q_1(3) = 0$.
```mermaid
graph LR
    A[ "A√ß√£o A_t" ] -->| "Recompensa R_t" | B( "Obter Recompensa" )
    B --> C{ "Atualizar Estimativa Q_t(a)" }
    C --> D[ "Pr√≥xima A√ß√£o" ]
    D --> A
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#ff9,stroke:#333,stroke-width:2px
```

#### Explora√ß√£o vs. Explota√ß√£o
A escolha entre **explorar** e **explorar** o conhecimento atual √© fundamental. A **explota√ß√£o** envolve selecionar a√ß√µes com as maiores estimativas de valor ($Q_t(a)$), chamadas a√ß√µes *greedy*. A **explora√ß√£o**, por outro lado, envolve selecionar a√ß√µes n√£o- *greedy* para melhorar as estimativas do valor [^2]. A explota√ß√£o maximiza a recompensa em uma √∫nica etapa, enquanto a explora√ß√£o pode levar a uma maior recompensa total a longo prazo. Existe um conflito inerente entre essas duas abordagens, pois a explora√ß√£o reduz a recompensa no curto prazo, mas beneficia o aprendizado futuro [^2].
**Proposi√ß√£o 1:** A escolha √≥tima entre explora√ß√£o e explota√ß√£o √© dependente do horizonte temporal do problema. Em problemas com poucos passos de tempo restantes, a explota√ß√£o tende a ser mais ben√©fica, enquanto a explora√ß√£o torna-se mais vantajosa em problemas com um grande n√∫mero de passos.
*Prova:* Se o n√∫mero de passos de tempo restantes √© pequeno, investir na explora√ß√£o pode n√£o ser t√£o vantajoso, j√° que a recompensa m√°xima poss√≠vel nesse curto espa√ßo de tempo ser√° limitada. Por outro lado, se um grande n√∫mero de passos de tempo ainda precisa ser feito, ent√£o a explora√ß√£o inicial pode ser usada para refinar as estimativas de valor, levando a um melhor desempenho ao longo do tempo. $\blacksquare$
```mermaid
graph LR
    subgraph "Explora√ß√£o"
    A("A√ß√µes N√£o-Greedy") --> B("Melhorar Q_t(a)")
    end
    subgraph "Explota√ß√£o"
     C("A√ß√µes Greedy") --> D("Maximizar Recompensa Imediata")
    end
    B --> E("Poss√≠vel Recompensa Superior")
    E --> C
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#ff9,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Suponha que temos 10 passos de tempo restantes e conhecemos as estimativas de valor das a√ß√µes como: $Q(1)=0.9$, $Q(2)=0.6$, $Q(3)=0.7$. Uma estrat√©gia puramente explorat√≥ria (por exemplo, escolhendo cada bra√ßo igualmente) pode n√£o ser ideal pois arriscar√≠amos escolher os bra√ßos 2 e 3 que rendem menos recompensa. Por outro lado, se tiv√©ssemos 1000 passos, investir na explora√ß√£o no in√≠cio permitiria refinar estas estimativas e talvez descobrir um bra√ßo com valor real maior do que 0.9, mesmo que este bra√ßo tenha um valor estimado inicial menor.

#### O 10-armed Testbed
Para avaliar os m√©todos de a√ß√£o-valor *greedy* e *-greedy*, foi criado um conjunto de 2000 problemas *k-armed bandit* com *k = 10*, chamado de *10-armed testbed* [^4]. Em cada problema, o valor verdadeiro de cada a√ß√£o, $q_*(a)$, foi sorteado de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1. As recompensas reais $R_t$ foram sorteadas de uma distribui√ß√£o normal com m√©dia $q_*(a)$ e vari√¢ncia 1 [^4]. Este testbed permite avaliar o desempenho dos algoritmos com base em como eles melhoram ao longo de 1000 etapas [^4]. A m√©dia de resultados de 2000 execu√ß√µes independentes, cada uma com um problema *bandit* diferente, √© utilizada para avaliar o comportamento m√©dio do algoritmo [^5].
**Lema 1.1:** A escolha da distribui√ß√£o de recompensas (neste caso, uma normal) e seus par√¢metros influenciam diretamente a dificuldade de um problema *k-armed bandit*. Distribui√ß√µes com vari√¢ncia maior tornam a explora√ß√£o mais crucial.
*Prova:* Uma maior vari√¢ncia na distribui√ß√£o de recompensas significa que uma √∫nica observa√ß√£o da recompensa fornece menos informa√ß√µes sobre o verdadeiro valor da a√ß√£o. Isso torna a estimativa mais dif√≠cil e a explora√ß√£o, portanto, mais importante para encontrar a a√ß√£o √≥tima. $\blacksquare$
```mermaid
graph LR
    A["Gera√ß√£o de Problemas (2000)"] --> B("Distribui√ß√£o q*(a) ~ N(0,1)")
    B --> C("Distribui√ß√£o R_t ~ N(q*(a),1)")
    C --> D["Avaliar Algoritmo (1000 etapas)"]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#ff9,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Considere dois bra√ßos, A e B. Para o bra√ßo A, a recompensa m√©dia $q_*(A) = 1.0$, e as recompensas s√£o sorteadas de uma normal com vari√¢ncia 1. Para o bra√ßo B, $q_*(B) = 1.2$ e as recompensas de uma normal com vari√¢ncia 4. As recompensas do bra√ßo B s√£o mais "ruidosas", o que significa que ser√° necess√°rio mais explora√ß√£o (e mais amostras) para estimar com precis√£o seu verdadeiro valor, em compara√ß√£o com o bra√ßo A.

#### An√°lise do Desempenho do M√©todo Ganancioso e …õ-Greedy no 10-armed Testbed
No *10-armed testbed*, o m√©todo *greedy* melhorou rapidamente no in√≠cio, mas depois estabilizou-se em um n√≠vel inferior em compara√ß√£o com as abordagens *-greedy* [^5]. O m√©todo *greedy* obteve uma recompensa por passo de apenas 1, em compara√ß√£o com o valor √≥timo de aproximadamente 1,54 neste testbed [^5]. Isso aconteceu porque o m√©todo *greedy* muitas vezes ficava preso na sele√ß√£o de a√ß√µes sub√≥timas, n√£o explorando outras a√ß√µes que poderiam ter recompensas maiores [^5].

Em contrapartida, os m√©todos *-greedy* (com *Œµ* = 0.01 e *Œµ* = 0.1) continuaram a explorar e, eventualmente, tiveram um desempenho melhor, reconhecendo as a√ß√µes √≥timas com maior frequ√™ncia [^6]. O m√©todo *Œµ*=0.1 explorou mais e descobriu a√ß√µes √≥timas mais rapidamente, mas a sele√ß√£o da a√ß√£o √≥tima n√£o excedeu 91% do tempo [^6]. O m√©todo *Œµ*=0.01 melhorou mais lentamente, mas, por fim, superou o m√©todo com *Œµ*=0.1 tanto em termos de recompensa quanto em probabilidade de selecionar a a√ß√£o √≥tima [^6]. Em cen√°rios com ru√≠do nas recompensas (por exemplo, uma vari√¢ncia de recompensa maior), espera-se que o m√©todo *-greedy* tenha um desempenho melhor em rela√ß√£o ao m√©todo *greedy*, pois a explora√ß√£o √© ainda mais crucial [^6].
```mermaid
sequenceDiagram
    participant M√©todo Greedy
    participant M√©todo Œµ-Greedy (Œµ=0.1)
    participant M√©todo Œµ-Greedy (Œµ=0.01)
    
    loop Etapas de Tempo
    
        M√©todo Greedy->>M√©todo Greedy: Selecionar a√ß√£o com maior Q_t(a)
        activate M√©todo Greedy
        M√©todo Greedy-->>M√©todo Greedy: Obt√©m R_t e Atualiza Q_t(a)
        deactivate M√©todo Greedy
       
    
        M√©todo Œµ-Greedy (Œµ=0.1) ->> M√©todo Œµ-Greedy (Œµ=0.1): Explorar com prob. 0.1, Exploitar com prob. 0.9
         activate M√©todo Œµ-Greedy (Œµ=0.1)
        M√©todo Œµ-Greedy (Œµ=0.1) -->> M√©todo Œµ-Greedy (Œµ=0.1): Obt√©m R_t e Atualiza Q_t(a)
          deactivate M√©todo Œµ-Greedy (Œµ=0.1)
         
        M√©todo Œµ-Greedy (Œµ=0.01) ->> M√©todo Œµ-Greedy (Œµ=0.01): Explorar com prob. 0.01, Exploitar com prob. 0.99
         activate M√©todo Œµ-Greedy (Œµ=0.01)
        M√©todo Œµ-Greedy (Œµ=0.01) -->> M√©todo Œµ-Greedy (Œµ=0.01): Obt√©m R_t e Atualiza Q_t(a)
           deactivate M√©todo Œµ-Greedy (Œµ=0.01)
    end
```

> üí° **Exemplo Num√©rico:** Suponha que em um determinado problema do *10-armed testbed*, o bra√ßo 5 tem um valor verdadeiro de $q_*(5) = 1.5$. Inicialmente, todas as estimativas Q(a) s√£o 0. No primeiro passo, o m√©todo *greedy* pode escolher qualquer bra√ßo (digamos bra√ßo 1), e obt√©m uma recompensa $R_1$. Suponha que na segunda itera√ß√£o o m√©todo greedy encontra um bra√ßo com estimativa ligeiramente maior (digamos $Q_2(3)=0.2$). O m√©todo *greedy* ir√° escolher o bra√ßo 3, enquanto um m√©todo *Œµ-greedy* pode ainda escolher o bra√ßo 5 com probabilidade *Œµ*. Se *Œµ* = 0.1, isso significa que o m√©todo *Œµ-greedy* tem 10% de chance de selecionar qualquer bra√ßo aleatoriamente, permitindo eventualmente encontrar o bra√ßo 5, com o valor verdadeiro maior. Com muitas itera√ß√µes, o m√©todo *greedy* pode ficar preso em um √≥timo local enquanto o *Œµ-greedy* ir√° convergir para o √≥timo global.

**Lemma 1:** O m√©todo ganancioso, enquanto maximiza a recompensa imediata, pode ficar preso em √≥timos locais devido √† falta de explora√ß√£o, especialmente quando as a√ß√µes sub√≥timas t√™m alguma incerteza associada a suas recompensas.
*Prova:* O m√©todo ganancioso seleciona a a√ß√£o com a maior estimativa de valor atual, ignorando outras a√ß√µes com potencial de serem melhores. Se as estimativas iniciais s√£o ruins ou uma a√ß√£o sub√≥tima tem uma recompensa inicialmente maior, o m√©todo ganancioso n√£o explorar√° outras op√ß√µes. $\blacksquare$

**Corol√°rio 1:** Em ambientes determin√≠sticos, onde a recompensa para cada a√ß√£o √© conhecida ap√≥s uma √∫nica tentativa, o m√©todo ganancioso pode ser suficiente, mas √© importante notar que, mesmo em casos determin√≠sticos, a explora√ß√£o pode ser necess√°ria se os valores das a√ß√µes mudarem ao longo do tempo. [^6]
**Teorema 1:** O valor de *Œµ* nos m√©todos *Œµ-greedy* √© um hiperpar√¢metro que afeta diretamente a taxa de converg√™ncia e a recompensa assint√≥tica. Um valor de *Œµ* maior acelera a explora√ß√£o inicial, mas pode levar a uma converg√™ncia mais lenta para a a√ß√£o √≥tima.
*Prova (Esbo√ßo):* Um valor de *Œµ* maior permite que o agente explore um maior n√∫mero de a√ß√µes, resultando em uma converg√™ncia mais r√°pida para a√ß√µes que oferecem recompensas maiores. Entretanto, ap√≥s um tempo, essa taxa de explora√ß√£o excessiva prejudica a capacidade de converg√™ncia para as a√ß√µes √≥timas. Um valor de *Œµ* menor promove uma converg√™ncia mais lenta, mas permite o desenvolvimento de uma explota√ß√£o mais refinada. $\blacksquare$
```mermaid
graph LR
    subgraph "Œµ alto"
        A("Explora√ß√£o Inicial R√°pida") --> B("Converg√™ncia Mais Lenta")
    end
     subgraph "Œµ baixo"
        C("Explora√ß√£o Lenta") --> D("Converg√™ncia Mais Refinada")
    end
    B --> F[Tradeoff]
    D --> F
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#ff9,stroke:#333,stroke-width:2px
    style F fill:#afa,stroke:#333,stroke-width:2px

```

> üí° **Exemplo Num√©rico:** Vamos simular um problema com 3 bra√ßos para ilustrar o efeito do Œµ em uma s√©rie de 100 passos.
```python
import numpy as np
import matplotlib.pyplot as plt

def run_bandit_simulation(epsilon, num_steps=100):
    q_true = [1.0, 2.0, 1.5] # True means for 3 arms
    q_est = [0.0, 0.0, 0.0] # Initial value estimates
    N = [0, 0, 0] # counts of each arm
    rewards = []
    for t in range(num_steps):
      if np.random.rand() < epsilon:
        action = np.random.randint(3) # explore
      else:
        action = np.argmax(q_est) # exploit
      reward = np.random.normal(q_true[action], 1) # sample reward with noise
      N[action]+=1
      q_est[action] = q_est[action] + (reward-q_est[action])/N[action]
      rewards.append(reward)
    return rewards, q_est


epsilons = [0, 0.1, 0.01]
all_rewards = []
for eps in epsilons:
  rewards,_ = run_bandit_simulation(eps)
  all_rewards.append(rewards)

plt.figure(figsize=(10, 6))
for i, rewards in enumerate(all_rewards):
    plt.plot(range(len(rewards)), np.cumsum(rewards), label=f'Œµ = {epsilons[i]}')
plt.xlabel("Passos de Tempo")
plt.ylabel("Recompensa Cumulativa")
plt.title("Compara√ß√£o Œµ-greedy: Recompensa Cumulativa")
plt.legend()
plt.grid(True)
plt.show()

```
O gr√°fico demonstra que o m√©todo guloso (Œµ=0) apresenta um crescimento lento da recompensa cumulativa e fica preso em um √≥timo local. O m√©todo Œµ=0.1 explora mais no in√≠cio e rapidamente encontra um melhor padr√£o de recompensa. J√° o m√©todo Œµ=0.01 tem um desempenho inicial um pouco pior, mas converge para um valor melhor de recompensa total com mais tempo.

### Conclus√£o
O estudo do *10-armed testbed* ilustra claramente a import√¢ncia do equil√≠brio entre explora√ß√£o e explota√ß√£o em aprendizado por refor√ßo [^1]. O m√©todo *greedy*, embora simples e eficaz em situa√ß√µes determin√≠sticas onde a informa√ß√£o √© perfeita desde o in√≠cio, n√£o consegue atingir o desempenho m√°ximo em ambientes incertos e explorat√≥rios [^6]. Os m√©todos *-greedy*, ao introduzirem uma pequena probabilidade de explora√ß√£o aleat√≥ria, conseguem escapar dos √≥timos locais e melhorar seu desempenho a longo prazo [^5]. Isso destaca a necessidade de estrat√©gias que permitam ao agente n√£o apenas explorar a√ß√µes de alto valor esperado, mas tamb√©m buscar ativamente novas op√ß√µes com potencial de recompensa superior. Em problemas de aprendizado por refor√ßo, especialmente naqueles n√£o estacion√°rios, a explora√ß√£o √© um componente vital para o sucesso [^6].
**Corol√°rio 1.1** A escolha do valor ideal de Œµ nos m√©todos *Œµ-greedy* √© um problema complexo e n√£o possui uma solu√ß√£o universal, sendo dependente do ambiente espec√≠fico e dos objetivos de aprendizado, exigindo muitas vezes a experimenta√ß√£o e otimiza√ß√£o do hiperpar√¢metro.
```mermaid
graph LR
    A["Ambiente"] --> B{"Escolher Œµ"}
    B --> |"Œµ adequado"| C["Desempenho Ideal"]
    B --> |"Œµ inadequado"| D["Desempenho Sub-√ìtimo"]
    C --> E("Otimiza√ß√£o")
    D --> E
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
     style D fill:#ff9,stroke:#333,stroke-width:2px
    style E fill:#afa,stroke:#333,stroke-width:2px
```

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^3]: "If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^4]: "To roughly assess the relative effectiveness of the greedy and …õ-greedy action-value methods, we compared them numerically on a suite of test problems. This was a set of 2000 randomly generated k-armed bandit problems with k = 10." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^5]: "Figure 2.2 compares a greedy method with two …õ-greedy methods (Œµ=0.01 and Œµ=0.1), as described above, on the 10-armed testbed." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^6]: "The advantage of …õ-greedy over greedy methods depends on the task. For example, suppose the reward variance had been larger, say 10 instead of 1." *(Trecho de Chapter 2: Multi-armed Bandits)*
