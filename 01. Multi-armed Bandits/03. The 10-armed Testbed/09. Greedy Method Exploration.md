## Explorando o Dilema da ExploraÃ§Ã£o-ExplotaÃ§Ã£o no Testbed de 10 BraÃ§os
### IntroduÃ§Ã£o
O conceito de **Multi-armed Bandits** Ã© central para entender a dinÃ¢mica entre **exploraÃ§Ã£o** e **explotaÃ§Ã£o** em aprendizado por reforÃ§o. Este capÃ­tulo foca no aspecto avaliativo do aprendizado por reforÃ§o, ou seja, em como as aÃ§Ãµes sÃ£o avaliadas com base no *feedback* recebido, e nÃ£o em instruÃ§Ãµes sobre qual aÃ§Ã£o tomar [^1]. O **k-armed bandit problem**, que serÃ¡ explorado aqui, Ã© uma versÃ£o simplificada, onde o agente precisa escolher entre *k* aÃ§Ãµes, cada uma fornecendo recompensas com distribuiÃ§Ãµes de probabilidade estacionÃ¡rias, a fim de maximizar a recompensa total esperada [^1]. O objetivo Ã© entender como o feedback avaliativo difere e se combina com o feedback instrutivo, evitando a complexidade do problema completo de aprendizado por reforÃ§o [^1]. O presente texto irÃ¡ se aprofundar na anÃ¡lise do **10-armed testbed**, avaliando o desempenho do mÃ©todo ganancioso em comparaÃ§Ã£o com as abordagens *-greedy*, particularmente em cenÃ¡rios determinÃ­sticos e nÃ£o estacionÃ¡rios [^2].

### Conceitos Fundamentais
#### O Problema k-armed Bandit
O problema *k-armed bandit* envolve um agente tomando decisÃµes repetidamente entre *k* opÃ§Ãµes, ou aÃ§Ãµes. ApÃ³s cada escolha, uma recompensa numÃ©rica Ã© obtida, extraÃ­da de uma distribuiÃ§Ã£o de probabilidade estacionÃ¡ria que depende da aÃ§Ã£o selecionada. O objetivo Ã© maximizar a recompensa total esperada em um perÃ­odo especÃ­fico, como 1000 seleÃ§Ãµes de aÃ§Ã£o ou passos de tempo [^2]. Cada aÃ§Ã£o tem um valor esperado, *q*(a), denotado como $$ q_*(a) = E[R_t | A_t = a] $$ [^2]. Onde $A_t$ Ã© a aÃ§Ã£o selecionada no tempo *t* e $R_t$ Ã© a recompensa correspondente. Em cenÃ¡rios ideais, com conhecimento de todos os valores *q*(a), o problema se tornaria trivial, consistindo em selecionar sempre a aÃ§Ã£o com o maior valor. No entanto, esses valores sÃ£o desconhecidos e precisam ser estimados [^2]. As estimativas sÃ£o denotadas como $Q_t(a)$, e o objetivo Ã© que $Q_t(a)$ se aproxime de $q_*(a)$ [^2].

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine um problema de 3 braÃ§os (k=3). As recompensas verdadeiras mÃ©dias sÃ£o $q_*(1) = 1.0$, $q_*(2) = 2.0$, e $q_*(3) = 1.5$. Inicialmente, nÃ£o sabemos esses valores, e nossas estimativas $Q_t(a)$ sÃ£o todas 0. Se selecionarmos o braÃ§o 1 na primeira iteraÃ§Ã£o ($A_1=1$) e recebermos uma recompensa $R_1=0.8$, entÃ£o nossa estimativa para o braÃ§o 1, $Q_1(1)$, serÃ¡ atualizada. Vamos usar a mÃ©dia amostral como estimador $Q_t(a)$. Depois de uma iteraÃ§Ã£o, $Q_1(1) = 0.8$, $Q_1(2) = 0$, e $Q_1(3) = 0$.
```mermaid
graph LR
    A[ "AÃ§Ã£o A_t" ] -->| "Recompensa R_t" | B( "Obter Recompensa" )
    B --> C{ "Atualizar Estimativa Q_t(a)" }
    C --> D[ "PrÃ³xima AÃ§Ã£o" ]
    D --> A
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#ff9,stroke:#333,stroke-width:2px
```

#### ExploraÃ§Ã£o vs. ExplotaÃ§Ã£o
A escolha entre **explorar** e **explorar** o conhecimento atual Ã© fundamental. A **explotaÃ§Ã£o** envolve selecionar aÃ§Ãµes com as maiores estimativas de valor ($Q_t(a)$), chamadas aÃ§Ãµes *greedy*. A **exploraÃ§Ã£o**, por outro lado, envolve selecionar aÃ§Ãµes nÃ£o- *greedy* para melhorar as estimativas do valor [^2]. A explotaÃ§Ã£o maximiza a recompensa em uma Ãºnica etapa, enquanto a exploraÃ§Ã£o pode levar a uma maior recompensa total a longo prazo. Existe um conflito inerente entre essas duas abordagens, pois a exploraÃ§Ã£o reduz a recompensa no curto prazo, mas beneficia o aprendizado futuro [^2].
**ProposiÃ§Ã£o 1:** A escolha Ã³tima entre exploraÃ§Ã£o e explotaÃ§Ã£o Ã© dependente do horizonte temporal do problema. Em problemas com poucos passos de tempo restantes, a explotaÃ§Ã£o tende a ser mais benÃ©fica, enquanto a exploraÃ§Ã£o torna-se mais vantajosa em problemas com um grande nÃºmero de passos.
*Prova:* Se o nÃºmero de passos de tempo restantes Ã© pequeno, investir na exploraÃ§Ã£o pode nÃ£o ser tÃ£o vantajoso, jÃ¡ que a recompensa mÃ¡xima possÃ­vel nesse curto espaÃ§o de tempo serÃ¡ limitada. Por outro lado, se um grande nÃºmero de passos de tempo ainda precisa ser feito, entÃ£o a exploraÃ§Ã£o inicial pode ser usada para refinar as estimativas de valor, levando a um melhor desempenho ao longo do tempo. $\blacksquare$
```mermaid
graph LR
    subgraph "ExploraÃ§Ã£o"
    A("AÃ§Ãµes NÃ£o-Greedy") --> B("Melhorar Q_t(a)")
    end
    subgraph "ExplotaÃ§Ã£o"
     C("AÃ§Ãµes Greedy") --> D("Maximizar Recompensa Imediata")
    end
    B --> E("PossÃ­vel Recompensa Superior")
    E --> C
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#ff9,stroke:#333,stroke-width:2px
```

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos 10 passos de tempo restantes e conhecemos as estimativas de valor das aÃ§Ãµes como: $Q(1)=0.9$, $Q(2)=0.6$, $Q(3)=0.7$. Uma estratÃ©gia puramente exploratÃ³ria (por exemplo, escolhendo cada braÃ§o igualmente) pode nÃ£o ser ideal pois arriscarÃ­amos escolher os braÃ§os 2 e 3 que rendem menos recompensa. Por outro lado, se tivÃ©ssemos 1000 passos, investir na exploraÃ§Ã£o no inÃ­cio permitiria refinar estas estimativas e talvez descobrir um braÃ§o com valor real maior do que 0.9, mesmo que este braÃ§o tenha um valor estimado inicial menor.

#### O 10-armed Testbed
Para avaliar os mÃ©todos de aÃ§Ã£o-valor *greedy* e *-greedy*, foi criado um conjunto de 2000 problemas *k-armed bandit* com *k = 10*, chamado de *10-armed testbed* [^4]. Em cada problema, o valor verdadeiro de cada aÃ§Ã£o, $q_*(a)$, foi sorteado de uma distribuiÃ§Ã£o normal com mÃ©dia 0 e variÃ¢ncia 1. As recompensas reais $R_t$ foram sorteadas de uma distribuiÃ§Ã£o normal com mÃ©dia $q_*(a)$ e variÃ¢ncia 1 [^4]. Este testbed permite avaliar o desempenho dos algoritmos com base em como eles melhoram ao longo de 1000 etapas [^4]. A mÃ©dia de resultados de 2000 execuÃ§Ãµes independentes, cada uma com um problema *bandit* diferente, Ã© utilizada para avaliar o comportamento mÃ©dio do algoritmo [^5].
**Lema 1.1:** A escolha da distribuiÃ§Ã£o de recompensas (neste caso, uma normal) e seus parÃ¢metros influenciam diretamente a dificuldade de um problema *k-armed bandit*. DistribuiÃ§Ãµes com variÃ¢ncia maior tornam a exploraÃ§Ã£o mais crucial.
*Prova:* Uma maior variÃ¢ncia na distribuiÃ§Ã£o de recompensas significa que uma Ãºnica observaÃ§Ã£o da recompensa fornece menos informaÃ§Ãµes sobre o verdadeiro valor da aÃ§Ã£o. Isso torna a estimativa mais difÃ­cil e a exploraÃ§Ã£o, portanto, mais importante para encontrar a aÃ§Ã£o Ã³tima. $\blacksquare$
```mermaid
graph LR
    A["GeraÃ§Ã£o de Problemas (2000)"] --> B("DistribuiÃ§Ã£o q*(a) ~ N(0,1)")
    B --> C("DistribuiÃ§Ã£o R_t ~ N(q*(a),1)")
    C --> D["Avaliar Algoritmo (1000 etapas)"]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#ff9,stroke:#333,stroke-width:2px
```

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere dois braÃ§os, A e B. Para o braÃ§o A, a recompensa mÃ©dia $q_*(A) = 1.0$, e as recompensas sÃ£o sorteadas de uma normal com variÃ¢ncia 1. Para o braÃ§o B, $q_*(B) = 1.2$ e as recompensas de uma normal com variÃ¢ncia 4. As recompensas do braÃ§o B sÃ£o mais "ruidosas", o que significa que serÃ¡ necessÃ¡rio mais exploraÃ§Ã£o (e mais amostras) para estimar com precisÃ£o seu verdadeiro valor, em comparaÃ§Ã£o com o braÃ§o A.

#### AnÃ¡lise do Desempenho do MÃ©todo Ganancioso e É›-Greedy no 10-armed Testbed
No *10-armed testbed*, o mÃ©todo *greedy* melhorou rapidamente no inÃ­cio, mas depois estabilizou-se em um nÃ­vel inferior em comparaÃ§Ã£o com as abordagens *-greedy* [^5]. O mÃ©todo *greedy* obteve uma recompensa por passo de apenas 1, em comparaÃ§Ã£o com o valor Ã³timo de aproximadamente 1,54 neste testbed [^5]. Isso aconteceu porque o mÃ©todo *greedy* muitas vezes ficava preso na seleÃ§Ã£o de aÃ§Ãµes subÃ³timas, nÃ£o explorando outras aÃ§Ãµes que poderiam ter recompensas maiores [^5].

Em contrapartida, os mÃ©todos *-greedy* (com *Îµ* = 0.01 e *Îµ* = 0.1) continuaram a explorar e, eventualmente, tiveram um desempenho melhor, reconhecendo as aÃ§Ãµes Ã³timas com maior frequÃªncia [^6]. O mÃ©todo *Îµ*=0.1 explorou mais e descobriu aÃ§Ãµes Ã³timas mais rapidamente, mas a seleÃ§Ã£o da aÃ§Ã£o Ã³tima nÃ£o excedeu 91% do tempo [^6]. O mÃ©todo *Îµ*=0.01 melhorou mais lentamente, mas, por fim, superou o mÃ©todo com *Îµ*=0.1 tanto em termos de recompensa quanto em probabilidade de selecionar a aÃ§Ã£o Ã³tima [^6]. Em cenÃ¡rios com ruÃ­do nas recompensas (por exemplo, uma variÃ¢ncia de recompensa maior), espera-se que o mÃ©todo *-greedy* tenha um desempenho melhor em relaÃ§Ã£o ao mÃ©todo *greedy*, pois a exploraÃ§Ã£o Ã© ainda mais crucial [^6].
```mermaid
sequenceDiagram
    participant MÃ©todo Greedy
    participant MÃ©todo Îµ-Greedy (Îµ=0.1)
    participant MÃ©todo Îµ-Greedy (Îµ=0.01)
    
    loop Etapas de Tempo
    
        MÃ©todo Greedy->>MÃ©todo Greedy: Selecionar aÃ§Ã£o com maior Q_t(a)
        activate MÃ©todo Greedy
        MÃ©todo Greedy-->>MÃ©todo Greedy: ObtÃ©m R_t e Atualiza Q_t(a)
        deactivate MÃ©todo Greedy
       
    
        MÃ©todo Îµ-Greedy (Îµ=0.1) ->> MÃ©todo Îµ-Greedy (Îµ=0.1): Explorar com prob. 0.1, Exploitar com prob. 0.9
         activate MÃ©todo Îµ-Greedy (Îµ=0.1)
        MÃ©todo Îµ-Greedy (Îµ=0.1) -->> MÃ©todo Îµ-Greedy (Îµ=0.1): ObtÃ©m R_t e Atualiza Q_t(a)
          deactivate MÃ©todo Îµ-Greedy (Îµ=0.1)
         
        MÃ©todo Îµ-Greedy (Îµ=0.01) ->> MÃ©todo Îµ-Greedy (Îµ=0.01): Explorar com prob. 0.01, Exploitar com prob. 0.99
         activate MÃ©todo Îµ-Greedy (Îµ=0.01)
        MÃ©todo Îµ-Greedy (Îµ=0.01) -->> MÃ©todo Îµ-Greedy (Îµ=0.01): ObtÃ©m R_t e Atualiza Q_t(a)
           deactivate MÃ©todo Îµ-Greedy (Îµ=0.01)
    end
```

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que em um determinado problema do *10-armed testbed*, o braÃ§o 5 tem um valor verdadeiro de $q_*(5) = 1.5$. Inicialmente, todas as estimativas Q(a) sÃ£o 0. No primeiro passo, o mÃ©todo *greedy* pode escolher qualquer braÃ§o (digamos braÃ§o 1), e obtÃ©m uma recompensa $R_1$. Suponha que na segunda iteraÃ§Ã£o o mÃ©todo greedy encontra um braÃ§o com estimativa ligeiramente maior (digamos $Q_2(3)=0.2$). O mÃ©todo *greedy* irÃ¡ escolher o braÃ§o 3, enquanto um mÃ©todo *Îµ-greedy* pode ainda escolher o braÃ§o 5 com probabilidade *Îµ*. Se *Îµ* = 0.1, isso significa que o mÃ©todo *Îµ-greedy* tem 10% de chance de selecionar qualquer braÃ§o aleatoriamente, permitindo eventualmente encontrar o braÃ§o 5, com o valor verdadeiro maior. Com muitas iteraÃ§Ãµes, o mÃ©todo *greedy* pode ficar preso em um Ã³timo local enquanto o *Îµ-greedy* irÃ¡ convergir para o Ã³timo global.

**Lemma 1:** O mÃ©todo ganancioso, enquanto maximiza a recompensa imediata, pode ficar preso em Ã³timos locais devido Ã  falta de exploraÃ§Ã£o, especialmente quando as aÃ§Ãµes subÃ³timas tÃªm alguma incerteza associada a suas recompensas.
*Prova:* O mÃ©todo ganancioso seleciona a aÃ§Ã£o com a maior estimativa de valor atual, ignorando outras aÃ§Ãµes com potencial de serem melhores. Se as estimativas iniciais sÃ£o ruins ou uma aÃ§Ã£o subÃ³tima tem uma recompensa inicialmente maior, o mÃ©todo ganancioso nÃ£o explorarÃ¡ outras opÃ§Ãµes. $\blacksquare$

**CorolÃ¡rio 1:** Em ambientes determinÃ­sticos, onde a recompensa para cada aÃ§Ã£o Ã© conhecida apÃ³s uma Ãºnica tentativa, o mÃ©todo ganancioso pode ser suficiente, mas Ã© importante notar que, mesmo em casos determinÃ­sticos, a exploraÃ§Ã£o pode ser necessÃ¡ria se os valores das aÃ§Ãµes mudarem ao longo do tempo. [^6]
**Teorema 1:** O valor de *Îµ* nos mÃ©todos *Îµ-greedy* Ã© um hiperparÃ¢metro que afeta diretamente a taxa de convergÃªncia e a recompensa assintÃ³tica. Um valor de *Îµ* maior acelera a exploraÃ§Ã£o inicial, mas pode levar a uma convergÃªncia mais lenta para a aÃ§Ã£o Ã³tima.
*Prova (EsboÃ§o):* Um valor de *Îµ* maior permite que o agente explore um maior nÃºmero de aÃ§Ãµes, resultando em uma convergÃªncia mais rÃ¡pida para aÃ§Ãµes que oferecem recompensas maiores. Entretanto, apÃ³s um tempo, essa taxa de exploraÃ§Ã£o excessiva prejudica a capacidade de convergÃªncia para as aÃ§Ãµes Ã³timas. Um valor de *Îµ* menor promove uma convergÃªncia mais lenta, mas permite o desenvolvimento de uma explotaÃ§Ã£o mais refinada. $\blacksquare$
```mermaid
graph LR
    subgraph "Îµ alto"
        A("ExploraÃ§Ã£o Inicial RÃ¡pida") --> B("ConvergÃªncia Mais Lenta")
    end
     subgraph "Îµ baixo"
        C("ExploraÃ§Ã£o Lenta") --> D("ConvergÃªncia Mais Refinada")
    end
    B --> F[Tradeoff]
    D --> F
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
    style D fill:#ff9,stroke:#333,stroke-width:2px
    style F fill:#afa,stroke:#333,stroke-width:2px

```

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos simular um problema com 3 braÃ§os para ilustrar o efeito do Îµ em uma sÃ©rie de 100 passos.
```python
import numpy as np
import matplotlib.pyplot as plt

def run_bandit_simulation(epsilon, num_steps=100):
    q_true = [1.0, 2.0, 1.5] # True means for 3 arms
    q_est = [0.0, 0.0, 0.0] # Initial value estimates
    N = [0, 0, 0] # counts of each arm
    rewards = []
    for t in range(num_steps):
      if np.random.rand() < epsilon:
        action = np.random.randint(3) # explore
      else:
        action = np.argmax(q_est) # exploit
      reward = np.random.normal(q_true[action], 1) # sample reward with noise
      N[action]+=1
      q_est[action] = q_est[action] + (reward-q_est[action])/N[action]
      rewards.append(reward)
    return rewards, q_est


epsilons = [0, 0.1, 0.01]
all_rewards = []
for eps in epsilons:
  rewards,_ = run_bandit_simulation(eps)
  all_rewards.append(rewards)

plt.figure(figsize=(10, 6))
for i, rewards in enumerate(all_rewards):
    plt.plot(range(len(rewards)), np.cumsum(rewards), label=f'Îµ = {epsilons[i]}')
plt.xlabel("Passos de Tempo")
plt.ylabel("Recompensa Cumulativa")
plt.title("ComparaÃ§Ã£o Îµ-greedy: Recompensa Cumulativa")
plt.legend()
plt.grid(True)
plt.show()

```
O grÃ¡fico demonstra que o mÃ©todo guloso (Îµ=0) apresenta um crescimento lento da recompensa cumulativa e fica preso em um Ã³timo local. O mÃ©todo Îµ=0.1 explora mais no inÃ­cio e rapidamente encontra um melhor padrÃ£o de recompensa. JÃ¡ o mÃ©todo Îµ=0.01 tem um desempenho inicial um pouco pior, mas converge para um valor melhor de recompensa total com mais tempo.

### ConclusÃ£o
O estudo do *10-armed testbed* ilustra claramente a importÃ¢ncia do equilÃ­brio entre exploraÃ§Ã£o e explotaÃ§Ã£o em aprendizado por reforÃ§o [^1]. O mÃ©todo *greedy*, embora simples e eficaz em situaÃ§Ãµes determinÃ­sticas onde a informaÃ§Ã£o Ã© perfeita desde o inÃ­cio, nÃ£o consegue atingir o desempenho mÃ¡ximo em ambientes incertos e exploratÃ³rios [^6]. Os mÃ©todos *-greedy*, ao introduzirem uma pequena probabilidade de exploraÃ§Ã£o aleatÃ³ria, conseguem escapar dos Ã³timos locais e melhorar seu desempenho a longo prazo [^5]. Isso destaca a necessidade de estratÃ©gias que permitam ao agente nÃ£o apenas explorar aÃ§Ãµes de alto valor esperado, mas tambÃ©m buscar ativamente novas opÃ§Ãµes com potencial de recompensa superior. Em problemas de aprendizado por reforÃ§o, especialmente naqueles nÃ£o estacionÃ¡rios, a exploraÃ§Ã£o Ã© um componente vital para o sucesso [^6].
**CorolÃ¡rio 1.1** A escolha do valor ideal de Îµ nos mÃ©todos *Îµ-greedy* Ã© um problema complexo e nÃ£o possui uma soluÃ§Ã£o universal, sendo dependente do ambiente especÃ­fico e dos objetivos de aprendizado, exigindo muitas vezes a experimentaÃ§Ã£o e otimizaÃ§Ã£o do hiperparÃ¢metro.
```mermaid
graph LR
    A["Ambiente"] --> B{"Escolher Îµ"}
    B --> |"Îµ adequado"| C["Desempenho Ideal"]
    B --> |"Îµ inadequado"| D["Desempenho Sub-Ã“timo"]
    C --> E("OtimizaÃ§Ã£o")
    D --> E
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
     style D fill:#ff9,stroke:#333,stroke-width:2px
    style E fill:#afa,stroke:#333,stroke-width:2px
```

### ReferÃªncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^3]: "If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^4]: "To roughly assess the relative effectiveness of the greedy and É›-greedy action-value methods, we compared them numerically on a suite of test problems. This was a set of 2000 randomly generated k-armed bandit problems with k = 10." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^5]: "Figure 2.2 compares a greedy method with two É›-greedy methods (Îµ=0.01 and Îµ=0.1), as described above, on the 10-armed testbed." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^6]: "The advantage of É›-greedy over greedy methods depends on the task. For example, suppose the reward variance had been larger, say 10 instead of 1." *(Trecho de Chapter 2: Multi-armed Bandits)*
