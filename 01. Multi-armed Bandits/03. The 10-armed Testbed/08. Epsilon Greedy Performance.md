## O Testbed de 10-BraÃ§os e o Comportamento Îµ-Ganancioso

### IntroduÃ§Ã£o

O estudo de *multi-armed bandits* (bandidos de mÃºltiplos braÃ§os) fornece uma base para a compreensÃ£o do aprendizado por reforÃ§o, particularmente no que diz respeito ao *trade-off* entre exploraÃ§Ã£o e explotaÃ§Ã£o [^1]. No contexto de *reinforcement learning*, a distinÃ§Ã£o crucial Ã© que o aprendizado ocorre por meio da avaliaÃ§Ã£o das aÃ§Ãµes tomadas, e nÃ£o pela instruÃ§Ã£o direta sobre as aÃ§Ãµes corretas [^1]. MÃ©todos que exploram, ou seja, tentam novas aÃ§Ãµes, podem levar a melhores resultados no longo prazo, embora reduzam a recompensa imediata [^2]. Para demonstrar o impacto da exploraÃ§Ã£o, o *10-armed testbed* (banco de testes de 10 braÃ§os) Ã© utilizado como um exemplo prÃ¡tico. Este capÃ­tulo se aprofundarÃ¡ na anÃ¡lise deste testbed e nas caracterÃ­sticas dos mÃ©todos Îµ-gananciosos.

### Conceitos Fundamentais

O *k-armed bandit problem* (problema do bandido de k braÃ§os) consiste em escolher repetidamente entre *k* diferentes opÃ§Ãµes, ou aÃ§Ãµes, cada uma com uma distribuiÃ§Ã£o de probabilidade estacionÃ¡ria associada, resultando em uma recompensa numÃ©rica. O objetivo Ã© maximizar a recompensa total esperada ao longo de um perÃ­odo [^1]. O *10-armed testbed* Ã© uma instancia desse problema com *k = 10*, onde cada aÃ§Ã£o *a* tem um valor verdadeiro associado, $q_*(a)$, definido como o valor esperado da recompensa quando a aÃ§Ã£o *a* Ã© selecionada:

$$q_*(a) = \mathbb{E}[R_t | A_t = a]$$

onde $A_t$ Ã© a aÃ§Ã£o selecionada no tempo *t* e $R_t$ Ã© a recompensa correspondente [^2]. Esses valores sÃ£o amostrados de uma distribuiÃ§Ã£o normal com mÃ©dia 0 e variÃ¢ncia 1. As recompensas reais tambÃ©m sÃ£o amostradas de uma distribuiÃ§Ã£o normal com mÃ©dia $q_*(a)$ e variÃ¢ncia 1 [^4].

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos supor que, para o braÃ§o 1 (a=1), o valor verdadeiro $q_*(1)$ seja 0.5. Isso significa que, em mÃ©dia, ao escolher o braÃ§o 1, esperamos receber uma recompensa de 0.5. No entanto, cada vez que selecionamos este braÃ§o, a recompensa real $R_t$ serÃ¡ sorteada de uma distribuiÃ§Ã£o normal com mÃ©dia 0.5 e variÃ¢ncia 1, entÃ£o poderÃ­amos obter valores como 1.2, -0.1, 0.8, etc. De forma semelhante, o braÃ§o 2 (a=2) poderia ter $q_*(2) = -0.2$ e, ao selecionÃ¡-lo, obterÃ­amos recompensas como -0.5, 0.3, -1.1, e assim por diante. Cada braÃ§o tem seu prÃ³prio valor esperado $q_*(a)$ e sua distribuiÃ§Ã£o de recompensas correspondente.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> import seaborn as sns
>
> np.random.seed(42) # Para reprodutibilidade
>
> # Valores verdadeiros dos 10 braÃ§os
> q_star = np.random.normal(0, 1, 10)
>
> # Amostras de recompensas para alguns braÃ§os
> arm1_rewards = np.random.normal(q_star[0], 1, 100) # 100 recompensas para o braÃ§o 1
> arm2_rewards = np.random.normal(q_star[1], 1, 100) # 100 recompensas para o braÃ§o 2
>
> # VisualizaÃ§Ã£o da distribuiÃ§Ã£o das recompensas
> sns.kdeplot(arm1_rewards, label=f"BraÃ§o 1 (q*={q_star[0]:.2f})")
> sns.kdeplot(arm2_rewards, label=f"BraÃ§o 2 (q*={q_star[1]:.2f})")
> plt.xlabel("Recompensa")
> plt.ylabel("Densidade")
> plt.title("DistribuiÃ§Ã£o das Recompensas para Dois BraÃ§os")
> plt.legend()
> plt.show()
> ```
> Este cÃ³digo gera um grÃ¡fico mostrando a distribuiÃ§Ã£o de recompensas para dois braÃ§os, demonstrando visualmente que, embora os valores esperados sejam diferentes ($q_*(a)$), as recompensas reais variam de acordo com a distribuiÃ§Ã£o normal.

**ProposiÃ§Ã£o 1.** *A distribuiÃ§Ã£o das recompensas $R_t$ dado uma aÃ§Ã£o $A_t = a$ Ã© uma distribuiÃ§Ã£o normal com mÃ©dia $q_*(a)$ e variÃ¢ncia 1, ou seja, $R_t | A_t = a \sim \mathcal{N}(q_*(a), 1)$.*

*Esta proposiÃ§Ã£o formaliza a descriÃ§Ã£o das recompensas como amostras de uma distribuiÃ§Ã£o normal centrada no valor verdadeiro da aÃ§Ã£o, estabelecendo uma base para anÃ¡lise estatÃ­stica subsequente.*

```mermaid
graph LR
    A("AÃ§Ã£o a") --> B("Valor Verdadeiro q*(a)");
    B --> C("DistribuiÃ§Ã£o Normal N(q*(a), 1)");
    C --> D("Recompensa Rt");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#aaf,stroke:#333,stroke-width:2px
    style D fill:#a0f,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3 stroke:black,stroke-width:2px;
```

Os mÃ©todos de *action-value* (valor da aÃ§Ã£o) estimam os valores das aÃ§Ãµes e utilizam essas estimativas para tomar decisÃµes sobre a seleÃ§Ã£o das aÃ§Ãµes. Uma forma comum de estimar o valor de uma aÃ§Ã£o Ã© atravÃ©s da mÃ©dia das recompensas obtidas ao selecionar essa aÃ§Ã£o:

$$Q_t(a) = \frac{\text{soma das recompensas quando } a \text{ foi tomada antes de } t}{\text{nÃºmero de vezes que } a \text{ foi tomada antes de } t} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}$$

onde $\mathbb{1}_{\text{predicate}}$ Ã© igual a 1 se *predicate* for verdadeiro, e 0 caso contrÃ¡rio [^3].

> ðŸ’¡ **Exemplo NumÃ©rico:**  Imagine que escolhemos o braÃ§o 1 (a=1) trÃªs vezes. As recompensas obtidas foram $R_1 = 0.8$, $R_2 = 0.2$, e $R_3 = 1.1$.  O valor estimado para o braÃ§o 1 apÃ³s essas trÃªs seleÃ§Ãµes seria:
>
> $$Q_4(1) = \frac{0.8 + 0.2 + 1.1}{3} = \frac{2.1}{3} = 0.7$$
>
> Se na prÃ³xima iteraÃ§Ã£o escolhermos novamente o braÃ§o 1 e obtivermos uma recompensa de $R_4 = 0.5$, o valor estimado seria atualizado:
>
> $$Q_5(1) = \frac{0.8 + 0.2 + 1.1 + 0.5}{4} = \frac{2.6}{4} = 0.65$$
>
> A estimativa $Q_t(a)$ se ajusta conforme obtemos mais informaÃ§Ãµes.
>
> ```python
> rewards_arm1 = [0.8, 0.2, 1.1, 0.5]
> q_estimates = np.cumsum(rewards_arm1) / np.arange(1, len(rewards_arm1) + 1)
> print("Estimativas de Q(1) ao longo do tempo:", q_estimates)
> ```

**Lema 1.** *A estimativa $Q_t(a)$ converge para $q_*(a)$ quando o nÃºmero de vezes que a aÃ§Ã£o 'a' Ã© tomada tende ao infinito, assumindo que as recompensas sÃ£o limitadas. Este resultado Ã© uma consequÃªncia da lei dos grandes nÃºmeros.*

*Este lema fornece uma garantia teÃ³rica de que as estimativas de valor da aÃ§Ã£o convergem para os valores verdadeiros, essencial para a eficÃ¡cia dos mÃ©todos de aprendizado por reforÃ§o baseados em estimativas de valor.*

```mermaid
graph LR
    A("AÃ§Ã£o a") --> B("Recompensas Obtidas");
    B --> C("Soma das Recompensas");
    B --> D("NÃºmero de vezes que 'a' foi tomada");
    C --> E("Qt(a) = Soma das Recompensas / NÃºmero de vezes");
    D --> E
    E --> F("ConvergÃªncia para q*(a)");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#aaf,stroke:#333,stroke-width:2px
    style D fill:#a0f,stroke:#333,stroke-width:2px
    style E fill:#afa,stroke:#333,stroke-width:2px
     style F fill:#aff,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3,4,5 stroke:black,stroke-width:2px;
```

Os mÃ©todos Îµ-gananciosos sÃ£o uma abordagem simples para equilibrar exploraÃ§Ã£o e explotaÃ§Ã£o. Nesses mÃ©todos, a aÃ§Ã£o com o maior valor estimado (aÃ§Ã£o *greedy*) Ã© selecionada com probabilidade $1-\epsilon$, e uma aÃ§Ã£o aleatÃ³ria Ã© selecionada com probabilidade Îµ [^3]. O parÃ¢metro Îµ controla a frequÃªncia da exploraÃ§Ã£o: valores mais altos de Îµ implicam maior exploraÃ§Ã£o. O texto enfatiza que, ao analisar o comportamento dos mÃ©todos Îµ-gananciosos, Ã© possÃ­vel observar um *trade-off* entre a velocidade de descoberta de aÃ§Ãµes Ã³timas e o desempenho no longo prazo [^6].

> ðŸ’¡ **Exemplo NumÃ©rico:**  Suponha que, apÃ³s algumas iteraÃ§Ãµes, tenhamos as seguintes estimativas de valor para 3 dos 10 braÃ§os: $Q_t(1) = 0.6$, $Q_t(2) = 0.8$, e $Q_t(3) = 0.5$. Se utilizarmos um mÃ©todo Îµ-ganancioso com $\epsilon = 0.2$, a probabilidade de escolher o braÃ§o 2 (o de maior valor) Ã© $1 - 0.2 = 0.8$. HÃ¡ tambÃ©m uma probabilidade de $0.2$ de escolher aleatoriamente um dos 10 braÃ§os, incluindo o braÃ§o 2, mas com uma chance menor. Portanto, a cada iteraÃ§Ã£o, 80% das vezes escolheremos o braÃ§o 2 e 20% das vezes escolheremos um braÃ§o aleatoriamente. Esta aleatoriedade garante que outros braÃ§os tambÃ©m sejam explorados, permitindo que a gente descubra um braÃ§o melhor.
>
> ```python
> def epsilon_greedy_action(q_values, epsilon):
>    if np.random.rand() < epsilon:
>        return np.random.randint(0, len(q_values))  # AÃ§Ã£o aleatÃ³ria
>    else:
>        return np.argmax(q_values) # AÃ§Ã£o greedy
>
> q_values = [0.6, 0.8, 0.5]  # Estimativas de valor para os braÃ§os 1, 2 e 3
> epsilon = 0.2
>
> actions = [epsilon_greedy_action(q_values, epsilon) for _ in range(100)]
> unique_actions, counts = np.unique(actions, return_counts=True)
> for action, count in zip(unique_actions, counts):
>    print(f"BraÃ§o {action + 1} escolhido {count} vezes")
>
> ```
> Este cÃ³digo simula a seleÃ§Ã£o de aÃ§Ãµes por 100 iteraÃ§Ãµes com um mÃ©todo $\epsilon$-ganancioso com $\epsilon=0.2$, mostrando a frequÃªncia com que cada braÃ§o Ã© selecionado. Note que, em mÃ©dia, o braÃ§o 2 (de maior valor) Ã© selecionado com mais frequÃªncia.

**DefiniÃ§Ã£o 1.** *Uma polÃ­tica $\pi$ Ã© dita $\epsilon$-gananciosa se, para cada estado $s \in \mathcal{S}$, a aÃ§Ã£o $a$ Ã© escolhida seguindo:*

$$\pi(a|s) = \begin{cases} 1 - \epsilon + \frac{\epsilon}{|\mathcal{A}(s)|} & \text{se } a = \arg\max_{a' \in \mathcal{A}(s)} Q(s, a') \\ \frac{\epsilon}{|\mathcal{A}(s)|} & \text{caso contrÃ¡rio} \end{cases}$$

*onde $\mathcal{A}(s)$ Ã© o conjunto de aÃ§Ãµes disponÃ­veis no estado $s$, e $|\mathcal{A}(s)|$ denota sua cardinalidade. Esta definiÃ§Ã£o formaliza a probabilidade de seleÃ§Ã£o de aÃ§Ãµes nos mÃ©todos $\epsilon$-gananciosos.*

```mermaid
graph LR
    A("Estado s") --> B("Conjunto de AÃ§Ãµes A(s)");
    B --> C("Valores Q(s, a')");
    C --> D{"argmax Q(s, a')"};
     D --> E("AÃ§Ã£o Greedy com prob. 1-Îµ + Îµ/|A(s)|");
     B --> F("AÃ§Ãµes NÃ£o-Greedy com prob. Îµ/|A(s)|");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#aaf,stroke:#333,stroke-width:2px
    style D fill:#a0f,stroke:#333,stroke-width:2px
    style E fill:#afa,stroke:#333,stroke-width:2px
     style F fill:#aff,stroke:#333,stroke-width:2px
        linkStyle 0,1,2,3,4 stroke:black,stroke-width:2px;
```

### AnÃ¡lise do Comportamento Îµ-Ganancioso no Testbed

No *10-armed testbed*, o desempenho dos mÃ©todos gananciosos e Îµ-gananciosos Ã© comparado empiricamente. A Figura 2.2 no contexto [^5] demonstra que o mÃ©todo puramente ganancioso inicialmente melhora mais rapidamente, mas estagna em um nÃ­vel de recompensa inferior, pois fica preso em aÃ§Ãµes subÃ³timas [^5]. Os mÃ©todos Îµ-gananciosos, por outro lado, exploram o espaÃ§o de aÃ§Ã£o, melhorando as chances de encontrar a aÃ§Ã£o ideal [^6].

Especificamente, o texto aborda que o mÃ©todo Îµ-ganancioso com Îµ=0.1 explora mais e geralmente encontra a aÃ§Ã£o Ã³tima mais rapidamente, mas nunca a seleciona mais que 91% das vezes. Em contraste, o mÃ©todo com Îµ=0.01 melhora mais lentamente, mas eventualmente supera o mÃ©todo com Îµ=0.1, nas mÃ©tricas de desempenho e probabilidade de seleÃ§Ã£o da aÃ§Ã£o Ã³tima, sugerindo que um valor menor de Îµ leva a uma exploraÃ§Ã£o menos frequente, mas mais refinada ao longo do tempo [^6]. Isso demonstra que:

> *Os mÃ©todos  Îµ-gananciosos eventualmente apresentam um desempenho melhor porque eles continuam explorando e melhorando suas chances de reconhecer a aÃ§Ã£o Ã³tima.* [^6]

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos simular um cenÃ¡rio onde temos 10 braÃ§os, onde um dos braÃ§os tem um valor esperado de 1.5 e os outros tÃªm um valor de 0.5. Para um mÃ©todo puramente ganancioso ($\epsilon=0$), uma vez que ele encontrar um braÃ§o com um valor de recompensa razoavelmente bom, ele nÃ£o explorarÃ¡ mais. Isso significa que, se um braÃ§o com valor de 0.5 for escolhido cedo e der uma recompensa relativamente alta, ele vai "prender" naquele braÃ§o, pois ele nÃ£o tem mecanismo de exploraÃ§Ã£o. Um mÃ©todo com $\epsilon = 0.1$ explorarÃ¡ com mais frequÃªncia, eventualmente encontrando o braÃ§o com recompensa de 1.5. No longo prazo, o mÃ©todo com $\epsilon = 0.1$ apresentarÃ¡ um desempenho melhor.
>
> ```python
> def simulate_bandit(q_star, num_steps, epsilon):
>    q_estimates = np.zeros(len(q_star))
>    counts = np.zeros(len(q_star))
>    rewards = []
>
>    for _ in range(num_steps):
>        if np.random.rand() < epsilon:
>            action = np.random.randint(0, len(q_star))
>        else:
>            action = np.argmax(q_estimates)
>
>        reward = np.random.normal(q_star[action], 1)
>        rewards.append(reward)
>        counts[action] += 1
>        q_estimates[action] = q_estimates[action] + (reward - q_estimates[action]) / counts[action]
>    return rewards
>
> num_arms = 10
> q_star_values = np.zeros(num_arms) + 0.5
> q_star_values[0] = 1.5
> num_steps = 1000
>
> rewards_greedy = simulate_bandit(q_star_values, num_steps, 0)
> rewards_epsilon_01 = simulate_bandit(q_star_values, num_steps, 0.1)
> rewards_epsilon_001 = simulate_bandit(q_star_values, num_steps, 0.01)
>
> plt.figure(figsize=(10, 6))
> plt.plot(np.cumsum(rewards_greedy) / np.arange(1, num_steps+1), label="Greedy (Îµ=0)")
> plt.plot(np.cumsum(rewards_epsilon_01) / np.arange(1, num_steps+1), label="Îµ-greedy (Îµ=0.1)")
> plt.plot(np.cumsum(rewards_epsilon_001) / np.arange(1, num_steps+1), label="Îµ-greedy (Îµ=0.01)")
> plt.xlabel("Steps")
> plt.ylabel("Average Reward")
> plt.title("Comparison of Epsilon-Greedy Methods")
> plt.legend()
> plt.show()
> ```
> Este cÃ³digo simula a execuÃ§Ã£o dos mÃ©todos gananciosos e Îµ-gananciosos em um ambiente simples, demonstrando que os mÃ©todos Îµ-gananciosos tÃªm melhor desempenho no longo prazo.

**Teorema 1.** *Em um problema *k*-armed bandit, um mÃ©todo $\epsilon$-ganancioso com $\epsilon > 0$ garante que cada aÃ§Ã£o serÃ¡ explorada um nÃºmero infinito de vezes com probabilidade 1, se o nÃºmero de passos for infinito. Isso implica que a estimativa de valor de cada aÃ§Ã£o convergiria eventualmente para o seu valor verdadeiro.*

*Prova: A cada passo, existe uma probabilidade $\epsilon/k > 0$ de qualquer aÃ§Ã£o ser escolhida. Como o nÃºmero de passos tende ao infinito, o nÃºmero de vezes que cada aÃ§Ã£o Ã© selecionada tambÃ©m tende ao infinito, o que Ã© suficiente para a convergÃªncia da estimativa do valor de cada aÃ§Ã£o. Este resultado demonstra formalmente a garantia de exploraÃ§Ã£o dos mÃ©todos $\epsilon$-gananciosos.*

```mermaid
graph LR
    A("Îµ > 0") --> B("Probabilidade Îµ/k de cada aÃ§Ã£o");
    B --> C("NÃºmero de passos â†’ âˆž");
    C --> D("Cada aÃ§Ã£o explorada âˆž vezes");
    D --> E("Qt(a) â†’ q*(a)");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#aaf,stroke:#333,stroke-width:2px
    style D fill:#a0f,stroke:#333,stroke-width:2px
     style E fill:#afa,stroke:#333,stroke-width:2px
        linkStyle 0,1,2,3 stroke:black,stroke-width:2px;
```

Ã‰ importante notar que a vantagem dos mÃ©todos Îµ-gananciosos depende da natureza da tarefa. Em ambientes ruidosos, onde a variÃ¢ncia das recompensas Ã© maior, Ã© necessÃ¡rio mais exploraÃ§Ã£o para encontrar a aÃ§Ã£o Ã³tima, tornando os mÃ©todos Îµ-gananciosos ainda mais eficazes em relaÃ§Ã£o aos mÃ©todos puramente gananciosos [^6]. Contudo, em cenÃ¡rios determinÃ­sticos, onde a variÃ¢ncia Ã© zero, o mÃ©todo ganancioso poderia apresentar o melhor desempenho, pois ele identificaria rapidamente a aÃ§Ã£o Ã³tima, sem precisar explorar [^6].

> ðŸ’¡ **Exemplo NumÃ©rico:** Em um ambiente onde as recompensas sÃ£o determinÃ­sticas (sem ruÃ­do), o braÃ§o com a maior recompensa sempre entregarÃ¡ aquela recompensa. O mÃ©todo ganancioso, se tiver sorte de selecionar o braÃ§o certo em um dos primeiros passos, rapidamente irÃ¡ convergir para o braÃ§o correto, sem necessidade de explorar.
>
> ```python
> def deterministic_bandit(q_star, num_steps, epsilon):
>    q_estimates = np.zeros(len(q_star))
>    counts = np.zeros(len(q_star))
>    rewards = []
>
>    for _ in range(num_steps):
>        if np.random.rand() < epsilon:
>             action = np.random.randint(0, len(q_star))
>        else:
>            action = np.argmax(q_estimates)
>        reward = q_star[action] # Recompensa determinÃ­stica
>        rewards.append(reward)
>        counts[action] += 1
>        q_estimates[action] = q_estimates[action] + (reward - q_estimates[action]) / counts[action]
>    return rewards
>
> num_arms = 10
> q_star_values = np.zeros(num_arms)
> q_star_values[0] = 1.5
> num_steps = 1000
>
> rewards_greedy = deterministic_bandit(q_star_values, num_steps, 0)
> rewards_epsilon_01 = deterministic_bandit(q_star_values, num_steps, 0.1)
>
> plt.figure(figsize=(10, 6))
> plt.plot(np.cumsum(rewards_greedy) / np.arange(1, num_steps+1), label="Greedy (Îµ=0)")
> plt.plot(np.cumsum(rewards_epsilon_01) / np.arange(1, num_steps+1), label="Îµ-greedy (Îµ=0.1)")
> plt.xlabel("Steps")
> plt.ylabel("Average Reward")
> plt.title("Comparison in a Deterministic Environment")
> plt.legend()
> plt.show()
>
> ```
> Este exemplo demonstra como, em ambientes determinÃ­sticos, o mÃ©todo ganancioso pode convergir mais rÃ¡pido, jÃ¡ que ele nÃ£o tem o custo de exploraÃ§Ã£o.

**Lema 1.1.** *Se o ambiente Ã© determinÃ­stico, i.e., a variÃ¢ncia das recompensas Ã© zero, um mÃ©todo puramente guloso converge para a aÃ§Ã£o Ã³tima mais rapidamente do que um mÃ©todo $\epsilon$-ganancioso com $\epsilon > 0$.*

*Este lema formaliza que em ambientes determinÃ­sticos, a exploraÃ§Ã£o introduzida pelo $\epsilon$ Ã© desnecessÃ¡ria, e um mÃ©todo ganancioso converge mais rÃ¡pido.*

```mermaid
graph LR
    A("Ambiente DeterminÃ­stico") --> B("VariÃ¢ncia da Recompensa = 0");
    B --> C("MÃ©todo Guloso");
    C --> D("ConvergÃªncia RÃ¡pida");
    A --> E("MÃ©todo Îµ-Ganancioso");
    E --> F("ExploraÃ§Ã£o DesnecessÃ¡ria");
    F --> G("ConvergÃªncia Mais Lenta");
    D -.-> G;
        style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#aaf,stroke:#333,stroke-width:2px
    style D fill:#a0f,stroke:#333,stroke-width:2px
       style E fill:#aff,stroke:#333,stroke-width:2px
     style F fill:#afa,stroke:#333,stroke-width:2px
    style G fill:#aea,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3,4,5,6 stroke:black,stroke-width:2px;
```

Contudo, em ambientes nÃ£o-estacionÃ¡rios, onde as aÃ§Ãµes Ã³timas podem variar com o tempo, a exploraÃ§Ã£o torna-se essencial, mesmo em cenÃ¡rios determinÃ­sticos [^6]. Esta observaÃ§Ã£o destaca a importÃ¢ncia de um equilÃ­brio adequado entre exploraÃ§Ã£o e exploraÃ§Ã£o no aprendizado por reforÃ§o.

**Teorema 1.1** *Em ambientes nÃ£o-estacionÃ¡rios, um mÃ©todo $\epsilon$-ganancioso com um valor de $\epsilon$ suficientemente grande pode se adaptar Ã s mudanÃ§as do ambiente mais rapidamente do que um mÃ©todo puramente ganancioso. Contudo, um valor muito grande de $\epsilon$ pode dificultar a convergÃªncia para a aÃ§Ã£o Ã³tima.*

*Este teorema enfatiza a importÃ¢ncia da exploraÃ§Ã£o em ambientes nÃ£o-estacionÃ¡rios e a necessidade de um ajuste adequado do parÃ¢metro $\epsilon$ para obter um desempenho ideal.*
```mermaid
graph LR
    A("Ambiente NÃ£o-EstacionÃ¡rio") --> B("AÃ§Ãµes Ã“timas Variam");
    B --> C("MÃ©todo Îµ-Ganancioso (Îµ Suficientemente Grande)");
    C --> D("AdaptaÃ§Ã£o RÃ¡pida Ã s MudanÃ§as");
    A --> E("MÃ©todo Guloso");
    E --> F("AdaptaÃ§Ã£o Lenta Ã s MudanÃ§as");
    C --> G("Îµ Muito Grande");
    G --> H("Dificulta a ConvergÃªncia");
        style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#aaf,stroke:#333,stroke-width:2px
    style D fill:#a0f,stroke:#333,stroke-width:2px
       style E fill:#aff,stroke:#333,stroke-width:2px
     style F fill:#afa,stroke:#333,stroke-width:2px
    style G fill:#aea,stroke:#333,stroke-width:2px
        style H fill:#fea,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3,4,5,6 stroke:black,stroke-width:2px;
```

### ConclusÃ£o

A anÃ¡lise do *10-armed testbed* ilustra o compromisso entre exploraÃ§Ã£o e exploraÃ§Ã£o no aprendizado por reforÃ§o. Os mÃ©todos Îµ-gananciosos demonstram que valores maiores de Îµ levam a uma exploraÃ§Ã£o inicial mais rÃ¡pida, mas a uma seleÃ§Ã£o menos frequente da aÃ§Ã£o Ã³tima no longo prazo [^6]. JÃ¡, valores menores de Îµ levam a uma exploraÃ§Ã£o mais lenta, mas a um melhor desempenho a longo prazo. A escolha do valor de Îµ, portanto, Ã© crucial e deve ser adaptada Ã  tarefa especÃ­fica. Este estudo fornece *insights* valiosos sobre as caracterÃ­sticas de diferentes estratÃ©gias de exploraÃ§Ã£o e destaca a importÃ¢ncia de escolher a abordagem correta para diferentes problemas de aprendizado por reforÃ§o.

**CorolÃ¡rio 1.** *O valor Ã³timo do parÃ¢metro Îµ em um problema *k*-armed bandit Ã© dependente das caracterÃ­sticas do ambiente, como o grau de ruÃ­do e o grau de nÃ£o estacionariedade. NÃ£o hÃ¡ um valor de Îµ que seja universalmente Ã³timo para todos os problemas.*

*Este corolÃ¡rio resume a necessidade de um ajuste cuidadoso do hiperparÃ¢metro Îµ, dependendo das caracterÃ­sticas do ambiente.*
```mermaid
graph LR
    A("Problema k-Armed Bandit") --> B("Valor Ã“timo de Îµ");
    B --> C("CaracterÃ­sticas do Ambiente");
    C --> D("Grau de RuÃ­do");
    C --> E("Grau de NÃ£o Estacionaridade");
    B --> F("NÃ£o hÃ¡ Îµ Universalmente Ã“timo");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#aaf,stroke:#333,stroke-width:2px
    style D fill:#a0f,stroke:#333,stroke-width:2px
    style E fill:#afa,stroke:#333,stroke-width:2px
     style F fill:#aea,stroke:#333,stroke-width:2px
    linkStyle 0,1,2 stroke:black,stroke-width:2px;
```

### ReferÃªncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Multi-armed Bandits)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action." *(Trecho de Multi-armed Bandits)*
[^3]: "The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section." *(Trecho de Multi-armed Bandits)*
[^4]: "Then, when a learning method applied to that problem selected action At at time step t, the actual reward, Rt, was selected from a normal distribution with mean q*(At) and variance 1." *(Trecho de Multi-armed Bandits)*
[^5]: "Figure 2.2 compares a greedy method with two Îµ-greedy methods (Îµ=0.01 and Îµ=0.1), as described above, on the 10-armed testbed." *(Trecho de Multi-armed Bandits)*
[^6]: "The Îµ = 0.1 method explored more, and usually found the optimal action earlier, but it never selected that action more than 91% of the time." *(Trecho de Multi-armed Bandits)*
