## Gradient Bandit Algorithms com Baseline

### Introdu√ß√£o
No contexto do aprendizado por refor√ßo, os **algoritmos bandit** s√£o modelos que lidam com o dilema de explora√ß√£o versus explota√ß√£o em ambientes simples, onde as a√ß√µes t√™m efeitos isolados e imediatos [1]. Em vez de estimar os valores das a√ß√µes, como nos m√©todos action-value, os algoritmos gradient bandit aprendem prefer√™ncias num√©ricas para cada a√ß√£o. Estas prefer√™ncias indicam a frequ√™ncia com que cada a√ß√£o deve ser selecionada, sem estabelecer uma rela√ß√£o direta com a recompensa esperada. Essas prefer√™ncias s√£o ent√£o usadas para determinar as probabilidades de sele√ß√£o de a√ß√£o atrav√©s de uma distribui√ß√£o softmax [1], um tipo de distribui√ß√£o probabil√≠stica que aloca probabilidades maiores para a√ß√µes com prefer√™ncias mais elevadas. A principal distin√ß√£o desse m√©todo √© que ele n√£o estima valores de a√ß√£o, mas sim aprende prefer√™ncias, usando estas para tomar decis√µes [1].

### Conceitos Fundamentais
O cerne dos algoritmos gradient bandit reside na aprendizagem das **prefer√™ncias de a√ß√£o** $H_t(a) \in \mathbb{R}$ [1]. Estas prefer√™ncias s√£o atualizadas atrav√©s de um m√©todo de **ascens√£o de gradiente estoc√°stico**. As probabilidades de sele√ß√£o de a√ß√£o s√£o dadas pela distribui√ß√£o softmax:
$$
\Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)
$$
onde $\pi_t(a)$ denota a probabilidade de selecionar a a√ß√£o $a$ no tempo $t$, e $k$ √© o n√∫mero total de a√ß√µes [1]. As prefer√™ncias de a√ß√£o s√£o inicialmente configuradas como iguais para todas as a√ß√µes, por exemplo, $H_1(a) = 0$ para todo $a$, garantindo que todas as a√ß√µes tenham inicialmente a mesma probabilidade de serem selecionadas [1]. As prefer√™ncias de a√ß√£o s√£o atualizadas com base na recompensa recebida ap√≥s a sele√ß√£o da a√ß√£o, e um termo chamado **baseline**. Esta atualiza√ß√£o √© dada por:
```mermaid
flowchart LR
    A[A√ß√£o A_t Selecionada] -->|Recompensa R_t| B(Atualizar Prefer√™ncias H_t(a));
    B --> C{H_t+1(A_t) = H_t(A_t) + alpha * (R_t - R_bar_t) * (1 - pi_t(A_t))};
    B --> D{H_t+1(a) = H_t(a) - alpha * (R_t - R_bar_t) * pi_t(a), para a != A_t};
    C --> E(Nova Prefer√™ncia H_t+1(A_t));
    D --> F(Novas Prefer√™ncias H_t+1(a), para a != A_t);
    E --> G(Pr√≥xima Itera√ß√£o);
    F --> G;
    style C fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
```
onde $\alpha > 0$ √© o par√¢metro step-size, $R_t$ √© a recompensa recebida ap√≥s selecionar a a√ß√£o $A_t$, e $\bar{R}_t$ √© a m√©dia das recompensas at√© o instante $t$ [1]. Esta m√©dia atua como um baseline. O baseline √© importante pois ele auxilia a dar sentido ao valor de $R_t$. Se a recompensa $R_t$ √© maior que o baseline $\bar{R}_t$, a probabilidade de selecionar a a√ß√£o $A_t$ √© aumentada; caso contr√°rio, ela √© diminu√≠da. As a√ß√µes n√£o selecionadas s√£o ajustadas na dire√ß√£o oposta [1].

> üí° **Exemplo Num√©rico:** Vamos considerar um problema com 3 a√ß√µes ($k=3$) e um step-size $\alpha = 0.1$. Inicialmente, as prefer√™ncias s√£o $H_1(a_1) = H_1(a_2) = H_1(a_3) = 0$, o que implica probabilidades iniciais $\pi_1(a_1) = \pi_1(a_2) = \pi_1(a_3) = 1/3$. No tempo $t=1$, a a√ß√£o $a_2$ √© selecionada (i.e., $A_1 = a_2$) e recebe uma recompensa $R_1 = 1$. Como este √© o primeiro passo, o baseline $\bar{R}_1 = 0$. A atualiza√ß√£o das prefer√™ncias √©:
>
> $H_{2}(a_2) = H_1(a_2) + \alpha(R_1 - \bar{R}_1)(1 - \pi_1(a_2)) = 0 + 0.1(1 - 0)(1 - 1/3) = 0.0667$
>
> $H_{2}(a_1) = H_1(a_1) - \alpha(R_1 - \bar{R}_1)\pi_1(a_1) = 0 - 0.1(1 - 0)(1/3) = -0.0333$
>
> $H_{2}(a_3) = H_1(a_3) - \alpha(R_1 - \bar{R}_1)\pi_1(a_3) = 0 - 0.1(1 - 0)(1/3) = -0.0333$
>
> As novas prefer√™ncias s√£o $H_2(a_1) = -0.0333$, $H_2(a_2) = 0.0667$, e $H_2(a_3) = -0.0333$. As novas probabilidades podem ser calculadas atrav√©s da distribui√ß√£o softmax.

**Lema 1:** *O uso da fun√ß√£o softmax para obter as probabilidades de a√ß√µes √© equivalente √† fun√ß√£o log√≠stica, ou sigmoide, no caso de duas a√ß√µes*.

*Prova:* Considere um cen√°rio com duas a√ß√µes, $a_1$ e $a_2$. De acordo com a distribui√ß√£o softmax, temos que:
$$\pi_t(a_1) = \frac{e^{H_t(a_1)}}{e^{H_t(a_1)} + e^{H_t(a_2)}}$$
Dividindo o numerador e o denominador por $e^{H_t(a_1)}$, obtemos:
$$\pi_t(a_1) = \frac{1}{1 + e^{H_t(a_2) - H_t(a_1)}}$$
Se definirmos a diferen√ßa entre as prefer√™ncias como $z = H_t(a_2) - H_t(a_1)$, podemos reescrever a equa√ß√£o como:
$$\pi_t(a_1) = \frac{1}{1 + e^z}$$
Esta √© precisamente a forma da fun√ß√£o log√≠stica ou sigmoide, onde $z$ representa a diferen√ßa entre as prefer√™ncias das duas a√ß√µes. $\blacksquare$
```mermaid
flowchart LR
    A["Fun√ß√£o Softmax (k=2)"] --> B["pi_t(a_1) = e^(H_t(a_1)) / (e^(H_t(a_1)) + e^(H_t(a_2)))"];
    B --> C["Dividindo por e^(H_t(a_1))"];
    C --> D["pi_t(a_1) = 1 / (1 + e^(H_t(a_2) - H_t(a_1)))"];
    D --> E["z = H_t(a_2) - H_t(a_1)"];
    E --> F["pi_t(a_1) = 1 / (1 + e^z) = Fun√ß√£o Log√≠stica"];
    style F fill:#ccf,stroke:#333,stroke-width:2px
```
> üí° **Exemplo Num√©rico:** Se tivermos duas a√ß√µes com prefer√™ncias $H_t(a_1) = 1$ e $H_t(a_2) = 2$, ent√£o $z = H_t(a_2) - H_t(a_1) = 2 - 1 = 1$.  Assim, $\pi_t(a_1) = \frac{1}{1 + e^1} \approx 0.269$ e $\pi_t(a_2) = 1 - \pi_t(a_1) \approx 0.731$.  A a√ß√£o $a_2$ √© mais prov√°vel de ser escolhida, devido √† sua maior prefer√™ncia, e a probabilidade √© calculada exatamente pela fun√ß√£o log√≠stica.

**Lema 1.1:** *A atualiza√ß√£o das prefer√™ncias de a√ß√£o pode ser expressa de forma mais concisa usando uma nota√ß√£o indicadora*.

*Prova:* Defina $I(A_t = a)$ como uma fun√ß√£o indicadora que retorna 1 se $A_t = a$ e 0 caso contr√°rio. Ent√£o a atualiza√ß√£o das prefer√™ncias de a√ß√£o pode ser escrita como:
$$ H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t)(I(A_t = a) - \pi_t(a)) $$
Esta express√£o condensa as duas atualiza√ß√µes em uma √∫nica f√≥rmula. Note que quando $a = A_t$, temos $I(A_t = a) = 1$ e obtemos a atualiza√ß√£o para a a√ß√£o selecionada, e quando $a \neq A_t$, temos $I(A_t = a) = 0$ e obtemos a atualiza√ß√£o para as demais a√ß√µes. $\blacksquare$
```mermaid
flowchart LR
    A["Nota√ß√£o Original para Atualiza√ß√£o de Prefer√™ncias"] --> B{ "H_{t+1}(A_t) = H_t(A_t) + alpha * (R_t - R_bar_t) * (1 - pi_t(A_t))" };
    A --> C{ "H_{t+1}(a) = H_t(a) - alpha * (R_t - R_bar_t) * pi_t(a), para a != A_t" };
    B --> D["Atualiza√ß√£o da a√ß√£o selecionada A_t"];
    C --> E["Atualiza√ß√£o das outras a√ß√µes a"];
    F["Fun√ß√£o Indicadora I(A_t = a)"] --> G{ "H_{t+1}(a) = H_t(a) + alpha * (R_t - R_bar_t) * (I(A_t = a) - pi_t(a))" };
    G --> H["Nota√ß√£o Concisa"];
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior com 3 a√ß√µes e ap√≥s a atualiza√ß√£o, no tempo t=2, temos $H_2(a_1) = -0.0333$, $H_2(a_2) = 0.0667$ e $H_2(a_3) = -0.0333$. Se no instante t=2, a a√ß√£o $a_3$ √© selecionada ($A_2 = a_3$) e recebe uma recompensa $R_2 = 0.5$, e vamos calcular o baseline atualizado como $\bar{R}_2 = (1+0.5)/2 = 0.75$, e as probabilidades de cada a√ß√£o atrav√©s da softmax, que s√£o, aproximadamente, $\pi_2(a_1) \approx 0.29, \pi_2(a_2) \approx 0.40, \pi_2(a_3) \approx 0.31$.  A atualiza√ß√£o da prefer√™ncia para a a√ß√£o $a_3$ usando a nota√ß√£o indicadora √©:
>
> $H_{3}(a_3) = H_2(a_3) + \alpha(R_2 - \bar{R}_2)(I(A_2 = a_3) - \pi_2(a_3)) = -0.0333 + 0.1(0.5 - 0.75)(1 - 0.31) = -0.0333 -0.01725 = -0.05055$
>
> Para as a√ß√µes n√£o selecionadas, $a_1$ e $a_2$:
>
> $H_{3}(a_1) = H_2(a_1) + \alpha(R_2 - \bar{R}_2)(I(A_2 = a_1) - \pi_2(a_1)) = -0.0333 + 0.1(0.5 - 0.75)(0 - 0.29) = -0.0333 + 0.00725 = -0.02605$
>
> $H_{3}(a_2) = H_2(a_2) + \alpha(R_2 - \bar{R}_2)(I(A_2 = a_2) - \pi_2(a_2)) = 0.0667 + 0.1(0.5 - 0.75)(0 - 0.40) = 0.0667 + 0.01000 = 0.0767$
>
> A atualiza√ß√£o usando a nota√ß√£o indicadora resulta nos mesmos valores que a nota√ß√£o original, mas de forma mais compacta e com menos c√°lculos repetitivos.

A import√¢ncia do baseline  $\bar{R}_t$ reside em seu papel como ponto de refer√™ncia para a recompensa. Se o baseline n√£o fosse usado (isto √©, $\bar{R}_t$ fosse igual a zero), o algoritmo ainda funcionaria como um m√©todo de ascens√£o de gradiente estoc√°stico [1], mas a converg√™ncia seria mais lenta. Isso ocorre porque o baseline serve para adaptar-se ao novo n√≠vel de recompensas, sendo que, sem ele, a performance do algoritmo se degradaria [1]. O uso do baseline n√£o afeta o valor esperado das atualiza√ß√µes, mas reduz a vari√¢ncia dessas atualiza√ß√µes [1].

> üí° **Exemplo Num√©rico:** Para ilustrar o efeito do baseline na vari√¢ncia, considere uma sequ√™ncia de recompensas onde todas as a√ß√µes tem valor esperado de recompensa 1.  Sem o baseline,  uma recompensa de 2  aumentaria a prefer√™ncia da a√ß√£o mesmo que essa fosse uma flutua√ß√£o aleat√≥ria.  Com um baseline (digamos 1), o efeito da mesma recompensa (2) na prefer√™ncia seria menor, pois seria $2 - 1 = 1$, indicando que esta recompensa √© apenas acima do esperado, e n√£o uma recompensa excepcional. O baseline age como uma normaliza√ß√£o, reduzindo o impacto de recompensas aleat√≥rias na prefer√™ncia de a√ß√£o.
```mermaid
flowchart LR
    A["Recompensa R_t"] --> B{ "Sem Baseline" };
    A --> C{ "Com Baseline R_bar_t" };
    B --> D["Aumento Direto da Prefer√™ncia"];
    C --> E["Aumento da Prefer√™ncia = R_t - R_bar_t"];
    D --> F["Maior Vari√¢ncia"];
    E --> G["Menor Vari√¢ncia"];
    H["R_bar_t = M√©dia das Recompensas"]
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

**Observa√ß√£o 1:** *A escolha do baseline como a m√©dia das recompensas anteriores √© apenas uma das op√ß√µes. Outros baselines, como a mediana das recompensas, ou uma m√©dia m√≥vel exponencial, podem ser utilizados, com impactos na converg√™ncia e estabilidade do algoritmo.* A m√©dia das recompensas √© uma op√ß√£o comum devido √† sua simplicidade e efic√°cia em muitos casos. No entanto, para ambientes n√£o estacion√°rios, um baseline que atribua maior import√¢ncia √†s recompensas mais recentes pode ser mais adequado, assim como os m√©todos de atualiza√ß√£o da m√©dia.

> üí° **Exemplo Num√©rico:** Suponha que em um ambiente n√£o estacion√°rio, as recompensas mudam abruptamente no tempo $t = 100$. Antes de $t=100$, a recompensa m√©dia era $\bar{R}_{99} = 1$.  Ap√≥s a mudan√ßa, as novas recompensas tem m√©dia 3.  Um baseline que usa uma m√©dia simples, levaria algum tempo at√© refletir essa mudan√ßa. Um baseline que usa uma m√©dia m√≥vel exponencial com um fator de decaimento $\lambda$ daria mais peso √†s recompensas mais recentes, permitindo que o baseline se adapte mais rapidamente a novas m√©dias, por exemplo, $\bar{R}_t = \lambda \bar{R}_{t-1} + (1-\lambda)R_t$, com $\lambda$ perto de 0.
```mermaid
flowchart LR
    A["Ambiente N√£o Estacion√°rio"] --> B["Mudan√ßa Abrupta de Recompensas"];
    B --> C{ "Baseline como M√©dia Simples" };
    B --> D{ "Baseline como M√©dia M√≥vel Exponencial" };
    C --> E["Adapta√ß√£o Lenta √† Mudan√ßa"];
    D --> F["Adapta√ß√£o R√°pida √† Mudan√ßa"];
    G["R_bar_t = lambda * R_bar_{t-1} + (1-lambda) * R_t"]
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

**Teorema 1:** *O algoritmo gradient bandit, com a atualiza√ß√£o de prefer√™ncia e o baseline como m√©dia das recompensas, converge para um √≥timo local sob certas condi√ß√µes de suavidade e step-size*.

*Prova (esbo√ßo):* A prova desse teorema geralmente envolve a an√°lise da converg√™ncia do processo de ascens√£o de gradiente estoc√°stico. Sob condi√ß√µes apropriadas (como um step-size $\alpha$ decrescente ao longo do tempo, satisfazendo as condi√ß√µes de Robbins-Monro, e uma fun√ß√£o de recompensa diferenci√°vel), pode-se mostrar que as prefer√™ncias de a√ß√£o convergem para um ponto onde o gradiente da recompensa esperada √© zero. Formalmente, este resultado requer ferramentas de an√°lise estoc√°stica e n√£o ser√° detalhado aqui. No entanto, a intui√ß√£o √© que o algoritmo ajusta iterativamente as prefer√™ncias na dire√ß√£o de recompensas maiores, at√© atingir um ponto estacion√°rio. $\blacksquare$

**Teorema 1.1:** *No caso de um ambiente estacion√°rio, a m√©dia das recompensas $\bar{R}_t$ converge para o valor esperado da recompensa sob a pol√≠tica √≥tima.*

*Prova (esbo√ßo):* No caso estacion√°rio, a recompensa m√©dia converge para um valor constante conforme $t$ tende ao infinito, dado que o algoritmo explora adequadamente todas as a√ß√µes. O baseline $\bar{R}_t$ converge para a recompensa m√©dia sob a pol√≠tica que o algoritmo segue. Com o tempo e a converg√™ncia da pol√≠tica para a √≥tima, a m√©dia das recompensas converge tamb√©m para a recompensa esperada sob a pol√≠tica √≥tima. A prova formal envolve o uso de leis dos grandes n√∫meros para processos estoc√°sticos. $\blacksquare$
```mermaid
flowchart LR
    A["Ambiente Estacion√°rio"] --> B["Algoritmo Explora A√ß√µes"];
    B --> C["M√©dia das Recompensas R_bar_t Converge"];
    C --> D["Pol√≠tica do Algoritmo Converge para √ìtima"];
    D --> E["R_bar_t Converge para Recompensa Esperada"];
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Suponha que ap√≥s um grande n√∫mero de itera√ß√µes, o algoritmo esteja em um estado em que sempre escolhe a a√ß√£o $a_1$, que tem recompensa m√©dia 2. O baseline $\bar{R}_t$ ir√° convergir para 2 com o tempo. Se de repente o algoritmo come√ßar a escolher outras a√ß√µes menos vantajosas (em uma fase de explora√ß√£o, por exemplo), o baseline tamb√©m se ajustar√° a este novo comportamento. Eventualmente, quando o algoritmo retornar para a a√ß√£o √≥tima $a_1$, o baseline voltar√° a convergir para o valor esperado da recompensa √≥tima, no caso, 2.

### Conclus√£o
Os gradient bandit algorithms representam uma abordagem alternativa aos m√©todos action-value, aprendendo diretamente as prefer√™ncias de a√ß√£o em vez de estimar os valores das a√ß√µes. A inclus√£o de um baseline na atualiza√ß√£o das prefer√™ncias de a√ß√£o, com a m√©dia das recompensas anteriores, √© importante para melhorar a taxa de converg√™ncia e a robustez do algoritmo. O baseline atua como um ponto de refer√™ncia adaptativo que ajuda o algoritmo a distinguir melhor as recompensas boas das ruins, acelerando o processo de aprendizagem. Esta abordagem √© especialmente √∫til em situa√ß√µes onde a interpreta√ß√£o direta das recompensas √© mais importante do que a predi√ß√£o de recompensas futuras.

### Refer√™ncias
[^1]: "So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. In this section we consider learning a numerical preference for each action a, which we denote $H_t(a) \in \mathbb{R}$. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:
$\Pr\{A_t=a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)$,
where here we have also introduced a useful new notation, $\pi_t(a)$, for the probability of taking action a at time t. Initially all action preferences are the same (e.g., $H_1(a) = 0$, for all a) so that all actions have an equal probability of being selected. There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action $A_t$ and receiving the reward $R_t$, the action preferences are updated by:
$H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t ‚Äì R_t) (1 ‚Äì \pi_t(A_t))$, and for all $a \neq A_t$,
$H_{t+1}(a) = H_t(a) ‚Äì \alpha(R_t ‚Äì R_t)\pi_t(a)$,
where $\alpha > 0$ is a step-size parameter, and $R_t \in \mathbb{R}$ is the average of the rewards up to but not including time t (with $R_1 = R_1$), which can be computed incrementally as described in Section 2.4 (or Section 2.5 if the problem is nonstationary)." *(Trecho de Multi-armed Bandits)*
