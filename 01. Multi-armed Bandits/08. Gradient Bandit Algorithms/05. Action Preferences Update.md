## Gradient Bandit Algorithms com Baseline

### IntroduÃ§Ã£o
No contexto do aprendizado por reforÃ§o, os **algoritmos bandit** sÃ£o modelos que lidam com o dilema de exploraÃ§Ã£o versus explotaÃ§Ã£o em ambientes simples, onde as aÃ§Ãµes tÃªm efeitos isolados e imediatos [1]. Em vez de estimar os valores das aÃ§Ãµes, como nos mÃ©todos action-value, os algoritmos gradient bandit aprendem preferÃªncias numÃ©ricas para cada aÃ§Ã£o. Estas preferÃªncias indicam a frequÃªncia com que cada aÃ§Ã£o deve ser selecionada, sem estabelecer uma relaÃ§Ã£o direta com a recompensa esperada. Essas preferÃªncias sÃ£o entÃ£o usadas para determinar as probabilidades de seleÃ§Ã£o de aÃ§Ã£o atravÃ©s de uma distribuiÃ§Ã£o softmax [1], um tipo de distribuiÃ§Ã£o probabilÃ­stica que aloca probabilidades maiores para aÃ§Ãµes com preferÃªncias mais elevadas. A principal distinÃ§Ã£o desse mÃ©todo Ã© que ele nÃ£o estima valores de aÃ§Ã£o, mas sim aprende preferÃªncias, usando estas para tomar decisÃµes [1].

### Conceitos Fundamentais
O cerne dos algoritmos gradient bandit reside na aprendizagem das **preferÃªncias de aÃ§Ã£o** $H_t(a) \in \mathbb{R}$ [1]. Estas preferÃªncias sÃ£o atualizadas atravÃ©s de um mÃ©todo de **ascensÃ£o de gradiente estocÃ¡stico**. As probabilidades de seleÃ§Ã£o de aÃ§Ã£o sÃ£o dadas pela distribuiÃ§Ã£o softmax:
$$
\Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)
$$
onde $\pi_t(a)$ denota a probabilidade de selecionar a aÃ§Ã£o $a$ no tempo $t$, e $k$ Ã© o nÃºmero total de aÃ§Ãµes [1]. As preferÃªncias de aÃ§Ã£o sÃ£o inicialmente configuradas como iguais para todas as aÃ§Ãµes, por exemplo, $H_1(a) = 0$ para todo $a$, garantindo que todas as aÃ§Ãµes tenham inicialmente a mesma probabilidade de serem selecionadas [1]. As preferÃªncias de aÃ§Ã£o sÃ£o atualizadas com base na recompensa recebida apÃ³s a seleÃ§Ã£o da aÃ§Ã£o, e um termo chamado **baseline**. Esta atualizaÃ§Ã£o Ã© dada por:
```mermaid
flowchart LR
    A[AÃ§Ã£o A_t Selecionada] -->|Recompensa R_t| B(Atualizar PreferÃªncias H_t(a));
    B --> C{H_t+1(A_t) = H_t(A_t) + alpha * (R_t - R_bar_t) * (1 - pi_t(A_t))};
    B --> D{H_t+1(a) = H_t(a) - alpha * (R_t - R_bar_t) * pi_t(a), para a != A_t};
    C --> E(Nova PreferÃªncia H_t+1(A_t));
    D --> F(Novas PreferÃªncias H_t+1(a), para a != A_t);
    E --> G(PrÃ³xima IteraÃ§Ã£o);
    F --> G;
    style C fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
```
onde $\alpha > 0$ Ã© o parÃ¢metro step-size, $R_t$ Ã© a recompensa recebida apÃ³s selecionar a aÃ§Ã£o $A_t$, e $\bar{R}_t$ Ã© a mÃ©dia das recompensas atÃ© o instante $t$ [1]. Esta mÃ©dia atua como um baseline. O baseline Ã© importante pois ele auxilia a dar sentido ao valor de $R_t$. Se a recompensa $R_t$ Ã© maior que o baseline $\bar{R}_t$, a probabilidade de selecionar a aÃ§Ã£o $A_t$ Ã© aumentada; caso contrÃ¡rio, ela Ã© diminuÃ­da. As aÃ§Ãµes nÃ£o selecionadas sÃ£o ajustadas na direÃ§Ã£o oposta [1].

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos considerar um problema com 3 aÃ§Ãµes ($k=3$) e um step-size $\alpha = 0.1$. Inicialmente, as preferÃªncias sÃ£o $H_1(a_1) = H_1(a_2) = H_1(a_3) = 0$, o que implica probabilidades iniciais $\pi_1(a_1) = \pi_1(a_2) = \pi_1(a_3) = 1/3$. No tempo $t=1$, a aÃ§Ã£o $a_2$ Ã© selecionada (i.e., $A_1 = a_2$) e recebe uma recompensa $R_1 = 1$. Como este Ã© o primeiro passo, o baseline $\bar{R}_1 = 0$. A atualizaÃ§Ã£o das preferÃªncias Ã©:
>
> $H_{2}(a_2) = H_1(a_2) + \alpha(R_1 - \bar{R}_1)(1 - \pi_1(a_2)) = 0 + 0.1(1 - 0)(1 - 1/3) = 0.0667$
>
> $H_{2}(a_1) = H_1(a_1) - \alpha(R_1 - \bar{R}_1)\pi_1(a_1) = 0 - 0.1(1 - 0)(1/3) = -0.0333$
>
> $H_{2}(a_3) = H_1(a_3) - \alpha(R_1 - \bar{R}_1)\pi_1(a_3) = 0 - 0.1(1 - 0)(1/3) = -0.0333$
>
> As novas preferÃªncias sÃ£o $H_2(a_1) = -0.0333$, $H_2(a_2) = 0.0667$, e $H_2(a_3) = -0.0333$. As novas probabilidades podem ser calculadas atravÃ©s da distribuiÃ§Ã£o softmax.

**Lema 1:** *O uso da funÃ§Ã£o softmax para obter as probabilidades de aÃ§Ãµes Ã© equivalente Ã  funÃ§Ã£o logÃ­stica, ou sigmoide, no caso de duas aÃ§Ãµes*.

*Prova:* Considere um cenÃ¡rio com duas aÃ§Ãµes, $a_1$ e $a_2$. De acordo com a distribuiÃ§Ã£o softmax, temos que:
$$\pi_t(a_1) = \frac{e^{H_t(a_1)}}{e^{H_t(a_1)} + e^{H_t(a_2)}}$$
Dividindo o numerador e o denominador por $e^{H_t(a_1)}$, obtemos:
$$\pi_t(a_1) = \frac{1}{1 + e^{H_t(a_2) - H_t(a_1)}}$$
Se definirmos a diferenÃ§a entre as preferÃªncias como $z = H_t(a_2) - H_t(a_1)$, podemos reescrever a equaÃ§Ã£o como:
$$\pi_t(a_1) = \frac{1}{1 + e^z}$$
Esta Ã© precisamente a forma da funÃ§Ã£o logÃ­stica ou sigmoide, onde $z$ representa a diferenÃ§a entre as preferÃªncias das duas aÃ§Ãµes. $\blacksquare$
```mermaid
flowchart LR
    A["FunÃ§Ã£o Softmax (k=2)"] --> B["pi_t(a_1) = e^(H_t(a_1)) / (e^(H_t(a_1)) + e^(H_t(a_2)))"];
    B --> C["Dividindo por e^(H_t(a_1))"];
    C --> D["pi_t(a_1) = 1 / (1 + e^(H_t(a_2) - H_t(a_1)))"];
    D --> E["z = H_t(a_2) - H_t(a_1)"];
    E --> F["pi_t(a_1) = 1 / (1 + e^z) = FunÃ§Ã£o LogÃ­stica"];
    style F fill:#ccf,stroke:#333,stroke-width:2px
```
> ðŸ’¡ **Exemplo NumÃ©rico:** Se tivermos duas aÃ§Ãµes com preferÃªncias $H_t(a_1) = 1$ e $H_t(a_2) = 2$, entÃ£o $z = H_t(a_2) - H_t(a_1) = 2 - 1 = 1$.  Assim, $\pi_t(a_1) = \frac{1}{1 + e^1} \approx 0.269$ e $\pi_t(a_2) = 1 - \pi_t(a_1) \approx 0.731$.  A aÃ§Ã£o $a_2$ Ã© mais provÃ¡vel de ser escolhida, devido Ã  sua maior preferÃªncia, e a probabilidade Ã© calculada exatamente pela funÃ§Ã£o logÃ­stica.

**Lema 1.1:** *A atualizaÃ§Ã£o das preferÃªncias de aÃ§Ã£o pode ser expressa de forma mais concisa usando uma notaÃ§Ã£o indicadora*.

*Prova:* Defina $I(A_t = a)$ como uma funÃ§Ã£o indicadora que retorna 1 se $A_t = a$ e 0 caso contrÃ¡rio. EntÃ£o a atualizaÃ§Ã£o das preferÃªncias de aÃ§Ã£o pode ser escrita como:
$$ H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t)(I(A_t = a) - \pi_t(a)) $$
Esta expressÃ£o condensa as duas atualizaÃ§Ãµes em uma Ãºnica fÃ³rmula. Note que quando $a = A_t$, temos $I(A_t = a) = 1$ e obtemos a atualizaÃ§Ã£o para a aÃ§Ã£o selecionada, e quando $a \neq A_t$, temos $I(A_t = a) = 0$ e obtemos a atualizaÃ§Ã£o para as demais aÃ§Ãµes. $\blacksquare$
```mermaid
flowchart LR
    A["NotaÃ§Ã£o Original para AtualizaÃ§Ã£o de PreferÃªncias"] --> B{ "H_{t+1}(A_t) = H_t(A_t) + alpha * (R_t - R_bar_t) * (1 - pi_t(A_t))" };
    A --> C{ "H_{t+1}(a) = H_t(a) - alpha * (R_t - R_bar_t) * pi_t(a), para a != A_t" };
    B --> D["AtualizaÃ§Ã£o da aÃ§Ã£o selecionada A_t"];
    C --> E["AtualizaÃ§Ã£o das outras aÃ§Ãµes a"];
    F["FunÃ§Ã£o Indicadora I(A_t = a)"] --> G{ "H_{t+1}(a) = H_t(a) + alpha * (R_t - R_bar_t) * (I(A_t = a) - pi_t(a))" };
    G --> H["NotaÃ§Ã£o Concisa"];
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

> ðŸ’¡ **Exemplo NumÃ©rico:** Usando o exemplo anterior com 3 aÃ§Ãµes e apÃ³s a atualizaÃ§Ã£o, no tempo t=2, temos $H_2(a_1) = -0.0333$, $H_2(a_2) = 0.0667$ e $H_2(a_3) = -0.0333$. Se no instante t=2, a aÃ§Ã£o $a_3$ Ã© selecionada ($A_2 = a_3$) e recebe uma recompensa $R_2 = 0.5$, e vamos calcular o baseline atualizado como $\bar{R}_2 = (1+0.5)/2 = 0.75$, e as probabilidades de cada aÃ§Ã£o atravÃ©s da softmax, que sÃ£o, aproximadamente, $\pi_2(a_1) \approx 0.29, \pi_2(a_2) \approx 0.40, \pi_2(a_3) \approx 0.31$.  A atualizaÃ§Ã£o da preferÃªncia para a aÃ§Ã£o $a_3$ usando a notaÃ§Ã£o indicadora Ã©:
>
> $H_{3}(a_3) = H_2(a_3) + \alpha(R_2 - \bar{R}_2)(I(A_2 = a_3) - \pi_2(a_3)) = -0.0333 + 0.1(0.5 - 0.75)(1 - 0.31) = -0.0333 -0.01725 = -0.05055$
>
> Para as aÃ§Ãµes nÃ£o selecionadas, $a_1$ e $a_2$:
>
> $H_{3}(a_1) = H_2(a_1) + \alpha(R_2 - \bar{R}_2)(I(A_2 = a_1) - \pi_2(a_1)) = -0.0333 + 0.1(0.5 - 0.75)(0 - 0.29) = -0.0333 + 0.00725 = -0.02605$
>
> $H_{3}(a_2) = H_2(a_2) + \alpha(R_2 - \bar{R}_2)(I(A_2 = a_2) - \pi_2(a_2)) = 0.0667 + 0.1(0.5 - 0.75)(0 - 0.40) = 0.0667 + 0.01000 = 0.0767$
>
> A atualizaÃ§Ã£o usando a notaÃ§Ã£o indicadora resulta nos mesmos valores que a notaÃ§Ã£o original, mas de forma mais compacta e com menos cÃ¡lculos repetitivos.

A importÃ¢ncia do baseline  $\bar{R}_t$ reside em seu papel como ponto de referÃªncia para a recompensa. Se o baseline nÃ£o fosse usado (isto Ã©, $\bar{R}_t$ fosse igual a zero), o algoritmo ainda funcionaria como um mÃ©todo de ascensÃ£o de gradiente estocÃ¡stico [1], mas a convergÃªncia seria mais lenta. Isso ocorre porque o baseline serve para adaptar-se ao novo nÃ­vel de recompensas, sendo que, sem ele, a performance do algoritmo se degradaria [1]. O uso do baseline nÃ£o afeta o valor esperado das atualizaÃ§Ãµes, mas reduz a variÃ¢ncia dessas atualizaÃ§Ãµes [1].

> ðŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar o efeito do baseline na variÃ¢ncia, considere uma sequÃªncia de recompensas onde todas as aÃ§Ãµes tem valor esperado de recompensa 1.  Sem o baseline,  uma recompensa de 2  aumentaria a preferÃªncia da aÃ§Ã£o mesmo que essa fosse uma flutuaÃ§Ã£o aleatÃ³ria.  Com um baseline (digamos 1), o efeito da mesma recompensa (2) na preferÃªncia seria menor, pois seria $2 - 1 = 1$, indicando que esta recompensa Ã© apenas acima do esperado, e nÃ£o uma recompensa excepcional. O baseline age como uma normalizaÃ§Ã£o, reduzindo o impacto de recompensas aleatÃ³rias na preferÃªncia de aÃ§Ã£o.
```mermaid
flowchart LR
    A["Recompensa R_t"] --> B{ "Sem Baseline" };
    A --> C{ "Com Baseline R_bar_t" };
    B --> D["Aumento Direto da PreferÃªncia"];
    C --> E["Aumento da PreferÃªncia = R_t - R_bar_t"];
    D --> F["Maior VariÃ¢ncia"];
    E --> G["Menor VariÃ¢ncia"];
    H["R_bar_t = MÃ©dia das Recompensas"]
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

**ObservaÃ§Ã£o 1:** *A escolha do baseline como a mÃ©dia das recompensas anteriores Ã© apenas uma das opÃ§Ãµes. Outros baselines, como a mediana das recompensas, ou uma mÃ©dia mÃ³vel exponencial, podem ser utilizados, com impactos na convergÃªncia e estabilidade do algoritmo.* A mÃ©dia das recompensas Ã© uma opÃ§Ã£o comum devido Ã  sua simplicidade e eficÃ¡cia em muitos casos. No entanto, para ambientes nÃ£o estacionÃ¡rios, um baseline que atribua maior importÃ¢ncia Ã s recompensas mais recentes pode ser mais adequado, assim como os mÃ©todos de atualizaÃ§Ã£o da mÃ©dia.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que em um ambiente nÃ£o estacionÃ¡rio, as recompensas mudam abruptamente no tempo $t = 100$. Antes de $t=100$, a recompensa mÃ©dia era $\bar{R}_{99} = 1$.  ApÃ³s a mudanÃ§a, as novas recompensas tem mÃ©dia 3.  Um baseline que usa uma mÃ©dia simples, levaria algum tempo atÃ© refletir essa mudanÃ§a. Um baseline que usa uma mÃ©dia mÃ³vel exponencial com um fator de decaimento $\lambda$ daria mais peso Ã s recompensas mais recentes, permitindo que o baseline se adapte mais rapidamente a novas mÃ©dias, por exemplo, $\bar{R}_t = \lambda \bar{R}_{t-1} + (1-\lambda)R_t$, com $\lambda$ perto de 0.
```mermaid
flowchart LR
    A["Ambiente NÃ£o EstacionÃ¡rio"] --> B["MudanÃ§a Abrupta de Recompensas"];
    B --> C{ "Baseline como MÃ©dia Simples" };
    B --> D{ "Baseline como MÃ©dia MÃ³vel Exponencial" };
    C --> E["AdaptaÃ§Ã£o Lenta Ã  MudanÃ§a"];
    D --> F["AdaptaÃ§Ã£o RÃ¡pida Ã  MudanÃ§a"];
    G["R_bar_t = lambda * R_bar_{t-1} + (1-lambda) * R_t"]
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

**Teorema 1:** *O algoritmo gradient bandit, com a atualizaÃ§Ã£o de preferÃªncia e o baseline como mÃ©dia das recompensas, converge para um Ã³timo local sob certas condiÃ§Ãµes de suavidade e step-size*.

*Prova (esboÃ§o):* A prova desse teorema geralmente envolve a anÃ¡lise da convergÃªncia do processo de ascensÃ£o de gradiente estocÃ¡stico. Sob condiÃ§Ãµes apropriadas (como um step-size $\alpha$ decrescente ao longo do tempo, satisfazendo as condiÃ§Ãµes de Robbins-Monro, e uma funÃ§Ã£o de recompensa diferenciÃ¡vel), pode-se mostrar que as preferÃªncias de aÃ§Ã£o convergem para um ponto onde o gradiente da recompensa esperada Ã© zero. Formalmente, este resultado requer ferramentas de anÃ¡lise estocÃ¡stica e nÃ£o serÃ¡ detalhado aqui. No entanto, a intuiÃ§Ã£o Ã© que o algoritmo ajusta iterativamente as preferÃªncias na direÃ§Ã£o de recompensas maiores, atÃ© atingir um ponto estacionÃ¡rio. $\blacksquare$

**Teorema 1.1:** *No caso de um ambiente estacionÃ¡rio, a mÃ©dia das recompensas $\bar{R}_t$ converge para o valor esperado da recompensa sob a polÃ­tica Ã³tima.*

*Prova (esboÃ§o):* No caso estacionÃ¡rio, a recompensa mÃ©dia converge para um valor constante conforme $t$ tende ao infinito, dado que o algoritmo explora adequadamente todas as aÃ§Ãµes. O baseline $\bar{R}_t$ converge para a recompensa mÃ©dia sob a polÃ­tica que o algoritmo segue. Com o tempo e a convergÃªncia da polÃ­tica para a Ã³tima, a mÃ©dia das recompensas converge tambÃ©m para a recompensa esperada sob a polÃ­tica Ã³tima. A prova formal envolve o uso de leis dos grandes nÃºmeros para processos estocÃ¡sticos. $\blacksquare$
```mermaid
flowchart LR
    A["Ambiente EstacionÃ¡rio"] --> B["Algoritmo Explora AÃ§Ãµes"];
    B --> C["MÃ©dia das Recompensas R_bar_t Converge"];
    C --> D["PolÃ­tica do Algoritmo Converge para Ã“tima"];
    D --> E["R_bar_t Converge para Recompensa Esperada"];
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que apÃ³s um grande nÃºmero de iteraÃ§Ãµes, o algoritmo esteja em um estado em que sempre escolhe a aÃ§Ã£o $a_1$, que tem recompensa mÃ©dia 2. O baseline $\bar{R}_t$ irÃ¡ convergir para 2 com o tempo. Se de repente o algoritmo comeÃ§ar a escolher outras aÃ§Ãµes menos vantajosas (em uma fase de exploraÃ§Ã£o, por exemplo), o baseline tambÃ©m se ajustarÃ¡ a este novo comportamento. Eventualmente, quando o algoritmo retornar para a aÃ§Ã£o Ã³tima $a_1$, o baseline voltarÃ¡ a convergir para o valor esperado da recompensa Ã³tima, no caso, 2.

### ConclusÃ£o
Os gradient bandit algorithms representam uma abordagem alternativa aos mÃ©todos action-value, aprendendo diretamente as preferÃªncias de aÃ§Ã£o em vez de estimar os valores das aÃ§Ãµes. A inclusÃ£o de um baseline na atualizaÃ§Ã£o das preferÃªncias de aÃ§Ã£o, com a mÃ©dia das recompensas anteriores, Ã© importante para melhorar a taxa de convergÃªncia e a robustez do algoritmo. O baseline atua como um ponto de referÃªncia adaptativo que ajuda o algoritmo a distinguir melhor as recompensas boas das ruins, acelerando o processo de aprendizagem. Esta abordagem Ã© especialmente Ãºtil em situaÃ§Ãµes onde a interpretaÃ§Ã£o direta das recompensas Ã© mais importante do que a prediÃ§Ã£o de recompensas futuras.

### ReferÃªncias
[^1]: "So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. In this section we consider learning a numerical preference for each action a, which we denote $H_t(a) \in \mathbb{R}$. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:
$\Pr\{A_t=a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)$,
where here we have also introduced a useful new notation, $\pi_t(a)$, for the probability of taking action a at time t. Initially all action preferences are the same (e.g., $H_1(a) = 0$, for all a) so that all actions have an equal probability of being selected. There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action $A_t$ and receiving the reward $R_t$, the action preferences are updated by:
$H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t â€“ R_t) (1 â€“ \pi_t(A_t))$, and for all $a \neq A_t$,
$H_{t+1}(a) = H_t(a) â€“ \alpha(R_t â€“ R_t)\pi_t(a)$,
where $\alpha > 0$ is a step-size parameter, and $R_t \in \mathbb{R}$ is the average of the rewards up to but not including time t (with $R_1 = R_1$), which can be computed incrementally as described in Section 2.4 (or Section 2.5 if the problem is nonstationary)." *(Trecho de Multi-armed Bandits)*
