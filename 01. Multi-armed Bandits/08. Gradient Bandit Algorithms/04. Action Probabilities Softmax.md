## Gradient Bandit Algorithms e DistribuiÃ§Ã£o Soft-Max

### IntroduÃ§Ã£o

O aprendizado por reforÃ§o se distingue de outras formas de aprendizado por usar informaÃ§Ãµes de treinamento que avaliam as aÃ§Ãµes tomadas, em vez de instruir atravÃ©s de aÃ§Ãµes corretas [^1]. Isso introduz a necessidade de **exploraÃ§Ã£o ativa** para encontrar bons comportamentos, onde a *purely evaluative feedback* indica a qualidade de uma aÃ§Ã£o tomada sem necessariamente indicar a melhor aÃ§Ã£o possÃ­vel [^1]. Em contraste, o feedback instrutivo indica a aÃ§Ã£o correta a ser tomada, independentemente da aÃ§Ã£o tomada, caracterÃ­stica do aprendizado supervisionado [^1]. Este capÃ­tulo explora o aspecto avaliativo do aprendizado por reforÃ§o em um ambiente simplificado, utilizando o **problema do *k*-armed bandit** como um ponto de partida [^1]. O problema do *k*-armed bandit consiste em escolher repetidamente entre *k* opÃ§Ãµes, cada uma fornecendo uma recompensa numÃ©rica de uma distribuiÃ§Ã£o de probabilidade estacionÃ¡ria dependente da aÃ§Ã£o escolhida [^1].

### Conceitos Fundamentais

No problema do *k*-armed bandit, cada aÃ§Ã£o tem um valor esperado ou recompensa mÃ©dia, dado que essa aÃ§Ã£o Ã© selecionada [^2]. Esse valor Ã© denotado por **q*(a)** [^2], onde *a* representa a aÃ§Ã£o. A aÃ§Ã£o selecionada no tempo *t* Ã© denotada por **At**, e a recompensa correspondente por **Rt** [^2]. O objetivo Ã© maximizar a recompensa total esperada ao longo de um perÃ­odo de tempo [^2].

```mermaid
graph LR
    A[/"AÃ§Ã£o 'a'"/] -->|recompensa esperada| B("q*(a)");
    C[/"AÃ§Ã£o selecionada no tempo 't' (At)"/] -->|recompensa| D("Rt");
    E("Objetivo") --> F("Maximizar recompensa total esperada");
```

Uma das estratÃ©gias de soluÃ§Ã£o para o *k*-armed bandit Ã© o uso de **mÃ©todos *action-value*** [^3], onde a estimativa do valor de uma aÃ§Ã£o *a* no tempo *t*, denotada por **Qt(a)**, busca se aproximar de q*(a) [^2]. MÃ©todos de *action-value* comuns incluem o mÃ©todo *sample-average*, que calcula a mÃ©dia das recompensas obtidas para cada aÃ§Ã£o [^3].

No entanto, hÃ¡ uma distinÃ§Ã£o fundamental entre **exploraÃ§Ã£o e *exploitation*** [^2]. *Exploitation* significa selecionar as aÃ§Ãµes com os maiores valores estimados, que sÃ£o as aÃ§Ãµes *greedy* [^2]. Por outro lado, explorar significa selecionar aÃ§Ãµes nÃ£o-*greedy* para melhorar a estimativa de seus valores, com o intuito de encontrar aÃ§Ãµes melhores a longo prazo [^2]. O equilÃ­brio entre exploraÃ§Ã£o e *exploitation* Ã© crucial para resolver o *k*-armed bandit de forma eficaz [^2].

Nesta seÃ§Ã£o, o foco serÃ¡ sobre os **gradient bandit algorithms**, que nÃ£o estimam valores de aÃ§Ã£o diretamente, mas aprendem uma **preferÃªncia numÃ©rica para cada aÃ§Ã£o**, denotada por **Ht(a)** [^13]. A preferÃªncia Ã© usada para determinar a probabilidade de selecionar uma aÃ§Ã£o usando uma **distribuiÃ§Ã£o *soft-max***, tambÃ©m conhecida como distribuiÃ§Ã£o Gibbs ou Boltzmann [^13]:

$$ P\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} = \pi_t(a) $$ [^13]

onde $\pi_t(a)$ representa a probabilidade de selecionar a aÃ§Ã£o *a* no tempo *t*, e as preferÃªncias de todas as aÃ§Ãµes sÃ£o exponenciadas e normalizadas para gerar uma distribuiÃ§Ã£o de probabilidade [^13]. Inicialmente, todas as preferÃªncias sÃ£o iguais, resultando em probabilidades iguais para todas as aÃ§Ãµes [^13].

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Imagine um problema de *3-armed bandit* (k=3) onde temos trÃªs aÃ§Ãµes (a=1, a=2, a=3). Inicialmente, as preferÃªncias $H_t(a)$ para todas as aÃ§Ãµes sÃ£o zero, ou seja, $H_1(1) = H_1(2) = H_1(3) = 0$.
>
> Calculando as probabilidades iniciais usando a distribuiÃ§Ã£o soft-max:
>
> $$ \pi_1(1) = \frac{e^{0}}{e^{0} + e^{0} + e^{0}} = \frac{1}{3} $$
> $$ \pi_1(2) = \frac{e^{0}}{e^{0} + e^{0} + e^{0}} = \frac{1}{3} $$
> $$ \pi_1(3) = \frac{e^{0}}{e^{0} + e^{0} + e^{0}} = \frac{1}{3} $$
>
> Inicialmente, cada aÃ§Ã£o tem a mesma probabilidade de ser escolhida.

### DerivaÃ§Ã£o da DistribuiÃ§Ã£o Soft-Max

A distribuiÃ§Ã£o *soft-max* Ã© usada para transformar preferÃªncias em probabilidades, sendo que a preferÃªncia de uma aÃ§Ã£o sobre outra Ã© relativa. A adiÃ§Ã£o de uma constante a todas as preferÃªncias nÃ£o muda as probabilidades de aÃ§Ã£o [^13]. A distribuiÃ§Ã£o *soft-max* Ã© dada por:

$$ \pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} $$ [^13]

Este tipo de funÃ§Ã£o Ã© tambÃ©m conhecida como **funÃ§Ã£o logÃ­stica**, ou **sigmoide**, quando aplicada a dois casos [^13].

```mermaid
graph LR
    subgraph "DistribuiÃ§Ã£o Soft-Max"
        A("PreferÃªncia Ht(a)") --> B("ExponenciaÃ§Ã£o: e^Ht(a)");
        B --> C("Soma de exponenciaÃ§Ãµes: Î£ e^Ht(b)");
        C --> D("NormalizaÃ§Ã£o: e^Ht(a) / Î£ e^Ht(b)");
        D --> E("Probabilidade Ï€t(a)");
    end
```

#### Lemma 1: RelaÃ§Ã£o com a FunÃ§Ã£o Sigmoide (Caso de Duas AÃ§Ãµes)
*DeclaraÃ§Ã£o:* No caso de duas aÃ§Ãµes, a distribuiÃ§Ã£o *soft-max* Ã© equivalente Ã  funÃ§Ã£o logÃ­stica.

*Prova:*
Seja *k* = 2. A probabilidade de selecionar a aÃ§Ã£o 1 Ã©:

$$ \pi_t(1) = \frac{e^{H_t(1)}}{e^{H_t(1)} + e^{H_t(2)}} $$

Dividindo o numerador e o denominador por $e^{H_t(1)}$:

$$ \pi_t(1) = \frac{1}{1 + e^{H_t(2) - H_t(1)}} $$

Se definirmos $z = H_t(1) - H_t(2)$, temos:

$$ \pi_t(1) = \frac{1}{1 + e^{-z}} $$

Esta Ã© a forma da funÃ§Ã£o sigmoide, onde $z$ representa a diferenÃ§a entre as preferÃªncias das aÃ§Ãµes.

$\blacksquare$

**Lema 1.1:** *InvariÃ¢ncia da DistribuiÃ§Ã£o Soft-Max sob AdiÃ§Ã£o de Constantes*
*DeclaraÃ§Ã£o:* A adiÃ§Ã£o de uma constante *c* a todas as preferÃªncias $H_t(a)$ nÃ£o altera as probabilidades de aÃ§Ã£o $\pi_t(a)$ geradas pela distribuiÃ§Ã£o *soft-max*.

*Prova:*
Seja $H'_t(a) = H_t(a) + c$ para todas as aÃ§Ãµes *a*. A nova probabilidade de selecionar a aÃ§Ã£o *a* Ã©:
$$
\pi'_t(a) = \frac{e^{H'_t(a)}}{\sum_{b=1}^k e^{H'_t(b)}} = \frac{e^{H_t(a) + c}}{\sum_{b=1}^k e^{H_t(b) + c}} = \frac{e^{H_t(a)}e^c}{\sum_{b=1}^k e^{H_t(b)}e^c}
$$
Fatorando $e^c$ do numerador e denominador, temos:
$$
\pi'_t(a) = \frac{e^c e^{H_t(a)}}{e^c \sum_{b=1}^k e^{H_t(b)}} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)
$$
Portanto, as probabilidades de aÃ§Ã£o permanecem as mesmas, demonstrando a invariÃ¢ncia da distribuiÃ§Ã£o *soft-max* sob adiÃ§Ã£o de constantes Ã s preferÃªncias.

$\blacksquare$
> ðŸ’¡ **Exemplo NumÃ©rico (Lema 1.1):**
>
> Continuando o exemplo anterior de *3-armed bandit*, suponha que apÃ³s algumas iteraÃ§Ãµes, as preferÃªncias sejam: $H_t(1) = 1$, $H_t(2) = 0.5$, $H_t(3) = -0.5$.
>
>  As probabilidades sÃ£o:
>
> $$ \pi_t(1) = \frac{e^{1}}{e^{1} + e^{0.5} + e^{-0.5}} \approx \frac{2.718}{2.718+1.649+0.607} \approx 0.52 $$
> $$ \pi_t(2) = \frac{e^{0.5}}{e^{1} + e^{0.5} + e^{-0.5}} \approx \frac{1.649}{2.718+1.649+0.607} \approx 0.32 $$
> $$ \pi_t(3) = \frac{e^{-0.5}}{e^{1} + e^{0.5} + e^{-0.5}} \approx \frac{0.607}{2.718+1.649+0.607} \approx 0.12 $$
>
> Agora, adicionemos uma constante, digamos *c = 2*, a todas as preferÃªncias: $H'_t(1) = 3$, $H'_t(2) = 2.5$, $H'_t(3) = 1.5$. As novas probabilidades sÃ£o:
>
> $$ \pi'_t(1) = \frac{e^{3}}{e^{3} + e^{2.5} + e^{1.5}} \approx \frac{20.086}{20.086+12.182+4.482} \approx 0.52 $$
> $$ \pi'_t(2) = \frac{e^{2.5}}{e^{3} + e^{2.5} + e^{1.5}} \approx \frac{12.182}{20.086+12.182+4.482} \approx 0.32 $$
> $$ \pi'_t(3) = \frac{e^{1.5}}{e^{3} + e^{2.5} + e^{1.5}} \approx \frac{4.482}{20.086+12.182+4.482} \approx 0.12 $$
>
> As probabilidades permanecem as mesmas, demonstrando que a adiÃ§Ã£o de uma constante nÃ£o afeta a probabilidade de seleÃ§Ã£o da aÃ§Ã£o.

### AtualizaÃ§Ã£o das PreferÃªncias

Os gradientes sÃ£o a base para a atualizaÃ§Ã£o das preferÃªncias [^13]. O algoritmo de aprendizado atualiza as preferÃªncias atravÃ©s do mÃ©todo **stochastic gradient ascent** [^14]. ApÃ³s selecionar a aÃ§Ã£o At e receber a recompensa Rt, as preferÃªncias de aÃ§Ã£o sÃ£o atualizadas como [^13]:

$$ H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \overline{R}_t)(1 - \pi_t(A_t)) $$ [^13]

$$ H_{t+1}(a) = H_t(a) - \alpha(R_t - \overline{R}_t)\pi_t(a), \quad \text{para todo } a \neq A_t $$ [^13]

onde $\alpha$ Ã© o tamanho do passo (step-size), $\overline{R}_t$ Ã© uma recompensa mÃ©dia atÃ© o tempo *t*, servindo como *baseline* [^13]. Se a recompensa *R* Ã© maior que a *baseline*, a probabilidade de tomar aquela aÃ§Ã£o aumenta. Se for menor, diminui. A derivaÃ§Ã£o da equaÃ§Ã£o de atualizaÃ§Ã£o das preferÃªncias usando stochastic gradient ascent Ã© feita a partir da anÃ¡lise do gradiente da performance esperada, como mostrado nas equaÃ§Ãµes (2.13) a (2.15) do contexto [^14, 15].

```mermaid
graph LR
    subgraph "AtualizaÃ§Ã£o de PreferÃªncias"
        A("AÃ§Ã£o At selecionada, Recompensa Rt") --> B("Calcular Baseline: RÌ„t");
        B --> C("Atualizar preferÃªncia Ht(At): Ht+1(At) = Ht(At) + Î±(Rt - RÌ„t)(1 - Ï€t(At))");
         B --> D("Atualizar outras preferencias Ht(a): Ht+1(a) = Ht(a) - Î±(Rt - RÌ„t)Ï€t(a), para a â‰  At");
         C --> E("Novas PreferÃªncias Ht+1(a)");
         D --> E
    end
```

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que estamos no tempo *t*, a aÃ§Ã£o selecionada foi $A_t = 1$, e recebemos uma recompensa de $R_t = 1$. A recompensa mÃ©dia atÃ© o momento Ã© $\overline{R}_t = 0.5$. O tamanho do passo Ã© $\alpha = 0.1$. Usando as preferÃªncias do exemplo anterior: $H_t(1) = 1$, $H_t(2) = 0.5$, $H_t(3) = -0.5$, e as probabilidades calculadas $\pi_t(1) \approx 0.52$, $\pi_t(2) \approx 0.32$, $\pi_t(3) \approx 0.12$.
>
> Atualizando as preferÃªncias:
>
> $$ H_{t+1}(1) = H_t(1) + \alpha(R_t - \overline{R}_t)(1 - \pi_t(1)) $$
> $$ H_{t+1}(1) = 1 + 0.1(1 - 0.5)(1 - 0.52) = 1 + 0.1 * 0.5 * 0.48 = 1 + 0.024 = 1.024 $$
>
> Para as outras aÃ§Ãµes:
>
> $$ H_{t+1}(2) = H_t(2) - \alpha(R_t - \overline{R}_t)\pi_t(2) $$
> $$ H_{t+1}(2) = 0.5 - 0.1(1 - 0.5)(0.32) = 0.5 - 0.1 * 0.5 * 0.32 = 0.5 - 0.016 = 0.484 $$
>
> $$ H_{t+1}(3) = H_t(3) - \alpha(R_t - \overline{R}_t)\pi_t(3) $$
> $$ H_{t+1}(3) = -0.5 - 0.1(1 - 0.5)(0.12) = -0.5 - 0.1 * 0.5 * 0.12 = -0.5 - 0.006 = -0.506 $$
>
> Observe que a preferÃªncia da aÃ§Ã£o 1, que foi selecionada e teve uma recompensa acima da mÃ©dia, aumentou. As outras preferÃªncias diminuÃ­ram.
>
> As novas probabilidades podem ser calculadas usando a nova preferÃªncia.
>
> Calculando as probabilidades para o tempo t+1 usando as novas preferencias $H_{t+1}(1) = 1.024$, $H_{t+1}(2) = 0.484$, $H_{t+1}(3) = -0.506$:
>
> $$ \pi_{t+1}(1) = \frac{e^{1.024}}{e^{1.024} + e^{0.484} + e^{-0.506}} \approx \frac{2.784}{2.784+1.623+0.603} \approx 0.528 $$
> $$ \pi_{t+1}(2) = \frac{e^{0.484}}{e^{1.024} + e^{0.484} + e^{-0.506}} \approx \frac{1.623}{2.784+1.623+0.603} \approx 0.309 $$
> $$ \pi_{t+1}(3) = \frac{e^{-0.506}}{e^{1.024} + e^{0.484} + e^{-0.506}} \approx \frac{0.603}{2.784+1.623+0.603} \approx 0.115 $$
> A probabilidade da aÃ§Ã£o 1 aumentou, enquanto as outras diminuÃ­ram.
>

#### Lemma 2: RelaÃ§Ã£o com Stochastic Gradient Ascent

*DeclaraÃ§Ã£o:* A atualizaÃ§Ã£o das preferÃªncias no gradient bandit algorithm pode ser derivada como uma aproximaÃ§Ã£o estocÃ¡stica do gradiente da performance esperada.

*Prova:*
O objetivo Ã© maximizar a recompensa esperada $E[R_t]$, que Ã© dada por:

$$ E[R_t] = \sum_{x} \pi_t(x)q_*(x) $$ [^14]

A atualizaÃ§Ã£o das preferÃªncias no gradient bandit algorithm Ã© baseada no conceito de stochastic gradient ascent, onde a preferÃªncia de aÃ§Ã£o Ã© incrementada proporcionalmente ao efeito desse incremento na performance [^14]:

$$H_{t+1}(a) = H_t(a) + \alpha \frac{\partial E[R_t]}{\partial H_t(a)}$$ [^14]

Ao derivar $\frac{\partial E[R_t]}{\partial H_t(a)}$ [^14, 15], utilizando a regra do quociente, obtemos:

$$ \frac{\partial E[R_t]}{\partial H_t(a)} = \sum_{x}(q_*(x) - \overline{R}_t) \frac{\partial \pi_t(x)}{\partial H_t(a)} $$

onde o *baseline* $\overline{R}_t$ nÃ£o interfere no gradiente [^15]. Ao usar a distribuiÃ§Ã£o *soft-max* para  $\pi_t(x)$, o gradiente resulta em:
$$ \frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x)(\mathbb{1}_{a=x} - \pi_t(a))$$ [^16]

Substituindo, obtemos a forma de atualizaÃ§Ã£o das preferÃªncias para o gradient bandit algorithm, onde cada atualizaÃ§Ã£o das preferÃªncias Ã© proporcional a uma amostra do gradiente esperado:

$$ H_{t+1}(a) = H_t(a) + \alpha (R_t - \overline{R}_t)(\mathbb{1}_{a=A_t} - \pi_t(a)) $$
$\blacksquare$

**Lema 2.1**: *AtualizaÃ§Ã£o Equivalente para Todas as AÃ§Ãµes*
*DeclaraÃ§Ã£o:* A atualizaÃ§Ã£o das preferÃªncias pode ser expressa de forma equivalente para todas as aÃ§Ãµes, usando o indicador $\mathbb{1}_{a=A_t}$.

*Prova:*
A atualizaÃ§Ã£o das preferÃªncias Ã© dada por:
$$
H_{t+1}(a) = H_t(a) + \alpha(R_t - \overline{R}_t)(\mathbb{1}_{a=A_t} - \pi_t(a))
$$
Analisando o caso em que $a = A_t$:
$$
H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \overline{R}_t)(1 - \pi_t(A_t))
$$
E quando $a \neq A_t$:
$$
H_{t+1}(a) = H_t(a) - \alpha(R_t - \overline{R}_t)\pi_t(a)
$$
Observe que a equaÃ§Ã£o geral inclui ambos os casos. Quando $a = A_t$, $\mathbb{1}_{a=A_t} = 1$, e a equaÃ§Ã£o coincide com a primeira atualizaÃ§Ã£o. Quando $a \neq A_t$, $\mathbb{1}_{a=A_t} = 0$, e a equaÃ§Ã£o se torna igual a segunda atualizaÃ§Ã£o. Portanto, a equaÃ§Ã£o geral pode ser usada para atualizar as preferÃªncias de todas as aÃ§Ãµes.

$\blacksquare$
> ðŸ’¡ **Exemplo NumÃ©rico (Lema 2.1):**
>
> Usando o mesmo cenÃ¡rio do exemplo anterior, onde $A_t = 1$, $R_t = 1$, $\overline{R}_t = 0.5$, $\alpha = 0.1$ e preferÃªncias $H_t(1) = 1$, $H_t(2) = 0.5$, $H_t(3) = -0.5$, com as probabilidades $\pi_t(1) \approx 0.52$, $\pi_t(2) \approx 0.32$, $\pi_t(3) \approx 0.12$.
>
> Usando a equaÃ§Ã£o equivalente:
>
> $$ H_{t+1}(a) = H_t(a) + \alpha(R_t - \overline{R}_t)(\mathbb{1}_{a=A_t} - \pi_t(a)) $$
>
> Para $a=1$:
>
> $$ H_{t+1}(1) = 1 + 0.1(1-0.5)(1-0.52) = 1 + 0.1 * 0.5 * 0.48 = 1.024 $$
>
> Para $a=2$:
>
> $$ H_{t+1}(2) = 0.5 + 0.1(1-0.5)(0 - 0.32) = 0.5 - 0.1 * 0.5 * 0.32 = 0.484 $$
>
> Para $a=3$:
>
> $$ H_{t+1}(3) = -0.5 + 0.1(1-0.5)(0-0.12) = -0.5 - 0.1 * 0.5 * 0.12 = -0.506 $$
>
> Os resultados sÃ£o idÃªnticos ao exemplo anterior, mostrando que a equaÃ§Ã£o equivalente leva Ã s mesmas atualizaÃ§Ãµes de preferÃªncia.

**ProposiÃ§Ã£o 1:** *RelaÃ§Ã£o com a Regra de Aprendizado de Widrow-Hoff*
*DeclaraÃ§Ã£o:* A atualizaÃ§Ã£o das preferÃªncias do gradient bandit algorithm, quando expandida, possui uma semelhanÃ§a com a regra de aprendizado de Widrow-Hoff, tambÃ©m conhecida como regra delta.
*Prova:*
Reescrevendo a equaÃ§Ã£o de atualizaÃ§Ã£o para uma aÃ§Ã£o arbitrÃ¡ria *a*:
$$H_{t+1}(a) = H_t(a) + \alpha(R_t - \overline{R}_t)(\mathbb{1}_{a=A_t} - \pi_t(a))$$
Expandindo a expressÃ£o, obtemos:
$$H_{t+1}(a) = H_t(a) + \alpha(R_t - \overline{R}_t)\mathbb{1}_{a=A_t} - \alpha(R_t - \overline{R}_t)\pi_t(a)$$
A expressÃ£o acima pode ser interpretada como:
- $H_t(a)$: O valor atual da preferÃªncia.
- $\alpha(R_t - \overline{R}_t)\mathbb{1}_{a=A_t}$: A atualizaÃ§Ã£o na preferÃªncia quando a aÃ§Ã£o *a* foi selecionada. A atualizaÃ§Ã£o Ã© proporcional Ã  diferenÃ§a entre a recompensa e o *baseline*.
- $\alpha(R_t - \overline{R}_t)\pi_t(a)$: Um ajuste de probabilidade, que reduz a preferÃªncia das aÃ§Ãµes que nÃ£o foram selecionadas, e que reduz a preferÃªncia da aÃ§Ã£o selecionada proporcionalmente a sua probabilidade.

Esta estrutura se assemelha Ã  regra de Widrow-Hoff (ou regra delta), que ajusta os pesos em uma rede neural com base no erro (diferenÃ§a entre a saÃ­da desejada e a saÃ­da real) e na entrada. A similaridade reside no ajuste proporcional ao erro (recompensa menos *baseline*) e na consideraÃ§Ã£o do valor atual da preferÃªncia/peso. No gradient bandit algorithm, a "entrada" Ã© representada pelo indicador e pelas probabilidades $\pi_t(a)$, enquanto na regra de Widrow-Hoff, a entrada Ã© o valor fornecido Ã  rede neural.

$\blacksquare$

### ConclusÃ£o

Os **gradient bandit algorithms** representam uma abordagem distinta em comparaÃ§Ã£o com os **action-value methods** para a soluÃ§Ã£o de problemas do tipo *k*-armed bandit. Ao invÃ©s de estimar os valores de aÃ§Ãµes, esses algoritmos aprendem preferÃªncias de aÃ§Ã£o. Utilizando uma **distribuiÃ§Ã£o soft-max** para determinar as probabilidades de aÃ§Ã£o, eles equilibram a exploraÃ§Ã£o e a *exploitation*. A implementaÃ§Ã£o da atualizaÃ§Ã£o de preferÃªncias atravÃ©s do mÃ©todo stochastic gradient ascent garante a convergÃªncia e adaptaÃ§Ã£o ao ambiente do problema. A capacidade de adaptar as preferÃªncias de aÃ§Ãµes de forma contÃ­nua, junto com o uso de uma *baseline* para a comparaÃ§Ã£o de recompensas, permite que estes algoritmos lidem com mudanÃ§as no ambiente e aprendam as melhores aÃ§Ãµes para maximizar o retorno a longo prazo.

### ReferÃªncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions."
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt."
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods."
[^13]: "In this section we consider learning a numerical preference for each action a, which we denote Ht(a) \in R. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows: Pr{At=a} = eHt(a) / \sumk b=1eHt(b) = \pi_t(a),..."
[^14]: "One can gain a deeper insight into the gradient bandit algorithm by understanding it as a stochastic approximation to gradient ascent. In exact gradient ascent, each action preference Ht(a) would be incremented in proportion to the increment's effect on performance: Ht+1(a) = Ht(a) + \alpha \partialE[Rt]/\partialHt(a), where the measure of performance here is the expected reward: E[Rt] = \sumx \pi_t(x)q_*(x), and the measure of the increment's effect is the partial derivative of this performance measure with respect to the action preference."
[^15]: "Next we multiply each term of the sum by \pi_t(x)/\pi_t(x): \partialE[Rt]/\partialHt(a) = \sumx \pi_t(x) (q_*(x) âˆ’ Bt) \partial\pi_t(x)/\partialHt(a) /\pi_t(x). The equation is now in the form of an expectation, summing over all possible values x of the random variable At, then multiplying by the probability of taking those values."
[^16]: "Thus it remains only to show that \partial\pi_t(x)/\partialHt(a) = \pi_t(x)(1_{a=x} âˆ’ \pi_t(a)), as we assumed. Recall the standard quotient rule for derivatives:  \partial/\partialx [f(x)/g(x)] = (\partialf(x)/\partialx g(x)âˆ’f(x) \partialg(x)/\partialx) /g(x)^2. Using this, we can write \partial\pi_t(x)/\partialHt(a) = \partial/\partialHt(a)  [ eHt(x) / \sumk y=1 eHt(y)]  ..."
