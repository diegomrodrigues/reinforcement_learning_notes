## Gradient Bandit Algorithms e Distribui√ß√£o Soft-Max

### Introdu√ß√£o

O aprendizado por refor√ßo se distingue de outras formas de aprendizado por usar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de instruir atrav√©s de a√ß√µes corretas [^1]. Isso introduz a necessidade de **explora√ß√£o ativa** para encontrar bons comportamentos, onde a *purely evaluative feedback* indica a qualidade de uma a√ß√£o tomada sem necessariamente indicar a melhor a√ß√£o poss√≠vel [^1]. Em contraste, o feedback instrutivo indica a a√ß√£o correta a ser tomada, independentemente da a√ß√£o tomada, caracter√≠stica do aprendizado supervisionado [^1]. Este cap√≠tulo explora o aspecto avaliativo do aprendizado por refor√ßo em um ambiente simplificado, utilizando o **problema do *k*-armed bandit** como um ponto de partida [^1]. O problema do *k*-armed bandit consiste em escolher repetidamente entre *k* op√ß√µes, cada uma fornecendo uma recompensa num√©rica de uma distribui√ß√£o de probabilidade estacion√°ria dependente da a√ß√£o escolhida [^1].

### Conceitos Fundamentais

No problema do *k*-armed bandit, cada a√ß√£o tem um valor esperado ou recompensa m√©dia, dado que essa a√ß√£o √© selecionada [^2]. Esse valor √© denotado por **q*(a)** [^2], onde *a* representa a a√ß√£o. A a√ß√£o selecionada no tempo *t* √© denotada por **At**, e a recompensa correspondente por **Rt** [^2]. O objetivo √© maximizar a recompensa total esperada ao longo de um per√≠odo de tempo [^2].

```mermaid
graph LR
    A[/"A√ß√£o 'a'"/] -->|recompensa esperada| B("q*(a)");
    C[/"A√ß√£o selecionada no tempo 't' (At)"/] -->|recompensa| D("Rt");
    E("Objetivo") --> F("Maximizar recompensa total esperada");
```

Uma das estrat√©gias de solu√ß√£o para o *k*-armed bandit √© o uso de **m√©todos *action-value*** [^3], onde a estimativa do valor de uma a√ß√£o *a* no tempo *t*, denotada por **Qt(a)**, busca se aproximar de q*(a) [^2]. M√©todos de *action-value* comuns incluem o m√©todo *sample-average*, que calcula a m√©dia das recompensas obtidas para cada a√ß√£o [^3].

No entanto, h√° uma distin√ß√£o fundamental entre **explora√ß√£o e *exploitation*** [^2]. *Exploitation* significa selecionar as a√ß√µes com os maiores valores estimados, que s√£o as a√ß√µes *greedy* [^2]. Por outro lado, explorar significa selecionar a√ß√µes n√£o-*greedy* para melhorar a estimativa de seus valores, com o intuito de encontrar a√ß√µes melhores a longo prazo [^2]. O equil√≠brio entre explora√ß√£o e *exploitation* √© crucial para resolver o *k*-armed bandit de forma eficaz [^2].

Nesta se√ß√£o, o foco ser√° sobre os **gradient bandit algorithms**, que n√£o estimam valores de a√ß√£o diretamente, mas aprendem uma **prefer√™ncia num√©rica para cada a√ß√£o**, denotada por **Ht(a)** [^13]. A prefer√™ncia √© usada para determinar a probabilidade de selecionar uma a√ß√£o usando uma **distribui√ß√£o *soft-max***, tamb√©m conhecida como distribui√ß√£o Gibbs ou Boltzmann [^13]:

$$ P\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} = \pi_t(a) $$ [^13]

onde $\pi_t(a)$ representa a probabilidade de selecionar a a√ß√£o *a* no tempo *t*, e as prefer√™ncias de todas as a√ß√µes s√£o exponenciadas e normalizadas para gerar uma distribui√ß√£o de probabilidade [^13]. Inicialmente, todas as prefer√™ncias s√£o iguais, resultando em probabilidades iguais para todas as a√ß√µes [^13].

> üí° **Exemplo Num√©rico:**
>
> Imagine um problema de *3-armed bandit* (k=3) onde temos tr√™s a√ß√µes (a=1, a=2, a=3). Inicialmente, as prefer√™ncias $H_t(a)$ para todas as a√ß√µes s√£o zero, ou seja, $H_1(1) = H_1(2) = H_1(3) = 0$.
>
> Calculando as probabilidades iniciais usando a distribui√ß√£o soft-max:
>
> $$ \pi_1(1) = \frac{e^{0}}{e^{0} + e^{0} + e^{0}} = \frac{1}{3} $$
> $$ \pi_1(2) = \frac{e^{0}}{e^{0} + e^{0} + e^{0}} = \frac{1}{3} $$
> $$ \pi_1(3) = \frac{e^{0}}{e^{0} + e^{0} + e^{0}} = \frac{1}{3} $$
>
> Inicialmente, cada a√ß√£o tem a mesma probabilidade de ser escolhida.

### Deriva√ß√£o da Distribui√ß√£o Soft-Max

A distribui√ß√£o *soft-max* √© usada para transformar prefer√™ncias em probabilidades, sendo que a prefer√™ncia de uma a√ß√£o sobre outra √© relativa. A adi√ß√£o de uma constante a todas as prefer√™ncias n√£o muda as probabilidades de a√ß√£o [^13]. A distribui√ß√£o *soft-max* √© dada por:

$$ \pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} $$ [^13]

Este tipo de fun√ß√£o √© tamb√©m conhecida como **fun√ß√£o log√≠stica**, ou **sigmoide**, quando aplicada a dois casos [^13].

```mermaid
graph LR
    subgraph "Distribui√ß√£o Soft-Max"
        A("Prefer√™ncia Ht(a)") --> B("Exponencia√ß√£o: e^Ht(a)");
        B --> C("Soma de exponencia√ß√µes: Œ£ e^Ht(b)");
        C --> D("Normaliza√ß√£o: e^Ht(a) / Œ£ e^Ht(b)");
        D --> E("Probabilidade œÄt(a)");
    end
```

#### Lemma 1: Rela√ß√£o com a Fun√ß√£o Sigmoide (Caso de Duas A√ß√µes)
*Declara√ß√£o:* No caso de duas a√ß√µes, a distribui√ß√£o *soft-max* √© equivalente √† fun√ß√£o log√≠stica.

*Prova:*
Seja *k* = 2. A probabilidade de selecionar a a√ß√£o 1 √©:

$$ \pi_t(1) = \frac{e^{H_t(1)}}{e^{H_t(1)} + e^{H_t(2)}} $$

Dividindo o numerador e o denominador por $e^{H_t(1)}$:

$$ \pi_t(1) = \frac{1}{1 + e^{H_t(2) - H_t(1)}} $$

Se definirmos $z = H_t(1) - H_t(2)$, temos:

$$ \pi_t(1) = \frac{1}{1 + e^{-z}} $$

Esta √© a forma da fun√ß√£o sigmoide, onde $z$ representa a diferen√ßa entre as prefer√™ncias das a√ß√µes.

$\blacksquare$

**Lema 1.1:** *Invari√¢ncia da Distribui√ß√£o Soft-Max sob Adi√ß√£o de Constantes*
*Declara√ß√£o:* A adi√ß√£o de uma constante *c* a todas as prefer√™ncias $H_t(a)$ n√£o altera as probabilidades de a√ß√£o $\pi_t(a)$ geradas pela distribui√ß√£o *soft-max*.

*Prova:*
Seja $H'_t(a) = H_t(a) + c$ para todas as a√ß√µes *a*. A nova probabilidade de selecionar a a√ß√£o *a* √©:
$$
\pi'_t(a) = \frac{e^{H'_t(a)}}{\sum_{b=1}^k e^{H'_t(b)}} = \frac{e^{H_t(a) + c}}{\sum_{b=1}^k e^{H_t(b) + c}} = \frac{e^{H_t(a)}e^c}{\sum_{b=1}^k e^{H_t(b)}e^c}
$$
Fatorando $e^c$ do numerador e denominador, temos:
$$
\pi'_t(a) = \frac{e^c e^{H_t(a)}}{e^c \sum_{b=1}^k e^{H_t(b)}} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)
$$
Portanto, as probabilidades de a√ß√£o permanecem as mesmas, demonstrando a invari√¢ncia da distribui√ß√£o *soft-max* sob adi√ß√£o de constantes √†s prefer√™ncias.

$\blacksquare$
> üí° **Exemplo Num√©rico (Lema 1.1):**
>
> Continuando o exemplo anterior de *3-armed bandit*, suponha que ap√≥s algumas itera√ß√µes, as prefer√™ncias sejam: $H_t(1) = 1$, $H_t(2) = 0.5$, $H_t(3) = -0.5$.
>
>  As probabilidades s√£o:
>
> $$ \pi_t(1) = \frac{e^{1}}{e^{1} + e^{0.5} + e^{-0.5}} \approx \frac{2.718}{2.718+1.649+0.607} \approx 0.52 $$
> $$ \pi_t(2) = \frac{e^{0.5}}{e^{1} + e^{0.5} + e^{-0.5}} \approx \frac{1.649}{2.718+1.649+0.607} \approx 0.32 $$
> $$ \pi_t(3) = \frac{e^{-0.5}}{e^{1} + e^{0.5} + e^{-0.5}} \approx \frac{0.607}{2.718+1.649+0.607} \approx 0.12 $$
>
> Agora, adicionemos uma constante, digamos *c = 2*, a todas as prefer√™ncias: $H'_t(1) = 3$, $H'_t(2) = 2.5$, $H'_t(3) = 1.5$. As novas probabilidades s√£o:
>
> $$ \pi'_t(1) = \frac{e^{3}}{e^{3} + e^{2.5} + e^{1.5}} \approx \frac{20.086}{20.086+12.182+4.482} \approx 0.52 $$
> $$ \pi'_t(2) = \frac{e^{2.5}}{e^{3} + e^{2.5} + e^{1.5}} \approx \frac{12.182}{20.086+12.182+4.482} \approx 0.32 $$
> $$ \pi'_t(3) = \frac{e^{1.5}}{e^{3} + e^{2.5} + e^{1.5}} \approx \frac{4.482}{20.086+12.182+4.482} \approx 0.12 $$
>
> As probabilidades permanecem as mesmas, demonstrando que a adi√ß√£o de uma constante n√£o afeta a probabilidade de sele√ß√£o da a√ß√£o.

### Atualiza√ß√£o das Prefer√™ncias

Os gradientes s√£o a base para a atualiza√ß√£o das prefer√™ncias [^13]. O algoritmo de aprendizado atualiza as prefer√™ncias atrav√©s do m√©todo **stochastic gradient ascent** [^14]. Ap√≥s selecionar a a√ß√£o At e receber a recompensa Rt, as prefer√™ncias de a√ß√£o s√£o atualizadas como [^13]:

$$ H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \overline{R}_t)(1 - \pi_t(A_t)) $$ [^13]

$$ H_{t+1}(a) = H_t(a) - \alpha(R_t - \overline{R}_t)\pi_t(a), \quad \text{para todo } a \neq A_t $$ [^13]

onde $\alpha$ √© o tamanho do passo (step-size), $\overline{R}_t$ √© uma recompensa m√©dia at√© o tempo *t*, servindo como *baseline* [^13]. Se a recompensa *R* √© maior que a *baseline*, a probabilidade de tomar aquela a√ß√£o aumenta. Se for menor, diminui. A deriva√ß√£o da equa√ß√£o de atualiza√ß√£o das prefer√™ncias usando stochastic gradient ascent √© feita a partir da an√°lise do gradiente da performance esperada, como mostrado nas equa√ß√µes (2.13) a (2.15) do contexto [^14, 15].

```mermaid
graph LR
    subgraph "Atualiza√ß√£o de Prefer√™ncias"
        A("A√ß√£o At selecionada, Recompensa Rt") --> B("Calcular Baseline: RÃÑt");
        B --> C("Atualizar prefer√™ncia Ht(At): Ht+1(At) = Ht(At) + Œ±(Rt - RÃÑt)(1 - œÄt(At))");
         B --> D("Atualizar outras preferencias Ht(a): Ht+1(a) = Ht(a) - Œ±(Rt - RÃÑt)œÄt(a), para a ‚â† At");
         C --> E("Novas Prefer√™ncias Ht+1(a)");
         D --> E
    end
```

> üí° **Exemplo Num√©rico:**
>
> Suponha que estamos no tempo *t*, a a√ß√£o selecionada foi $A_t = 1$, e recebemos uma recompensa de $R_t = 1$. A recompensa m√©dia at√© o momento √© $\overline{R}_t = 0.5$. O tamanho do passo √© $\alpha = 0.1$. Usando as prefer√™ncias do exemplo anterior: $H_t(1) = 1$, $H_t(2) = 0.5$, $H_t(3) = -0.5$, e as probabilidades calculadas $\pi_t(1) \approx 0.52$, $\pi_t(2) \approx 0.32$, $\pi_t(3) \approx 0.12$.
>
> Atualizando as prefer√™ncias:
>
> $$ H_{t+1}(1) = H_t(1) + \alpha(R_t - \overline{R}_t)(1 - \pi_t(1)) $$
> $$ H_{t+1}(1) = 1 + 0.1(1 - 0.5)(1 - 0.52) = 1 + 0.1 * 0.5 * 0.48 = 1 + 0.024 = 1.024 $$
>
> Para as outras a√ß√µes:
>
> $$ H_{t+1}(2) = H_t(2) - \alpha(R_t - \overline{R}_t)\pi_t(2) $$
> $$ H_{t+1}(2) = 0.5 - 0.1(1 - 0.5)(0.32) = 0.5 - 0.1 * 0.5 * 0.32 = 0.5 - 0.016 = 0.484 $$
>
> $$ H_{t+1}(3) = H_t(3) - \alpha(R_t - \overline{R}_t)\pi_t(3) $$
> $$ H_{t+1}(3) = -0.5 - 0.1(1 - 0.5)(0.12) = -0.5 - 0.1 * 0.5 * 0.12 = -0.5 - 0.006 = -0.506 $$
>
> Observe que a prefer√™ncia da a√ß√£o 1, que foi selecionada e teve uma recompensa acima da m√©dia, aumentou. As outras prefer√™ncias diminu√≠ram.
>
> As novas probabilidades podem ser calculadas usando a nova prefer√™ncia.
>
> Calculando as probabilidades para o tempo t+1 usando as novas preferencias $H_{t+1}(1) = 1.024$, $H_{t+1}(2) = 0.484$, $H_{t+1}(3) = -0.506$:
>
> $$ \pi_{t+1}(1) = \frac{e^{1.024}}{e^{1.024} + e^{0.484} + e^{-0.506}} \approx \frac{2.784}{2.784+1.623+0.603} \approx 0.528 $$
> $$ \pi_{t+1}(2) = \frac{e^{0.484}}{e^{1.024} + e^{0.484} + e^{-0.506}} \approx \frac{1.623}{2.784+1.623+0.603} \approx 0.309 $$
> $$ \pi_{t+1}(3) = \frac{e^{-0.506}}{e^{1.024} + e^{0.484} + e^{-0.506}} \approx \frac{0.603}{2.784+1.623+0.603} \approx 0.115 $$
> A probabilidade da a√ß√£o 1 aumentou, enquanto as outras diminu√≠ram.
>

#### Lemma 2: Rela√ß√£o com Stochastic Gradient Ascent

*Declara√ß√£o:* A atualiza√ß√£o das prefer√™ncias no gradient bandit algorithm pode ser derivada como uma aproxima√ß√£o estoc√°stica do gradiente da performance esperada.

*Prova:*
O objetivo √© maximizar a recompensa esperada $E[R_t]$, que √© dada por:

$$ E[R_t] = \sum_{x} \pi_t(x)q_*(x) $$ [^14]

A atualiza√ß√£o das prefer√™ncias no gradient bandit algorithm √© baseada no conceito de stochastic gradient ascent, onde a prefer√™ncia de a√ß√£o √© incrementada proporcionalmente ao efeito desse incremento na performance [^14]:

$$H_{t+1}(a) = H_t(a) + \alpha \frac{\partial E[R_t]}{\partial H_t(a)}$$ [^14]

Ao derivar $\frac{\partial E[R_t]}{\partial H_t(a)}$ [^14, 15], utilizando a regra do quociente, obtemos:

$$ \frac{\partial E[R_t]}{\partial H_t(a)} = \sum_{x}(q_*(x) - \overline{R}_t) \frac{\partial \pi_t(x)}{\partial H_t(a)} $$

onde o *baseline* $\overline{R}_t$ n√£o interfere no gradiente [^15]. Ao usar a distribui√ß√£o *soft-max* para  $\pi_t(x)$, o gradiente resulta em:
$$ \frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x)(\mathbb{1}_{a=x} - \pi_t(a))$$ [^16]

Substituindo, obtemos a forma de atualiza√ß√£o das prefer√™ncias para o gradient bandit algorithm, onde cada atualiza√ß√£o das prefer√™ncias √© proporcional a uma amostra do gradiente esperado:

$$ H_{t+1}(a) = H_t(a) + \alpha (R_t - \overline{R}_t)(\mathbb{1}_{a=A_t} - \pi_t(a)) $$
$\blacksquare$

**Lema 2.1**: *Atualiza√ß√£o Equivalente para Todas as A√ß√µes*
*Declara√ß√£o:* A atualiza√ß√£o das prefer√™ncias pode ser expressa de forma equivalente para todas as a√ß√µes, usando o indicador $\mathbb{1}_{a=A_t}$.

*Prova:*
A atualiza√ß√£o das prefer√™ncias √© dada por:
$$
H_{t+1}(a) = H_t(a) + \alpha(R_t - \overline{R}_t)(\mathbb{1}_{a=A_t} - \pi_t(a))
$$
Analisando o caso em que $a = A_t$:
$$
H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \overline{R}_t)(1 - \pi_t(A_t))
$$
E quando $a \neq A_t$:
$$
H_{t+1}(a) = H_t(a) - \alpha(R_t - \overline{R}_t)\pi_t(a)
$$
Observe que a equa√ß√£o geral inclui ambos os casos. Quando $a = A_t$, $\mathbb{1}_{a=A_t} = 1$, e a equa√ß√£o coincide com a primeira atualiza√ß√£o. Quando $a \neq A_t$, $\mathbb{1}_{a=A_t} = 0$, e a equa√ß√£o se torna igual a segunda atualiza√ß√£o. Portanto, a equa√ß√£o geral pode ser usada para atualizar as prefer√™ncias de todas as a√ß√µes.

$\blacksquare$
> üí° **Exemplo Num√©rico (Lema 2.1):**
>
> Usando o mesmo cen√°rio do exemplo anterior, onde $A_t = 1$, $R_t = 1$, $\overline{R}_t = 0.5$, $\alpha = 0.1$ e prefer√™ncias $H_t(1) = 1$, $H_t(2) = 0.5$, $H_t(3) = -0.5$, com as probabilidades $\pi_t(1) \approx 0.52$, $\pi_t(2) \approx 0.32$, $\pi_t(3) \approx 0.12$.
>
> Usando a equa√ß√£o equivalente:
>
> $$ H_{t+1}(a) = H_t(a) + \alpha(R_t - \overline{R}_t)(\mathbb{1}_{a=A_t} - \pi_t(a)) $$
>
> Para $a=1$:
>
> $$ H_{t+1}(1) = 1 + 0.1(1-0.5)(1-0.52) = 1 + 0.1 * 0.5 * 0.48 = 1.024 $$
>
> Para $a=2$:
>
> $$ H_{t+1}(2) = 0.5 + 0.1(1-0.5)(0 - 0.32) = 0.5 - 0.1 * 0.5 * 0.32 = 0.484 $$
>
> Para $a=3$:
>
> $$ H_{t+1}(3) = -0.5 + 0.1(1-0.5)(0-0.12) = -0.5 - 0.1 * 0.5 * 0.12 = -0.506 $$
>
> Os resultados s√£o id√™nticos ao exemplo anterior, mostrando que a equa√ß√£o equivalente leva √†s mesmas atualiza√ß√µes de prefer√™ncia.

**Proposi√ß√£o 1:** *Rela√ß√£o com a Regra de Aprendizado de Widrow-Hoff*
*Declara√ß√£o:* A atualiza√ß√£o das prefer√™ncias do gradient bandit algorithm, quando expandida, possui uma semelhan√ßa com a regra de aprendizado de Widrow-Hoff, tamb√©m conhecida como regra delta.
*Prova:*
Reescrevendo a equa√ß√£o de atualiza√ß√£o para uma a√ß√£o arbitr√°ria *a*:
$$H_{t+1}(a) = H_t(a) + \alpha(R_t - \overline{R}_t)(\mathbb{1}_{a=A_t} - \pi_t(a))$$
Expandindo a express√£o, obtemos:
$$H_{t+1}(a) = H_t(a) + \alpha(R_t - \overline{R}_t)\mathbb{1}_{a=A_t} - \alpha(R_t - \overline{R}_t)\pi_t(a)$$
A express√£o acima pode ser interpretada como:
- $H_t(a)$: O valor atual da prefer√™ncia.
- $\alpha(R_t - \overline{R}_t)\mathbb{1}_{a=A_t}$: A atualiza√ß√£o na prefer√™ncia quando a a√ß√£o *a* foi selecionada. A atualiza√ß√£o √© proporcional √† diferen√ßa entre a recompensa e o *baseline*.
- $\alpha(R_t - \overline{R}_t)\pi_t(a)$: Um ajuste de probabilidade, que reduz a prefer√™ncia das a√ß√µes que n√£o foram selecionadas, e que reduz a prefer√™ncia da a√ß√£o selecionada proporcionalmente a sua probabilidade.

Esta estrutura se assemelha √† regra de Widrow-Hoff (ou regra delta), que ajusta os pesos em uma rede neural com base no erro (diferen√ßa entre a sa√≠da desejada e a sa√≠da real) e na entrada. A similaridade reside no ajuste proporcional ao erro (recompensa menos *baseline*) e na considera√ß√£o do valor atual da prefer√™ncia/peso. No gradient bandit algorithm, a "entrada" √© representada pelo indicador e pelas probabilidades $\pi_t(a)$, enquanto na regra de Widrow-Hoff, a entrada √© o valor fornecido √† rede neural.

$\blacksquare$

### Conclus√£o

Os **gradient bandit algorithms** representam uma abordagem distinta em compara√ß√£o com os **action-value methods** para a solu√ß√£o de problemas do tipo *k*-armed bandit. Ao inv√©s de estimar os valores de a√ß√µes, esses algoritmos aprendem prefer√™ncias de a√ß√£o. Utilizando uma **distribui√ß√£o soft-max** para determinar as probabilidades de a√ß√£o, eles equilibram a explora√ß√£o e a *exploitation*. A implementa√ß√£o da atualiza√ß√£o de prefer√™ncias atrav√©s do m√©todo stochastic gradient ascent garante a converg√™ncia e adapta√ß√£o ao ambiente do problema. A capacidade de adaptar as prefer√™ncias de a√ß√µes de forma cont√≠nua, junto com o uso de uma *baseline* para a compara√ß√£o de recompensas, permite que estes algoritmos lidem com mudan√ßas no ambiente e aprendam as melhores a√ß√µes para maximizar o retorno a longo prazo.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions."
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt."
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods."
[^13]: "In this section we consider learning a numerical preference for each action a, which we denote Ht(a) \in R. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows: Pr{At=a} = eHt(a) / \sumk b=1eHt(b) = \pi_t(a),..."
[^14]: "One can gain a deeper insight into the gradient bandit algorithm by understanding it as a stochastic approximation to gradient ascent. In exact gradient ascent, each action preference Ht(a) would be incremented in proportion to the increment's effect on performance: Ht+1(a) = Ht(a) + \alpha \partialE[Rt]/\partialHt(a), where the measure of performance here is the expected reward: E[Rt] = \sumx \pi_t(x)q_*(x), and the measure of the increment's effect is the partial derivative of this performance measure with respect to the action preference."
[^15]: "Next we multiply each term of the sum by \pi_t(x)/\pi_t(x): \partialE[Rt]/\partialHt(a) = \sumx \pi_t(x) (q_*(x) ‚àí Bt) \partial\pi_t(x)/\partialHt(a) /\pi_t(x). The equation is now in the form of an expectation, summing over all possible values x of the random variable At, then multiplying by the probability of taking those values."
[^16]: "Thus it remains only to show that \partial\pi_t(x)/\partialHt(a) = \pi_t(x)(1_{a=x} ‚àí \pi_t(a)), as we assumed. Recall the standard quotient rule for derivatives:  \partial/\partialx [f(x)/g(x)] = (\partialf(x)/\partialx g(x)‚àíf(x) \partialg(x)/\partialx) /g(x)^2. Using this, we can write \partial\pi_t(x)/\partialHt(a) = \partial/\partialHt(a)  [ eHt(x) / \sumk y=1 eHt(y)]  ..."
