## Gradient Bandit Algorithms: Prefer√™ncias Relativas e Aprendizado por Gradiente

### Introdu√ß√£o
No campo do aprendizado por refor√ßo, os m√©todos para estimar **valores de a√ß√£o** e us√°-los para selecionar a√ß√µes s√£o abordagens comuns, mas n√£o as √∫nicas [^1]. Uma alternativa √© aprender uma **prefer√™ncia num√©rica** para cada a√ß√£o, denotada por $H_t(a)$, onde $a$ representa uma a√ß√£o e $t$ o passo de tempo [^1]. Uma caracter√≠stica crucial desses m√©todos √© que as **prefer√™ncias de a√ß√£o** $H_t(a)$ n√£o possuem uma interpreta√ß√£o direta em termos de recompensa; em vez disso, apenas a prefer√™ncia relativa de uma a√ß√£o em rela√ß√£o a outra √© significativa [^1]. O foco n√£o √© em quanto uma a√ß√£o √© "boa" em termos absolutos, mas sim em como ela se compara a outras a√ß√µes poss√≠veis [^1]. Isso leva a uma abordagem distinta para a sele√ß√£o de a√ß√µes, baseada em uma **distribui√ß√£o soft-max**, onde a probabilidade de escolher uma a√ß√£o √© proporcional √† sua prefer√™ncia exponencial [^1].

### Conceitos Fundamentais

#### Prefer√™ncias Relativas de A√ß√£o
Diferentemente dos m√©todos baseados em valores de a√ß√£o, os algoritmos **Gradient Bandit** aprendem as **prefer√™ncias de a√ß√£o** $H_t(a)$, que s√£o valores num√©ricos que indicam a conveni√™ncia relativa de cada a√ß√£o [^1]. Quanto maior a prefer√™ncia, mais frequentemente uma a√ß√£o √© escolhida [^1]. No entanto, √© fundamental ressaltar que a magnitude de $H_t(a)$ n√£o corresponde a uma recompensa esperada ou a qualquer m√©trica de valor absoluto [^1]. O que importa √© a diferen√ßa entre as prefer√™ncias de diferentes a√ß√µes [^1]. Por exemplo, se adicionarmos 1000 a todas as prefer√™ncias, as probabilidades de a√ß√£o n√£o ser√£o afetadas [^1]. Isso ocorre porque a sele√ß√£o da a√ß√£o √© baseada na compara√ß√£o relativa dessas prefer√™ncias, o que √© formalizado pela distribui√ß√£o soft-max [^1].

> üí° **Exemplo Num√©rico:** Imagine que temos 3 a√ß√µes, A, B e C, com prefer√™ncias iniciais $H_t(A) = 1$, $H_t(B) = 2$ e $H_t(C) = 0$. Se adicionarmos 1000 a todas as prefer√™ncias, teremos $H_t(A) = 1001$, $H_t(B) = 1002$ e $H_t(C) = 1000$. A probabilidade de sele√ß√£o de cada a√ß√£o, calculada pela distribui√ß√£o soft-max, permanece a mesma em ambos os casos, mostrando a import√¢ncia da prefer√™ncia relativa em vez do valor absoluto.

#### Distribui√ß√£o Soft-Max
A probabilidade de selecionar uma a√ß√£o $a$ no tempo $t$, denotada por $\pi_t(a)$, √© determinada usando uma distribui√ß√£o **soft-max** (tamb√©m conhecida como distribui√ß√£o Gibbs ou Boltzmann) [^1]:
$$
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}}
$$
onde $k$ √© o n√∫mero total de a√ß√µes poss√≠veis [^1]. Esta equa√ß√£o transforma as prefer√™ncias de a√ß√£o $H_t(a)$ em probabilidades, garantindo que a probabilidade de selecionar uma a√ß√£o seja proporcional √† sua prefer√™ncia, mas que todas as probabilidades somem 1 [^1]. No in√≠cio, as prefer√™ncias de a√ß√£o s√£o inicializadas de forma que todas as a√ß√µes tenham a mesma probabilidade de serem selecionadas, por exemplo, $H_1(a) = 0$ para todas as a√ß√µes [^1].

```mermaid
graph LR
    A["H_t(a) : Prefer√™ncias de A√ß√£o"]
    B["Exp(H_t(a)) : Exponencial das Prefer√™ncias"]
    C["Soma das Exponenciais : \sum(Exp(H_t(b)))"]
    D["pi_t(a) : Probabilidade Soft-Max"]
    A --> B
    B --> C
    B --> D
    C --> D
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3 stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Considere novamente as 3 a√ß√µes (A, B, C) com prefer√™ncias $H_t(A) = 1$, $H_t(B) = 2$ e $H_t(C) = 0$. Usando a distribui√ß√£o soft-max:
>
> $\pi_t(A) = \frac{e^1}{e^1 + e^2 + e^0} \approx \frac{2.718}{2.718 + 7.389 + 1} \approx \frac{2.718}{11.107} \approx 0.245$
>
> $\pi_t(B) = \frac{e^2}{e^1 + e^2 + e^0} \approx \frac{7.389}{11.107} \approx 0.665$
>
> $\pi_t(C) = \frac{e^0}{e^1 + e^2 + e^0} \approx \frac{1}{11.107} \approx 0.090$
>
> Observe que a a√ß√£o B, com a maior prefer√™ncia, tem a maior probabilidade de ser selecionada. As probabilidades somam aproximadamente 1.

#### Aprendizado por Gradiente Estoc√°stico
O algoritmo de aprendizagem para as prefer√™ncias de a√ß√£o √© baseado na ideia de **ascens√£o do gradiente estoc√°stico** [^1]. Em cada passo, ap√≥s selecionar uma a√ß√£o $A_t$ e receber a recompensa $R_t$, as prefer√™ncias de a√ß√£o s√£o atualizadas usando a seguinte regra:
$$
H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - \bar{R}_t) (1 - \pi_t(A_t)),
$$
e, para todas as outras a√ß√µes $a \neq A_t$:
$$
H_{t+1}(a) = H_t(a) - \alpha (R_t - \bar{R}_t) \pi_t(a),
$$
onde $\alpha > 0$ √© um par√¢metro de tamanho do passo (step-size), e $\bar{R}_t$ √© a recompensa m√©dia at√© o tempo $t$ (que serve como linha de base, *baseline*) [^1]. A *baseline*  $\bar{R}_t$ √© crucial para o desempenho do algoritmo [^1]. Se a recompensa $R_t$ for maior que a *baseline*, a probabilidade de escolher a a√ß√£o $A_t$ no futuro aumenta, enquanto se a recompensa for menor que a *baseline*, a probabilidade diminui [^1]. As a√ß√µes n√£o selecionadas se movem na dire√ß√£o oposta [^1]. A atualiza√ß√£o das prefer√™ncias garante que as a√ß√µes que levaram a recompensas acima da m√©dia (relativa ao *baseline*) ser√£o mais propensas a serem selecionadas, e aquelas que levaram a recompensas abaixo da m√©dia ser√£o menos propensas a serem selecionadas no futuro. A forma como essa *baseline* √© calculada pode ser incrementalmente com um par√¢metro step-size $\alpha$ como descrito na se√ß√£o 2.4 e 2.5 [^1].

```mermaid
sequenceDiagram
    participant Agente
    participant Ambiente
    Agente->>Ambiente: "Seleciona A√ß√£o A_t"
    Ambiente->>Agente: "Retorna Recompensa R_t"
    Agente->>Agente: "Calcula Baseline R_bar_t"
    Agente->>Agente: "Calcula Diferen√ßa (R_t - R_bar_t)"
    Agente->>Agente: "Atualiza H_t+1(A_t) = H_t(A_t) + alpha*(R_t - R_bar_t)*(1 - pi_t(A_t))"
    Agente->>Agente: "Atualiza H_t+1(a) = H_t(a) - alpha*(R_t - R_bar_t)*pi_t(a), para a!=A_t"
```

> üí° **Exemplo Num√©rico:** Vamos considerar um cen√°rio onde temos duas a√ß√µes (A e B) e a cada passo selecionamos uma a√ß√£o, recebemos uma recompensa e atualizamos as preferencias:
>
> **Passo 1:**
> *   Inicializamos: $H_1(A) = 0$, $H_1(B) = 0$, $\alpha=0.1$, $\bar{R}_1 = 0$
> *   Selecionamos a a√ß√£o A.
> *   Recebemos uma recompensa $R_1 = 1$
> *   Calculamos $\pi_1(A)$ e $\pi_1(B)$ usando a distribui√ß√£o soft-max: $\pi_1(A) = \frac{e^0}{e^0 + e^0} = 0.5$ e $\pi_1(B) = 0.5$
> *   Atualizamos as prefer√™ncias:
>     $H_2(A) = 0 + 0.1 * (1 - 0) * (1 - 0.5) = 0.05$
>     $H_2(B) = 0 - 0.1 * (1-0) * 0.5 = -0.05$
> *  Atualizamos a baseline $\bar{R}_2 = \frac{0+1}{2}=0.5$
>
> **Passo 2:**
> *   Selecionamos a a√ß√£o B.
> *   Recebemos uma recompensa $R_2 = 0$.
> *   Calculamos  $\pi_2(A) = \frac{e^{0.05}}{e^{0.05} + e^{-0.05}} \approx 0.512$ e $\pi_2(B) = 1 - \pi_2(A) \approx 0.488$
> *   Atualizamos as prefer√™ncias:
>      $H_3(A) = 0.05 - 0.1 * (0-0.5) * 0.512 = 0.0756$
>      $H_3(B) = -0.05 + 0.1 * (0 - 0.5) * (1-0.488) \approx -0.0756$
> *  Atualizamos a baseline $\bar{R}_3 = \frac{0+1+0}{3}=\frac{1}{3}\approx 0.33$
>
>   Note como as prefer√™ncias se ajustam de acordo com as recompensas e como a baseline influencia a magnitude do ajuste.

**Proposi√ß√£o 1**
O algoritmo Gradient Bandit pode ser visto como uma inst√¢ncia de *policy gradient*, onde a pol√≠tica √© dada pela distribui√ß√£o soft-max.

**Proof**
O algoritmo atualiza as prefer√™ncias de a√ß√£o $H_t(a)$ de forma a aumentar a probabilidade de a√ß√µes que levaram a recompensas acima da m√©dia e diminuir a probabilidade de a√ß√µes que levaram a recompensas abaixo da m√©dia. Essa atualiza√ß√£o √© feita na dire√ß√£o do gradiente da recompensa esperada, como demonstrado no Lema 1. Em m√©todos *policy gradient*, o objetivo √© otimizar diretamente a pol√≠tica, e n√£o os valores de a√ß√£o. O Gradient Bandit atinge isso otimizando as prefer√™ncias, que, por sua vez, definem a pol√≠tica atrav√©s da distribui√ß√£o soft-max. Portanto, o algoritmo se encaixa no *framework* de *policy gradient* $\blacksquare$

```mermaid
graph LR
    A["Gradient Bandit Algorithm"]
    B["Otimiza√ß√£o das Prefer√™ncias H_t(a)"]
    C["Distribui√ß√£o Soft-Max pi_t(a)"]
    D["Pol√≠tica de A√ß√£o"]
    E["Framework Policy Gradient"]
    A --> B
    B --> C
    C --> D
    D --> E
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3 stroke-width:2px
```

#### Lemma 1
O uso da distribui√ß√£o soft-max como mostrado na equa√ß√£o:
$$
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}}
$$
e a atualiza√ß√£o das prefer√™ncias de a√ß√£o em dire√ß√£o ao gradiente da recompensa esperada, faz com que o algoritmo possa ser considerado como um **ascens√£o de gradiente estoc√°stico**.

##### *Proof*
A atualiza√ß√£o das prefer√™ncias de a√ß√£o em dire√ß√£o ao gradiente da recompensa esperada pode ser expressa por:
$$
H_{t+1}(a) = H_t(a) + \alpha\frac{\partial E[R_t]}{\partial H_t(a)}
$$
onde a recompensa esperada √© dada por:
$$
E[R_t] = \sum_x \pi_t(x) q_*(x)
$$
e o gradiente da recompensa esperada com respeito a uma prefer√™ncia de a√ß√£o √©:
$$
\frac{\partial E[R_t]}{\partial H_t(a)} =  \sum_x q_*(x) \frac{\partial \pi_t(x)}{\partial H_t(a)}
$$
Usando o conceito de baseline como descrito em [^1] e multiplicando por $\frac{\pi_t(x)}{\pi_t(x)}$ :
$$
\frac{\partial E[R_t]}{\partial H_t(a)} =  \sum_x  \frac{\pi_t(x)}{\pi_t(x)} (q_*(x) - B_t)  \frac{\partial \pi_t(x)}{\partial H_t(a)}
$$
Isso resulta em:
$$
=  E\left[(R_t - \bar{R}_t) \frac{\partial \pi_t(A_t)}{\partial H_t(a)}  \frac{1}{\pi_t(A_t)} \right]
$$
Aplicando a regra do quociente e manipulando as derivadas, conforme feito em [^1] chega-se ao seguinte resultado:
$$
\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) (\mathbb{1}_{a=x} - \pi_t(a))
$$
Onde  $\mathbb{1}_{a=x}$ √© 1 quando a=x, e zero quando $a \ne x$. Inserindo esse resultado na equa√ß√£o anterior temos:

$$
= E\left[(R_t - \bar{R}_t) ( \mathbb{1}_{a=A_t} - \pi_t(a) ) \right]
$$

Substituindo o resultado no algoritmo de ascens√£o do gradiente:

$$
H_{t+1}(a) = H_t(a) + \alpha (R_t - \bar{R}_t) (\mathbb{1}_{a=A_t} - \pi_t(a))
$$
Que √© equivalente a (2.12), provando assim que o m√©todo √© uma instancia de ascens√£o do gradiente estoc√°stico [^1]. $\blacksquare$

```mermaid
graph LR
    A["Gradiente da Recompensa Esperada: $\\frac{\\partial E[R_t]}{\\partial H_t(a)}$"]
    B["Derivada da Probabilidade: $\\frac{\\partial \\pi_t(x)}{\\partial H_t(a)}$"]
     C["Gradiente da Recompensa Esperada = $E[(R_t - \\bar{R}_t) (1_{a=A_t} - \\pi_t(a))]$"]
    D["Atualiza√ß√£o de Prefer√™ncias: $H_{t+1}(a) = H_t(a) + \\alpha * (R_t - \\bar{R}_t) * (1_{a=A_t} - \\pi_t(a))$"]
    A --> B
    B --> C
    C --> D
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
     linkStyle 0,1,2 stroke-width:2px
```

**Lema 1.1**
A equival√™ncia demonstrada no Lemma 1, entre a atualiza√ß√£o do algoritmo Gradient Bandit e a ascens√£o de gradiente estoc√°stico, implica que o algoritmo converge para um √≥timo local, sob certas condi√ß√µes sobre o tamanho do passo $\alpha$ e a suavidade da fun√ß√£o de recompensa esperada.

**Proof**
Este resultado decorre diretamente da teoria de otimiza√ß√£o por gradiente estoc√°stico. Quando a fun√ß√£o objetivo (neste caso, a recompensa esperada) √© suave e o tamanho do passo √© apropriadamente ajustado (por exemplo, decrescente ao longo do tempo), o algoritmo de ascens√£o de gradiente estoc√°stico converge para um √≥timo local. Como demonstrado no Lemma 1, a atualiza√ß√£o das prefer√™ncias de a√ß√£o do Gradient Bandit √© uma inst√¢ncia de ascens√£o de gradiente estoc√°stico, logo, as garantias de converg√™ncia do gradiente estoc√°stico se aplicam ao Gradient Bandit.  $\blacksquare$

#### Corol√°rio 1
A escolha da *baseline* √© um ponto crucial no algoritmo, e a escolha da m√©dia de todas as recompensas at√© aquele ponto √© uma boa escolha porque n√£o afeta a performance do algoritmo, mas diminui a vari√¢ncia da atualiza√ß√£o, e, portanto, melhora a taxa de converg√™ncia.

**Corol√°rio 1.1**
A escolha de uma *baseline* diferente de zero, mesmo que constante, n√£o afeta o resultado final do algoritmo, pois as prefer√™ncias de a√ß√£o $H_t(a)$ s√£o relativas. No entanto, uma *baseline* constante pode levar a um comportamento de explora√ß√£o/explota√ß√£o diferente durante a fase de aprendizado inicial.

**Proof**
Se adicionarmos um valor constante $C$ √† *baseline*, a atualiza√ß√£o das prefer√™ncias se torna:
$H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - (\bar{R}_t + C)) (1 - \pi_t(A_t))$ e
$H_{t+1}(a) = H_t(a) - \alpha (R_t - (\bar{R}_t + C)) \pi_t(a)$. Isso √© equivalente a $H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - \bar{R}_t)(1 - \pi_t(A_t)) - \alpha C (1 - \pi_t(A_t))$ e  $H_{t+1}(a) = H_t(a) - \alpha (R_t - \bar{R}_t)\pi_t(a) + \alpha C \pi_t(a)$. Observe que o termo  $-\alpha C (1 - \pi_t(A_t))$ e $\alpha C \pi_t(a)$ somam zero quando computamos a varia√ß√£o total da prefer√™ncia. Ou seja, o efeito de uma baseline constante √© somente adicionar um termo constante em cada itera√ß√£o. Como as prefer√™ncias de a√ß√£o s√£o relativas, essa constante n√£o afeta o resultado final. Contudo, durante a fase inicial de aprendizado, uma *baseline* constante pode aumentar ou diminuir o qu√£o r√°pido as prefer√™ncias de a√ß√£o mudam, o que pode afetar o trade-off entre explora√ß√£o e explota√ß√£o. $\blacksquare$

```mermaid
graph LR
    A["Baseline R_bar_t"]
    B["Adi√ß√£o de Constante C"]
    C["Atualiza√ß√£o da Prefer√™ncia com Baseline Alterada"]
    D["Efeito Relativo das Prefer√™ncias"]
    E["Impacto no Trade-off Explora√ß√£o/Explota√ß√£o"]
    A --> B
    B --> C
    C --> D
    D --> E
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2,3 stroke-width:2px
```

### Conclus√£o
Em resumo, os algoritmos **Gradient Bandit** s√£o uma abordagem alternativa para a sele√ß√£o de a√ß√µes no aprendizado por refor√ßo, onde o foco √© nas prefer√™ncias relativas de a√ß√£o, ao inv√©s de valores de a√ß√£o. A combina√ß√£o da distribui√ß√£o soft-max com o aprendizado por gradiente estoc√°stico permite que esses algoritmos explorem e encontrem as a√ß√µes que levam a melhores resultados, adaptando as prefer√™ncias relativas ao longo do tempo com base nas recompensas obtidas. A formula√ß√£o desse algoritmo usando o framework de ascens√£o de gradiente estoc√°stico garante robustez e boas propriedades de converg√™ncia.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. ... In this section we consider learning a numerical preference for each action a, which we denote H≈Ç(a) ‚àà R. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows: ... There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action At and receiving the reward Rt, the action preferences are updated by:
Ht+1(At) = Ht(At) + a(Rt ‚Äì Rt) (1 ‚Äì œÄœÑ(At)),
Ht+1(a) = H+(a) ‚Äì Œ±(Rt ‚Äì Rt)œÄŒπ(Œ±),
for all a ‚â† At," *(Trecho de Multi-armed Bandits)*
