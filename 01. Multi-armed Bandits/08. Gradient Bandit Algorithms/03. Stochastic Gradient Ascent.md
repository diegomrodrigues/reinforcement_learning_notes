## Gradient Bandit Algorithms e Ascens√£o de Gradiente Estoc√°stico

### Introdu√ß√£o

Este cap√≠tulo explora o conceito de **Multi-armed Bandits (MAB)** no contexto de *reinforcement learning*, onde o aprendizado ocorre por meio de avalia√ß√µes das a√ß√µes tomadas, em vez de instru√ß√µes diretas sobre a√ß√µes corretas [1]. Abordamos o problema do MAB, uma vers√£o simplificada do *reinforcement learning* que permite explorar como o feedback avaliativo funciona e como pode ser combinado com feedback instrutivo [1]. O foco aqui √© o **Gradient Bandit Algorithms**, que aprendem prefer√™ncias num√©ricas para cada a√ß√£o, em vez de valores de a√ß√£o [2]. Esta se√ß√£o espec√≠fica aprofunda o entendimento de como esses algoritmos podem ser entendidos como uma aproxima√ß√£o estoc√°stica da ascens√£o de gradiente.

### Conceitos Fundamentais

Em contraste com os m√©todos que estimam os valores de a√ß√£o, os **Gradient Bandit Algorithms** aprendem uma **prefer√™ncia num√©rica** $H_t(a) \in \mathbb{R}$ para cada a√ß√£o *a* [2]. Essa prefer√™ncia indica a frequ√™ncia com que uma a√ß√£o √© escolhida, sem uma interpreta√ß√£o direta em termos de recompensa [2]. As probabilidades de escolha da a√ß√£o s√£o determinadas usando uma **distribui√ß√£o soft-max**, tamb√©m conhecida como distribui√ß√£o de Gibbs ou Boltzmann, dada por:

$$
Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)
$$

onde $\pi_t(a)$ denota a probabilidade de tomar a a√ß√£o *a* no tempo *t*, e *k* √© o n√∫mero total de a√ß√µes [2]. Inicialmente, todas as prefer√™ncias s√£o iguais, resultando em uma probabilidade igual de sele√ß√£o para todas as a√ß√µes [2].

> üí° **Exemplo Num√©rico:** Considere um cen√°rio com 3 a√ß√µes (k=3). Inicialmente, as prefer√™ncias podem ser definidas como $H_1(1) = 0$, $H_1(2) = 0$, e $H_1(3) = 0$. As probabilidades iniciais de cada a√ß√£o, calculadas com a f√≥rmula softmax, seriam:
>
>  $\pi_1(1) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.33$
>
> $\pi_1(2) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.33$
>
> $\pi_1(3) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.33$
>
> Isso significa que todas as a√ß√µes t√™m a mesma probabilidade de serem escolhidas no in√≠cio.

O aprendizado das prefer√™ncias de a√ß√£o √© baseado na ideia da **ascens√£o de gradiente estoc√°stico** [2]. Ap√≥s selecionar uma a√ß√£o $A_t$ e receber a recompensa $R_t$, as prefer√™ncias s√£o atualizadas da seguinte forma:

```mermaid
flowchart LR
    A[ "Iniciar: H_t(a) para cada a√ß√£o a" ] --> B{ "Selecionar A√ß√£o A_t" };
    B --> C{ "Receber Recompensa R_t" };
    C --> D{ "Calcular R_t (m√©dia das recompensas)"};
    D --> E{ "Atualizar H_t(A_t): H_{t+1}(A_t) = H_t(A_t) + Œ±(R_t - R_t)(1 - œÄ_t(A_t))" };
    E --> F{ "Atualizar H_t(a) para a != A_t: H_{t+1}(a) = H_t(a) - Œ±(R_t - R_t)œÄ_t(a)"};
    F --> G[ "H_t(a) atualizadas para o pr√≥ximo passo" ];
    G --> B;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

$$
H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t))
$$
$$
H_{t+1}(a) = H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a), \quad \text{para todo } a \neq A_t
$$

onde $\alpha > 0$ √© o *step-size parameter*, e $\bar{R}_t$ √© uma linha de base que corresponde √† m√©dia das recompensas at√© o tempo *t* [2]. A linha de base serve para comparar a recompensa atual; se a recompensa for maior que a linha de base, a probabilidade de tomar aquela a√ß√£o no futuro aumenta, e vice-versa [2].

> üí° **Exemplo Num√©rico:** Suponha que a a√ß√£o $A_t = 1$ seja selecionada em *t=1*, e a recompensa recebida seja $R_1 = 1$, com $\alpha = 0.1$ e $\bar{R}_1 = 0$ (j√° que √© a primeira recompensa). A probabilidade de escolher a a√ß√£o 1 foi previamente calculada como $\pi_1(1) \approx 0.33$. As prefer√™ncias s√£o ent√£o atualizadas:
>
> $H_{2}(1) = H_1(1) + \alpha(R_1 - \bar{R}_1)(1 - \pi_1(1)) = 0 + 0.1(1-0)(1-0.33) = 0 + 0.1 * 0.67 = 0.067$
>
> Para as outras a√ß√µes, com a premissa de que $H_1(2)=0$ e $H_1(3)=0$ e $\pi_1(2) \approx 0.33$ e $\pi_1(3) \approx 0.33$:
>
> $H_{2}(2) = H_1(2) - \alpha(R_1 - \bar{R}_1)\pi_1(2) = 0 - 0.1(1-0)(0.33) = 0 - 0.033 = -0.033$
>
> $H_{2}(3) = H_1(3) - \alpha(R_1 - \bar{R}_1)\pi_1(3) = 0 - 0.1(1-0)(0.33) = 0 - 0.033 = -0.033$
>
> Ap√≥s esta atualiza√ß√£o, a prefer√™ncia da a√ß√£o 1 aumentou, enquanto as prefer√™ncias das outras a√ß√µes diminu√≠ram, o que, por sua vez, impactar√° as probabilidades de sele√ß√£o de cada a√ß√£o no pr√≥ximo passo.

**Proposi√ß√£o 1**
A atualiza√ß√£o das prefer√™ncias pode ser expressa de forma compacta usando a nota√ß√£o de fun√ß√£o indicadora:
$$
H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t)(\mathbb{1}\{a = A_t\} - \pi_t(a))
$$
*Proof Outline:* This compact form is simply a restatement of the two-part update rule, combining the case when $a = A_t$ (where the indicator is 1) and the case when $a \neq A_t$ (where the indicator is 0) into a single expression.

A conex√£o entre os Gradient Bandit Algorithms e a ascens√£o de gradiente estoc√°stico pode ser demonstrada ao mostrar que a atualiza√ß√£o das prefer√™ncias de a√ß√£o √© uma aproxima√ß√£o estoc√°stica do gradiente da performance esperada. O gradiente exato da performance √© dado por:

```mermaid
flowchart LR
  subgraph "Deriva√ß√£o do Gradiente da Performance Esperada"
    A[ "Gradiente da Performance Esperada:" ] --> B("‚àÇE[R_t]/‚àÇH_t(a) = Œ£_x (q_*(x) - B_t) * ‚àÇœÄ_t(x)/‚àÇH_t(a)");
    B --> C("Multiplicar por œÄ_t(x)/œÄ_t(x):");
    C --> D("‚àÇE[R_t]/‚àÇH_t(a) = Œ£_x  (‚àÇœÄ_t(x)/‚àÇH_t(a) * œÄ_t(x) / œÄ_t(x)) * (q_*(x) - B_t)");
    D --> E("Forma de Esperan√ßa:");
    E --> F("‚àÇE[R_t]/‚àÇH_t(a) = E[(R_t - RÃÑ_t) * (‚àÇœÄ_t(A_t)/‚àÇH_t(a) * 1/œÄ_t(A_t))]");
  end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = \sum_x (q_*(x) - B_t) \frac{\partial \pi_t(x)}{\partial H_t(a)}
$$

onde $B_t$ √© uma linha de base que n√£o depende de *x*. Multiplicando e dividindo cada termo por $\pi_t(x)$, a equa√ß√£o se torna:

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = \sum_x \frac{\partial \pi_t(x)}{\partial H_t(a)} \frac{\pi_t(x)}{\pi_t(x)} (q_*(x) - B_t)
$$

Esta equa√ß√£o est√° agora na forma de uma esperan√ßa, que pode ser amostrada substituindo $q_*(A_t)$ por $R_t$ e definindo a linha de base como a m√©dia das recompensas [2]:

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E \left[\left(R_t - \bar{R}_t\right) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} \frac{1}{\pi_t(A_t)} \right]
$$

Para chegar ao algoritmo de atualiza√ß√£o apresentado anteriormente, √© preciso demonstrar que $\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x)(1_{a=x} - \pi_t(a))$, onde $1_{a=x}$ √© 1 se $a = x$ e 0 caso contr√°rio.  A derivada da probabilidade com rela√ß√£o √† prefer√™ncia pode ser encontrada usando a regra do quociente:

```mermaid
flowchart LR
    A[ "Regra do Quociente:" ] --> B("‚àÇ/‚àÇx [f(x)/g(x)] = (f'(x)g(x) - f(x)g'(x)) / g(x)^2");
    B --> C{ "Aplicar √† derivada de œÄ_t(x) com respeito a H_t(a)"};
    C --> D("‚àÇœÄ_t(x)/‚àÇH_t(a) =  (‚àÇ/‚àÇH_t(a) * e^H_t(x) * Œ£_y e^H_t(y) - e^H_t(x) * ‚àÇ/‚àÇH_t(a) * Œ£_y e^H_t(y) ) / (Œ£_y e^H_t(y))^2 ");
        D --> E("Simplificando a Derivada:");
    E --> F("‚àÇœÄ_t(x)/‚àÇH_t(a) =  (1_{a=x}* e^H_t(x) * Œ£_y e^H_t(y) - e^H_t(x) * e^H_t(a)) / (Œ£_y e^H_t(y))^2");
        F --> G("‚àÇœÄ_t(x)/‚àÇH_t(a) =  1_{a=x}* œÄ_t(x) - œÄ_t(x) * œÄ_t(a)");
     G --> H("‚àÇœÄ_t(x)/‚àÇH_t(a) =  œÄ_t(x) (1_{a=x} - œÄ_t(a))");
       style A fill:#f9f,stroke:#333,stroke-width:2px
        style H fill:#ccf,stroke:#333,stroke-width:2px
```

$$
\frac{\partial}{\partial x} \left[ \frac{f(x)}{g(x)} \right] = \frac{\frac{\partial f(x)}{\partial x} g(x) - f(x) \frac{\partial g(x)}{\partial x}}{g(x)^2}
$$

Aplicando essa regra, obtemos:

$$
\frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)} \left[ \frac{e^{H_t(x)}}{\sum_{y=1}^k e^{H_t(y)}} \right] = \frac{1_{a=x} e^{H_t(x)} \sum_{y=1}^k e^{H_t(y)} - e^{H_t(x)} e^{H_t(a)}}{ \left( \sum_{y=1}^k e^{H_t(y)} \right)^2 } =  \frac{1_{a=x} e^{H_t(x)}}{\sum_{y=1}^k e^{H_t(y)}} - \frac{e^{H_t(x)} e^{H_t(a)}}{\left( \sum_{y=1}^k e^{H_t(y)} \right)^2} = 1_{a=x} \pi_t(x) - \pi_t(x) \pi_t(a) = \pi_t(x)(1_{a=x} - \pi_t(a))
$$

Substituindo esse resultado na equa√ß√£o do gradiente, temos:

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E[(R_t - \bar{R}_t)(1_{a=A_t} - \pi_t(a))]
$$
**Lema 1**
The gradient of the log-probability of action *x* with respect to the preference *H_t(a)*, can be expressed as:
$$
    \frac{\partial}{\partial H_t(a)} \log(\pi_t(x)) = \mathbb{1}\{a=x\} - \pi_t(a)
$$
*Proof Outline:* This can be derived directly from the derivative of $\pi_t(x)$ with respect to *H_t(a)* and the property that $\frac{\partial}{\partial x}\log(f(x)) = \frac{f'(x)}{f(x)}$ and $\pi_t(x) = \frac{e^{H_t(x)}}{\sum_{y=1}^k e^{H_t(y)}}$.

A atualiza√ß√£o estoc√°stica por ascens√£o de gradiente √© obtida ao amostrar essa esperan√ßa e atualizar as prefer√™ncias:

```mermaid
flowchart LR
  subgraph "Atualiza√ß√£o das Prefer√™ncias (Gradient Ascent Estoc√°stico)"
    A[ "Gradiente da performance em rela√ß√£o √†s prefer√™ncias:" ] --> B("‚àÇE[R_t]/‚àÇH_t(a) = E[(R_t - RÃÑ_t) * (1_{a=A_t} - œÄ_t(a))]");
    B --> C("Atualiza√ß√£o Estoc√°stica:");
    C --> D("H_{t+1}(a) = H_t(a) + Œ± * (R_t - RÃÑ_t) * (1_{a=A_t} - œÄ_t(a))");

  end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```
$$
H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t)(1_{a=A_t} - \pi_t(a))
$$

Esta equa√ß√£o √© equivalente ao algoritmo de atualiza√ß√£o original [2].

**Teorema 1**
The gradient bandit update rule can be viewed as a stochastic approximation of the gradient ascent method on the expected reward, considering the softmax distribution.
*Proof Outline:* This result follows directly from the derivation that shows that the update of preferences, $H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t)(1_{a=A_t} - \pi_t(a))$, is proportional to the derivative of the expected reward with respect to the preferences, as derived in the preceding steps. The stochasticity arises from the sampling of the action $A_t$ and the reward $R_t$.

### Conclus√£o

Este aprofundamento do Gradient Bandit Algorithm revela sua natureza como uma **aproxima√ß√£o estoc√°stica da ascens√£o de gradiente**, demonstrando que a atualiza√ß√£o das prefer√™ncias de a√ß√£o segue uma dire√ß√£o que maximiza a recompensa esperada [2]. Essa compreens√£o n√£o apenas valida o algoritmo, mas tamb√©m oferece uma vis√£o sobre sua robustez e propriedades de converg√™ncia. A flexibilidade do uso de uma linha de base, como a m√©dia das recompensas, demonstra sua aplicabilidade pr√°tica, mesmo sem o conhecimento da fun√ß√£o de valor √≥tima.

### Refer√™ncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. [...]" *(Multi-armed Bandits)*
[^2]: "So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. [...] In this section we consider learning a numerical preference for each action a, which we denote H≈Ç(a) ‚àà R. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important [...]" *(Multi-armed Bandits)*
[^3]: "There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action At and receiving the reward Rt, the action preferences are updated by:
Ht+1(At) = Ht(At) + a(Rt ‚Äì Rt) (1 ‚Äì œÄœÑ(At)),
Ht+1(a) = H+(a) ‚Äì Œ±(Rt ‚Äì Rt)œÄŒπ(Œ±), and for all a ‚â† At, [...]" *(Multi-armed Bandits)*
[^4]: "In the empirical results in this chapter, the baseline Rt also included Rt." *(Multi-armed Bandits)*
[^5]: "steps. First we take a closer look at the exact performance gradient: [...]
Next we multiply each term of the sum by œÄt(x)/œÄt(x): [...]
The equation is now in the form of an expectation, summing over all possible values x of the random variable At, then multiplying by the probability of taking those values. Thus: [...]
Recall that our plan has been to write the performance gradient as an expectation of something that we can sample on each step, as we have just done, and then update on each step in proportion to the sample. Substituting a sample of the expectation above for the performance gradient in (2.13) yields:
Ht+1(a) = H+(a) + a(Rt ‚Äì Rt) (1a=At ‚Äì œÄœÑ(Œ±)), for all a,
which you may recognize as being equivalent to our original algorithm (2.12)." *(Multi-armed Bandits)*
[^6]: "Thus it remains only to show that aœÄt(x) = œÄt(x)(1a=x ‚Äì œÄt(Œ±)), as we assumed." *(Multi-armed Bandits)*
