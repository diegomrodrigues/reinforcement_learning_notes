## Gradient Bandit Algorithms e AscensÃ£o de Gradiente EstocÃ¡stico

### IntroduÃ§Ã£o

Este capÃ­tulo explora o conceito de **Multi-armed Bandits (MAB)** no contexto de *reinforcement learning*, onde o aprendizado ocorre por meio de avaliaÃ§Ãµes das aÃ§Ãµes tomadas, em vez de instruÃ§Ãµes diretas sobre aÃ§Ãµes corretas [1]. Abordamos o problema do MAB, uma versÃ£o simplificada do *reinforcement learning* que permite explorar como o feedback avaliativo funciona e como pode ser combinado com feedback instrutivo [1]. O foco aqui Ã© o **Gradient Bandit Algorithms**, que aprendem preferÃªncias numÃ©ricas para cada aÃ§Ã£o, em vez de valores de aÃ§Ã£o [2]. Esta seÃ§Ã£o especÃ­fica aprofunda o entendimento de como esses algoritmos podem ser entendidos como uma aproximaÃ§Ã£o estocÃ¡stica da ascensÃ£o de gradiente.

### Conceitos Fundamentais

Em contraste com os mÃ©todos que estimam os valores de aÃ§Ã£o, os **Gradient Bandit Algorithms** aprendem uma **preferÃªncia numÃ©rica** $H_t(a) \in \mathbb{R}$ para cada aÃ§Ã£o *a* [2]. Essa preferÃªncia indica a frequÃªncia com que uma aÃ§Ã£o Ã© escolhida, sem uma interpretaÃ§Ã£o direta em termos de recompensa [2]. As probabilidades de escolha da aÃ§Ã£o sÃ£o determinadas usando uma **distribuiÃ§Ã£o soft-max**, tambÃ©m conhecida como distribuiÃ§Ã£o de Gibbs ou Boltzmann, dada por:

$$
Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)
$$

onde $\pi_t(a)$ denota a probabilidade de tomar a aÃ§Ã£o *a* no tempo *t*, e *k* Ã© o nÃºmero total de aÃ§Ãµes [2]. Inicialmente, todas as preferÃªncias sÃ£o iguais, resultando em uma probabilidade igual de seleÃ§Ã£o para todas as aÃ§Ãµes [2].

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um cenÃ¡rio com 3 aÃ§Ãµes (k=3). Inicialmente, as preferÃªncias podem ser definidas como $H_1(1) = 0$, $H_1(2) = 0$, e $H_1(3) = 0$. As probabilidades iniciais de cada aÃ§Ã£o, calculadas com a fÃ³rmula softmax, seriam:
>
>  $\pi_1(1) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.33$
>
> $\pi_1(2) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.33$
>
> $\pi_1(3) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.33$
>
> Isso significa que todas as aÃ§Ãµes tÃªm a mesma probabilidade de serem escolhidas no inÃ­cio.

O aprendizado das preferÃªncias de aÃ§Ã£o Ã© baseado na ideia da **ascensÃ£o de gradiente estocÃ¡stico** [2]. ApÃ³s selecionar uma aÃ§Ã£o $A_t$ e receber a recompensa $R_t$, as preferÃªncias sÃ£o atualizadas da seguinte forma:

```mermaid
flowchart LR
    A[ "Iniciar: H_t(a) para cada aÃ§Ã£o a" ] --> B{ "Selecionar AÃ§Ã£o A_t" };
    B --> C{ "Receber Recompensa R_t" };
    C --> D{ "Calcular R_t (mÃ©dia das recompensas)"};
    D --> E{ "Atualizar H_t(A_t): H_{t+1}(A_t) = H_t(A_t) + Î±(R_t - R_t)(1 - Ï€_t(A_t))" };
    E --> F{ "Atualizar H_t(a) para a != A_t: H_{t+1}(a) = H_t(a) - Î±(R_t - R_t)Ï€_t(a)"};
    F --> G[ "H_t(a) atualizadas para o prÃ³ximo passo" ];
    G --> B;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

$$
H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t))
$$
$$
H_{t+1}(a) = H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a), \quad \text{para todo } a \neq A_t
$$

onde $\alpha > 0$ Ã© o *step-size parameter*, e $\bar{R}_t$ Ã© uma linha de base que corresponde Ã  mÃ©dia das recompensas atÃ© o tempo *t* [2]. A linha de base serve para comparar a recompensa atual; se a recompensa for maior que a linha de base, a probabilidade de tomar aquela aÃ§Ã£o no futuro aumenta, e vice-versa [2].

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que a aÃ§Ã£o $A_t = 1$ seja selecionada em *t=1*, e a recompensa recebida seja $R_1 = 1$, com $\alpha = 0.1$ e $\bar{R}_1 = 0$ (jÃ¡ que Ã© a primeira recompensa). A probabilidade de escolher a aÃ§Ã£o 1 foi previamente calculada como $\pi_1(1) \approx 0.33$. As preferÃªncias sÃ£o entÃ£o atualizadas:
>
> $H_{2}(1) = H_1(1) + \alpha(R_1 - \bar{R}_1)(1 - \pi_1(1)) = 0 + 0.1(1-0)(1-0.33) = 0 + 0.1 * 0.67 = 0.067$
>
> Para as outras aÃ§Ãµes, com a premissa de que $H_1(2)=0$ e $H_1(3)=0$ e $\pi_1(2) \approx 0.33$ e $\pi_1(3) \approx 0.33$:
>
> $H_{2}(2) = H_1(2) - \alpha(R_1 - \bar{R}_1)\pi_1(2) = 0 - 0.1(1-0)(0.33) = 0 - 0.033 = -0.033$
>
> $H_{2}(3) = H_1(3) - \alpha(R_1 - \bar{R}_1)\pi_1(3) = 0 - 0.1(1-0)(0.33) = 0 - 0.033 = -0.033$
>
> ApÃ³s esta atualizaÃ§Ã£o, a preferÃªncia da aÃ§Ã£o 1 aumentou, enquanto as preferÃªncias das outras aÃ§Ãµes diminuÃ­ram, o que, por sua vez, impactarÃ¡ as probabilidades de seleÃ§Ã£o de cada aÃ§Ã£o no prÃ³ximo passo.

**ProposiÃ§Ã£o 1**
A atualizaÃ§Ã£o das preferÃªncias pode ser expressa de forma compacta usando a notaÃ§Ã£o de funÃ§Ã£o indicadora:
$$
H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t)(\mathbb{1}\{a = A_t\} - \pi_t(a))
$$
*Proof Outline:* This compact form is simply a restatement of the two-part update rule, combining the case when $a = A_t$ (where the indicator is 1) and the case when $a \neq A_t$ (where the indicator is 0) into a single expression.

A conexÃ£o entre os Gradient Bandit Algorithms e a ascensÃ£o de gradiente estocÃ¡stico pode ser demonstrada ao mostrar que a atualizaÃ§Ã£o das preferÃªncias de aÃ§Ã£o Ã© uma aproximaÃ§Ã£o estocÃ¡stica do gradiente da performance esperada. O gradiente exato da performance Ã© dado por:

```mermaid
flowchart LR
  subgraph "DerivaÃ§Ã£o do Gradiente da Performance Esperada"
    A[ "Gradiente da Performance Esperada:" ] --> B("âˆ‚E[R_t]/âˆ‚H_t(a) = Î£_x (q_*(x) - B_t) * âˆ‚Ï€_t(x)/âˆ‚H_t(a)");
    B --> C("Multiplicar por Ï€_t(x)/Ï€_t(x):");
    C --> D("âˆ‚E[R_t]/âˆ‚H_t(a) = Î£_x  (âˆ‚Ï€_t(x)/âˆ‚H_t(a) * Ï€_t(x) / Ï€_t(x)) * (q_*(x) - B_t)");
    D --> E("Forma de EsperanÃ§a:");
    E --> F("âˆ‚E[R_t]/âˆ‚H_t(a) = E[(R_t - RÌ„_t) * (âˆ‚Ï€_t(A_t)/âˆ‚H_t(a) * 1/Ï€_t(A_t))]");
  end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = \sum_x (q_*(x) - B_t) \frac{\partial \pi_t(x)}{\partial H_t(a)}
$$

onde $B_t$ Ã© uma linha de base que nÃ£o depende de *x*. Multiplicando e dividindo cada termo por $\pi_t(x)$, a equaÃ§Ã£o se torna:

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = \sum_x \frac{\partial \pi_t(x)}{\partial H_t(a)} \frac{\pi_t(x)}{\pi_t(x)} (q_*(x) - B_t)
$$

Esta equaÃ§Ã£o estÃ¡ agora na forma de uma esperanÃ§a, que pode ser amostrada substituindo $q_*(A_t)$ por $R_t$ e definindo a linha de base como a mÃ©dia das recompensas [2]:

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E \left[\left(R_t - \bar{R}_t\right) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} \frac{1}{\pi_t(A_t)} \right]
$$

Para chegar ao algoritmo de atualizaÃ§Ã£o apresentado anteriormente, Ã© preciso demonstrar que $\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x)(1_{a=x} - \pi_t(a))$, onde $1_{a=x}$ Ã© 1 se $a = x$ e 0 caso contrÃ¡rio.  A derivada da probabilidade com relaÃ§Ã£o Ã  preferÃªncia pode ser encontrada usando a regra do quociente:

```mermaid
flowchart LR
    A[ "Regra do Quociente:" ] --> B("âˆ‚/âˆ‚x [f(x)/g(x)] = (f'(x)g(x) - f(x)g'(x)) / g(x)^2");
    B --> C{ "Aplicar Ã  derivada de Ï€_t(x) com respeito a H_t(a)"};
    C --> D("âˆ‚Ï€_t(x)/âˆ‚H_t(a) =  (âˆ‚/âˆ‚H_t(a) * e^H_t(x) * Î£_y e^H_t(y) - e^H_t(x) * âˆ‚/âˆ‚H_t(a) * Î£_y e^H_t(y) ) / (Î£_y e^H_t(y))^2 ");
        D --> E("Simplificando a Derivada:");
    E --> F("âˆ‚Ï€_t(x)/âˆ‚H_t(a) =  (1_{a=x}* e^H_t(x) * Î£_y e^H_t(y) - e^H_t(x) * e^H_t(a)) / (Î£_y e^H_t(y))^2");
        F --> G("âˆ‚Ï€_t(x)/âˆ‚H_t(a) =  1_{a=x}* Ï€_t(x) - Ï€_t(x) * Ï€_t(a)");
     G --> H("âˆ‚Ï€_t(x)/âˆ‚H_t(a) =  Ï€_t(x) (1_{a=x} - Ï€_t(a))");
       style A fill:#f9f,stroke:#333,stroke-width:2px
        style H fill:#ccf,stroke:#333,stroke-width:2px
```

$$
\frac{\partial}{\partial x} \left[ \frac{f(x)}{g(x)} \right] = \frac{\frac{\partial f(x)}{\partial x} g(x) - f(x) \frac{\partial g(x)}{\partial x}}{g(x)^2}
$$

Aplicando essa regra, obtemos:

$$
\frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)} \left[ \frac{e^{H_t(x)}}{\sum_{y=1}^k e^{H_t(y)}} \right] = \frac{1_{a=x} e^{H_t(x)} \sum_{y=1}^k e^{H_t(y)} - e^{H_t(x)} e^{H_t(a)}}{ \left( \sum_{y=1}^k e^{H_t(y)} \right)^2 } =  \frac{1_{a=x} e^{H_t(x)}}{\sum_{y=1}^k e^{H_t(y)}} - \frac{e^{H_t(x)} e^{H_t(a)}}{\left( \sum_{y=1}^k e^{H_t(y)} \right)^2} = 1_{a=x} \pi_t(x) - \pi_t(x) \pi_t(a) = \pi_t(x)(1_{a=x} - \pi_t(a))
$$

Substituindo esse resultado na equaÃ§Ã£o do gradiente, temos:

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E[(R_t - \bar{R}_t)(1_{a=A_t} - \pi_t(a))]
$$
**Lema 1**
The gradient of the log-probability of action *x* with respect to the preference *H_t(a)*, can be expressed as:
$$
    \frac{\partial}{\partial H_t(a)} \log(\pi_t(x)) = \mathbb{1}\{a=x\} - \pi_t(a)
$$
*Proof Outline:* This can be derived directly from the derivative of $\pi_t(x)$ with respect to *H_t(a)* and the property that $\frac{\partial}{\partial x}\log(f(x)) = \frac{f'(x)}{f(x)}$ and $\pi_t(x) = \frac{e^{H_t(x)}}{\sum_{y=1}^k e^{H_t(y)}}$.

A atualizaÃ§Ã£o estocÃ¡stica por ascensÃ£o de gradiente Ã© obtida ao amostrar essa esperanÃ§a e atualizar as preferÃªncias:

```mermaid
flowchart LR
  subgraph "AtualizaÃ§Ã£o das PreferÃªncias (Gradient Ascent EstocÃ¡stico)"
    A[ "Gradiente da performance em relaÃ§Ã£o Ã s preferÃªncias:" ] --> B("âˆ‚E[R_t]/âˆ‚H_t(a) = E[(R_t - RÌ„_t) * (1_{a=A_t} - Ï€_t(a))]");
    B --> C("AtualizaÃ§Ã£o EstocÃ¡stica:");
    C --> D("H_{t+1}(a) = H_t(a) + Î± * (R_t - RÌ„_t) * (1_{a=A_t} - Ï€_t(a))");

  end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```
$$
H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t)(1_{a=A_t} - \pi_t(a))
$$

Esta equaÃ§Ã£o Ã© equivalente ao algoritmo de atualizaÃ§Ã£o original [2].

**Teorema 1**
The gradient bandit update rule can be viewed as a stochastic approximation of the gradient ascent method on the expected reward, considering the softmax distribution.
*Proof Outline:* This result follows directly from the derivation that shows that the update of preferences, $H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t)(1_{a=A_t} - \pi_t(a))$, is proportional to the derivative of the expected reward with respect to the preferences, as derived in the preceding steps. The stochasticity arises from the sampling of the action $A_t$ and the reward $R_t$.

### ConclusÃ£o

Este aprofundamento do Gradient Bandit Algorithm revela sua natureza como uma **aproximaÃ§Ã£o estocÃ¡stica da ascensÃ£o de gradiente**, demonstrando que a atualizaÃ§Ã£o das preferÃªncias de aÃ§Ã£o segue uma direÃ§Ã£o que maximiza a recompensa esperada [2]. Essa compreensÃ£o nÃ£o apenas valida o algoritmo, mas tambÃ©m oferece uma visÃ£o sobre sua robustez e propriedades de convergÃªncia. A flexibilidade do uso de uma linha de base, como a mÃ©dia das recompensas, demonstra sua aplicabilidade prÃ¡tica, mesmo sem o conhecimento da funÃ§Ã£o de valor Ã³tima.

### ReferÃªncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. [...]" *(Multi-armed Bandits)*
[^2]: "So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. [...] In this section we consider learning a numerical preference for each action a, which we denote HÅ‚(a) âˆˆ R. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important [...]" *(Multi-armed Bandits)*
[^3]: "There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action At and receiving the reward Rt, the action preferences are updated by:
Ht+1(At) = Ht(At) + a(Rt â€“ Rt) (1 â€“ Ï€Ï„(At)),
Ht+1(a) = H+(a) â€“ Î±(Rt â€“ Rt)Ï€Î¹(Î±), and for all a â‰  At, [...]" *(Multi-armed Bandits)*
[^4]: "In the empirical results in this chapter, the baseline Rt also included Rt." *(Multi-armed Bandits)*
[^5]: "steps. First we take a closer look at the exact performance gradient: [...]
Next we multiply each term of the sum by Ï€t(x)/Ï€t(x): [...]
The equation is now in the form of an expectation, summing over all possible values x of the random variable At, then multiplying by the probability of taking those values. Thus: [...]
Recall that our plan has been to write the performance gradient as an expectation of something that we can sample on each step, as we have just done, and then update on each step in proportion to the sample. Substituting a sample of the expectation above for the performance gradient in (2.13) yields:
Ht+1(a) = H+(a) + a(Rt â€“ Rt) (1a=At â€“ Ï€Ï„(Î±)), for all a,
which you may recognize as being equivalent to our original algorithm (2.12)." *(Multi-armed Bandits)*
[^6]: "Thus it remains only to show that aÏ€t(x) = Ï€t(x)(1a=x â€“ Ï€t(Î±)), as we assumed." *(Multi-armed Bandits)*
