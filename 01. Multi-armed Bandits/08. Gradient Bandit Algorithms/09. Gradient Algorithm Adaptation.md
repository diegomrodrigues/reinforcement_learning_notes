## Gradient Bandit Algorithms: Adapta√ß√£o Instant√¢nea com Linha de Base de Recompensa
### Introdu√ß√£o
O aprendizado por refor√ßo se distingue de outras formas de aprendizado por usar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de instruir por meio de a√ß√µes corretas [^1]. Isso introduz a necessidade de explora√ß√£o ativa, buscando explicitamente um bom comportamento. Este cap√≠tulo foca no aspecto avaliativo do aprendizado por refor√ßo em um cen√°rio simplificado, o problema do **k-armed bandit**, onde o objetivo √© maximizar a recompensa esperada total ao longo do tempo por meio de tentativas e erros [^1]. Os algoritmos de *action-value*, que estimam os valores das a√ß√µes, s√£o uma abordagem comum para resolver este problema [^3]. Contudo, esta se√ß√£o foca em uma abordagem alternativa: aprender prefer√™ncias num√©ricas para cada a√ß√£o, usando o algoritmo de *gradient bandit* e como ele usa a linha de base de recompensa para adapta√ß√£o instant√¢nea a novos n√≠veis de recompensa.

### Conceitos Fundamentais
Nesta se√ß√£o, exploramos o algoritmo *gradient bandit*, uma abordagem alternativa aos m√©todos de *action-value*. Em vez de estimar valores de a√ß√µes, o algoritmo aprende uma prefer√™ncia num√©rica $H_t(a)$ para cada a√ß√£o $a$ [^13]. A probabilidade de selecionar uma a√ß√£o $a$ no tempo $t$, denotada como $\pi_t(a)$, √© dada por uma distribui√ß√£o *soft-max* (tamb√©m conhecida como distribui√ß√£o de Gibbs ou Boltzmann):
$$
Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = \pi_t(a)
$$
[^13] onde $k$ √© o n√∫mero total de a√ß√µes. Inicialmente, todas as prefer√™ncias s√£o iguais (por exemplo, $H_1(a) = 0$ para todas as a√ß√µes), o que garante que todas as a√ß√µes tenham uma probabilidade igual de serem selecionadas [^13].
```mermaid
graph LR
    A[In√≠cio] --> B("Inicializar prefer√™ncias H_1(a) = 0 para todas as a√ß√µes");
    B --> C("Calcular probabilidades de a√ß√£o œÄ_t(a) usando Softmax");
    C --> D("Selecionar a√ß√£o A_t com base em œÄ_t(a)");
    D --> E("Receber recompensa R_t");
    E --> F("Atualizar prefer√™ncias H_t(a) com base em R_t e linha de base");
    F --> C;
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de 3-armed bandit (k=3). Inicialmente, as prefer√™ncias s√£o $H_1(a_1) = 0$, $H_1(a_2) = 0$, e $H_1(a_3) = 0$. As probabilidades de sele√ß√£o das a√ß√µes s√£o calculadas usando a fun√ß√£o softmax:
>
>  $\pi_1(a_1) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3}$
>
>  $\pi_1(a_2) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3}$
>
>  $\pi_1(a_3) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3}$
>
> Isso confirma que inicialmente todas as a√ß√µes s√£o igualmente prov√°veis de serem escolhidas.

O aprendizado das prefer√™ncias √© baseado na ideia de *stochastic gradient ascent*. Ap√≥s selecionar uma a√ß√£o $A_t$ e receber uma recompensa $R_t$, as prefer√™ncias de a√ß√£o s√£o atualizadas da seguinte maneira:
$$
H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t))
$$
$$
H_{t+1}(a) = H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a), \quad \text{para todo} \quad a \neq A_t
$$
[^13] onde $\alpha > 0$ √© um par√¢metro de tamanho do passo e $\bar{R}_t$ √© a m√©dia das recompensas recebidas at√© o tempo $t$, ou seja, o baseline [^13]. √â crucial notar que o termo $\bar{R}_t$ serve como um *baseline* com o qual a recompensa $R_t$ √© comparada [^13]. Se a recompensa for maior que a linha de base, a probabilidade de sele√ß√£o de $A_t$ aumenta; caso contr√°rio, a probabilidade diminui. As a√ß√µes n√£o selecionadas se movem na dire√ß√£o oposta [^13].
```mermaid
sequenceDiagram
    participant Agente
    participant Ambiente
    Agente->>Ambiente: Seleciona A_t
    Ambiente-->>Agente: Retorna R_t
    Agente->>Agente: Calcula  "H_{t+1}(A_t)"
    Agente->>Agente: Calcula "H_{t+1}(a)" para todo a != A_t
    Agente->>Agente: Atualiza  "œÄ_{t+1}(a)" usando softmax
    loop Para cada passo
        Agente->>Ambiente: Seleciona A_t
        Ambiente-->>Agente: Retorna R_t
        Agente->>Agente: Calcula  "H_{t+1}(A_t)"
        Agente->>Agente: Calcula "H_{t+1}(a)" para todo a != A_t
        Agente->>Agente: Atualiza  "œÄ_{t+1}(a)" usando softmax
    end
```

> üí° **Exemplo Num√©rico:**  Continuando o exemplo anterior, suponha que no tempo t=1, a a√ß√£o $A_1 = a_2$ foi selecionada e recebemos uma recompensa $R_1 = 1$. Assumindo um tamanho de passo $\alpha = 0.1$, e que a m√©dia das recompensas at√© agora √© $\bar{R}_1 = 0$ (pois esta √© a primeira recompensa), as prefer√™ncias s√£o atualizadas:
>
>  $H_2(a_2) = H_1(a_2) + \alpha(R_1 - \bar{R}_1)(1 - \pi_1(a_2)) = 0 + 0.1(1 - 0)(1 - \frac{1}{3}) = 0 + 0.1 * \frac{2}{3} \approx 0.067$
>
>  $H_2(a_1) = H_1(a_1) - \alpha(R_1 - \bar{R}_1)\pi_1(a_1) = 0 - 0.1(1 - 0)\frac{1}{3} \approx -0.033$
>
>  $H_2(a_3) = H_1(a_3) - \alpha(R_1 - \bar{R}_1)\pi_1(a_3) = 0 - 0.1(1 - 0)\frac{1}{3} \approx -0.033$
>
> Observe que a prefer√™ncia da a√ß√£o $a_2$ (a a√ß√£o selecionada) aumentou, enquanto as prefer√™ncias das a√ß√µes n√£o selecionadas diminu√≠ram.

**Lemma 1:** O algoritmo *gradient bandit* se adapta instantaneamente √†s mudan√ßas nos n√≠veis de recompensa devido ao termo de baseline $\bar{R}_t$.
*Prova*: O termo *baseline* $\bar{R}_t$ √© a m√©dia das recompensas at√© o instante $t$ [^13]. Se a recompensa m√©dia esperada em todas as a√ß√µes aumentar para um novo n√≠vel, o baseline se ajustar√° para refletir este novo n√≠vel [^13]. Portanto, a atualiza√ß√£o das prefer√™ncias ser√° relativa a este novo baseline, mantendo as rela√ß√µes de prefer√™ncia entre as a√ß√µes, sem deixar que o aumento nos valores das recompensas afete o aprendizado do algoritmo. Isso permite que o algoritmo *gradient bandit* continue aprendendo e ajustando as probabilidades de a√ß√£o, mesmo que os n√≠veis absolutos de recompensa mudem. Consequentemente, o algoritmo *gradient bandit* √© invariante a mudan√ßas no n√≠vel das recompensas, desde que a rela√ß√£o relativa entre as recompensas seja mantida [^13].
$\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que, ap√≥s v√°rias itera√ß√µes, o baseline $\bar{R}_t$ se estabilizou em 2.0, indicando que, em m√©dia, as recompensas est√£o em torno de 2. Agora, considere que todas as recompensas aumentam por 3, de forma que a recompensa m√©dia esperada se torne 5. Inicialmente, as recompensas observadas $R_t$ ser√£o maiores do que o baseline, fazendo com que as prefer√™ncias da a√ß√£o selecionada aumentem significativamente. No entanto, o baseline $\bar{R}_t$ rapidamente come√ßar√° a se ajustar para cima, refletindo o novo n√≠vel de recompensa. Com o novo baseline se aproximando de 5, o termo $(R_t - \bar{R}_t)$ voltar√° a ser pequeno. Isso mostra que o algoritmo se adapta rapidamente √† mudan√ßa de n√≠vel de recompensa, mantendo as rela√ß√µes de prefer√™ncia entre as a√ß√µes.
>
> Para ver o impacto sem o baseline, considere o caso onde o baseline √© sempre 0. Se a recompensa de uma a√ß√£o √© 5, o termo $(R_t - \bar{R}_t)$ √© sempre 5, levando a um aumento constante da prefer√™ncia dessa a√ß√£o.  Se posteriormente a recompensa volta a um valor menor, as prefer√™ncias ainda teriam que diminuir e levar um certo tempo para se ajustarem novamente. Isso demonstra como o baseline permite uma adapta√ß√£o mais r√°pida e est√°vel.
```mermaid
graph LR
    A["Recompensa R_t"] --> B("Calcular R_t - baseline");
    B --> C{R_t > baseline?};
    C -- "Sim" --> D("Aumentar H_t(A_t)");
    C -- "N√£o" --> E("Diminuir H_t(A_t)");
     D --> F("Ajustar baseline R_t");
     E --> F
    F --> G("Atualizar œÄ_t(a) com Softmax");
    G --> H["Pr√≥xima itera√ß√£o"];
    style A fill:#ccf,stroke:#333,stroke-width:2px
```

**Lema 1.1:**  O algoritmo *gradient bandit* pode ser adaptado para utilizar diferentes *baselines*.
*Prova:*  O baseline $\bar{R}_t$ n√£o precisa ser necessariamente a m√©dia das recompensas passadas. Podemos usar qualquer estimativa que n√£o dependa da a√ß√£o escolhida $A_t$. Por exemplo, uma linha de base constante $b$ ou uma m√©dia ponderada exponencial das recompensas passadas, onde recompensas mais recentes t√™m maior peso. A demonstra√ß√£o do Lemma 1 se mant√©m, contanto que o baseline seja independente da a√ß√£o selecionada, e o algoritmo ainda ser√° uma inst√¢ncia do *stochastic gradient ascent* (conforme [^16]). A escolha de um baseline diferente pode afetar a vari√¢ncia das atualiza√ß√µes e, portanto, a velocidade de converg√™ncia. $\blacksquare$

> üí° **Exemplo Num√©rico:** Em vez de usar a m√©dia simples das recompensas, $\bar{R}_t$, podemos usar uma m√©dia m√≥vel exponencial:
>  $\bar{R}_t = \beta\bar{R}_{t-1} + (1-\beta)R_t$, onde $\beta$ √© um fator de pondera√ß√£o (por exemplo, 0.9). Inicialmente, $\bar{R}_0 = 0$. Suponha que as recompensas em tr√™s passos sejam $R_1=1$, $R_2=3$ e $R_3=2$. A m√©dia m√≥vel exponencial seria:
>
>  $\bar{R}_1 = 0.9 * 0 + 0.1 * 1 = 0.1$
>
>  $\bar{R}_2 = 0.9 * 0.1 + 0.1 * 3 = 0.39$
>
>  $\bar{R}_3 = 0.9 * 0.39 + 0.1 * 2 = 0.551$
>
>  Observe como a m√©dia m√≥vel exponencial d√° mais peso √†s recompensas mais recentes, adaptando-se mais rapidamente √†s mudan√ßas no n√≠vel de recompensa do que a m√©dia simples. Este tipo de baseline pode ser mais adequado em ambientes n√£o estacion√°rios.

Para demonstrar a import√¢ncia do baseline, o texto compara o desempenho do algoritmo com e sem o termo de baseline em uma vers√£o do *10-armed testbed* [^13]. Nesta vers√£o, as recompensas esperadas foram selecionadas com base em uma distribui√ß√£o normal com m√©dia +4, em vez de zero [^13]. A mudan√ßa para uma m√©dia +4 n√£o afeta o algoritmo *gradient bandit* porque o baseline $\bar{R}_t$ ajusta-se instantaneamente ao novo n√≠vel, conforme demonstrado pelo Lemma 1 [^13]. Entretanto, sem o baseline, o desempenho do algoritmo diminui significativamente [^13].

**Proposi√ß√£o 1:**  O algoritmo *gradient bandit*, com a escolha de um *baseline* adequado, pode convergir para uma pol√≠tica √≥tima sob certas condi√ß√µes.
*Prova:* O algoritmo *gradient bandit* implementa o *stochastic gradient ascent* no espa√ßo de prefer√™ncias.  Sob certas condi√ß√µes, como tamanho de passo decrescente $(\alpha)$ e uma explora√ß√£o suficiente, o algoritmo converge para um √≥timo local. Se a fun√ß√£o de recompensa esperada √© convexa (ou c√¥ncava, dependendo da dire√ß√£o da otimiza√ß√£o) no espa√ßo de prefer√™ncias, ent√£o este √≥timo local √© tamb√©m um √≥timo global [^17]. A escolha de um baseline apropriado pode reduzir a vari√¢ncia das atualiza√ß√µes e acelerar a converg√™ncia para a pol√≠tica √≥tima. $\blacksquare$
```mermaid
graph LR
    subgraph "Stochastic Gradient Ascent"
    A["Prefer√™ncias H_t(a)"]-->B("Calcular Gradiente da Recompensa Esperada");
    B-->C("Atualizar H_t(a) com tamanho de passo Œ±");
     C --> D("Convergir para um √ìtimo Local");
      D --> E("√ìtimo Local = √ìtimo Global se a Fun√ß√£o for Convexa");
    end
    subgraph "Baseline"
    F["Baseline adequado"]-->G("Reduzir vari√¢ncia das atualiza√ß√µes");
     G --> H("Acelerar converg√™ncia para a pol√≠tica √≥tima");
    end
    style A fill:#aaf,stroke:#333,stroke-width:2px
    style F fill:#afa,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Suponha que temos um 2-armed bandit com recompensas esperadas de 1 para a√ß√£o $a_1$ e 2 para a√ß√£o $a_2$.  Inicialmente, as prefer√™ncias s√£o $H_t(a_1)=0$ e $H_t(a_2)=0$.  Com um tamanho de passo decrescente $(\alpha)$, o algoritmo converge para uma pol√≠tica na qual a a√ß√£o $a_2$ √© escolhida com maior probabilidade. Se a fun√ß√£o de recompensa for suave o suficiente, a converg√™ncia para essa pol√≠tica √© garantida. Um baseline que n√£o dependa da a√ß√£o escolhida ajuda a garantir que as atualiza√ß√µes de prefer√™ncia se concentrem na melhoria da a√ß√£o em si, em vez de apenas nos n√≠veis de recompensa.

**Corol√°rio 1:** A escolha do baseline n√£o afeta o valor esperado da atualiza√ß√£o do algoritmo, apenas sua vari√¢ncia.
*Prova*: O algoritmo *gradient bandit* √© uma inst√¢ncia do *stochastic gradient ascent*. Como a prova no contexto demonstra [^15], a atualiza√ß√£o esperada das prefer√™ncias, $H_t(a)$, √© igual ao gradiente da recompensa esperada [^15]. Portanto, a escolha do baseline, contanto que seja independente da a√ß√£o escolhida, n√£o altera a dire√ß√£o em que as prefer√™ncias s√£o atualizadas no valor esperado [^16]. No entanto, o baseline afeta a vari√¢ncia das atualiza√ß√µes [^16]. Um bom baseline reduzir√° essa vari√¢ncia, levando a um aprendizado mais r√°pido e est√°vel. Por exemplo, usar a m√©dia das recompensas como baseline, conforme feito no texto, pode levar a uma vari√¢ncia menor do que usar um baseline constante [^16]. $\blacksquare$

> üí° **Exemplo Num√©rico:** Para ilustrar a vari√¢ncia, suponha que usamos um baseline constante de $b=0$ em vez da m√©dia $\bar{R}_t$.  Em alguns passos, a recompensa $R_t$ pode ser muito maior do que 0, causando grandes atualiza√ß√µes em $H_t(a)$, e em outros passos ela pode ser menor, resultando em atualiza√ß√µes na dire√ß√£o oposta. Isso resulta em um caminho de aprendizado com alta variabilidade. Em contraste, usar $\bar{R}_t$ como baseline suaviza as atualiza√ß√µes, pois o baseline acompanha a m√©dia das recompensas, resultando em menor vari√¢ncia e uma converg√™ncia mais est√°vel.

**Corol√°rio 1.1:**  Um baseline com baixa vari√¢ncia contribui para um aprendizado mais est√°vel e r√°pido do algoritmo *gradient bandit*.
*Prova:* Conforme o Corol√°rio 1, a escolha do baseline n√£o influencia a dire√ß√£o do *stochastic gradient ascent* no valor esperado, mas afeta a vari√¢ncia da atualiza√ß√£o. Um baseline que tenha uma vari√¢ncia menor, como a m√©dia m√≥vel de recompensas, reduz a vari√¢ncia da atualiza√ß√£o das prefer√™ncias $H_t(a)$. Isso leva a um caminho mais suave no espa√ßo de prefer√™ncias e uma converg√™ncia mais r√°pida para o √≥timo, melhorando a estabilidade do aprendizado do algoritmo.  $\blacksquare$

> üí° **Exemplo Num√©rico:**  Comparando dois baselines, a m√©dia simples $\bar{R}_t$ e um baseline constante $b=1$, em um problema de 3-armed bandit.  Vamos supor que as recompensas geradas em uma sequ√™ncia sejam 2, 4, 1, 3, 5.  A m√©dia simples ter√° valores de baseline que variam mais lentamente: 0, 1, 2, 8/3, 10/4 , 15/5.  O baseline constante, obviamente, permanece constante.  Se a m√©dia das recompensas come√ßar a aumentar, o baseline constante n√£o ir√° ajustar e causar√° maior vari√¢ncia nas atualiza√ß√µes das prefer√™ncias. Por outro lado, a m√©dia se ajustar√° e diminuir√° a vari√¢ncia das atualiza√ß√µes de $H_t(a)$, levando a uma converg√™ncia mais r√°pida e est√°vel.

### Conclus√£o
O algoritmo *gradient bandit* apresenta uma abordagem √∫nica para o problema do *k-armed bandit*, aprendendo prefer√™ncias de a√ß√£o em vez de estimativas de valor. A introdu√ß√£o de um baseline na atualiza√ß√£o da prefer√™ncia permite que o algoritmo se adapte instantaneamente a novos n√≠veis de recompensa, demonstrando a robustez do m√©todo. Essa propriedade √© crucial em ambientes n√£o estacion√°rios ou quando as recompensas sofrem altera√ß√µes significativas ao longo do tempo. A capacidade de o *gradient bandit* ajustar-se rapidamente a n√≠veis de recompensa vari√°veis, mantendo a rela√ß√£o relativa entre as a√ß√µes, o torna uma ferramenta valiosa no aprendizado por refor√ßo.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Multi-armed Bandits)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods." *(Multi-armed Bandits)*
[^13]: "So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. In this section we consider learning a numerical preference for each action a, which we denote Ht(a) ‚àà R. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:" *(Multi-armed Bandits)*
[^15]: "First we take a closer look at the exact performance gradient: ...The equation is now in the form of an expectation, summing over all possible values x of the random variable At, then multiplying by the probability of taking those values." *(Multi-armed Bandits)*
[^16]: "Note that we did not require any properties of the reward baseline other than that it does not depend on the selected action. For example, we could have set it to zero, or to 1000, and the algorithm would still be an instance of stochastic gradient ascent. The choice of the baseline does not affect the expected update of the algorithm, but it does affect the variance of the update and thus the rate of convergence (as shown, for example, in Figure 2.5)." *(Multi-armed Bandits)*
[^17]: Bertsekas, Dimitri P. *Nonlinear Programming*. Athena Scientific, 1999.
