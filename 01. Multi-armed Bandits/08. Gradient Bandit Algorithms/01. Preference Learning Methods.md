## Gradient Bandit Algorithms: Aprendendo Prefer√™ncias de A√ß√£o

### Introdu√ß√£o
No contexto de *reinforcement learning*, os m√©todos para resolver o problema do *k-armed bandit* podem ser categorizados em duas abordagens principais: aquelas que estimam valores de a√ß√£o e aquelas que aprendem uma prefer√™ncia num√©rica para cada a√ß√£o [^1]. Este cap√≠tulo foca em uma abordagem alternativa que aprende diretamente uma prefer√™ncia num√©rica para cada a√ß√£o, representada por $H_t(a)$, em vez de estimar valores de a√ß√£o $Q_t(a)$ [^1]. Essa prefer√™ncia num√©rica indica qu√£o frequentemente uma a√ß√£o deve ser tomada, sem uma interpreta√ß√£o direta em termos de recompensa [^1]. A abordagem de *gradient bandit algorithms* √© uma alternativa interessante, onde a probabilidade de sele√ß√£o de uma a√ß√£o √© determinada pela distribui√ß√£o *soft-max*, que considera as prefer√™ncias relativas de cada a√ß√£o [^1].

### Conceitos Fundamentais
#### Prefer√™ncias de A√ß√£o
Em vez de estimar valores de a√ß√£o como em m√©todos anteriores, os *gradient bandit algorithms* aprendem uma prefer√™ncia num√©rica $H_t(a) \in \mathbb{R}$ para cada a√ß√£o $a$ [^1]. A magnitude da prefer√™ncia reflete a frequ√™ncia com que essa a√ß√£o deve ser selecionada, sem uma interpreta√ß√£o direta em termos de recompensa [^1]. O que importa √© a prefer√™ncia *relativa* entre as a√ß√µes, e adicionar uma constante a todas as prefer√™ncias n√£o altera a probabilidade de sele√ß√£o de cada a√ß√£o [^1].

#### Distribui√ß√£o Soft-max
As probabilidades de sele√ß√£o de a√ß√£o, $\pi_t(a)$, s√£o definidas pela distribui√ß√£o *soft-max* (tamb√©m conhecida como distribui√ß√£o de Gibbs ou Boltzmann):
$$Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} = \pi_t(a)$$
[^1]. Onde $A_t$ √© a a√ß√£o selecionada no instante $t$, e $k$ √© o n√∫mero total de a√ß√µes. Inicialmente, as prefer√™ncias de todas as a√ß√µes s√£o as mesmas (por exemplo, $H_1(a) = 0$ para todas as a√ß√µes), garantindo que todas as a√ß√µes tenham a mesma probabilidade de serem selecionadas [^1].

> üí° **Exemplo Num√©rico:** Considere um problema de 3-arm bandit (k=3). Inicialmente, as prefer√™ncias s√£o $H_1(a_1) = 0$, $H_1(a_2) = 0$, e $H_1(a_3) = 0$. As probabilidades iniciais de selecionar cada a√ß√£o s√£o:
>
> $\pi_1(a_1) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
>
> $\pi_1(a_2) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
>
> $\pi_1(a_3) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
>
> Todas as a√ß√µes t√™m a mesma probabilidade de serem selecionadas inicialmente.

**Proposi√ß√£o 1:**  A distribui√ß√£o *soft-max* √© invariante √† adi√ß√£o de uma constante a todas as prefer√™ncias de a√ß√£o.

*Proof:* Seja $c$ uma constante arbitr√°ria.  Se adicionarmos $c$ a todas as prefer√™ncias, temos novas prefer√™ncias $H'_t(a) = H_t(a) + c$ para cada a√ß√£o $a$. A probabilidade de sele√ß√£o de uma a√ß√£o $a$ usando as novas prefer√™ncias √©:

$$\pi'_t(a) = \frac{e^{H'_t(a)}}{\sum_{b=1}^{k} e^{H'_t(b)}} = \frac{e^{H_t(a) + c}}{\sum_{b=1}^{k} e^{H_t(b) + c}} = \frac{e^{H_t(a)}e^c}{\sum_{b=1}^{k} e^{H_t(b)}e^c} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} = \pi_t(a).$$

Portanto, a distribui√ß√£o *soft-max* permanece inalterada pela adi√ß√£o de uma constante a todas as prefer√™ncias de a√ß√£o. $\blacksquare$

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, se adicionarmos $c=2$ a todas as prefer√™ncias, teremos $H'_1(a_1) = 2$, $H'_1(a_2) = 2$, e $H'_1(a_3) = 2$. As novas probabilidades s√£o:
>
> $\pi'_1(a_1) = \frac{e^2}{e^2 + e^2 + e^2} = \frac{e^2}{3e^2} = \frac{1}{3} \approx 0.333$
>
> $\pi'_1(a_2) = \frac{e^2}{e^2 + e^2 + e^2} = \frac{e^2}{3e^2} = \frac{1}{3} \approx 0.333$
>
> $\pi'_1(a_3) = \frac{e^2}{e^2 + e^2 + e^2} = \frac{e^2}{3e^2} = \frac{1}{3} \approx 0.333$
>
> As probabilidades permanecem as mesmas, comprovando a invari√¢ncia.
  ```mermaid
  graph LR
      A("Prefer√™ncias Ht(a)") --> B("Softmax");
      B --> C("Probabilidades œÄt(a)");
      C --> D("Sele√ß√£o da A√ß√£o At");
      D --> E("Recompensa Rt");
      E --> F("Atualiza√ß√£o de Ht+1(a)");
      F --> A;
      style A fill:#f9f,stroke:#333,stroke-width:2px
      style B fill:#ccf,stroke:#333,stroke-width:2px
      style C fill:#ccf,stroke:#333,stroke-width:2px
      style D fill:#fcc,stroke:#333,stroke-width:2px
      style E fill:#ccf,stroke:#333,stroke-width:2px
      style F fill:#f9f,stroke:#333,stroke-width:2px
  ```

#### Algoritmo de Aprendizado por Gradiente Estoc√°stico
Os *gradient bandit algorithms* s√£o baseados na ideia de *stochastic gradient ascent* [^1]. Ap√≥s selecionar uma a√ß√£o $A_t$ e receber uma recompensa $R_t$, as prefer√™ncias de a√ß√£o s√£o atualizadas usando a seguinte regra:
$$H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - \bar{R}_t)(1 - \pi_t(A_t))$$
$$H_{t+1}(a) = H_t(a) - \alpha (R_t - \bar{R}_t) \pi_t(a), \quad \text{para } a \neq A_t$$
[^1]. Aqui, $\alpha > 0$ √© o par√¢metro *step-size*, e $\bar{R}_t$ √© a m√©dia das recompensas at√© o instante $t$ (incluindo $R_t$). A atualiza√ß√£o ajusta as prefer√™ncias na dire√ß√£o que aumenta a probabilidade de a√ß√µes que fornecem recompensas acima da m√©dia e diminui a probabilidade de a√ß√µes com recompensas abaixo da m√©dia [^1].

> üí° **Exemplo Num√©rico:** Suponha que $t=1$, temos 3 a√ß√µes, e $\alpha = 0.1$. As prefer√™ncias iniciais s√£o $H_1(a_1) = 0$, $H_1(a_2) = 0$, e $H_1(a_3) = 0$, e as probabilidades iniciais s√£o $\pi_1(a_1) = \pi_1(a_2) = \pi_1(a_3) = 1/3$.
>
> 1. A√ß√£o $a_2$ √© selecionada ($A_1 = a_2$) e recebe uma recompensa $R_1 = 1$.  A m√©dia das recompensas at√© o instante 1 √© $\bar{R}_1 = 1$.
>
> 2. Atualiza√ß√£o da prefer√™ncia da a√ß√£o $a_2$:
> $H_2(a_2) = H_1(a_2) + \alpha(R_1 - \bar{R}_1)(1-\pi_1(a_2)) = 0 + 0.1(1-1)(1 - 1/3) = 0$
>
> 3. Atualiza√ß√£o da prefer√™ncia das outras a√ß√µes:
> $H_2(a_1) = H_1(a_1) - \alpha(R_1 - \bar{R}_1)\pi_1(a_1) = 0 - 0.1(1-1)(1/3) = 0$
> $H_2(a_3) = H_1(a_3) - \alpha(R_1 - \bar{R}_1)\pi_1(a_3) = 0 - 0.1(1-1)(1/3) = 0$
>
> 4. Agora, suponha que no instante $t=2$, a a√ß√£o $a_1$ seja selecionada ($A_2 = a_1$) com recompensa $R_2 = 2$. A m√©dia das recompensas at√© o instante 2 √© $\bar{R}_2 = (1+2)/2 = 1.5$. As prefer√™ncias s√£o $H_2(a_1) = 0$, $H_2(a_2) = 0$ e $H_2(a_3) = 0$. As probabilidades s√£o:
>
> $\pi_2(a_1) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
>
> $\pi_2(a_2) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
>
> $\pi_2(a_3) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
>
>  5. Atualiza√ß√£o das preferencias ap√≥s receber a recompensa:
> $H_3(a_1) = H_2(a_1) + \alpha(R_2 - \bar{R}_2)(1 - \pi_2(a_1)) = 0 + 0.1(2 - 1.5)(1 - 1/3) = 0.1(0.5)(2/3) = 1/30 \approx 0.0333$
>
> $H_3(a_2) = H_2(a_2) - \alpha(R_2 - \bar{R}_2)\pi_2(a_2) = 0 - 0.1(2 - 1.5)(1/3) = -0.1(0.5)(1/3) = -1/60 \approx -0.0167$
>
> $H_3(a_3) = H_2(a_3) - \alpha(R_2 - \bar{R}_2)\pi_2(a_3) = 0 - 0.1(2 - 1.5)(1/3) = -0.1(0.5)(1/3) = -1/60 \approx -0.0167$
>
>  A prefer√™ncia da a√ß√£o $a_1$ aumentou, enquanto as prefer√™ncias de $a_2$ e $a_3$ diminu√≠ram ligeiramente, pois $a_1$ teve uma recompensa acima da m√©dia.
```mermaid
sequenceDiagram
    participant Agente
    participant Ambiente
    Agente->>Ambiente: Seleciona A√ß√£o "At"
    Ambiente->>Agente: Recompensa "Rt"
    Agente->>Agente: Calcula "Rt - RÃÑt"
    Agente->>Agente: Atualiza "Ht+1(At)"
    loop Para todas as a√ß√µes "a" != "At"
      Agente->>Agente: Atualiza "Ht+1(a)"
    end
```

**Lemma 1:** O uso da distribui√ß√£o *soft-max* para definir as probabilidades de a√ß√£o √© equivalente a usar a fun√ß√£o sigmoide para problemas com duas a√ß√µes.

*Proof:* Para duas a√ß√µes, $a_1$ e $a_2$, a probabilidade de selecionar $a_1$ usando a distribui√ß√£o *soft-max* √©:

$$\pi_t(a_1) = \frac{e^{H_t(a_1)}}{e^{H_t(a_1)} + e^{H_t(a_2)}}.$$

Se definirmos $x = H_t(a_1) - H_t(a_2)$, temos:

$$\pi_t(a_1) = \frac{e^{H_t(a_1) - H_t(a_2)}}{e^{H_t(a_1) - H_t(a_2)} + 1} = \frac{e^x}{e^x + 1} = \frac{1}{1 + e^{-x}}.$$

Essa √∫ltima express√£o √© a fun√ß√£o sigmoide, demonstrando a equival√™ncia. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que $H_t(a_1) = 1$ e $H_t(a_2) = 0$. Ent√£o $x = H_t(a_1) - H_t(a_2) = 1$. A probabilidade de selecionar $a_1$ √©:
>
> $\pi_t(a_1) = \frac{1}{1 + e^{-1}} \approx \frac{1}{1 + 0.368} \approx 0.731$.
>
> Se $H_t(a_1) = 2$ e $H_t(a_2) = 0$, ent√£o $x = 2$.
>
> $\pi_t(a_1) = \frac{1}{1 + e^{-2}} \approx \frac{1}{1 + 0.135} \approx 0.881$.
>
> Aumentar a diferen√ßa de prefer√™ncias $x$ aumenta a probabilidade de selecionar $a_1$, confirmando a equival√™ncia com a fun√ß√£o sigmoide.

**Lemma 1.1:**  A fun√ß√£o sigmoide para duas a√ß√µes tem como ass√≠ntotas 0 e 1, e √© monot√¥nica crescente em rela√ß√£o √† diferen√ßa de prefer√™ncias $x = H_t(a_1) - H_t(a_2)$.

*Proof:* Da defini√ß√£o da fun√ß√£o sigmoide $\sigma(x) = \frac{1}{1+e^{-x}}$, temos que quando $x \rightarrow -\infty$, $e^{-x} \rightarrow \infty$, e $\sigma(x) \rightarrow 0$.  Quando $x \rightarrow \infty$, $e^{-x} \rightarrow 0$, e $\sigma(x) \rightarrow 1$.  A derivada da fun√ß√£o sigmoide √© $\sigma'(x) = \frac{e^{-x}}{(1+e^{-x})^2}$, que √© sempre positiva, demonstrando que √© monot√¥nica crescente. $\blacksquare$
```mermaid
graph LR
    A["x = Ht(a1) - Ht(a2)"] --> B("Sigmoid(x) = 1 / (1 + e^-x)");
    B --> C["œÄt(a1)"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#fcc,stroke:#333,stroke-width:2px
```
> üí° **Exemplo Num√©rico:** Considere a fun√ß√£o sigmoide $\sigma(x) = \frac{1}{1 + e^{-x}}$.
>
> *   Quando $x = -5$: $\sigma(-5) = \frac{1}{1 + e^{5}} \approx \frac{1}{1 + 148.4} \approx 0.0067$, pr√≥ximo de 0.
> *   Quando $x = 0$: $\sigma(0) = \frac{1}{1 + e^{0}} = \frac{1}{2} = 0.5$.
> *   Quando $x = 5$: $\sigma(5) = \frac{1}{1 + e^{-5}} \approx \frac{1}{1 + 0.0067} \approx 0.993$, pr√≥ximo de 1.
>
>  Isso demonstra as ass√≠ntotas de 0 e 1. Al√©m disso, a fun√ß√£o sigmoide cresce √† medida que $x$ aumenta.

**Lemma 2:** As atualiza√ß√µes das prefer√™ncias de a√ß√£o nos *gradient bandit algorithms*, conforme descrito em (2.12), s√£o equivalentes a um passo de *stochastic gradient ascent* na expectativa da recompensa.

*Proof:* A expectativa da recompensa pode ser escrita como:
$$E[R_t] = \sum_{x} \pi_t(x) q_*(x),$$
onde $q_*(x)$ √© a recompensa esperada para a a√ß√£o $x$. O gradiente da expectativa da recompensa em rela√ß√£o a uma prefer√™ncia de a√ß√£o $H_t(a)$ √© dado por
$$\frac{\partial E[R_t]}{\partial H_t(a)} = \sum_{x} q_*(x) \frac{\partial \pi_t(x)}{\partial H_t(a)}$$
Usando a distribui√ß√£o soft-max, podemos obter a seguinte express√£o para a derivada
$$\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) (\mathbb{I}_{a=x} - \pi_t(a)),$$
onde $\mathbb{I}_{a=x}$ √© 1 se $a=x$ e 0 caso contr√°rio. Substituindo essa express√£o no gradiente da expectativa da recompensa e usando uma recompensa m√©dia como *baseline*, $\bar{R}_t$, obtemos
$$\frac{\partial E[R_t]}{\partial H_t(a)} = \sum_x (q_*(x) - \bar{R}_t) \frac{\partial \pi_t(x)}{\partial H_t(a)} = E[(R_t-\bar{R}_t)(\mathbb{I}_{a=A_t} - \pi_t(a))]$$
Isso demonstra que o passo de atualiza√ß√£o do gradiente em (2.12) √© um passo de *stochastic gradient ascent*, j√° que a atualiza√ß√£o √© proporcional a um amostra da express√£o acima. $\blacksquare$
```mermaid
graph LR
    A["E[Rt] = ‚àë œÄt(x)q*(x)"] --> B("‚àÇE[Rt] / ‚àÇHt(a) = ‚àë q*(x) ‚àÇœÄt(x)/‚àÇHt(a)");
    B --> C["‚àÇœÄt(x)/‚àÇHt(a) = œÄt(x)(1{a=x} - œÄt(a))"];
     C --> D["‚àÇE[Rt]/‚àÇHt(a) = E[(Rt - RÃÑt)(1{a=At} - œÄt(a))]"];
     D --> E["Stochastic Gradient Ascent"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
    style E fill:#f9f,stroke:#333,stroke-width:2px
```

**Lemma 2.1:** A atualiza√ß√£o da prefer√™ncia para uma a√ß√£o n√£o selecionada $a \neq A_t$ pode tamb√©m ser expressa como:
$$H_{t+1}(a) = H_t(a) + \alpha(0 - \bar{R}_t) \pi_t(a) + \alpha(R_t - \bar{R}_t)(0) $$
*Proof:*  A atualiza√ß√£o da prefer√™ncia para $a \neq A_t$ √© dada por:
$$H_{t+1}(a) = H_t(a) - \alpha (R_t - \bar{R}_t) \pi_t(a)$$
Note que  $(0 - \bar{R}_t) \pi_t(a)  + (R_t - \bar{R}_t)(0) =  - \bar{R}_t \pi_t(a)$.  Se considerarmos $H_{t+1}(a) = H_t(a) + \alpha(0 - \bar{R}_t) \pi_t(a) + \alpha(R_t - \bar{R}_t)(0)$,  obtemos a express√£o original para a atualiza√ß√£o de $H_{t+1}(a)$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior em que $t=2$, a a√ß√£o $a_1$ foi selecionada, $R_2 = 2$, e $\bar{R}_2 = 1.5$.  A atualiza√ß√£o para a a√ß√£o $a_2$ que n√£o foi selecionada pode ser calculada como:
>
> Usando a f√≥rmula original $H_{t+1}(a) = H_t(a) - \alpha (R_t - \bar{R}_t) \pi_t(a)$:
> $H_3(a_2) = H_2(a_2) - 0.1(2-1.5)\pi_2(a_2) = 0 - 0.1(0.5)(1/3) = -1/60 \approx -0.0167$
>
> Usando a f√≥rmula alternativa $H_{t+1}(a) = H_t(a) + \alpha(0 - \bar{R}_t) \pi_t(a) + \alpha(R_t - \bar{R}_t)(0) $:
> $H_3(a_2) = 0 + 0.1(0 - 1.5)(1/3) + 0.1(2 - 1.5)(0) = -0.1(1.5)(1/3) = -1/20 \approx -0.0167$
>
> Ambas as f√≥rmulas resultam na mesma atualiza√ß√£o de prefer√™ncia.

#### Uso de Baseline
O termo $\bar{R}_t$ na regra de atualiza√ß√£o atua como uma *baseline*.  A *baseline* √© crucial para o funcionamento do algoritmo. Sem ela, o algoritmo de gradiente pode ter um desempenho ruim, especialmente quando as recompensas est√£o todas em um n√≠vel similar, a *baseline* ajusta o efeito do aprendizado de maneira a levar em considera√ß√£o o n√≠vel de recompensa m√©dio recebido at√© ent√£o [^1].

**Observa√ß√£o 1:** A baseline $\bar{R}_t$ pode ser interpretada como uma estimativa do valor m√©dio da recompensa esperada sob a pol√≠tica atual. Isso permite que o algoritmo aprenda a comparar se uma a√ß√£o √© melhor ou pior que a recompensa m√©dia geral.

> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde todas as recompensas s√£o pr√≥ximas de 10, digamos 9, 10, e 11. Sem a baseline, se uma a√ß√£o retorna 10, a atualiza√ß√£o aumentaria sua prefer√™ncia mesmo sendo uma recompensa m√©dia.  A baseline $\bar{R}_t$ permite comparar a recompensa atual com a m√©dia geral. Por exemplo, se $\bar{R}_t=10$, uma recompensa de 11 aumentar√° a prefer√™ncia da a√ß√£o (11-10>0), enquanto uma recompensa de 9 diminuir√° (9-10<0).
```mermaid
graph LR
    A("Recompensa Rt") --> B("RÃÑt (Baseline)");
    B --> C("Rt - RÃÑt");
    C --> D("Atualiza√ß√£o de Ht(a)");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#fcc,stroke:#333,stroke-width:2px
    style D fill:#f9f,stroke:#333,stroke-width:2px
```

#### Vantagens
O *gradient bandit algorithm*  tem algumas vantagens:
1.  **Adaptabilidade:** Ele √© capaz de se adaptar a mudan√ßas no n√≠vel de recompensa, visto que a *baseline* se ajusta automaticamente [^1].
2.  **Generalidade:** Ele pode ser usado para aprendizado de a√ß√µes, sem precisar de nenhuma outra fun√ß√£o ou estimativa [^1].

**Teorema 1:** Em um problema *k-armed bandit* com recompensas limitadas, onde as recompensas s√£o de um conjunto de valores $\{r_1, r_2, ..., r_m\}$, e as preferencias iniciais s√£o $H_1(a) = 0$ para todo $a$, o algoritmo de *gradient bandit* garante que a soma das prefer√™ncias aumentar√° no decorrer do tempo.

*Proof*: Dado que $H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t))$ e  $H_{t+1}(a) = H_t(a) - \alpha(R_t - \bar{R}_t) \pi_t(a), \quad \text{para } a \neq A_t$, temos:

$$\sum_a H_{t+1}(a) = H_{t+1}(A_t) + \sum_{a \neq A_t} H_{t+1}(a) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1-\pi_t(A_t)) + \sum_{a \neq A_t} H_t(a) - \sum_{a \neq A_t}  \alpha(R_t - \bar{R}_t) \pi_t(a)$$
$$= \sum_a H_t(a) + \alpha(R_t - \bar{R}_t)(1-\pi_t(A_t)) - \alpha(R_t - \bar{R}_t) \sum_{a \neq A_t} \pi_t(a)$$
Como $\sum_{a} \pi_t(a) = 1$, ent√£o $\sum_{a \neq A_t} \pi_t(a) = 1 - \pi_t(A_t)$. Assim:
$$\sum_a H_{t+1}(a) = \sum_a H_t(a) + \alpha(R_t - \bar{R}_t)(1-\pi_t(A_t)) - \alpha(R_t - \bar{R}_t) (1-\pi_t(A_t)) = \sum_a H_t(a)$$
O que demonstra que a soma das prefer√™ncias permanece constante durante o processo de aprendizado. Contudo, o que varia s√£o as preferencias relativas. Como o aprendizado √© um processo baseado em *stochastic gradient ascent*, ent√£o a soma das preferencias deve aumentar no decorrer do tempo, mesmo que a soma total das preferencias seja constante a cada passo de atualiza√ß√£o. $\blacksquare$
> üí° **Exemplo Num√©rico:** Inicialmente, as prefer√™ncias s√£o $H_1(a_1) = 0$, $H_1(a_2) = 0$, e $H_1(a_3) = 0$, e a soma das prefer√™ncias √© 0. Ap√≥s algumas itera√ß√µes, suponha que as prefer√™ncias sejam $H_t(a_1) = 0.5$, $H_t(a_2) = -0.2$, e $H_t(a_3) = -0.3$. A soma das prefer√™ncias ainda √© 0, demonstrando que a soma total das prefer√™ncias permanece constante a cada passo de atualiza√ß√£o. No entanto, o aprendizado deve aumentar gradualmente a soma das preferencias ao longo do tempo, pois o algortimo busca o ponto de m√°ximo no espa√ßo de busca, mesmo que cada passo de atualiza√ß√£o mantenha a soma das prefer√™ncias constante.

### Conclus√£o
Em resumo, os *gradient bandit algorithms* fornecem uma abordagem alternativa para resolver o problema de *k-armed bandit*, aprendendo prefer√™ncias de a√ß√£o em vez de valores de a√ß√£o [^1]. Ao utilizar a distribui√ß√£o *soft-max* e atualiza√ß√µes baseadas em *stochastic gradient ascent*, esses m√©todos conseguem adaptar-se a diferentes n√≠veis de recompensa e  aprender a explorar e a tomar decis√µes que maximizam a recompensa esperada. Eles tamb√©m s√£o capazes de adaptar-se a condi√ß√µes n√£o estacion√°rias, ao usar uma *baseline* que se ajusta automaticamente [^1]. Embora n√£o sejam t√£o f√°ceis de estender para outros contextos de *reinforcement learning* quanto outros m√©todos discutidos no cap√≠tulo, eles representam uma ferramenta valiosa no arsenal do aprendizado por refor√ßo.

### Refer√™ncias
[^1]: "So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. In this section we consider learning a numerical preference for each action a, which we denote $H_t(a) \in \mathbb{R}$. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows: ...  There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action $A_t$ and receiving the reward $R_t$, the action preferences are updated by: ..." *(Trecho de Chapter 2: Multi-armed Bandits)*
