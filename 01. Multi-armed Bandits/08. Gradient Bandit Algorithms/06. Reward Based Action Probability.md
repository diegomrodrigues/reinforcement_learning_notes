## Gradient Bandit Algorithms: Ajustando Probabilidades de A√ß√£o Baseado na Recompensa

### Introdu√ß√£o

Este cap√≠tulo explora algoritmos que aprendem prefer√™ncias num√©ricas para cada a√ß√£o, em vez de estimar valores de a√ß√£o. O foco est√° em como essas prefer√™ncias s√£o utilizadas para selecionar a√ß√µes, utilizando uma distribui√ß√£o softmax, que √© um m√©todo probabil√≠stico. Essa abordagem contrasta com os m√©todos de valor de a√ß√£o que s√£o discutidos em outros cap√≠tulos [^1], e que visam estimar os valores das a√ß√µes para orientar a tomada de decis√£o. Uma das caracter√≠sticas distintas dos Gradient Bandit Algorithms √© o uso de uma *baseline* de recompensa, que permite ajustes mais precisos das probabilidades de a√ß√£o com base no desempenho relativo da a√ß√£o tomada.

### Conceitos Fundamentais

#### Prefer√™ncias de A√ß√£o e Distribui√ß√£o Softmax

Nos **Gradient Bandit Algorithms**, cada a√ß√£o $a$ √© associada a uma prefer√™ncia num√©rica $H_t(a) \in \mathbb{R}$ [^1]. Diferentemente dos valores de a√ß√£o, essas prefer√™ncias n√£o t√™m uma interpreta√ß√£o direta em termos de recompensa. O que importa √© a prefer√™ncia relativa entre as a√ß√µes. A probabilidade de selecionar uma a√ß√£o $a$ no instante $t$ √© dada pela distribui√ß√£o softmax:

$$
Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} = \pi_t(a)
$$

onde $k$ √© o n√∫mero total de a√ß√µes e $\pi_t(a)$ √© a probabilidade de selecionar a a√ß√£o $a$ no instante $t$ [^1]. Inicialmente, todas as prefer√™ncias s√£o iguais (por exemplo, $H_1(a) = 0$ para todas as a√ß√µes), resultando em uma probabilidade igual de sele√ß√£o para todas as a√ß√µes [^1].

> üí° **Exemplo Num√©rico:** Suponha que temos 3 a√ß√µes (k=3) e inicialmente, as prefer√™ncias s√£o $H_1(1) = 0$, $H_1(2) = 0$, e $H_1(3) = 0$. A probabilidade de selecionar cada a√ß√£o no primeiro passo ($t=1$) √©:
>
> $\pi_1(1) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
>
> $\pi_1(2) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
>
> $\pi_1(3) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
>
> Todas as a√ß√µes t√™m a mesma probabilidade de serem escolhidas inicialmente.

```mermaid
graph LR
    subgraph "Distribui√ß√£o Softmax"
    A("H_t(a)") -->|e^x| B("e^{H_t(a)}")
    C("Soma das exp preferencias") --> D("Œ£ e^{H_t(b)}")
    B --> E("Divis√£o")
    D --> E
    E --> F("œÄ_t(a) = Probabilidade da a√ß√£o a no tempo t")
     end
```

**Proposi√ß√£o 1:** *A distribui√ß√£o softmax garante que todas as a√ß√µes tenham uma probabilidade n√£o-nula de serem selecionadas, a menos que as prefer√™ncias sejam $-\infty$*.

*Proof:* A fun√ß√£o exponencial $e^{H_t(a)}$ √© sempre positiva para qualquer valor real de $H_t(a)$. Assim, o numerador da distribui√ß√£o softmax √© sempre positivo. O denominador √© a soma de termos positivos, ent√£o tamb√©m √© positivo. Portanto, a probabilidade $\pi_t(a)$ √© sempre maior que zero, a menos que $H_t(a) = -\infty$, o que na pr√°tica n√£o ocorre devido √† atualiza√ß√£o das prefer√™ncias. $\blacksquare$

#### Atualiza√ß√£o das Prefer√™ncias via Gradiente Estoc√°stico

As prefer√™ncias de a√ß√£o s√£o atualizadas usando um algoritmo de ascens√£o de gradiente estoc√°stico. Ap√≥s selecionar a a√ß√£o $A_t$ e receber a recompensa $R_t$, as prefer√™ncias s√£o atualizadas da seguinte forma:

$$
\begin{aligned}
H_{t+1}(A_t) &= H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t)) \\
H_{t+1}(a) &= H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a), \quad \forall a \ne A_t
\end{aligned}
$$

onde $\alpha > 0$ √© um par√¢metro de tamanho do passo e $\bar{R}_t$ √© a recompensa m√©dia at√© o instante $t$ [^1]. Este termo, $\bar{R}_t$, serve como *baseline*, onde $R_1$ √© o primeiro valor recebido.

> üí° **Exemplo Num√©rico:** Suponha que no instante $t=2$, a a√ß√£o selecionada foi $A_2=2$. As prefer√™ncias eram $H_2(1) = 0.1$, $H_2(2) = 0.2$, e $H_2(3) = -0.1$. A recompensa obtida foi $R_2 = 1$, e a recompensa m√©dia at√© agora √© $\bar{R}_2 = 0.5$ (supondo que $R_1 = 0$). Usamos um tamanho de passo $\alpha = 0.1$. Primeiro, calculamos as probabilidades de a√ß√£o:
>
>  $\pi_2(1) = \frac{e^{0.1}}{e^{0.1} + e^{0.2} + e^{-0.1}} \approx \frac{1.105}{1.105 + 1.221 + 0.905} \approx 0.336$
>
>  $\pi_2(2) = \frac{e^{0.2}}{e^{0.1} + e^{0.2} + e^{-0.1}} \approx \frac{1.221}{1.105 + 1.221 + 0.905} \approx 0.372$
>
>  $\pi_2(3) = \frac{e^{-0.1}}{e^{0.1} + e^{0.2} + e^{-0.1}} \approx \frac{0.905}{1.105 + 1.221 + 0.905} \approx 0.292$
>
> Agora, atualizamos as prefer√™ncias:
>
> $H_3(2) = 0.2 + 0.1 * (1 - 0.5) * (1 - 0.372) \approx 0.2 + 0.1 * 0.5 * 0.628 = 0.231$
>
> $H_3(1) = 0.1 - 0.1 * (1 - 0.5) * 0.336 \approx 0.1 - 0.1 * 0.5 * 0.336 = 0.083$
>
> $H_3(3) = -0.1 - 0.1 * (1 - 0.5) * 0.292 \approx -0.1 - 0.1 * 0.5 * 0.292 = -0.115$
>
> Observe que a prefer√™ncia da a√ß√£o escolhida (a√ß√£o 2) aumentou, pois $R_2 > \bar{R}_2$, enquanto as prefer√™ncias das a√ß√µes n√£o selecionadas (a√ß√µes 1 e 3) diminu√≠ram.

```mermaid
  flowchart LR
      subgraph "Atualiza√ß√£o da Prefer√™ncia"
      A("H_t(a)") --> B{{"A√ß√£o a == A_t?"}}
        B -- "Sim" --> C("H_{t+1}(A_t) = H_t(A_t) + Œ±(R_t -  RÃÑ_t)(1 - œÄ_t(A_t))")
      B -- "N√£o" --> D("H_{t+1}(a) = H_t(a) - Œ±(R_t - RÃÑ_t)œÄ_t(a)")
      C --> E("H_{t+1}(a)")
      D --> E
       end
```

**Lemma 1:** *A atualiza√ß√£o das prefer√™ncias de a√ß√£o nos Gradient Bandit Algorithms √© uma inst√¢ncia de ascens√£o de gradiente estoc√°stico*.

*Proof:* O objetivo √© maximizar o valor esperado da recompensa, $E[R_t]$. A atualiza√ß√£o das prefer√™ncias de a√ß√£o √© baseada no gradiente da recompensa esperada em rela√ß√£o √†s prefer√™ncias de a√ß√£o. A derivada parcial da recompensa esperada √© dada por [^1]:

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = \sum_x (q_*(x) - \bar{R}_t) \frac{\partial \pi_t(x)}{\partial H_t(a)}
$$

onde $q_*(x)$ √© o valor real da a√ß√£o $x$.

Substituindo $\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x)(\delta_{ax}-\pi_t(a))$, onde $\delta_{ax} = 1$ se $a=x$ e 0 caso contr√°rio, temos:

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = \sum_x (q_*(x) - \bar{R}_t) \pi_t(x)(\delta_{ax}-\pi_t(a))
$$

Essa express√£o pode ser reescrita como:

$$
E[(R_t - \bar{R}_t) (\delta_{aA_t} - \pi_t(a))]
$$

A atualiza√ß√£o das prefer√™ncias de a√ß√£o no algoritmo √© feita tomando um passo na dire√ß√£o desse gradiente:

$$
H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t) (\delta_{aA_t} - \pi_t(a))
$$
que √© equivalente √†s equa√ß√µes de atualiza√ß√£o apresentadas anteriormente, estabelecendo o algoritmo como uma inst√¢ncia de ascens√£o de gradiente estoc√°stico. $\blacksquare$

**Lemma 1.1:** *A atualiza√ß√£o das prefer√™ncias pode ser reescrita utilizando uma nota√ß√£o mais compacta.*

*Proof:* Podemos definir o vetor de prefer√™ncias $H_t = [H_t(1), H_t(2), \ldots, H_t(k)]$ e a probabilidade de a√ß√£o $\pi_t = [\pi_t(1), \pi_t(2), \ldots, \pi_t(k)]$. Ent√£o a atualiza√ß√£o das prefer√™ncias pode ser escrita de forma mais compacta como:

$$
H_{t+1} = H_t + \alpha(R_t - \bar{R}_t)(\delta_{A_t} - \pi_t)
$$
onde $\delta_{A_t}$ √© um vetor com todos os elementos iguais a 0 exceto o elemento correspondente √† a√ß√£o $A_t$ que √© 1. Essa nota√ß√£o compacta facilita a an√°lise e implementa√ß√£o do algoritmo. $\blacksquare$

> üí° **Exemplo Num√©rico (Nota√ß√£o Compacta):** Usando o exemplo anterior com $k=3$, podemos representar as prefer√™ncias e probabilidades como vetores:
>
> $H_2 = [0.1, 0.2, -0.1]$
>
> $\pi_2 = [0.336, 0.372, 0.292]$
>
> $\delta_{A_2} = [0, 1, 0]$ (pois $A_2 = 2$)
>
> $R_2 = 1$, $\bar{R}_2 = 0.5$, $\alpha = 0.1$
>
> A atualiza√ß√£o das prefer√™ncias em nota√ß√£o vetorial √©:
>
> $H_{3} = H_2 + \alpha(R_2 - \bar{R}_2)(\delta_{A_2} - \pi_2)$
>
> $H_{3} = [0.1, 0.2, -0.1] + 0.1*(1 - 0.5)*([0, 1, 0] - [0.336, 0.372, 0.292])$
>
> $H_{3} = [0.1, 0.2, -0.1] + 0.05*[-0.336, 0.628, -0.292]$
>
> $H_{3} = [0.1 - 0.0168, 0.2 + 0.0314, -0.1 - 0.0146]$
>
> $H_{3} = [0.0832, 0.2314, -0.1146]$
>
> Este resultado √© equivalente ao calculado anteriormente, demonstrando a utilidade da nota√ß√£o compacta.

```mermaid
graph LR
    subgraph "Atualiza√ß√£o de Prefer√™ncias - Nota√ß√£o Vetorial"
    H_t("H_t (Vetor de Prefer√™ncias)")
    pi_t("œÄ_t (Vetor de Probabilidades)")
    delta_A_t("Œ¥_{A_t} (Vetor Indicador A√ß√£o Selecionada)")
    alpha("Œ± (Taxa de Aprendizado)")
    R_t("R_t (Recompensa)")
    R_bar_t("RÃÑ_t (Recompensa M√©dia)")
    
    H_t -- "+" --> sum_1
    alpha -- "*" --> mult_1
    R_t -- "-" --> sub_1
    R_bar_t -- "+" --> sub_1
    
   sub_1 -- "*" --> mult_1
   
    delta_A_t -- "-" --> sub_2
    pi_t -- "+" --> sub_2
    
    sub_2 -- "*" --> mult_1
    mult_1 --> sum_1
    
   sum_1 --> H_t_plus_1("H_{t+1} (Novas Prefer√™ncias)")
    end
```

**Corol√°rio 1:** *O termo baseline, $\bar{R}_t$, n√£o afeta a converg√™ncia do algoritmo, mas sim a taxa de converg√™ncia*.

*Proof:* Conforme demonstrado no Lemma 1, a atualiza√ß√£o das prefer√™ncias √© um m√©todo de ascens√£o de gradiente estoc√°stico e independe do termo $\bar{R}_t$.  Qualquer valor constante de $\bar{R}_t$ n√£o altera o resultado da derivada, garantindo a converg√™ncia. Entretanto, o termo *baseline* reduz a vari√¢ncia da atualiza√ß√£o, resultando em uma taxa de converg√™ncia mais r√°pida e est√°vel. $\blacksquare$

**Corol√°rio 1.1:** *Se a recompensa $R_t$ for sempre igual a $\bar{R}_t$, ent√£o as prefer√™ncias n√£o s√£o modificadas no passo atual.*

*Proof:* Se $R_t = \bar{R}_t$, ent√£o o termo $(R_t - \bar{R}_t)$ √© zero. Consequentemente, a atualiza√ß√£o das prefer√™ncias $H_{t+1}(a)$ torna-se igual a $H_t(a)$ para toda a√ß√£o $a$, ou seja, $H_{t+1} = H_t$. Isso demonstra que quando a recompensa atual √© igual √† m√©dia, n√£o h√° ajustes nas prefer√™ncias, o que indica que o desempenho atual √© considerado "neutro" em rela√ß√£o ao passado.  $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que no tempo $t=3$, as prefer√™ncias s√£o $H_3(1)=0.083$, $H_3(2)=0.231$, $H_3(3)=-0.115$, e a a√ß√£o selecionada foi $A_3=1$. A recompensa obtida foi $R_3 = 0.6$ e a m√©dia das recompensas at√© o momento √© $\bar{R}_3 = 0.6$. Como $R_3=\bar{R}_3$, ent√£o:
>
> $H_{4}(1) = H_3(1) + \alpha(R_3 - \bar{R}_3)(1 - \pi_3(1)) = 0.083 + \alpha(0)(1 - \pi_3(1)) = 0.083$
>
> $H_{4}(2) = H_3(2) - \alpha(R_3 - \bar{R}_3)\pi_3(2) = 0.231 - \alpha(0)\pi_3(2) = 0.231$
>
> $H_{4}(3) = H_3(3) - \alpha(R_3 - \bar{R}_3)\pi_3(3) = -0.115 - \alpha(0)\pi_3(3) = -0.115$
>
> Todas as prefer√™ncias permanecem inalteradas, confirmando o Corol√°rio 1.1.

####  Impacto da Baseline

A recompensa m√©dia $\bar{R}_t$ serve como uma *baseline* com a qual a recompensa $R_t$ √© comparada [^1]. Se a recompensa obtida √© maior que a *baseline*, a probabilidade de escolher a a√ß√£o $A_t$ √© aumentada; caso contr√°rio, √© diminu√≠da. As a√ß√µes que n√£o foram selecionadas no tempo $t$ t√™m suas probabilidades ajustadas na dire√ß√£o oposta [^1]. Este mecanismo garante que o algoritmo aprenda quais a√ß√µes s√£o relativamente melhores ou piores, em vez de apenas aprender o valor absoluto das recompensas. Este ponto tamb√©m est√° descrito na se√ß√£o *2.8. Gradient Bandit Algorithms* do texto [^1].

#### Ajuste da Probabilidade de A√ß√£o com Base na Recompensa

A chave do Gradient Bandit Algorithm √© o ajuste da probabilidade de cada a√ß√£o baseada na recompensa recebida. Se a recompensa $R_t$ excede a baseline $\bar{R}_t$, a probabilidade $\pi_t(A_t)$ da a√ß√£o $A_t$ √© incrementada. Isso √© obtido adicionando $\alpha(R_t - \bar{R}_t)(1-\pi_t(A_t))$ √† prefer√™ncia $H_t(A_t)$. Para as a√ß√µes que n√£o foram selecionadas, a prefer√™ncia √© reduzida por $\alpha(R_t - \bar{R}_t)\pi_t(a)$. Assim, as a√ß√µes que levam a recompensas acima da baseline s√£o favorecidas ao longo do tempo, enquanto as a√ß√µes que levam a recompensas abaixo da baseline s√£o desfavorecidas. Este mecanismo assegura que o algoritmo de aprendizado se ajuste para as a√ß√µes mais recompensadoras.

**Lema 2:** *A soma das altera√ß√µes nas prefer√™ncias em um dado passo de tempo √© sempre zero.*

*Proof:* A altera√ß√£o na prefer√™ncia da a√ß√£o selecionada $A_t$ √© dada por $\alpha(R_t - \bar{R}_t)(1-\pi_t(A_t))$, enquanto que para qualquer outra a√ß√£o $a \neq A_t$, √© dada por $-\alpha(R_t - \bar{R}_t)\pi_t(a)$. A soma de todas as altera√ß√µes nas prefer√™ncias no passo $t$ √© ent√£o dada por:

$$
\alpha(R_t - \bar{R}_t)(1-\pi_t(A_t)) + \sum_{a \neq A_t} -\alpha(R_t - \bar{R}_t)\pi_t(a)
$$

Reescrevendo a soma:

$$
\alpha(R_t - \bar{R}_t) - \alpha(R_t - \bar{R}_t)\pi_t(A_t) - \alpha(R_t - \bar{R}_t)\sum_{a \neq A_t} \pi_t(a)
$$
Como $\sum_{a=1}^{k} \pi_t(a) = 1$, ent√£o $\sum_{a \neq A_t} \pi_t(a) = 1 - \pi_t(A_t)$. Substituindo:
$$
\alpha(R_t - \bar{R}_t) - \alpha(R_t - \bar{R}_t)\pi_t(A_t) - \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t))
$$
$$
\alpha(R_t - \bar{R}_t) - \alpha(R_t - \bar{R}_t)\pi_t(A_t) - \alpha(R_t - \bar{R}_t) + \alpha(R_t - \bar{R}_t)\pi_t(A_t) = 0
$$

Portanto, a soma das altera√ß√µes nas prefer√™ncias em um dado passo √© sempre zero. Isso garante que a m√©dia das prefer√™ncias n√£o se desloque arbitrariamente, focando na altera√ß√£o relativa entre as prefer√™ncias. $\blacksquare$

> üí° **Exemplo Num√©rico:** Retomando o exemplo do tempo t=2, onde $A_2=2$, $R_2 = 1$, $\bar{R}_2 = 0.5$, $\alpha=0.1$, $\pi_2 = [0.336, 0.372, 0.292]$, as atualiza√ß√µes foram:
>
> $\Delta H_2(2) = \alpha(R_2 - \bar{R}_2)(1 - \pi_2(2)) = 0.1 * (1 - 0.5) * (1 - 0.372) \approx 0.0314$
>
> $\Delta H_2(1) = -\alpha(R_2 - \bar{R}_2)\pi_2(1) = -0.1 * (1 - 0.5) * 0.336 \approx -0.0168$
>
> $\Delta H_2(3) = -\alpha(R_2 - \bar{R}_2)\pi_2(3) = -0.1 * (1 - 0.5) * 0.292 \approx -0.0146$
>
> A soma das altera√ß√µes √©: $0.0314 - 0.0168 - 0.0146 = 0$, confirmando o Lema 2.

```mermaid
graph LR
    subgraph "Soma das Altera√ß√µes nas Prefer√™ncias"
    A("ŒîH_t(A_t) = Œ±(R_t - RÃÑ_t)(1 - œÄ_t(A_t))")
    B("Œ£ ŒîH_t(a) para a != A_t = -Œ±(R_t - RÃÑ_t)Œ£œÄ_t(a)")
    A -- "+" --> C("Soma Total")
    B --> C
    C --> D("Soma = 0")
    end
```

### Conclus√£o

Os **Gradient Bandit Algorithms** oferecem uma abordagem alternativa para o problema de *k-armed bandits*, concentrando-se em prefer√™ncias de a√ß√£o em vez de valores de a√ß√£o. A utiliza√ß√£o de uma *baseline* de recompensa permite ajustar as probabilidades de a√ß√£o de forma mais precisa e eficiente. Essa abordagem √© uma inst√¢ncia de ascens√£o de gradiente estoc√°stico e possui propriedades de converg√™ncia robustas. O uso da *baseline* √© essencial para estabilizar o aprendizado, o que √© demonstrado no contexto [^1] quando os resultados de um teste de desempenho do algoritmo *gradient bandit* com e sem linha de base s√£o comparados. Ao ajustar as probabilidades de a√ß√£o com base no desempenho relativo das a√ß√µes tomadas, esses algoritmos se tornam muito eficazes no problema de aprendizado por refor√ßo.

### Refer√™ncias

[^1]: "So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. In this section we consider learning a numerical preference for each action a, which we denote H≈Ç(a) ‚àà R. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows: ... where here we have also introduced a useful new notation, œÄt(a), for the probability of taking action a at time t. Initially all action preferences are the same (e.g., H‚ÇÅ(a) = 0, for all a) so that all actions have an equal probability of being selected. ... There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action At and receiving the reward Rt, the action preferences are updated by:
Ht+1(At) = Ht(At) + Œ±(Rt ‚Äì Rt) (1 ‚Äì œÄœÑ(At)), and
Ht+1(a) = H+(a) ‚Äì Œ±(Rt ‚Äì Rt)œÄŒπ(Œ±), for all a ‚â† At,
where a > 0 is a step-size parameter, and Rt ‚àà R is the average of the rewards up to but not including time t (with R‚ÇÅ = R‚ÇÅ), which can be computed incrementally as described in Section 2.4 (or Section 2.5 if the problem is nonstationary).¬π The Rt term serves as a baseline with which the reward is compared. If the reward is higher than the baseline, then the probability of taking At in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite direction." *(Trecho de Multi-armed Bandits)*
