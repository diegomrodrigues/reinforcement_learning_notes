### Gradient Bandit Algorithms e o Impacto da Linha de Base

### Introdu√ß√£o
O aprendizado por refor√ßo (RL) difere de outros m√©todos de aprendizado por utilizar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de simplesmente instruir atrav√©s de a√ß√µes corretas [1]. Em um cen√°rio simplificado, exploramos o *k-armed bandit problem*, onde o objetivo √© maximizar a recompensa total esperada ao longo do tempo, escolhendo entre *k* diferentes op√ß√µes de a√ß√µes [1]. Este cap√≠tulo aborda m√©todos que estimam os valores de a√ß√£o para guiar a sele√ß√£o de a√ß√µes. Uma alternativa, explorada nas se√ß√µes finais deste cap√≠tulo, envolve aprender uma **prefer√™ncia num√©rica** para cada a√ß√£o, em vez de um valor estimado [1]. Essas prefer√™ncias, denotadas como $H_t(a)$, influenciam a probabilidade de sele√ß√£o de uma a√ß√£o atrav√©s da distribui√ß√£o soft-max [1]. Especificamente, este cap√≠tulo busca entender como a omiss√£o da linha de base em algoritmos de bandit gradiente afeta o desempenho do aprendizado.

### Conceitos Fundamentais
Os algoritmos de **bandit gradiente** s√£o uma abordagem alternativa aos m√©todos de valor de a√ß√£o para o *k-armed bandit problem*, onde o objetivo n√£o √© estimar o valor das a√ß√µes, mas sim aprender uma prefer√™ncia num√©rica para cada a√ß√£o [1]. Estas prefer√™ncias, $H_t(a) \in \mathbb{R}$, determinam a frequ√™ncia com que uma a√ß√£o √© selecionada. A probabilidade de selecionar uma a√ß√£o $a$ no tempo $t$ √© definida pela distribui√ß√£o *soft-max*:

$$
Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} = \pi_t(a)
$$

onde $\pi_t(a)$ denota a probabilidade de selecionar a a√ß√£o $a$ no tempo $t$ [1]. As prefer√™ncias das a√ß√µes s√£o atualizadas usando o conceito de **ascens√£o de gradiente estoc√°stico**. Ap√≥s selecionar a a√ß√£o $A_t$ e receber a recompensa $R_t$, as prefer√™ncias s√£o atualizadas da seguinte forma:

$$
H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t))
$$

$$
H_{t+1}(a) = H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a), \quad \text{para todo} \quad a \neq A_t
$$
```mermaid
graph LR
    A[A√ß√£o "A_t" Selecionada] -->|Recompensa "R_t" Recebida| B(Atualiza√ß√£o de Prefer√™ncias);
    B --> C{Calcular "H_{t+1}(A_t)"};
    C --> D{Calcular "H_{t+1}(a)" para "a != A_t"};
    D --> E[Prefer√™ncias "H_{t+1}(a)" Atualizadas];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```
Aqui, $\alpha > 0$ √© o par√¢metro de *step-size*, e $\bar{R}_t$ √© a recompensa m√©dia at√© o tempo $t$ [1]. A introdu√ß√£o da linha de base, $\bar{R}_t$, √© crucial. Se a recompensa $R_t$ for maior que a linha de base $\bar{R}_t$, a probabilidade de selecionar $A_t$ no futuro aumenta, caso contr√°rio, diminui [1].

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de 3-armed bandit (k=3). Inicialmente, as prefer√™ncias s√£o $H_1(1) = H_1(2) = H_1(3) = 0$.  Vamos usar $\alpha = 0.1$. No tempo $t=1$, a probabilidade de selecionar qualquer a√ß√£o √© igual: $\pi_1(1) = \pi_1(2) = \pi_1(3) = \frac{e^0}{e^0+e^0+e^0} = \frac{1}{3} \approx 0.33$. Suponha que escolhemos a a√ß√£o $A_1 = 2$ e recebemos uma recompensa $R_1 = 1$. A recompensa m√©dia inicial √© $\bar{R}_1 = 0$.
>
> Atualizando as prefer√™ncias:
>
> $\text{Passo 1: Atualizando } H_2(2)$:
>  $H_2(2) = H_1(2) + \alpha(R_1 - \bar{R}_1)(1 - \pi_1(2)) = 0 + 0.1(1-0)(1-1/3) = 0.1 * 1 * (2/3) \approx 0.067$
>
> $\text{Passo 2: Atualizando } H_2(1)$:
>  $H_2(1) = H_1(1) - \alpha(R_1 - \bar{R}_1)\pi_1(1) = 0 - 0.1(1-0)(1/3) \approx -0.033$
>
> $\text{Passo 3: Atualizando } H_2(3)$:
>  $H_2(3) = H_1(3) - \alpha(R_1 - \bar{R}_1)\pi_1(3) = 0 - 0.1(1-0)(1/3) \approx -0.033$
>
> As novas prefer√™ncias s√£o $H_2(1) \approx -0.033$, $H_2(2) \approx 0.067$ e $H_2(3) \approx -0.033$. A a√ß√£o 2 agora tem uma prefer√™ncia ligeiramente maior.
>  
>
> Vamos para o tempo $t=2$.  A probabilidade de selecionar cada a√ß√£o agora √©:
> $\pi_2(1) = \frac{e^{-0.033}}{e^{-0.033} + e^{0.067} + e^{-0.033}} \approx 0.30$
> $\pi_2(2) = \frac{e^{0.067}}{e^{-0.033} + e^{0.067} + e^{-0.033}} \approx 0.39$
> $\pi_2(3) = \frac{e^{-0.033}}{e^{-0.033} + e^{0.067} + e^{-0.033}} \approx 0.30$
>
>  Observe que a probabilidade de selecionar a a√ß√£o 2 aumentou em rela√ß√£o √†s outras a√ß√µes.

O texto afirma que a omiss√£o da linha de base, isto √©, configurar $\bar{R}_t = 0$ no algoritmo de bandit gradiente, levaria a uma **degrada√ß√£o significativa no desempenho** [1].  A raz√£o para essa degrada√ß√£o reside na capacidade da linha de base de adaptar-se ao n√≠vel m√©dio das recompensas, permitindo que o algoritmo distinga as a√ß√µes com base em recompensas relativas, n√£o absolutas. Este mecanismo √© essencial especialmente quando as recompensas verdadeiras das a√ß√µes s√£o positivas ou negativas, mas n√£o centradas em torno de zero.

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, vamos analisar o que aconteceria se us√°ssemos $\bar{R}_t = 0$.  No tempo $t=1$, ainda escolhemos a a√ß√£o $A_1 = 2$ e recebemos uma recompensa $R_1 = 1$.
>
> $\text{Passo 1: Atualizando } H_2(2)$:
>  $H_2(2) = H_1(2) + \alpha(R_1 - 0)(1 - \pi_1(2)) = 0 + 0.1(1)(1-1/3) = 0.1 * (2/3) \approx 0.067$
>
> $\text{Passo 2: Atualizando } H_2(1)$:
>  $H_2(1) = H_1(1) - \alpha(R_1 - 0)\pi_1(1) = 0 - 0.1(1)(1/3) \approx -0.033$
>
> $\text{Passo 3: Atualizando } H_2(3)$:
>  $H_2(3) = H_1(3) - \alpha(R_1 - 0)\pi_1(3) = 0 - 0.1(1)(1/3) \approx -0.033$
>
>  Neste caso, as prefer√™ncias s√£o as mesmas tanto para o caso com linha de base quanto sem linha de base no tempo $t=1$, **mas a interpreta√ß√£o √© muito diferente**. Sem linha de base o algoritmo est√° apenas considerando os valores absolutos das recompensas, e n√£o os valores relativos a m√©dia das recompensas. Se todas as recompensas forem positivas o algoritmo sem linha de base ir√° sempre aumentar a probabilidade de sele√ß√£o da a√ß√£o selecionada. Isso faz com que seja muito mais lento para o algoritmo convergir quando h√° a√ß√µes com recompensas menores. Imagine o cen√°rio em que uma a√ß√£o tem recompensa 1 e todas as outras tem recompensa 0. O algoritmo com linha de base tender√° a convergir mais r√°pido do que o sem linha de base, pois o algoritmo com linha de base ir√° diminuir a probabilidade de sele√ß√£o das a√ß√µes que tem recompensa zero.

**Observa√ß√£o 1:** A escolha da linha de base $\bar{R}_t$ como a m√©dia das recompensas at√© o tempo $t$ n√£o √© a √∫nica op√ß√£o. Outras linhas de base podem ser consideradas, como uma m√©dia m√≥vel das recompensas ou uma constante pr√©-definida, dependendo da aplica√ß√£o espec√≠fica. A escolha da linha de base pode afetar a velocidade de converg√™ncia e a estabilidade do algoritmo.

O texto tamb√©m deriva o algoritmo de bandit gradiente como uma instancia√ß√£o do m√©todo do gradiente estoc√°stico (2.13) e demonstra que a atualiza√ß√£o de preferencia por a√ß√£o est√° relacionada √† derivada parcial do desempenho com respeito √† prefer√™ncia por a√ß√£o [1].
```mermaid
graph LR
    A["Derivada parcial do desempenho com respeito √† prefer√™ncia de a√ß√£o"] --> B["Express√£o geral:  "$\frac{\partial E[R_t]}{\partial H_t(a)}$""];
    B --> C["Substitui√ß√£o da linha de base " $B_t$ " por " $\bar{R}_t$ ""];
    C --> D["Uso da esperan√ßa condicional e derivada da soft-max"];
    D --> E["Atualiza√ß√£o das prefer√™ncias de a√ß√£o"];
    E --> F["Algoritmo de bandit gradiente obtido via ascens√£o de gradiente estoc√°stico"];
   style A fill:#f9f,stroke:#333,stroke-width:2px
   style F fill:#ccf,stroke:#333,stroke-width:2px
```

**Lema 1:** A atualiza√ß√£o de prefer√™ncias de a√ß√£o no algoritmo bandit gradiente, especificamente,
$$
H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t) (1_{A_t=a}-\pi_t(a))
$$
√© consistente com o m√©todo de ascens√£o de gradiente estoc√°stico.
*Demonstra√ß√£o:* A derivada parcial do desempenho com respeito √† prefer√™ncia de a√ß√£o √© dada por:
$$
\frac{\partial E[R_t]}{\partial H_t(a)} = \sum_{x} (q_*(x)-B_t) \frac{\partial \pi_t(x)}{\partial H_t(a)}
$$

onde $B_t$ √© uma linha de base. O uso da linha de base √© permitido pois a soma do gradiente sobre todas as a√ß√µes √© zero [1]. Substituindo $B_t$ por $R_t$ e usando $E[R_t|A_t] = q_*(A_t)$ temos:

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E[(R_t-\bar{R_t}) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} / \pi_t(A_t) ]
$$
E atrav√©s do uso da regra do quociente e da derivada da distribui√ß√£o soft-max [1], se obt√©m:
$$
\frac{\partial \pi_t(A_t)}{\partial H_t(a)} = \pi_t(A_t)(1_{A_t=a}-\pi_t(a))
$$

Substituindo isso na equa√ß√£o anterior e atrav√©s da atualiza√ß√£o por amostra, obt√©m-se:

$$
H_{t+1}(a) = H_t(a) + \alpha(R_t-\bar{R_t})(1_{A_t=a}-\pi_t(a))
$$
$\blacksquare$
```mermaid
graph LR
    A["Derivada parcial: "$\frac{\partial E[R_t]}{\partial H_t(a)}$""] --> B["Substitui√ß√£o: "$\frac{\partial \pi_t(A_t)}{\partial H_t(a)}$""];
    B --> C["Regra do quociente e derivada soft-max: "$\pi_t(A_t)(1_{A_t=a}-\pi_t(a))$""];
    C --> D["Substitui√ß√£o na express√£o da derivada"];
    D --> E["Atualiza√ß√£o por amostra"];
    style A fill:#f0f,stroke:#333,stroke-width:2px
     style E fill:#aaf,stroke:#333,stroke-width:2px
```

**Lema 1.1:** A derivada parcial do desempenho com respeito √† prefer√™ncia de a√ß√£o pode ser expressa de forma alternativa, utilizando a esperan√ßa condicionada:
$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E_{A_t} \left[ (R_t-\bar{R_t}) (1_{A_t=a} - \pi_t(a)) \right]
$$
*Demonstra√ß√£o:*
Come√ßamos com a express√£o original da derivada parcial:
$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E[(R_t-\bar{R_t}) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} / \pi_t(A_t) ]
$$
Substituindo a express√£o da derivada da soft-max:
$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E[(R_t-\bar{R_t}) \frac{\pi_t(A_t)(1_{A_t=a}-\pi_t(a))}{\pi_t(A_t)} ]
$$
Simplificando:
$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E[(R_t-\bar{R_t}) (1_{A_t=a}-\pi_t(a)) ]
$$
Esta esperan√ßa pode ser interpretada como a esperan√ßa sobre a distribui√ß√£o de $A_t$. Portanto, podemos escrever:
$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E_{A_t} \left[ (R_t-\bar{R_t}) (1_{A_t=a} - \pi_t(a)) \right]
$$
$\blacksquare$

**Teorema 1:** Se $\bar{R}_t = 0$ para todo $t$, ent√£o a atualiza√ß√£o das prefer√™ncias de a√ß√£o n√£o corresponde ao gradiente de desempenho e, portanto, n√£o garante a converg√™ncia para um √≥timo local ou global.

*Demonstra√ß√£o:*
Do lema 1, sabemos que a atualiza√ß√£o das prefer√™ncias de a√ß√£o √© dada por:
$$
H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t) (1_{A_t=a}-\pi_t(a))
$$
E a derivada do desempenho com respeito a preferencia √©:
$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E_{A_t} \left[ (R_t-\bar{R_t}) (1_{A_t=a} - \pi_t(a)) \right]
$$
Se $\bar{R}_t = 0$ ent√£o:
$$
H_{t+1}(a) = H_t(a) + \alpha R_t (1_{A_t=a}-\pi_t(a))
$$
E
$$
\frac{\partial E[R_t]}{\partial H_t(a)} = E_{A_t} \left[ R_t(1_{A_t=a} - \pi_t(a)) \right]
$$
Note que, a atualiza√ß√£o de prefer√™ncias com $\bar{R}_t = 0$ ainda se assemelha ao gradiente, mas o valor $R_t$ sem a subtra√ß√£o da linha de base implica que a dire√ß√£o de atualiza√ß√£o n√£o √© mais baseada em recompensas relativas, mas sim valores absolutos. Consequentemente, a dire√ß√£o da atualiza√ß√£o n√£o segue a dire√ß√£o do verdadeiro gradiente de desempenho, o que impede a converg√™ncia para o ponto √≥timo. $\blacksquare$

### Conclus√£o
A inclus√£o da linha de base em algoritmos de bandit gradiente √© crucial para um aprendizado eficaz, especialmente quando as recompensas n√£o s√£o centradas em torno de zero. A linha de base permite que o algoritmo se adapte ao n√≠vel m√©dio das recompensas e aprenda com base nas recompensas relativas. Omitir a linha de base pode levar a um desempenho significativamente degradado, como demonstrado no experimento descrito no contexto [1]. A formula√ß√£o do algoritmo de bandit gradiente como uma instancia√ß√£o da ascens√£o de gradiente estoc√°stico assegura a sua estabilidade e converg√™ncia, destacando a import√¢ncia da linha de base no processo de otimiza√ß√£o. A adaptabilidade do algoritmo √†s recompensas m√©dias √© um fator cr√≠tico para sua efici√™ncia em uma variedade de cen√°rios de aprendizado por refor√ßo. Al√©m disso, a escolha apropriada da linha de base pode influenciar a converg√™ncia e desempenho do algoritmo.
```mermaid
graph LR
    A["Algoritmo de Bandit Gradiente com Linha de Base"] --"Recompensas relativas"--> B["Aprendizado Eficaz"];
    B --"Converg√™ncia"--> C["Desempenho Superior"];
    D["Algoritmo de Bandit Gradiente sem Linha de Base"] --"Recompensas absolutas"--> E["Aprendizado Ineficaz"];
    E --"Diverg√™ncia"--> F["Desempenho Degradado"];
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#afa,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
    style F fill:#faa,stroke:#333,stroke-width:2px
```

### Refer√™ncias
[^1]: "So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. In this section we consider learning a numerical preference for each action a, which we denote H≈Ç(a) ‚àà R. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:
Pr{At=a} = 
eHt(a)
Œ£=1eHt(b)
k
= œÄœÑ(Œ±),
where here we have also introduced a useful new notation, œÄŒπ(Œ±), for the probability of taking action a at time t. Initially all action preferences are the same (e.g., H‚ÇÅ(a) = 0, for all a) so that all actions have an equal probability of being selected.
[...] There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action A≈• and receiving the reward Rt, the action preferences are updated by:
Ht+1(At) = Ht(At) + a(Rt ‚Äì Rt) (1 ‚Äì œÄœÑ(At)),
Ht+1(a) = H+(a) ‚Äì Œ±(Rt ‚Äì Rt)œÄŒπ(Œ±),
and
for all a ‚â† At,
where a > 0 is a step-size parameter, and Rt ‚àà R is the average of the rewards up to but not including time t (with R‚ÇÅ = R‚ÇÅ), which can be computed incrementally as described in Section 2.4 (or Section 2.5 if the problem is nonstationary).1 The Rt term serves as a baseline with which the reward is compared. If the reward is higher than the baseline, then the probability of taking At in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite direction.
[...] if the baseline were omitted (that is, if Rt was taken to be constant zero in (2.12)), then performance would be significantly degraded, as shown in the figure." *(Trecho de <Chapter 2>)*
