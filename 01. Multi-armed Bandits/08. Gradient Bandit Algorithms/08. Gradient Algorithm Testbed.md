### Gradient Bandit Algorithms com Recompensas Deslocadas

### Introdu√ß√£o
Este cap√≠tulo explora o conceito de **Multi-armed Bandits** (MAB) e m√©todos para equilibrar explora√ß√£o e explota√ß√£o. Uma dessas abordagens √© o **Gradient Bandit Algorithm**, que n√£o estima valores de a√ß√£o diretamente, mas sim prefer√™ncias de a√ß√£o. Essas prefer√™ncias s√£o usadas para determinar probabilidades de a√ß√£o atrav√©s de uma distribui√ß√£o softmax [^1]. Este m√©todo ser√° o foco principal desta se√ß√£o, especialmente quando aplicado a uma variante do testbed de 10 bra√ßos.

### Conceitos Fundamentais
O **Gradient Bandit Algorithm** aprende prefer√™ncias num√©ricas $H_t(a) \in \mathbb{R}$ para cada a√ß√£o $a$. A probabilidade de selecionar uma a√ß√£o √© determinada pela distribui√ß√£o **soft-max**, tamb√©m conhecida como distribui√ß√£o de Gibbs ou Boltzmann [^1]:

$$
Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} = \pi_t(a) \quad (2.11)
$$

onde $k$ √© o n√∫mero total de a√ß√µes, e $\pi_t(a)$ representa a probabilidade de selecionar a a√ß√£o $a$ no tempo $t$. Inicialmente, as prefer√™ncias de todas as a√ß√µes s√£o iguais, por exemplo, $H_1(a) = 0$ para todo $a$, garantindo que todas as a√ß√µes tenham a mesma probabilidade de serem selecionadas [^1].

> üí° **Exemplo Num√©rico:** Vamos considerar um cen√°rio com 3 a√ß√µes (k=3). Inicialmente, $H_1(a_1) = H_1(a_2) = H_1(a_3) = 0$. Usando a equa√ß√£o (2.11), a probabilidade de selecionar cada a√ß√£o no tempo t=1 √©:
>
> $\pi_1(a_1) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
> $\pi_1(a_2) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
> $\pi_1(a_3) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
>
> Todas as a√ß√µes t√™m igual probabilidade de serem selecionadas no in√≠cio.

```mermaid
flowchart LR
    A[/"A√ß√µes (a)"/] --> B("H_1(a) = 0 para todas as a√ß√µes")
    B --> C{/"Calcular Probabilidade pi_1(a)"/};
     C --> D("pi_1(a) = 1/k")
    D --> E("Todas a√ß√µes equiprov√°veis");
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

#### Atualiza√ß√£o das Prefer√™ncias
As prefer√™ncias de a√ß√£o s√£o atualizadas usando um m√©todo de **stochastic gradient ascent**. Ap√≥s selecionar a a√ß√£o $A_t$ e receber a recompensa $R_t$, as prefer√™ncias s√£o atualizadas da seguinte forma [^1]:

$$
H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t)) \quad \text{e} \\
H_{t+1}(a) = H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a) \quad \text{para todo } a \ne A_t \quad (2.12)
$$

Aqui, $\alpha > 0$ √© o **step-size parameter**, que controla o tamanho do ajuste nas prefer√™ncias, e $\bar{R}_t$ √© a m√©dia das recompensas at√© o tempo $t$, excluindo a recompensa do tempo $t$. $\bar{R}_t$ serve como uma **baseline** com a qual a recompensa $R_t$ √© comparada. Se a recompensa √© maior que a baseline, a probabilidade de escolher $A_t$ no futuro √© aumentada. Se a recompensa √© menor que a baseline, a probabilidade √© diminu√≠da [^1].

> üí° **Exemplo Num√©rico:** Suponha que temos 3 a√ß√µes, com prefer√™ncias iniciais $H_1(a_1) = 0.1$, $H_1(a_2) = 0.2$, e $H_1(a_3) = -0.1$.  O par√¢metro de step-size $\alpha = 0.1$.
>
> 1.  **C√°lculo das probabilidades iniciais:**
>
> $\pi_1(a_1) = \frac{e^{0.1}}{e^{0.1} + e^{0.2} + e^{-0.1}} = \frac{1.105}{1.105 + 1.221 + 0.905} \approx 0.35$
> $\pi_1(a_2) = \frac{e^{0.2}}{e^{0.1} + e^{0.2} + e^{-0.1}} = \frac{1.221}{1.105 + 1.221 + 0.905} \approx 0.39$
> $\pi_1(a_3) = \frac{e^{-0.1}}{e^{0.1} + e^{0.2} + e^{-0.1}} = \frac{0.905}{1.105 + 1.221 + 0.905} \approx 0.26$
>
> 2.  **Sele√ß√£o da a√ß√£o:** Suponha que a a√ß√£o selecionada no tempo t=1 foi $a_2$ e a recompensa obtida foi $R_1 = 1$, e a baseline anterior $\bar{R}_1 = 0.5$.
>
> 3.  **Atualiza√ß√£o das prefer√™ncias:**
>
> $H_2(a_2) = 0.2 + 0.1(1-0.5)(1-0.39) \approx 0.2 + 0.1*0.5*0.61 = 0.2305$
>
> $H_2(a_1) = 0.1 - 0.1(1-0.5)(0.35) \approx 0.1 - 0.1*0.5*0.35 =  0.0825$
>
> $H_2(a_3) = -0.1 - 0.1(1-0.5)(0.26) \approx -0.1 - 0.1*0.5*0.26= -0.113$
>
> Observe que a prefer√™ncia da a√ß√£o $a_2$ aumentou, pois a recompensa foi maior que a baseline, enquanto as prefer√™ncias de $a_1$ e $a_3$ diminu√≠ram.

```mermaid
sequenceDiagram
    participant Agente
    participant Ambiente
    Agente ->> Ambiente: Seleciona a√ß√£o "A_t"
    Ambiente -->> Agente: Recompensa "R_t"
    Agente ->> Agente: Calcula "baseline R_t_barra"
    Agente ->> Agente: Atualiza "H_t+1(A_t)" usando "alpha", "R_t", "R_t_barra" e "pi_t(A_t)"
    Agente ->> Agente: Atualiza "H_t+1(a)" para "a != A_t"
    loop Continua
       Agente ->> Ambiente: Seleciona a√ß√£o "A_t"
       Ambiente -->> Agente: Recompensa "R_t"
        Agente ->> Agente: Calcula "baseline R_t_barra"
        Agente ->> Agente: Atualiza "H_t+1(A_t)" usando "alpha", "R_t", "R_t_barra" e "pi_t(A_t)"
        Agente ->> Agente: Atualiza "H_t+1(a)" para "a != A_t"
    end
```

**Observa√ß√£o 1:** √â importante notar que a atualiza√ß√£o das prefer√™ncias, como definida em (2.12), preserva a propriedade de que a soma das probabilidades de todas as a√ß√µes √© sempre igual a 1. Isso ocorre porque o aumento na prefer√™ncia da a√ß√£o selecionada √© compensado pela diminui√ß√£o nas prefer√™ncias das outras a√ß√µes, garantindo que a distribui√ß√£o de probabilidade soft-max permane√ßa v√°lida em todos os momentos. Al√©m disso, esta atualiza√ß√£o garante que as prefer√™ncias ser√£o sempre n√∫meros reais, mesmo ap√≥s repetidas aplica√ß√µes.

#### Lemma 1
Em um cen√°rio de duas a√ß√µes, a distribui√ß√£o soft-max √© equivalente √† fun√ß√£o log√≠stica, tamb√©m conhecida como fun√ß√£o sigm√≥ide, usada em estat√≠stica e redes neurais artificiais [^1].

*Prova:*
Seja $k = 2$ e as duas a√ß√µes $a$ e $b$. Ent√£o,
$$
\pi_t(a) = \frac{e^{H_t(a)}}{e^{H_t(a)} + e^{H_t(b)}}
$$
e
$$
\pi_t(b) = \frac{e^{H_t(b)}}{e^{H_t(a)} + e^{H_t(b)}} = 1 - \pi_t(a)
$$
Se dividirmos o numerador e o denominador de $\pi_t(a)$ por $e^{H_t(b)}$ teremos:
$$
\pi_t(a) = \frac{e^{H_t(a) - H_t(b)}}{e^{H_t(a) - H_t(b)} + 1}
$$
Definindo $z = H_t(a) - H_t(b)$, temos:
$$
\pi_t(a) = \frac{e^z}{e^z + 1} = \frac{1}{1 + e^{-z}}
$$
que √© a forma da fun√ß√£o log√≠stica. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere duas a√ß√µes, a e b, com prefer√™ncias $H_t(a) = 1$ e $H_t(b) = -1$. Ent√£o, $z = H_t(a) - H_t(b) = 1 - (-1) = 2$.
>
> Usando a fun√ß√£o log√≠stica, a probabilidade de selecionar a a√ß√£o 'a' √©:
>
> $\pi_t(a) = \frac{1}{1 + e^{-2}} \approx \frac{1}{1 + 0.135} \approx 0.88$
>
> A probabilidade de selecionar a a√ß√£o 'b' √©:
>
> $\pi_t(b) = 1 - \pi_t(a) \approx 1 - 0.88 = 0.12$
>
> A a√ß√£o 'a' tem uma probabilidade muito maior de ser selecionada. Isso demonstra como a diferen√ßa nas prefer√™ncias se traduz em probabilidades atrav√©s da fun√ß√£o log√≠stica.

```mermaid
flowchart LR
    subgraph "Distribui√ß√£o Softmax (2 a√ß√µes)"
        A["pi_t(a) = e^{H_t(a)} / (e^{H_t(a)} + e^{H_t(b)})"]
        B["pi_t(b) = e^{H_t(b)} / (e^{H_t(a)} + e^{H_t(b)})"]
        A --"Dividir por e^{H_t(b)}"-->C["pi_t(a) = e^{H_t(a)-H_t(b)} / (e^{H_t(a)-H_t(b)} + 1)"]
        C --"Definir z = H_t(a) - H_t(b)"-->D["pi_t(a) = e^z / (e^z + 1)"]
        D -->E["pi_t(a) = 1 / (1 + e^{-z})"]
    end
    E --> F("Fun√ß√£o Log√≠stica");
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#ccf,stroke:#333,stroke-width:2px
      style D fill:#ccf,stroke:#333,stroke-width:2px
       style E fill:#ccf,stroke:#333,stroke-width:2px
```

**Lemma 1.1:** A fun√ß√£o log√≠stica, como derivada no Lemma 1, possui uma propriedade importante: sua derivada √© dada por $\sigma(z)(1-\sigma(z))$, onde $\sigma(z) = \frac{1}{1 + e^{-z}}$. Esta propriedade √© essencial na deriva√ß√£o de muitos algoritmos de aprendizado de m√°quina, incluindo o algoritmo de backpropagation em redes neurais.
*Proof:*
Seja $\sigma(z) = \frac{1}{1+e^{-z}}$. Ent√£o,
$$
\frac{d\sigma(z)}{dz} = \frac{d}{dz} \left( \frac{1}{1+e^{-z}} \right) = \frac{0 - (-e^{-z})}{(1+e^{-z})^2} = \frac{e^{-z}}{(1+e^{-z})^2} = \frac{1}{1+e^{-z}}\frac{e^{-z}}{1+e^{-z}}
$$
Multiplicando e dividindo por $e^z$, obtemos
$$
\frac{d\sigma(z)}{dz} = \frac{1}{1+e^{-z}}\frac{1}{1+e^{z}} = \frac{1}{1+e^{-z}} \left( 1- \frac{1}{1+e^{-z}} \right) = \sigma(z)(1-\sigma(z))
$$
$\blacksquare$

```mermaid
flowchart LR
    A["sigma(z) = 1 / (1 + e^-z)"]
    B["d(sigma(z))/dz = d/dz (1/(1+e^-z))"]
    B --> C["d(sigma(z))/dz = e^-z / (1+e^-z)^2"]
    C --> D["d(sigma(z))/dz = (1/(1+e^-z)) * (e^-z/(1+e^-z))"]
    D --> E["d(sigma(z))/dz = (1/(1+e^-z)) * (1/(1+e^z))"]
     E --> F["d(sigma(z))/dz = (1/(1+e^-z)) * (1 - 1/(1+e^-z))"]
    F --> G["d(sigma(z))/dz = sigma(z) * (1 - sigma(z))"]
        style A fill:#aaf,stroke:#333,stroke-width:2px
        style B fill:#aaf,stroke:#333,stroke-width:2px
        style C fill:#aaf,stroke:#333,stroke-width:2px
        style D fill:#aaf,stroke:#333,stroke-width:2px
        style E fill:#aaf,stroke:#333,stroke-width:2px
        style F fill:#aaf,stroke:#333,stroke-width:2px
        style G fill:#aaf,stroke:#333,stroke-width:2px
```

#### Corol√°rio 1
O ajuste das prefer√™ncias no algoritmo gradiente de bandido, de acordo com a equa√ß√£o (2.12), constitui uma inst√¢ncia de stochastic gradient ascent, onde a dire√ß√£o do ajuste √© um passo na dire√ß√£o do gradiente da recompensa esperada [^1].

*Prova:*
A prova de que o algoritmo de bandido gradiente √© uma inst√¢ncia de stochastic gradient ascent est√° detalhada no texto, utilizando um c√°lculo de derivada parcial, para chegar na mesma equa√ß√£o (2.12).

O gradiente da recompensa esperada √© dada por:
$$ \frac{\partial E[R_t]}{\partial H_t(a)} = \sum_x \left(q_*(x) - \bar{R}_t\right) \frac{\partial \pi_t(x)}{\partial H_t(a)} $$
e foi demonstrado que $\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) \left( 1_{a=x} - \pi_t(a) \right)$, ent√£o:
$$ \frac{\partial E[R_t]}{\partial H_t(a)} =  \mathbb{E} \left[ (R_t - \bar{R}_t) (1_{a=A_t} - \pi_t(a)) \right] $$
Ao substituir essa equa√ß√£o em um m√©todo de stochastic gradient ascent, chegamos nas equa√ß√µes de atualiza√ß√£o (2.12). $\blacksquare$

```mermaid
flowchart LR
subgraph "Stochastic Gradient Ascent"
    A["Gradiente da recompensa esperada: ‚àÇE[R_t]/‚àÇH_t(a)"]
    B["‚àÇE[R_t]/‚àÇH_t(a) = ‚àë_x (q_*(x) - R_t_barra) * ‚àÇœÄ_t(x)/‚àÇH_t(a)"]
    C["‚àÇœÄ_t(x)/‚àÇH_t(a) = œÄ_t(x) * (1_{a=x} - œÄ_t(a))"]
        B --> D["‚àÇE[R_t]/‚àÇH_t(a) = E[(R_t - R_t_barra) * (1_{a=A_t} - œÄ_t(a))]"]
        D --> E["Atualiza√ß√£o H_{t+1}(a) ‚àù ‚àÇE[R_t]/‚àÇH_t(a)"]
        E --> F["Equa√ß√µes de Atualiza√ß√£o (2.12)"]
    end
    style A fill:#afa,stroke:#333,stroke-width:2px
     style B fill:#afa,stroke:#333,stroke-width:2px
      style C fill:#afa,stroke:#333,stroke-width:2px
     style D fill:#afa,stroke:#333,stroke-width:2px
      style E fill:#afa,stroke:#333,stroke-width:2px
      style F fill:#afa,stroke:#333,stroke-width:2px
```

**Corol√°rio 1.1** Uma consequ√™ncia direta do fato de que o algoritmo de gradiente de bandido √© uma forma de stochastic gradient ascent, √© que, sob certas condi√ß√µes (como um step size $\alpha$ adequadamente pequeno e um n√∫mero suficientemente grande de itera√ß√µes), ele deve convergir para um m√°ximo local da recompensa esperada. No entanto, devido √† natureza n√£o convexa do problema, n√£o h√° garantia de que ele convergiria para o m√°ximo global.

> üí° **Exemplo Num√©rico:** Imagine que, ap√≥s v√°rias itera√ß√µes, o algoritmo de bandido gradiente encontrou um conjunto de prefer√™ncias onde a recompensa esperada √© relativamente alta, mas n√£o necessariamente a mais alta poss√≠vel. Suponha que, neste estado, as prefer√™ncias e probabilidades sejam as seguintes:
>
> A√ß√µes: $a_1$, $a_2$, $a_3$
>
> Prefer√™ncias: $H(a_1) = 2.1$, $H(a_2) = 1.8$, $H(a_3) = 0.9$
>
> Probabilidades: $\pi(a_1) \approx 0.50$, $\pi(a_2) \approx 0.30$, $\pi(a_3) \approx 0.20$
>
> Recompensa Esperada Atual: $E[R] = 10$ (em uma escala arbitr√°ria)
>
> Este ponto pode ser um m√°ximo local. Mesmo que exista outro conjunto de prefer√™ncias que leve a uma recompensa esperada de 12, o algoritmo pode n√£o ser capaz de chegar at√© ele com um step size muito pequeno, uma vez que ele vai otimizar a recompensa atual ao inv√©s de explorar um espa√ßo mais amplo de solu√ß√µes. O algoritmo tender√° a melhorar as prefer√™ncias atuais, levando a pequenos ajustes em torno do m√°ximo local, mas n√£o a uma mudan√ßa dr√°stica para o m√°ximo global.

```mermaid
flowchart LR
    A[/"Stochastic Gradient Ascent"/] --> B("Converg√™ncia para m√°ximo local");
    B --> C{"Step size alpha pequeno"};
     C --> D{"N√∫mero de itera√ß√µes grande"};
        D --> E{"Garantia de m√°ximo global?"};
        E --> F("N√£o, natureza n√£o-convexa");
         style A fill:#ffc,stroke:#333,stroke-width:2px
```

### Aplica√ß√£o em um Testbed com Recompensas Deslocadas
Uma variante do **10-armed testbed** √© utilizada para testar o Gradient Bandit Algorithm. Nesta variante, as recompensas esperadas $q_*(a)$ s√£o selecionadas a partir de uma distribui√ß√£o normal com m√©dia +4 em vez de 0, como no testbed original [^1]. Este deslocamento nas recompensas n√£o tem impacto no desempenho do Gradient Bandit Algorithm devido ao uso do termo *reward baseline* ($\bar{R}_t$) [^1].

A baseline adapta-se instantaneamente ao novo n√≠vel de recompensa, permitindo que o algoritmo funcione de forma eficaz independentemente do deslocamento das recompensas. A figura 2.5 [^1] mostra os resultados do algoritmo com e sem a baseline.  O uso da baseline √© crucial para o desempenho do algoritmo em um cen√°rio de recompensas deslocadas; sem ela, o algoritmo tem um desempenho significativamente pior.

> üí° **Exemplo Num√©rico:** Num testbed de 10 bra√ßos com recompensas deslocadas, suponha que as recompensas esperadas de cada bra√ßo s√£o geradas por uma distribui√ß√£o normal com m√©dia +4 e desvio padr√£o 1. O bra√ßo 1 pode ter uma recompensa m√©dia de 4.2, o bra√ßo 2 de 3.8, o bra√ßo 3 de 4.5 e assim por diante. Sem a baseline, o algoritmo tentaria otimizar os valores absolutos das recompensas. No entanto, com a baseline $\bar{R}_t$, o algoritmo otimiza a diferen√ßa entre a recompensa obtida e a m√©dia de recompensas anteriores. Isso significa que, mesmo que as recompensas estejam todas deslocadas para cima, o algoritmo ainda procura as a√ß√µes que proporcionam recompensas acima da m√©dia atual. Por exemplo, se a baseline fosse 4, e o algoritmo escolhesse o bra√ßo 3 com recompensa 4.7, a recompensa de 0.7 acima da baseline seria usada para ajustar as prefer√™ncias, independentemente do deslocamento das recompensas.

```mermaid
flowchart LR
    subgraph "Testbed com Recompensas Deslocadas"
        A["q_*(a) ~ N(4, 1)"];
        B["Com Baseline R_t_barra"] --> C("Algoritmo otimiza R_t - R_t_barra");
    end
        D["Sem Baseline"] --> E("Algoritmo otimiza valores absolutos R_t");
    C --> F("Deslocamento n√£o impacta performance");
    E --> G("Performance degradada");

    style A fill:#faa,stroke:#333,stroke-width:2px
    style B fill:#faa,stroke:#333,stroke-width:2px
      style C fill:#faa,stroke:#333,stroke-width:2px
       style D fill:#faa,stroke:#333,stroke-width:2px
        style E fill:#faa,stroke:#333,stroke-width:2px
        style F fill:#faa,stroke:#333,stroke-width:2px
          style G fill:#faa,stroke:#333,stroke-width:2px
```

**Proposi√ß√£o 1:** O desempenho do Gradient Bandit Algorithm em um cen√°rio de recompensas deslocadas sem a baseline pode ser interpretado como um problema de otimiza√ß√£o onde o sinal do gradiente √© enviesado. A aus√™ncia da baseline faz com que o algoritmo aprenda as recompensas absolutas, em vez das recompensas relativas √† m√©dia. Consequentemente, o sinal do gradiente pode apontar na dire√ß√£o errada, levando a um aprendizado ineficaz.
*Proof:*
Quando a baseline n√£o √© usada, a atualiza√ß√£o das prefer√™ncias se torna:
$$ H_{t+1}(A_t) = H_t(A_t) + \alpha R_t(1 - \pi_t(A_t)) $$
$$ H_{t+1}(a) = H_t(a) - \alpha R_t\pi_t(a) \quad \text{para todo } a \ne A_t $$
Neste caso, o termo $R_t$ n√£o √© comparado com uma m√©dia m√≥vel das recompensas anteriores, mas sim com zero. Em um cen√°rio de recompensas deslocadas onde as recompensas esperadas $q_*(a)$ s√£o sempre positivas (por exemplo, com m√©dia +4), o termo $R_t$ ser√° sempre positivo. Isto leva ao algoritmo a sempre aumentar a probabilidade da a√ß√£o selecionada, sem levar em considera√ß√£o se essa a√ß√£o foi melhor ou pior que a m√©dia. Consequentemente, o algoritmo n√£o consegue convergir para as melhores a√ß√µes, uma vez que o sinal da atualiza√ß√£o estar√° sempre enviesado para cima. $\blacksquare$

> üí° **Exemplo Num√©rico:** Voltando ao exemplo do testbed de 10 bra√ßos com recompensas m√©dias de +4, sem a baseline, o algoritmo sempre recebe uma recompensa positiva. Suponha que, em uma itera√ß√£o, a a√ß√£o $a_1$ √© selecionada e gera uma recompensa $R_t = 4.5$. Sem a baseline, a atualiza√ß√£o de prefer√™ncia de $a_1$ seria:
>
> $H_{t+1}(a_1) = H_t(a_1) + \alpha \cdot 4.5 \cdot (1 - \pi_t(a_1))$
>
> Como $4.5$ √© sempre positivo, a prefer√™ncia de $a_1$ sempre aumenta, independentemente de a recompensa ser melhor ou pior que a m√©dia. Mesmo se outra a√ß√£o tivesse uma recompensa m√©dia de 4.8, o algoritmo sem baseline n√£o ajustaria as prefer√™ncias corretamente para a a√ß√£o melhor, porque todas as recompensas s√£o positivas e levam a incrementos na prefer√™ncia da a√ß√£o escolhida. Isso impede o algoritmo de aprender a diferen√ßa relativa entre as a√ß√µes, levando a um aprendizado ineficaz.
>

```mermaid
flowchart LR
    subgraph "Sem Baseline"
        A["Atualiza√ß√£o de prefer√™ncia: H_{t+1}(A_t) = H_t(A_t) + alpha * R_t * (1-pi_t(A_t))"];
        B["Recompensas R_t sempre positivas (ex: m√©dia +4)"];
        C["Prefer√™ncia da a√ß√£o selecionada SEMPRE aumenta"];
    end
    B-->C;
    C --> D["Gradiente enviesado, aprendizado ineficaz"];
    style A fill:#eef,stroke:#333,stroke-width:2px
      style B fill:#eef,stroke:#333,stroke-width:2px
      style C fill:#eef,stroke:#333,stroke-width:2px
        style D fill:#eef,stroke:#333,stroke-width:2px
```

### Conclus√£o
O Gradient Bandit Algorithm oferece uma abordagem eficaz para resolver problemas de MAB, especialmente quando adaptado com uma reward baseline. A capacidade de usar prefer√™ncias de a√ß√£o, em vez de valores de a√ß√£o, e ajustar essas prefer√™ncias com base no gradiente da recompensa esperada, torna o algoritmo adapt√°vel a diferentes cen√°rios de recompensa. A baseline desempenha um papel vital, adaptando-se a mudan√ßas nos n√≠veis de recompensa e garantindo um bom desempenho, mesmo com recompensas esperadas deslocadas, como demonstrado na variante do testbed de 10 bra√ßos.

### Refer√™ncias
[^1]: "So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. In this section we consider learning a numerical preference for each action a, which we denote H≈Ç(a) ‚àà R. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows: ... where here we have also introduced a useful new notation, œÄœÑ(Œ±), for the probability of taking action a at time t. Initially all action preferences are the same (e.g., H‚ÇÅ(a) = 0, for all a) so that all actions have an equal probability of being selected." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^3]: "There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action A≈• and receiving the reward Rt, the action preferences are updated by: ... where a > 0 is a step-size parameter, and Rt ‚àà R is the average of the rewards up to but not including time t (with R‚ÇÅ = R‚ÇÅ), which can be computed incrementally as described in Section 2.4 (or Section 2.5 if the problem is nonstationary).1 The Rt term serves as a baseline with which the reward is compared. If the reward is higher than the baseline, then the probability of taking At in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite direction." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^4]: "Figure 2.5 shows results with the gradient bandit algorithm on a variant of the 10-armed testbed in which the true expected rewards were selected according to a normal distribution with a mean of +4 instead of zero (and with unit variance as before). This shifting up of all the rewards has absolutely no effect on the gradient bandit algorithm because of the reward baseline term, which instantaneously adapts to the new level. But if the baseline were omitted (that is, if Rt was taken to be constant zero in (2.12)), then performance would be significantly degraded, as shown in the figure." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^5]: "One can gain a deeper insight into the gradient bandit algorithm by understanding it as a stochastic approximation to gradient ascent. In exact gradient ascent, each action preference H≈Ç(a) would be incremented in proportion to the increment's effect on performance: ..." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^6]: "The equation is now in the form of an expectation, summing over all possible values x of the random variable At, then multiplying by the probability of taking those values. Thus: ..." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^7]:  "Recall that our plan has been to write the performance gradient as an expectation of something that we can sample on each step, as we have just done, and then update on each step in proportion to the sample. Substituting a sample of the expectation above for the performance gradient in (2.13) yields: ...  for all a, which you may recognize as being equivalent to our original algorithm (2.12)." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^8]: "Thus it remains only to show that
Œ∏œÄt(x)
Œ∏Ht(a)
= œÄt(x)(1a=x ‚Äì œÄt(a)), as we assumed." *(Trecho de Chapter 2: Multi-armed Bandits)*
