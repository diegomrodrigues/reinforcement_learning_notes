## Gradient Bandit Algorithms e a M√©dia das Recompensas como Baseline

### Introdu√ß√£o
Este cap√≠tulo explora os **Gradient Bandit Algorithms**, uma abordagem alternativa para o aprendizado por refor√ßo, focando em como eles utilizam um conceito fundamental: a m√©dia das recompensas como *baseline*. Diferentemente dos m√©todos que estimam valores de a√ß√£o, os **Gradient Bandit Algorithms** aprendem prefer√™ncias num√©ricas para cada a√ß√£o, onde a probabilidade de selecionar uma a√ß√£o √© proporcional √† sua prefer√™ncia. O conceito de *baseline* desempenha um papel crucial nessa abordagem, permitindo que o algoritmo se adapte e melhore seu desempenho ao longo do tempo [^1].

### Conceitos Fundamentais
#### Prefer√™ncias de A√ß√£o e Soft-Max
Em vez de estimar os valores de a√ß√£o, $q_*(a)$, os **Gradient Bandit Algorithms** mant√™m uma prefer√™ncia num√©rica para cada a√ß√£o $a$, denotada por $H_t(a)$. Estas prefer√™ncias determinam as probabilidades de selecionar as a√ß√µes atrav√©s de uma distribui√ß√£o **soft-max**, tamb√©m conhecida como distribui√ß√£o de *Gibbs* ou de *Boltzmann*. A probabilidade de selecionar uma a√ß√£o $a$ no tempo $t$, denotada por $\pi_t(a)$, √© dada por:

$$
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}}  \qquad (2.11)
$$
onde $k$ √© o n√∫mero total de a√ß√µes [^1]. Inicialmente, todas as prefer√™ncias s√£o iguais ($H_1(a) = 0$), de modo que todas as a√ß√µes t√™m probabilidades iguais de serem selecionadas.

> üí° **Exemplo Num√©rico:** Suponha que temos tr√™s a√ß√µes (k=3) e no tempo t=1, todas as prefer√™ncias s√£o H_1(a) = 0 para a=1,2,3. Ent√£o, a probabilidade de cada a√ß√£o √©:
>
> $\pi_1(1) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
> $\pi_1(2) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
> $\pi_1(3) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3} \approx 0.333$
>
>  Inicialmente, todas as a√ß√µes t√™m a mesma probabilidade de serem escolhidas.

#### Atualiza√ß√£o das Prefer√™ncias
Ap√≥s selecionar uma a√ß√£o $A_t$ e receber uma recompensa $R_t$, as prefer√™ncias das a√ß√µes s√£o atualizadas com base em um processo de **stochastic gradient ascent**. A regra de atualiza√ß√£o √© definida por:

```mermaid
graph LR
    A[/"A√ß√£o A_t Selecionada"/] -->|/"Recompensa R_t"/| B("Calcular 'delta' = R_t - RÃÑ_t");
    B --> C{/"A_t == a ?"/};
    C -- Sim --> D[/"H_{t+1}(A_t) = H_t(A_t) + Œ± * delta * (1 - œÄ_t(A_t))"/];
    C -- N√£o --> E[/"H_{t+1}(a) = H_t(a) - Œ± * delta * œÄ_t(a)"/];
    D --> F(/"Atualizar H_t(a)"/);
     E --> F;
    F --> G[/"Pr√≥ximo passo"/];
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
        style E fill:#ccf,stroke:#333,stroke-width:2px
     style F fill:#ccf,stroke:#333,stroke-width:2px

```

$$
H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - \overline{R}_t) (1 - \pi_t(A_t))
$$
e
$$
H_{t+1}(a) = H_t(a) - \alpha (R_t - \overline{R}_t) \pi_t(a), \text{ para todo } a \neq A_t \qquad (2.12)
$$
onde $\alpha > 0$ √© o par√¢metro *step-size* que controla o tamanho da atualiza√ß√£o, e $\overline{R}_t$ √© a m√©dia das recompensas recebidas at√© o momento, que atua como *baseline* [^1].

> üí° **Exemplo Num√©rico:** Considere que temos 3 a√ß√µes e no tempo t=1, temos as prefer√™ncias H_1(1) = 0.2, H_1(2) = 0.1, H_1(3) = -0.1. Suponha que a a√ß√£o A_1 = 1 foi selecionada, e recebemos uma recompensa R_1 = 1. Inicialmente $\overline{R}_1 = R_1 = 1$. Usando $\alpha = 0.1$, e calculando as probabilidades com a equa√ß√£o (2.11):
>
> $\pi_1(1) = \frac{e^{0.2}}{e^{0.2} + e^{0.1} + e^{-0.1}} \approx \frac{1.221}{1.221+1.105+0.905} \approx 0.396$
> $\pi_1(2) = \frac{e^{0.1}}{e^{0.2} + e^{0.1} + e^{-0.1}} \approx \frac{1.105}{1.221+1.105+0.905} \approx 0.357$
> $\pi_1(3) = \frac{e^{-0.1}}{e^{0.2} + e^{0.1} + e^{-0.1}} \approx \frac{0.905}{1.221+1.105+0.905} \approx 0.292$
>
>  A atualiza√ß√£o das prefer√™ncias ser√°:
>
> $H_{2}(1) = H_{1}(1) + \alpha (R_1 - \overline{R}_1) (1 - \pi_1(1)) = 0.2 + 0.1 * (1 - 1) * (1 - 0.396) = 0.2$
> $H_{2}(2) = H_{1}(2) - \alpha (R_1 - \overline{R}_1) \pi_1(2) = 0.1 - 0.1 * (1 - 1) * 0.357 = 0.1$
> $H_{2}(3) = H_{1}(3) - \alpha (R_1 - \overline{R}_1) \pi_1(3) = -0.1 - 0.1 * (1 - 1) * 0.292 = -0.1$
>
> Neste caso espec√≠fico, como a recompensa √© igual ao baseline, as prefer√™ncias n√£o se alteram. Contudo, no passo seguinte, caso a recompensa seja diferente do baseline, as prefer√™ncias ser√£o atualizadas de acordo.
>

**Proposi√ß√£o 1.** *A atualiza√ß√£o das prefer√™ncias pode ser expressa de forma unificada, utilizando o conceito do delta de Kronecker* $\delta_{A_t, a}$, que √© igual a 1 se $A_t = a$ e 0 caso contr√°rio. A atualiza√ß√£o pode ser reescrita como:
$$
H_{t+1}(a) = H_t(a) + \alpha (R_t - \overline{R}_t) (\delta_{A_t, a} - \pi_t(a)) \qquad (2.13)
$$

*Demonstra√ß√£o:*
Quando $a = A_t$, o delta de Kronecker $\delta_{A_t, a}$ √© igual a 1, e a atualiza√ß√£o torna-se
$H_{t+1}(A_t) = H_t(A_t) + \alpha (R_t - \overline{R}_t) (1 - \pi_t(A_t))$, que √© a primeira parte da equa√ß√£o (2.12). Quando $a \neq A_t$,  $\delta_{A_t, a} = 0$, e a atualiza√ß√£o torna-se $H_{t+1}(a) = H_t(a) - \alpha (R_t - \overline{R}_t) \pi_t(a)$, que corresponde √† segunda parte da equa√ß√£o (2.12). Portanto, a equa√ß√£o (2.13) √© uma representa√ß√£o equivalente das equa√ß√µes (2.12).

#### A Import√¢ncia do Baseline
O *baseline* $\overline{R}_t$ √© um componente essencial da atualiza√ß√£o das prefer√™ncias no **Gradient Bandit Algorithm**. Ele fornece um ponto de refer√™ncia para a recompensa atual, $R_t$. Se a recompensa obtida √© maior que o *baseline* ($R_t > \overline{R}_t$), a prefer√™ncia da a√ß√£o selecionada $A_t$ √© aumentada, incentivando sua sele√ß√£o futura. Se a recompensa for menor que o *baseline* ($R_t < \overline{R}_t$), a prefer√™ncia da a√ß√£o selecionada √© diminu√≠da, desincentivando sua sele√ß√£o futura. A l√≥gica de usar o *baseline* faz com que as a√ß√µes com recompensas acima da m√©dia sejam mais prov√°veis de serem selecionadas e as recompensas abaixo da m√©dia menos prov√°veis de serem selecionadas. As a√ß√µes que n√£o foram selecionadas se movem na dire√ß√£o oposta.

> üí° **Exemplo Num√©rico:** Vamos continuar com o exemplo anterior. No passo t=2, suponha que a a√ß√£o A_2 = 2 foi selecionada e a recompensa R_2 = 0.5. O baseline ser√° atualizado (como veremos abaixo), e por enquanto vamos assumir que $\overline{R}_2=0.75$.  As prefer√™ncias s√£o atualizadas (usando $\alpha=0.1$) como:
>
> $H_{3}(1) = H_{2}(1) + \alpha (R_2 - \overline{R}_2) (\delta_{2,1} - \pi_2(1)) = 0.2 + 0.1 * (0.5 - 0.75) * (0 - \pi_2(1))$. Calculando $\pi_2(a)$:
>
>  Primeiro, com as prefer√™ncias H_2(1)=0.2, H_2(2)=0.1, H_2(3)=-0.1:
>
> $\pi_2(1) = \frac{e^{0.2}}{e^{0.2} + e^{0.1} + e^{-0.1}} \approx 0.396$
> $\pi_2(2) = \frac{e^{0.1}}{e^{0.2} + e^{0.1} + e^{-0.1}} \approx 0.357$
> $\pi_2(3) = \frac{e^{-0.1}}{e^{0.2} + e^{0.1} + e^{-0.1}} \approx 0.292$
>
>  Agora podemos concluir o c√°lculo:
>
> $H_{3}(1) = 0.2 + 0.1 * (-0.25) * (-0.396) \approx 0.2 + 0.0099 = 0.2099$
> $H_{3}(2) = 0.1 + 0.1 * (-0.25) * (1 - 0.357) \approx 0.1 - 0.016 = 0.084$
> $H_{3}(3) = -0.1 + 0.1 * (-0.25) * (0- 0.292) \approx -0.1 + 0.0073 = -0.0927$
>
> Como a recompensa da a√ß√£o 2 foi menor que o baseline, a prefer√™ncia da a√ß√£o 2 diminuiu, e as prefer√™ncias das outras a√ß√µes aumentaram, proporcionalmente √† probabilidade de escolha de cada uma.

#### C√°lculo Incremental do Baseline
O c√°lculo de $\overline{R}_t$  √© realizado de forma incremental, semelhante ao m√©todo usado para as estimativas de valores de a√ß√£o. Embora o texto n√£o forne√ßa uma f√≥rmula espec√≠fica para o c√°lculo incremental do baseline no contexto do Gradiente Bandido, o texto indica que o c√°lculo incremental de m√©dia de recompensas pode ser usado [^1]. Portanto, √© poss√≠vel usar uma f√≥rmula semelhante √† equa√ß√£o (2.3), com um passo adaptado para a m√©dia de recompensas:

```mermaid
graph LR
  A[/"Recompensa R_t recebida"/] --> B("Baseline anterior RÃÑ_t");
  B --> C[/"Calcular RÃÑ_{t+1} = RÃÑ_t + (1/t) * (R_t - RÃÑ_t)"/];
  C --> D[/"Atualizar RÃÑ_t"/];
   D --> E[/"Pr√≥ximo passo"/];
    style C fill:#ccf,stroke:#333,stroke-width:2px
     style D fill:#ccf,stroke:#333,stroke-width:2px

```

$$
\overline{R}_{t+1} = \overline{R}_t + \frac{1}{t} (R_t - \overline{R}_t)
$$
onde, inicialmente, $\overline{R}_1=R_1$.
O uso de um c√°lculo incremental garante que a complexidade computacional por passo seja constante, o que √© eficiente para problemas com um grande n√∫mero de passos [^1]. Alternativamente, pode-se utilizar tamb√©m uma m√©dia ponderada exponencial para o baseline, similar √† utilizada para o c√°lculo dos valores de a√ß√£o em problemas n√£o estacion√°rios, o que tamb√©m resultaria em um baseline com constante complexidade de tempo.

> üí° **Exemplo Num√©rico:**  Usando as recompensas dos exemplos anteriores, temos $R_1 = 1$ e $R_2 = 0.5$. Inicialmente, $\overline{R}_1 = R_1 = 1$. Ent√£o:
>
> $\overline{R}_2 = \overline{R}_1 + \frac{1}{2} (R_2 - \overline{R}_1) = 1 + \frac{1}{2} (0.5 - 1) = 1 - 0.25 = 0.75$.
>
>  Este √© o valor de baseline usado no exemplo anterior.
>
>  Agora, suponha que no passo t=3 a recompensa seja R_3= 1.2. Ent√£o:
>  $\overline{R}_3 = \overline{R}_2 + \frac{1}{3} (R_3 - \overline{R}_2) = 0.75 + \frac{1}{3} (1.2 - 0.75) = 0.75 + \frac{0.45}{3} = 0.75 + 0.15 = 0.9$
>
> O baseline se ajusta incrementalmente √†s recompensas recebidas.

**Lema 1.** *A m√©dia incremental de recompensas $\overline{R}_{t+1}$ pode ser expressa de forma recursiva utilizando um fator de decaimento $\beta_t = \frac{1}{t}$, como:*

```mermaid
graph LR
    A[/"Baseline anterior RÃÑ_t"/] --> B("Recompensa R_t");
    A --> C[/"Calcular (1 - Œ≤_t) * RÃÑ_t"/];
    B --> D[/"Calcular Œ≤_t * R_t"/];
    C --> E[/"Soma"/]
    D --> E
    E --> F[/"RÃÑ_{t+1} = (1 - Œ≤_t) * RÃÑ_t + Œ≤_t * R_t"/];
        style F fill:#ccf,stroke:#333,stroke-width:2px
        style C fill:#ccf,stroke:#333,stroke-width:2px
          style D fill:#ccf,stroke:#333,stroke-width:2px

```

$$
\overline{R}_{t+1} = (1-\beta_t)\overline{R}_t + \beta_t R_t
$$

*Demonstra√ß√£o:*
Reorganizando a f√≥rmula incremental $\overline{R}_{t+1} = \overline{R}_t + \frac{1}{t} (R_t - \overline{R}_t)$, temos:

$\overline{R}_{t+1} = \overline{R}_t + \frac{1}{t}R_t - \frac{1}{t}\overline{R}_t$

$\overline{R}_{t+1} = \overline{R}_t(1 - \frac{1}{t}) + \frac{1}{t}R_t $

Fazendo $\beta_t = \frac{1}{t}$, obtemos a forma recursiva:

$\overline{R}_{t+1} = (1-\beta_t)\overline{R}_t + \beta_t R_t$

**Observa√ß√£o:** Esta forma recursiva explicita como a m√©dia de recompensas $\overline{R}_{t+1}$ √© uma m√©dia ponderada entre a m√©dia anterior $\overline{R}_t$ e a recompensa atual $R_t$, com o fator de decaimento $\beta_t$ dando mais peso para a recompensa atual no in√≠cio do aprendizado e decaindo com o tempo.

**Teorema 1.** *O baseline $\overline{R}_t$ pode ser generalizado utilizando um fator de decaimento $\beta \in [0, 1]$ para uma m√©dia ponderada exponencial, resultando em:*
```mermaid
graph LR
    A[/"Baseline anterior RÃÑ_t"/] --> B("Recompensa R_t");
    A --> C[/"Calcular (1 - Œ≤) * RÃÑ_t"/];
    B --> D[/"Calcular Œ≤ * R_t"/];
    C --> E[/"Soma"/]
     D --> E
    E --> F[/"RÃÑ_{t+1} = (1 - Œ≤) * RÃÑ_t + Œ≤ * R_t"/];
     style F fill:#ccf,stroke:#333,stroke-width:2px
      style C fill:#ccf,stroke:#333,stroke-width:2px
          style D fill:#ccf,stroke:#333,stroke-width:2px
```

$$
\overline{R}_{t+1} = (1-\beta)\overline{R}_t + \beta R_t
$$
*Este tipo de baseline oferece maior flexibilidade e √© mais apropriado para problemas n√£o estacion√°rios.*

*Demonstra√ß√£o:*
A forma generalizada do baseline corresponde a uma m√©dia ponderada exponencial, onde o fator de decaimento $\beta$ permite que o baseline se adapte mais rapidamente a mudan√ßas na distribui√ß√£o de recompensas, ao dar maior peso √†s recompensas mais recentes. Quando $\beta = \frac{1}{t}$ , temos o c√°lculo incremental cl√°ssico. Para um $\beta$ constante, o baseline passa a ser uma m√©dia exponencialmente ponderada das recompensas, dando mais peso √†s recompensas recentes.

> üí° **Exemplo Num√©rico:**  Usando as recompensas anteriores, $R_1 = 1$, $R_2 = 0.5$, e $R_3 = 1.2$. Vamos usar um $\beta = 0.2$ para uma m√©dia ponderada exponencial. Inicialmente, $\overline{R}_1 = R_1 = 1$:
>
> $\overline{R}_2 = (1-0.2)\overline{R}_1 + 0.2 * R_2 = 0.8 * 1 + 0.2 * 0.5 = 0.8 + 0.1 = 0.9$
> $\overline{R}_3 = (1-0.2)\overline{R}_2 + 0.2 * R_3 = 0.8 * 0.9 + 0.2 * 1.2 = 0.72 + 0.24 = 0.96$
>
> Comparando com o c√°lculo incremental, temos $\overline{R}_2 = 0.75$ e  $\overline{R}_3 = 0.9$ no exemplo anterior.  A m√©dia ponderada exponencial com $\beta=0.2$ reage mais rapidamente √†s mudan√ßas, pois d√° mais peso √†s recompensas recentes. Para $\beta=0.2$, a m√©dia ponderada exponencial atribui 20% de peso √† recompensa atual e 80% ao valor anterior do baseline, resultando numa m√©dia que rapidamente se ajusta a varia√ß√µes nas recompensas. Em contraste, o m√©todo incremental, com $\beta_t = 1/t$, diminui gradualmente a import√¢ncia de novas recompensas no c√°lculo da m√©dia, tornando-o mais est√°vel, mas tamb√©m mais lento para se adaptar a mudan√ßas. Em problemas n√£o estacion√°rios, onde a distribui√ß√£o de recompensas pode mudar ao longo do tempo, a m√©dia ponderada exponencial com $\beta$ constante pode ser mais adequada, pois √© mais responsiva a varia√ß√µes recentes.

#### Deriva√ß√£o da Atualiza√ß√£o das Prefer√™ncias
O texto tamb√©m aborda a deriva√ß√£o da atualiza√ß√£o das prefer√™ncias, mostrando como ela se relaciona com o conceito de **stochastic gradient ascent**. A atualiza√ß√£o das prefer√™ncias,  dada por (2.12), √© uma aproxima√ß√£o da atualiza√ß√£o de gradient descent que utiliza o gradiente da recompensa esperada $E[R_t]$ em rela√ß√£o √† prefer√™ncia da a√ß√£o $H_t(a)$. O gradiente da recompensa esperada √© dado por:

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = \sum_x \pi_t(x) (q_*(x) - \overline{R}_t) \frac{\partial \pi_t(x)}{\partial H_t(a)}
$$

A an√°lise do texto detalha como a atualiza√ß√£o (2.12) se torna uma aproxima√ß√£o do gradient ascent ao substituir o valor da a√ß√£o $q_*(x)$ pela recompensa $R_t$ e ao expressar a derivada de $\pi_t(x)$ em rela√ß√£o a $H_t(a)$ explicitamente. Assim, pode-se concluir que o uso do *baseline* $\overline{R}_t$ n√£o afeta o valor esperado da atualiza√ß√£o, mas reduz a sua vari√¢ncia. A demonstra√ß√£o completa, apesar de complexa, est√° presente no texto [^1].

**Lema 2.** *A derivada da probabilidade $\pi_t(x)$ em rela√ß√£o √† prefer√™ncia $H_t(a)$ pode ser expressa como:*

```mermaid
graph LR
    A[/"Probabilidade œÄ_t(x)"/] --> B{/"x == a ?"/};
    B -- Sim --> C[/"œÄ_t(x) * (1 - œÄ_t(a))"/];
    B -- N√£o --> D[/"- œÄ_t(x) * œÄ_t(a)"/];
    C --> E[/"Resultado"/]
    D --> E
    style C fill:#ccf,stroke:#333,stroke-width:2px
       style D fill:#ccf,stroke:#333,stroke-width:2px
```

$$
\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) (\delta_{x,a} - \pi_t(a))
$$

*Demonstra√ß√£o:*
Utilizando a regra da cadeia e a defini√ß√£o de $\pi_t(x)$ na equa√ß√£o (2.11), temos:

$$
\frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)} \frac{e^{H_t(x)}}{\sum_{b=1}^k e^{H_t(b)}}
$$

Se $x = a$, ent√£o:

$$
\frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{e^{H_t(a)} \sum_{b=1}^k e^{H_t(b)} - e^{H_t(a)}e^{H_t(a)}}{(\sum_{b=1}^k e^{H_t(b)})^2} = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} \left( 1 -  \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} \right) = \pi_t(a)(1-\pi_t(a))
$$

Se $x \neq a$, ent√£o:
$$
\frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{0 - e^{H_t(x)}e^{H_t(a)}}{(\sum_{b=1}^k e^{H_t(b)})^2} = - \frac{e^{H_t(x)}}{\sum_{b=1}^k e^{H_t(b)}} \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} = -\pi_t(x)\pi_t(a)
$$
Combinando os casos, obtemos:
$$
\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) (\delta_{x,a} - \pi_t(a))
$$

**Teorema 2.** *Substituindo a derivada de* $\pi_t(x)$ *na express√£o do gradiente da recompensa esperada e usando a aproxima√ß√£o de substituir* $q_*(x)$ *por* $R_t$, *√© poss√≠vel derivar a atualiza√ß√£o das prefer√™ncias na equa√ß√£o (2.13).*

*Demonstra√ß√£o:*
Substituindo o resultado do Lema 2 na express√£o do gradiente, temos:

$$
\frac{\partial E[R_t]}{\partial H_t(a)} = \sum_x \pi_t(x) (q_*(x) - \overline{R}_t) \pi_t(x) (\delta_{x,a} - \pi_t(a))
$$
Aproximando $q_*(x)$ por $R_t$, temos
$$
\frac{\partial E[R_t]}{\partial H_t(a)} \approx \sum_x \pi_t(x) (R_t - \overline{R}_t) (\delta_{x,a} - \pi_t(a))
$$
Na soma acima, o termo $(R_t - \overline{R}_t)$ √© constante em rela√ß√£o a $x$, e a soma sobre todas as a√ß√µes √© igual a zero quando  $x \neq A_t$, e igual a 1 quando $x = A_t$

$$
\frac{\partial E[R_t]}{\partial H_t(a)} \approx  (R_t - \overline{R}_t) \sum_x \pi_t(x) (\delta_{x,a} - \pi_t(a))
$$
$$
\frac{\partial E[R_t]}{\partial H_t(a)} \approx  (R_t - \overline{R}_t) (\delta_{A_t,a} - \pi_t(a))
$$
Finalmente, aplicando um passo de tamanho $\alpha$, a atualiza√ß√£o das prefer√™ncias fica:

$$
H_{t+1}(a) = H_t(a) + \alpha (R_t - \overline{R}_t) (\delta_{A_t, a} - \pi_t(a))
$$
Que √© a atualiza√ß√£o mostrada na equa√ß√£o (2.13)

### Conclus√£o
O uso de uma m√©dia das recompensas como *baseline* nos **Gradient Bandit Algorithms** √© um conceito fundamental para seu funcionamento. A linha de base fornece um ponto de refer√™ncia para avaliar o qu√£o boa √© uma recompensa, permitindo que o algoritmo se ajuste e aprenda de forma eficaz. O algoritmo ajusta as prefer√™ncias de a√ß√£o, com base na compara√ß√£o entre as recompensas recebidas e o valor m√©dio acumulado das recompensas. Este mecanismo de compara√ß√£o possibilita uma atualiza√ß√£o mais precisa, e consequentemente uma converg√™ncia mais r√°pida do algoritmo. Embora o uso de um *baseline* seja cr√≠tico, a escolha de como calcul√°-lo (seja de forma incremental ou com uma m√©dia ponderada exponencial) n√£o afeta o fato de que o algoritmo √© uma inst√¢ncia de *stochastic gradient ascent*, garantindo que o algoritmo tenha boas propriedades de converg√™ncia. A utiliza√ß√£o da nota√ß√£o delta de Kronecker para descrever a atualiza√ß√£o das prefer√™ncias e a deriva√ß√£o do c√°lculo da derivada da probabilidade em rela√ß√£o √†s prefer√™ncias, demonstram a base matem√°tica da atualiza√ß√£o das prefer√™ncias em dire√ß√£o ao m√°ximo da recompensa esperada.

### Refer√™ncias
[^1]: "So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. In this section we consider learning a numerical preference for each action a, which we denote H≈Ç(a) ‚àà R. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no effect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:
Pr{At=a} =
eHt(a)
Œ£=1eHt(b)
k
= œÄœÑ(Œ±),
where here we have also introduced a useful new notation, œÄŒπ(Œ±), for the probability of taking action a at time t. Initially all action preferences are the same (e.g., H‚ÇÅ(a) = 0, for all a) so that all actions have an equal probability of being selected.
There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. On each step, after selecting action A≈• and receiving the reward Rt, the action preferences are updated by:
Ht+1(At) = Ht(At) + a(Rt ‚Äì Rt) (1 ‚Äì œÄœÑ(At)),
Ht+1(a) = H+(a) ‚Äì Œ±(Rt ‚Äì Rt)œÄŒπ(Œ±),
and
for all a ‚â† At,
where a > 0 is a step-size parameter, and Rt ‚àà R is the average of the rewards up to but not including time t (with R‚ÇÅ = R‚ÇÅ), which can be computed incrementally as described in Section 2.4 (or Section 2.5 if the problem is nonstationary). The Rt term serves as a baseline with which the reward is compared. If the reward is higher than the baseline, then the probability of taking At in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite direction." *(Trecho de /content/reinforcement_learning_notes/01. Multi-armed Bandits)*
