## Sum√°rio dos M√©todos de Explora√ß√£o-Explota√ß√£o em Multi-Armed Bandits

### Introdu√ß√£o

Este cap√≠tulo focou no problema fundamental de **multi-armed bandits** e explorou diversas estrat√©gias para balancear a **explora√ß√£o** (descobrir novas op√ß√µes) e a **explota√ß√£o** (utilizar o conhecimento atual para maximizar a recompensa imediata) [^1]. Dada a natureza *evaluativa* do feedback em reinforcement learning, essa dicotomia torna-se um desafio central. Em contraste com o aprendizado supervisionado, onde o feedback *instrui* sobre a a√ß√£o correta, o reinforcement learning requer que o agente ativamente busque e avalie diferentes a√ß√µes para otimizar o desempenho a longo prazo [^1].

**Proposi√ß√£o 1** (Trade-off Explora√ß√£o-Explota√ß√£o): O problema de balancear explora√ß√£o e explota√ß√£o √© inerente a qualquer sistema de aprendizado que interage com um ambiente desconhecido. A explora√ß√£o excessiva leva √† perda de recompensas imediatas, enquanto a explota√ß√£o excessiva impede a descoberta de estrat√©gias √≥timas a longo prazo.

*Prova (Argumentativa):* A explora√ß√£o, por defini√ß√£o, envolve a experimenta√ß√£o com a√ß√µes sub√≥timas (conforme o conhecimento atual) na esperan√ßa de descobrir a√ß√µes melhores. Cada a√ß√£o explorat√≥ria resulta em uma recompensa possivelmente menor do que a a√ß√£o atualmente considerada √≥tima. Por outro lado, a explota√ß√£o consiste em selecionar consistentemente a a√ß√£o que parece ser a melhor com base no conhecimento atual. Se o conhecimento atual for incompleto ou impreciso, a explota√ß√£o pode levar a um desempenho sub√≥timo a longo prazo, pois o sistema n√£o explora alternativas que poderiam ser superiores.

### M√©todos para Balancear Explora√ß√£o e Explota√ß√£o

V√°rios m√©todos para balancear explora√ß√£o e explota√ß√£o foram apresentados neste cap√≠tulo, cada um com suas pr√≥prias caracter√≠sticas e trade-offs [^42].

1.  **Œµ-Greedy Methods:**
    -   Estes m√©todos empregam uma abordagem simples: a maior parte do tempo, a *a√ß√£o gananciosa* (aquela com a maior estimativa de valor) √© selecionada [^27]. No entanto, com uma pequena probabilidade *Œµ*, uma a√ß√£o √© escolhida aleatoriamente [^27].
    -   A vantagem desses m√©todos √© a garantia de que, no limite, todas as a√ß√µes ser√£o amostradas um n√∫mero infinito de vezes, assegurando a converg√™ncia das estimativas de valor para seus valores verdadeiros $q_*(a)$ [^28].
    -   Entretanto, a probabilidade *Œµ* precisa ser cuidadosamente ajustada. Um *Œµ* muito grande pode levar a uma explora√ß√£o excessiva, enquanto um *Œµ* muito pequeno pode resultar na estagna√ß√£o em a√ß√µes sub√≥timas [^28].
    -   Como vimos, no *10-armed testbed*, o m√©todo *Œµ-greedy* com Œµ=0.1 explorou mais e encontrou a a√ß√£o √≥tima mais r√°pido, mas nunca a selecionou mais de 91% das vezes [^30]. J√° o m√©todo com Œµ=0.01 melhorou mais lentamente, mas superaria o outro a longo prazo [^30].

    > üí° **Exemplo Num√©rico:** Considere um problema de 3-armed bandit com recompensas m√©dias desconhecidas. Inicialmente, as estimativas de valor s√£o $Q(A_1) = 2$, $Q(A_2) = 3$, e $Q(A_3) = 4$. Se usarmos um Œµ-greedy com $\epsilon = 0.2$, em 20% das vezes, selecionaremos uma a√ß√£o aleatoriamente. Se no primeiro passo a a√ß√£o escolhida aleatoriamente for $A_1$ e a recompensa obtida for 1, a estimativa de valor de $A_1$ ser√° atualizada (usando uma m√©dia amostral) para um novo valor, influenciando as futuras decis√µes. Se $\epsilon = 0.01$, a a√ß√£o $A_1$ seria muito menos frequentemente selecionada aleatoriamente, resultando numa converg√™ncia mais lenta para o seu valor verdadeiro.

    **Lema 1.1:** A taxa de converg√™ncia do m√©todo $\epsilon$-greedy depende da distribui√ß√£o das recompensas e do valor de $\epsilon$.

    *Prova (Esbo√ßo):* Seja $\Delta_a = q^* - q(a)$ a diferen√ßa entre o valor √≥timo $q^*$ e o valor da a√ß√£o $a$, onde $q(a)$ √© uma estimativa. Quanto maior a diferen√ßa entre o valor da a√ß√£o sub√≥tima e a a√ß√£o √≥tima, mais tempo levar√° para o algoritmo convergir para a a√ß√£o √≥tima, especialmente para pequenos $\epsilon$, pois a a√ß√£o sub√≥tima precisa ser amostrada o suficiente para que sua estimativa se aproxime o suficiente de seu valor real.

    **Prova:**
    I. Seja $q^*(a)$ o valor verdadeiro da a√ß√£o $a$ e $Q_t(a)$ a estimativa do valor no tempo $t$. Definimos $\Delta_a = q^* - Q_t(a)$ como a diferen√ßa entre o valor √≥timo e a estimativa da a√ß√£o $a$.

    II. Para que a a√ß√£o √≥tima seja escolhida, sua estimativa de valor deve ser maior que a estimativa de valor de qualquer a√ß√£o sub√≥tima. Se $\epsilon$ √© muito pequeno, as a√ß√µes sub√≥timas s√£o exploradas raramente.

    III. Considere uma a√ß√£o sub√≥tima $a'$ tal que $q^*(a') < q^*$. A probabilidade de selecionar $a'$ √© $\frac{\epsilon}{k}$, onde $k$ √© o n√∫mero de bra√ßos.

    IV. Para que a a√ß√£o √≥tima seja consistentemente escolhida, $Q_t(a')$ deve convergir para $q^*(a')$. A taxa na qual $Q_t(a')$ converge depende da magnitude das recompensas e da frequ√™ncia com que a a√ß√£o $a'$ √© selecionada, que √© controlada por $\epsilon$.

    V. Se $\epsilon$ √© pequeno, $a'$ √© selecionada raramente, e a converg√™ncia de $Q_t(a')$ para $q^*(a')$ √© lenta. Se a diferen√ßa $\Delta_{a'}$ √© grande, ent√£o o algoritmo levar√° ainda mais tempo para convergir para a a√ß√£o √≥tima. Portanto, a taxa de converg√™ncia depende de $\epsilon$ e da distribui√ß√£o das recompensas. ‚ñ†



![Average performance of Œµ-greedy action-value methods on a 10-armed testbed, demonstrating the exploration-exploitation trade-off.](./../images/image6.png)

2.  **Upper-Confidence-Bound (UCB) Action Selection:**
    -   Diferentemente dos m√©todos *Œµ-greedy*, o UCB seleciona a√ß√µes de forma *determin√≠stica*, mas incentiva a explora√ß√£o [^42]. A a√ß√£o √© escolhida de acordo com a seguinte f√≥rmula:
        $$
        A_t = \underset{a}{\operatorname{argmax}} \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
        $$
        onde:
        -   $Q_t(a)$ √© a estimativa do valor da a√ß√£o *a* no tempo *t*.
        -   $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada antes do tempo *t*.
        -   $c > 0$ √© um par√¢metro que controla o grau de explora√ß√£o.
    -   O termo $\sqrt{\frac{\ln t}{N_t(a)}}$ adiciona um *limite de confian√ßa superior* √† estimativa do valor, refletindo a incerteza sobre o valor da a√ß√£o [^36]. A√ß√µes que foram menos frequentemente selecionadas t√™m um limite de confian√ßa superior maior, encorajando a explora√ß√£o.
    -   Como $ln(t)$ cresce lentamente, todas as a√ß√µes ser√£o eventualmente selecionadas [^36].
    -   O UCB demonstrou bom desempenho, mas √© mais complexo que o Œµ-greedy, dificultando sua extens√£o para cen√°rios mais gerais [^36].

    > üí° **Exemplo Num√©rico:** Suponha que temos dois bra√ßos ($k=2$). Ap√≥s 10 rodadas ($t=10$), o bra√ßo 1 foi puxado 1 vez ($N_t(1)=1$) com uma recompensa m√©dia de 4 ($Q_t(1)=4$), e o bra√ßo 2 foi puxado 9 vezes ($N_t(2)=9$) com uma recompensa m√©dia de 3 ($Q_t(2)=3$). Usando $c=2$, calculamos o UCB para cada bra√ßo:
    >
    > Bra√ßo 1: $4 + 2 * \sqrt{\frac{\ln 10}{1}} \approx 4 + 2 * \sqrt{2.3} \approx 7.03$
    >
    > Bra√ßo 2: $3 + 2 * \sqrt{\frac{\ln 10}{9}} \approx 3 + 2 * \sqrt{0.255} \approx 4.01$
    >
    > O UCB para o bra√ßo 1 √© maior, ent√£o o UCB selecionaria o bra√ßo 1, incentivando a explora√ß√£o mesmo que sua recompensa m√©dia seja inferior √† do bra√ßo 2.

    **Teorema 2.1:** (Regret Bound para UCB) Para um problema multi-armed bandit com $k$ bra√ßos e recompensas no intervalo $[0, 1]$, o *regret* cumulativo do algoritmo UCB √© limitado por $O(\sum_{a: \Delta_a > 0} \frac{\ln T}{\Delta_a})$, onde $\Delta_a$ √© a diferen√ßa entre a recompensa m√©dia do bra√ßo √≥timo e a recompensa m√©dia do bra√ßo $a$, e $T$ √© o n√∫mero de passos de tempo.

    *Prova (Esbo√ßo):* A prova envolve mostrar que o n√∫mero de vezes que uma a√ß√£o sub√≥tima $a$ √© selecionada √© limitado por uma fun√ß√£o logar√≠tmica de $T$. O *regret* √© ent√£o limitado pela soma das diferen√ßas de recompensa ponderadas pelo n√∫mero de vezes que cada a√ß√£o sub√≥tima √© selecionada. A prova completa pode ser encontrada em "Finite-time Analysis of the Multiarmed Bandit Problem" de Auer et al. (2002).

    **Prova:**
    I. Seja $a^*$ a a√ß√£o √≥tima e $a$ uma a√ß√£o sub√≥tima, tal que $q(a^*) > q(a)$. O *regret* √© definido como a diferen√ßa entre a recompensa esperada da a√ß√£o √≥tima e a recompensa esperada da a√ß√£o selecionada.

    II. Seja $T$ o n√∫mero total de passos de tempo. O *regret* cumulativo √© dado por:
    $$R(T) = \mathbb{E}\left[\sum_{t=1}^{T} (q(a^*) - q(A_t))\right]$$

    III. Decomponha o *regret* em termos das a√ß√µes sub√≥timas:
    $$R(T) = \sum_{a: \Delta_a > 0} \Delta_a \mathbb{E}[N_T(a)]$$
    onde $\Delta_a = q(a^*) - q(a)$ e $N_T(a)$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© o tempo $T$.

    IV. Usando a defini√ß√£o do UCB, uma a√ß√£o sub√≥tima $a$ √© selecionada no tempo $t$ somente se:
    $$Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \geq Q_t(a^*) + c \sqrt{\frac{\ln t}{N_t(a^*)}}$$

    V. Ap√≥s algumas manipula√ß√µes e usando o fato de que $Q_t(a)$ converge para $q(a)$, podemos mostrar que o n√∫mero esperado de vezes que a a√ß√£o $a$ √© selecionada √© limitado por:
    $$\mathbb{E}[N_T(a)] \leq O\left(\frac{\ln T}{\Delta_a^2}\right)$$

    VI. Substituindo este resultado na express√£o para o *regret* cumulativo, obtemos:
    $$R(T) = \sum_{a: \Delta_a > 0} \Delta_a \mathbb{E}[N_T(a)] \leq O\left(\sum_{a: \Delta_a > 0} \frac{\ln T}{\Delta_a}\right)$$

    Portanto, o *regret* cumulativo do algoritmo UCB √© limitado por $O(\sum_{a: \Delta_a > 0} \frac{\ln T}{\Delta_a})$. ‚ñ†

![Average performance comparison of UCB and Œµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

3.  **Gradient Bandit Algorithms:**
    -   Em vez de estimar os valores das a√ß√µes, esses algoritmos estimam as *prefer√™ncias* das a√ß√µes, denotadas por $H_t(a)$ [^37].
    -   A probabilidade de selecionar uma a√ß√£o √© determinada por uma fun√ß√£o *soft-max* das prefer√™ncias:
        $$
        Pr\{A_t = a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} = \pi_t(a)
        $$
    -   As prefer√™ncias s√£o atualizadas usando um m√©todo de *ascens√£o de gradiente estoc√°stico*:
        $$
        H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t))
        $$
        $$
         H_{t+1}(a) = H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a), \quad \text{para todo } a \neq A_t
        $$
        onde:
        -   $Œ±$ √© o tamanho do passo.
        -   $R_t$ √© a recompensa recebida no tempo *t*.
        -   $\bar{R}_t$ √© a *linha de base* (baseline), que pode ser a m√©dia das recompensas [^37].
    -   A inclus√£o da linha de base reduz a vari√¢ncia das atualiza√ß√µes e melhora a converg√™ncia.
    -   Uma das vantagens destes algoritmos √© que o *shifting* nas recompensas n√£o tem efeito no algoritmo, dado que a *baseline* se adapta imediatamente ao novo n√≠vel [^37].

    > üí° **Exemplo Num√©rico:** Considere um problema de 2-armed bandit com recompensas que variam ao redor de 1 e 2, respectivamente. Inicialmente, as prefer√™ncias s√£o $H_1(1) = 0$ e $H_1(2) = 0$. Se no tempo $t=1$, selecionarmos a a√ß√£o 1 e recebermos uma recompensa $R_1 = 1.2$, e a m√©dia das recompensas at√© agora for $\bar{R}_1=0$, e o tamanho do passo for $\alpha = 0.1$, ent√£o a probabilidade de selecionar a a√ß√£o 1 √© $\pi_1(1) = \frac{e^0}{e^0 + e^0} = 0.5$. As prefer√™ncias ser√£o atualizadas como:
    >
    > $H_2(1) = 0 + 0.1 * (1.2 - 0) * (1 - 0.5) = 0.06$
    >
    > $H_2(2) = 0 - 0.1 * (1.2 - 0) * 0.5 = -0.06$
    >
    > No tempo $t=2$, a probabilidade de selecionar a a√ß√£o 1 muda para $\pi_2(1) = \frac{e^{0.06}}{e^{0.06} + e^{-0.06}} \approx 0.515$, demonstrando como o algoritmo se adapta com base na recompensa recebida.

**Teorema 3.1:** (Converg√™ncia dos Gradient Bandit Algorithms) Sob certas condi√ß√µes de regularidade e para um tamanho de passo $\alpha$ suficientemente pequeno, os algoritmos Gradient Bandit convergem para uma pol√≠tica √≥tima.

*Prova (Esbo√ßo):* A prova se baseia na teoria de converg√™ncia de algoritmos de aproxima√ß√£o estoc√°stica. √â necess√°rio mostrar que a sequ√™ncia de prefer√™ncias $H_t(a)$ converge para um ponto fixo que corresponde a uma pol√≠tica √≥tima. Isso geralmente requer condi√ß√µes sobre o decaimento do tamanho do passo $Œ±$ e a suavidade da fun√ß√£o de recompensa. Uma an√°lise detalhada pode ser encontrada em "Reinforcement Learning: An Introduction" de Sutton e Barto.

**Prova:**

I. Seja $J(\theta)$ o objetivo de desempenho, onde $\theta$ representa os par√¢metros da pol√≠tica. No caso dos Gradient Bandit Algorithms, $\theta$ corresponde √†s prefer√™ncias $H_t(a)$.

II. O objetivo √© maximizar $J(\theta)$ usando o m√©todo de ascens√£o de gradiente:
$$\theta_{t+1} = \theta_t + \alpha \nabla J(\theta_t)$$
onde $\alpha$ √© o tamanho do passo e $\nabla J(\theta_t)$ √© o gradiente do objetivo de desempenho com respeito aos par√¢metros no tempo $t$.

III. No contexto dos Gradient Bandit Algorithms, o gradiente √© estimado usando amostras estoc√°sticas. A atualiza√ß√£o das prefer√™ncias √© uma aproxima√ß√£o estoc√°stica do gradiente do objetivo de desempenho.

IV. Para garantir a converg√™ncia, √© necess√°rio que o tamanho do passo $\alpha$ satisfa√ßa certas condi√ß√µes. Uma condi√ß√£o comum √© que $\alpha$ deve diminuir ao longo do tempo, mas n√£o muito rapidamente:
$$\sum_{t=1}^{\infty} \alpha_t = \infty \quad \text{e} \quad \sum_{t=1}^{\infty} \alpha_t^2 < \infty$$

V. Al√©m disso, a fun√ß√£o objetivo $J(\theta)$ deve ser suave e bem comportada. Se essas condi√ß√µes forem satisfeitas, a sequ√™ncia de par√¢metros $\theta_t$ converge para um ponto fixo $\theta^*$, que corresponde a um √≥timo local ou global do objetivo de desempenho.

VI. No caso dos Gradient Bandit Algorithms, a converg√™ncia para uma pol√≠tica √≥tima significa que as prefer√™ncias $H_t(a)$ convergem para valores que correspondem a uma pol√≠tica que maximiza a recompensa esperada. Portanto, sob certas condi√ß√µes, os algoritmos Gradient Bandit convergem para uma pol√≠tica √≥tima. ‚ñ†

![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 bra√ßos.](./../images/image1.png)

### Conclus√£o

Este cap√≠tulo apresentou um panorama de algoritmos para o problema de *k-armed bandit*. Os m√©todos *Œµ-greedy*, UCB e *gradient bandit algorithms* oferecem diferentes formas de equilibrar a explora√ß√£o e a explota√ß√£o. A escolha do algoritmo ideal depende das caracter√≠sticas do problema, como a estacionariedade das recompensas e a necessidade de um ajuste fino dos par√¢metros.

![Parameter study comparing bandit algorithms, showing average reward over 1000 steps as a function of algorithm-specific parameters.](./../images/image2.png)

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

![Pseudoc√≥digo de um algoritmo de bandit simples com estrat√©gia Œµ-greedy para explora√ß√£o e explota√ß√£o.](./../images/image4.png)

### Refer√™ncias

[^1]: Cap√≠tulo 2, Multi-armed Bandits.
[^27]: Se√ß√£o 2.2, Action-value Methods.
[^28]: Se√ß√£o 2.3, The 10-armed Testbed.
[^30]: Se√ß√£o 2.3, The 10-armed Testbed.
[^36]: Se√ß√£o 2.7, Upper-Confidence-Bound Action Selection.
[^37]: Se√ß√£o 2.8, Gradient Bandit Algorithms.
[^42]: Se√ß√£o 2.10, Summary.
<!-- END -->