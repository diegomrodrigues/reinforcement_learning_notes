## Sum√°rio das Abordagens de Explora√ß√£o-Explota√ß√£o em Bandidos Multi-armados: Œµ-greedy, UCB e Gradiente de Bandido

### Introdu√ß√£o
Este cap√≠tulo foca em m√©todos que equilibram a **explora√ß√£o** (descobrir a√ß√µes com recompensas potencialmente mais altas) e a **explota√ß√£o** (usar o conhecimento atual para maximizar a recompensa imediata) no contexto do problema de *k-armed bandit*. Exploramos tr√™s abordagens principais: Œµ-greedy, Upper-Confidence-Bound (UCB) e algoritmos de gradiente de bandido. Cada um deles oferece uma maneira diferente de lidar com o *trade-off* explora√ß√£o-explota√ß√£o, crucial no aprendizado por refor√ßo [^42].

### M√©todos Œµ-greedy
Os m√©todos **Œµ-greedy** s√£o caracterizados por uma abordagem simples e direta para a explora√ß√£o [^27]. Com probabilidade $\epsilon$, o agente seleciona uma a√ß√£o aleatoriamente, independentemente das estimativas de valor da a√ß√£o. Com probabilidade 1 - $\epsilon$, o agente age *greedy*, selecionando a a√ß√£o com a maior estimativa de valor atual [^27].

> üí° **Exemplo Num√©rico:** Imagine um cen√°rio com 5 bra√ßos ($k$=5). Definimos $\epsilon$ = 0.1. Isso significa que, em 10% das vezes, o agente selecionar√° um bra√ßo aleatoriamente (explora√ß√£o). Nos 90% restantes das vezes, escolher√° o bra√ßo que atualmente acredita ser o melhor (explota√ß√£o).

A atualiza√ß√£o para a estimativa de valor $Q_t(a)$ segue uma m√©dia amostral:

$$
Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)}[R_t - Q_t(a)]
$$

onde $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o tempo *t*, e $R_t$ √© a recompensa recebida no tempo *t* [^31].

> üí° **Exemplo Num√©rico:** Suponha que a a√ß√£o *a* (bra√ßo 2) foi selecionada 10 vezes at√© o tempo *t* ($N_t(a) = 10$) e sua estimativa de valor atual √© $Q_t(a) = 0.5$. No tempo *t*, selecionar a a√ß√£o *a* rende uma recompensa de $R_t = 1$. A nova estimativa de valor para a a√ß√£o *a* ser√°:
>
> $Q_{t+1}(a) = 0.5 + \frac{1}{10}[1 - 0.5] = 0.5 + 0.05 = 0.55$
>
> A estimativa do valor da a√ß√£o *a* aumentou porque a recompensa obtida ($R_t = 1$) foi maior do que a estimativa anterior ($Q_t(a) = 0.5$).

Para entender melhor como essa atualiza√ß√£o funciona, podemos mostrar que ela representa uma m√©dia amostral das recompensas recebidas para a a√ß√£o *a*.

**Prova:**
Provaremos que a atualiza√ß√£o iterativa da estimativa de valor $Q_{t+1}(a)$ √© equivalente ao c√°lculo da m√©dia amostral das recompensas recebidas para a a√ß√£o $a$ at√© o tempo $t+1$.

I. Seja $Q_{t+1}(a)$ a estimativa de valor da a√ß√£o $a$ ap√≥s $t+1$ itera√ß√µes, atualizada usando a regra fornecida:
   $$Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)}[R_t - Q_t(a)]$$

II. Multiplicando ambos os lados por $N_t(a)$:
    $$N_t(a) \cdot Q_{t+1}(a) = N_t(a) \cdot Q_t(a) + R_t - Q_t(a)$$
    $$N_t(a) \cdot Q_{t+1}(a) + Q_t(a) = N_t(a) \cdot Q_t(a) + R_t$$

III. Adicionando $N_t(a)Q_t(a)$ em ambos os lados e reorganizando, obtemos:
     $$Q_{t+1}(a)N_{t+1}(a) = Q_t(a)N_t(a) + R_t$$, onde $N_{t+1}(a) = N_t(a) + 1$

IV. Agora, expandindo recursivamente $Q_t(a)N_t(a)$, considerando que $Q_1(a)$ √© a recompensa inicial $R_1$ e $N_1(a) = 1$:
    $$Q_{t+1}(a)N_{t+1}(a) = R_1 + R_2 + \ldots + R_t = \sum_{i=1}^{t} R_i$$

V. Portanto, podemos expressar $Q_{t+1}(a)$ como:
    $$Q_{t+1}(a) = \frac{1}{N_{t+1}(a)} \sum_{i=1}^{N_{t+1}(a)} R_i$$
Isso mostra que $Q_{t+1}(a)$ √© a m√©dia amostral das recompensas recebidas para a a√ß√£o $a$ at√© o tempo $t+1$. $\blacksquare$

**Vantagens:**
*   Simplicidade e facilidade de implementa√ß√£o [^27].
*   Garantia de que, no limite, todas as a√ß√µes ser√£o amostradas infinitas vezes, garantindo a converg√™ncia de $Q_t(a)$ para o valor verdadeiro $q_*(a)$ [^28].

**Desvantagens:**
*   Explora√ß√£o indiscriminada, sem prefer√™ncia por a√ß√µes quase gananciosas ou com alta incerteza [^35].
*   A taxa de explora√ß√£o $\epsilon$ √© um hiperpar√¢metro fixo que pode ser dif√≠cil de ajustar idealmente [^28].

Uma varia√ß√£o comum do m√©todo Œµ-greedy √© o **Œµ-greedy com decaimento**. Nesta abordagem, o valor de $\epsilon$ diminui gradualmente ao longo do tempo. Isso permite uma explora√ß√£o mais agressiva no in√≠cio do aprendizado, quando a incerteza √© maior, e uma explota√ß√£o mais focada posteriormente, quando as estimativas de valor se tornam mais precisas. Uma forma comum de decaimento √© reduzir $\epsilon$ linearmente ou exponencialmente.

> üí° **Exemplo Num√©rico:** Inicializamos $\epsilon$ = 0.5 e definimos uma taxa de decaimento linear de 0.001 por passo. Ap√≥s 100 passos, $\epsilon$ = 0.5 - (100 * 0.001) = 0.4. Ap√≥s 500 passos, $\epsilon$ = 0.5 - (500 * 0.001) = 0.0. Isso significa que ap√≥s 500 passos, o agente estar√° apenas explotando.

**Proposi√ß√£o 1:** Uma estrat√©gia de decaimento para $\epsilon$ pode ser definida como $\epsilon_t = \epsilon_0 \cdot \gamma^t$, onde $\epsilon_0$ √© a taxa de explora√ß√£o inicial e $\gamma \in (0, 1]$ √© a taxa de decaimento.

> üí° **Exemplo Num√©rico:** Se $\epsilon_0 = 0.5$ e $\gamma = 0.99$, ent√£o ap√≥s 100 passos, $\epsilon_{100} = 0.5 \cdot 0.99^{100} \approx 0.1846$. Este decaimento exponencial suaviza a transi√ß√£o da explora√ß√£o para a explota√ß√£o.

**Vantagens do Œµ-greedy com decaimento:**
*   Adapta-se melhor √† natureza do problema, explorando mais no in√≠cio e explotando mais no final.
*   Pode levar a um desempenho superior em compara√ß√£o com o Œµ-greedy com um valor $\epsilon$ fixo.

**Desvantagens do Œµ-greedy com decaimento:**
*   Introduz um novo hiperpar√¢metro, a taxa de decaimento $\gamma$, que tamb√©m precisa ser ajustada.
*   A escolha da fun√ß√£o de decaimento (linear, exponencial, etc.) pode impactar o desempenho.



![Pseudoc√≥digo de um algoritmo de bandit simples com estrat√©gia Œµ-greedy para explora√ß√£o e explota√ß√£o.](./../images/image4.png)

### Upper-Confidence-Bound (UCB) Action Selection
Ao contr√°rio de Œµ-greedy, a sele√ß√£o de a√ß√£o **UCB** √© *determin√≠stica* e explora favorecendo sutilmente, a cada passo, as a√ß√µes que receberam menos amostras at√© agora [^42]. A ideia central √© selecionar a√ß√µes de acordo com:

$$
A_t = \text{argmax}_a \left[ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right]
$$

onde:
*   $Q_t(a)$ √© a estimativa de valor da a√ß√£o *a* no tempo *t* [^26].
*   $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada antes do tempo *t* [^35].
*   $c > 0$ controla o grau de explora√ß√£o [^36].
*   $\ln t$ √© o logaritmo natural de *t*.

O termo $\sqrt{\frac{\ln t}{N_t(a)}}$ √© uma medida da incerteza ou vari√¢ncia na estimativa do valor da a√ß√£o [^36].  Ele diminui √† medida que $N_t(a)$ aumenta, incentivando a explora√ß√£o de a√ß√µes menos amostradas [^36]. O par√¢metro *c* determina o n√≠vel de confian√ßa no limite superior do valor real da a√ß√£o [^36].

> üí° **Exemplo Num√©rico:** Assuma que *t* = 100, *c* = 2. Temos duas a√ß√µes: A√ß√£o 1 com $Q_t(1) = 0.6$ e $N_t(1) = 50$, e A√ß√£o 2 com $Q_t(2) = 0.5$ e $N_t(2) = 5$.
>
> Para A√ß√£o 1: $UCB_1 = 0.6 + 2 \sqrt{\frac{\ln 100}{50}} \approx 0.6 + 2 \sqrt{\frac{4.605}{50}} \approx 0.6 + 2(0.303) \approx 1.206$
> Para A√ß√£o 2: $UCB_2 = 0.5 + 2 \sqrt{\frac{\ln 100}{5}} \approx 0.5 + 2 \sqrt{\frac{4.605}{5}} \approx 0.5 + 2(0.96) \approx 2.42$
>
> Neste caso, a A√ß√£o 2 seria selecionada porque tem um UCB maior, mesmo que sua estimativa de valor ($Q_t(2) = 0.5$) seja menor que a da A√ß√£o 1 ($Q_t(1) = 0.6$). Isso ocorre porque a A√ß√£o 2 foi amostrada muito menos vezes, ent√£o h√° mais incerteza sobre seu verdadeiro valor, e o UCB incentiva sua explora√ß√£o.

**Vantagens:**
*   Aborda a explora√ß√£o de forma mais inteligente do que Œµ-greedy, equilibrando estimativas de valor e incerteza [^35].
*   O desempenho geralmente √© melhor do que Œµ-greedy [^36].

**Desvantagens:**
*   Mais dif√≠cil de estender al√©m dos bandidos para configura√ß√µes mais gerais de aprendizado por refor√ßo [^36].
*   Pode enfrentar dificuldades em problemas n√£o estacion√°rios [^36].

√â importante notar que o termo $\ln t$ no UCB garante que, no longo prazo, todas as a√ß√µes ser√£o exploradas.  No entanto, a taxa de explora√ß√£o diminui logaritmicamente, o que pode ser muito lento em alguns problemas. Uma alternativa para acelerar a explora√ß√£o √© usar uma variante chamada **UCB1-tuned**, que usa uma estimativa da vari√¢ncia da recompensa para cada a√ß√£o para refinar o limite de confian√ßa.

**Teorema 2:** A sele√ß√£o de a√ß√£o UCB1-tuned √© dada por:

$$
A_t = \text{argmax}_a \left[ Q_t(a) + \sqrt{\frac{\ln t}{N_t(a)} \min\left\{V_t(a), \frac{1}{4}\right\}} \right]
$$

onde $V_t(a)$ √© uma estimativa da vari√¢ncia da recompensa da a√ß√£o *a* no tempo *t*, dada por:

$$
V_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{N_t(a)} (R_{i,a} - Q_t(a))^2
$$

e $R_{i,a}$ representa a i-√©sima recompensa obtida ao selecionar a a√ß√£o *a*.

> üí° **Exemplo Num√©rico:** Considere a A√ß√£o 2 do exemplo anterior. Suponha que as 5 recompensas obtidas at√© o momento foram: 0.4, 0.6, 0.3, 0.7, 0.5. A estimativa de valor $Q_t(2)$ √© 0.5 (a m√©dia). A vari√¢ncia estimada √©:
>
> $V_t(2) = \frac{1}{5} [(0.4-0.5)^2 + (0.6-0.5)^2 + (0.3-0.5)^2 + (0.7-0.5)^2 + (0.5-0.5)^2] = \frac{1}{5} [0.01 + 0.01 + 0.04 + 0.04 + 0] = 0.02$.
>
> Usando UCB1-tuned: $UCB1_{tuned} = 0.5 + \sqrt{\frac{\ln 100}{5} \min\{0.02, \frac{1}{4}\}} = 0.5 + \sqrt{\frac{4.605}{5} \cdot 0.02} \approx 0.5 + \sqrt{0.01842} \approx 0.5 + 0.1357 \approx 0.6357$
>
> Comparando com o UCB original (2.42), o UCB1-tuned fornece um limite superior de confian√ßa muito menor, devido √† baixa vari√¢ncia estimada das recompensas para a A√ß√£o 2.

A intui√ß√£o por tr√°s do UCB1-tuned √© que, se uma a√ß√£o tem uma alta vari√¢ncia estimada, o limite de confian√ßa ser√° maior, incentivando a explora√ß√£o. No entanto, a vari√¢ncia √© limitada superiormente por 1/4, garantindo que a explora√ß√£o n√£o seja excessiva.

### Algoritmos de Gradiente de Bandido
Os **algoritmos de gradiente de bandido** estimam as *prefer√™ncias* de a√ß√£o, $H_t(a)$, em vez de estimativas de valor de a√ß√£o [^42]. A probabilidade de selecionar uma a√ß√£o *a* no tempo *t* √© determinada por uma distribui√ß√£o *softmax*:

$$
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}}
$$

> üí° **Exemplo Num√©rico:** Considere um problema com 3 bra√ßos. As prefer√™ncias de a√ß√£o atuais s√£o: $H_t(1) = 1$, $H_t(2) = 0.5$, e $H_t(3) = 0$. As probabilidades de sele√ß√£o s√£o:
>
> $\pi_t(1) = \frac{e^1}{e^1 + e^{0.5} + e^0} \approx \frac{2.718}{2.718 + 1.649 + 1} \approx \frac{2.718}{5.367} \approx 0.5065$
> $\pi_t(2) = \frac{e^{0.5}}{e^1 + e^{0.5} + e^0} \approx \frac{1.649}{5.367} \approx 0.3073$
> $\pi_t(3) = \frac{e^0}{e^1 + e^{0.5} + e^0} \approx \frac{1}{5.367} \approx 0.1863$
>
> Portanto, a A√ß√£o 1 tem a maior probabilidade de ser selecionada, seguida pela A√ß√£o 2 e, finalmente, a A√ß√£o 3.

As prefer√™ncias de a√ß√£o s√£o atualizadas usando um algoritmo de ascens√£o de gradiente estoc√°stico:

$$
H_{t+1}(a) = H_t(a) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(a)) \quad \text{se } a = A_t
$$

$$
H_{t+1}(a) = H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a) \quad \text{se } a \ne A_t
$$

onde:
*   $\alpha > 0$ √© um par√¢metro de tamanho do passo [^37].
*   $R_t$ √© a recompensa recebida no tempo *t* [^26].
*   $\bar{R}_t$ √© a recompensa m√©dia no tempo *t*, servindo como linha de base [^37].

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, suponha que a A√ß√£o 1 foi selecionada ($A_t = 1$) e a recompensa obtida foi $R_t = 0.8$. A recompensa m√©dia at√© o momento √© $\bar{R}_t = 0.4$.  O tamanho do passo √© $\alpha = 0.1$.
>
> Atualiza√ß√£o para a A√ß√£o 1: $H_{t+1}(1) = 1 + 0.1 (0.8 - 0.4)(1 - 0.5065) \approx 1 + 0.1 (0.4)(0.4935) \approx 1 + 0.01974 \approx 1.01974$
> Atualiza√ß√£o para a A√ß√£o 2: $H_{t+1}(2) = 0.5 - 0.1 (0.8 - 0.4)(0.3073) \approx 0.5 - 0.1(0.4)(0.3073) \approx 0.5 - 0.012292 \approx 0.487708$
> Atualiza√ß√£o para a A√ß√£o 3: $H_{t+1}(3) = 0 - 0.1 (0.8 - 0.4)(0.1863) \approx 0 - 0.1(0.4)(0.1863) \approx 0 - 0.007452 \approx -0.007452$
>
> A prefer√™ncia da A√ß√£o 1 aumentou porque sua recompensa foi maior que a m√©dia, enquanto as prefer√™ncias das outras a√ß√µes diminu√≠ram.

Podemos verificar que a atualiza√ß√£o das prefer√™ncias de a√ß√£o implementa, de fato, uma forma de ascens√£o de gradiente estoc√°stico.

**Prova:**
Demonstraremos que as regras de atualiza√ß√£o para $H_{t+1}(a)$ s√£o derivadas de uma ascens√£o de gradiente estoc√°stico no desempenho esperado.

I. O objetivo √© maximizar a recompensa esperada, que pode ser expressa como:
    $$J(H_t) = \mathbb{E}[R_t] = \sum_{a=1}^k \pi_t(a) q_*(a)$$
    onde $q_*(a)$ √© o verdadeiro valor da a√ß√£o *a*.

II. Queremos realizar uma ascens√£o de gradiente em $H_t(a)$, ent√£o precisamos calcular $\frac{\partial J(H_t)}{\partial H_t(a)}$. Usando a regra da cadeia:
     $$\frac{\partial J(H_t)}{\partial H_t(a)} = \sum_{b=1}^k q_*(b) \frac{\partial \pi_t(b)}{\partial H_t(a)}$$

III. Agora, precisamos calcular $\frac{\partial \pi_t(b)}{\partial H_t(a)}$.  Sabemos que $\pi_t(b) = \frac{e^{H_t(b)}}{\sum_{c=1}^k e^{H_t(c)}}$.  Portanto:
    *   Se $a = b$:
        $$\frac{\partial \pi_t(b)}{\partial H_t(a)} = \frac{e^{H_t(a)} \sum_{c=1}^k e^{H_t(c)} - e^{H_t(a)} e^{H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2} = \frac{e^{H_t(a)}}{\sum_{c=1}^k e^{H_t(c)}} \left( 1 - \frac{e^{H_t(a)}}{\sum_{c=1}^k e^{H_t(c)}} \right) = \pi_t(a)(1 - \pi_t(a))$$
    *   Se $a \ne b$:
        $$\frac{\partial \pi_t(b)}{\partial H_t(a)} = \frac{0 \cdot \sum_{c=1}^k e^{H_t(c)} - e^{H_t(b)} e^{H_t(a)}}{(\sum_{c=1}^k e^{H_t(c)})^2} = - \frac{e^{H_t(b)}}{\sum_{c=1}^k e^{H_t(c)}} \frac{e^{H_t(a)}}{\sum_{c=1}^k e^{H_t(c)}} = - \pi_t(b) \pi_t(a)$$

IV. Substituindo esses resultados de volta na derivada de $J(H_t)$:
    $$\frac{\partial J(H_t)}{\partial H_t(a)} = \sum_{b=1}^k q_*(b) \frac{\partial \pi_t(b)}{\partial H_t(a)} = q_*(a) \pi_t(a) (1 - \pi_t(a)) + \sum_{b \ne a} q_*(b) (-\pi_t(b) \pi_t(a))$$
    $$\frac{\partial J(H_t)}{\partial H_t(a)} = \pi_t(a) \left[ q_*(a) - \sum_{b=1}^k \pi_t(b) q_*(b) \right] = \pi_t(a) \left[ q_*(a) - J(H_t) \right]$$

V.  O algoritmo de ascens√£o de gradiente estoc√°stico √©:
     $$H_{t+1}(a) = H_t(a) + \alpha \frac{\partial J(H_t)}{\partial H_t(a)} = H_t(a) + \alpha \pi_t(a) [q_*(a) - J(H_t)]$$

VI. Como n√£o conhecemos $q_*(a)$ e $J(H_t)$, usamos amostras para estim√°-los. Usamos $R_t$ como uma amostra de $q_*(A_t)$ e $\bar{R}_t$ como uma amostra de $J(H_t)$.  Portanto, a atualiza√ß√£o se torna:
    $$H_{t+1}(a) = H_t(a) + \alpha (R_t - \bar{R}_t)(1 - \pi_t(a)) \quad \text{se } a = A_t$$
    $$H_{t+1}(a) = H_t(a) - \alpha (R_t - \bar{R}_t)\pi_t(a) \quad \text{se } a \ne A_t$$
    Isso confirma que as atualiza√ß√µes de $H_{t+1}(a)$ s√£o uma forma de ascens√£o de gradiente estoc√°stico. $\blacksquare$

**Vantagens:**
*   N√£o requer estimativas de valor de a√ß√£o, permitindo maior flexibilidade [^37].
*   Robusto a mudan√ßas na escala das recompensas [^37].

**Desvantagens:**
*   O desempenho depende fortemente da escolha da linha de base [^37].
*   Pode convergir mais lentamente do que os m√©todos de valor de a√ß√£o [^37].

Uma escolha comum para a linha de base $\bar{R}_t$ √© a m√©dia amostral de todas as recompensas recebidas at√© o tempo *t*. No entanto, outras linhas de base podem ser consideradas, dependendo do problema espec√≠fico. Por exemplo, em ambientes n√£o estacion√°rios, uma m√©dia m√≥vel exponencial das recompensas pode ser mais apropriada.

**Proposi√ß√£o 3:** Uma m√©dia m√≥vel exponencial para a linha de base pode ser definida como:

$$
\bar{R}_{t+1} = \beta \bar{R}_t + (1 - \beta) R_t
$$

onde $\beta \in [0, 1]$ √© um par√¢metro que controla o peso dado √†s recompensas passadas. Um valor de $\beta$ pr√≥ximo de 1 d√° mais peso √†s recompensas passadas, enquanto um valor pr√≥ximo de 0 d√° mais peso √† recompensa mais recente.

> üí° **Exemplo Num√©rico:** Se $\bar{R}_t = 0.4$, $R_t = 0.8$ e $\beta = 0.9$, ent√£o $\bar{R}_{t+1} = 0.9 \cdot 0.4 + (1 - 0.9) \cdot 0.8 = 0.36 + 0.08 = 0.44$. A m√©dia m√≥vel exponencial se ajusta lentamente √† recompensa mais recente. Se $\beta = 0.1$, ent√£o $\bar{R}_{t+1} = 0.1 \cdot 0.4 + (1 - 0.1) \cdot 0.8 = 0.04 + 0.72 = 0.76$. Neste caso, a m√©dia m√≥vel exponencial se ajusta muito mais rapidamente √† recompensa mais recente.

Usar uma m√©dia m√≥vel exponencial permite que o algoritmo se adapte mais rapidamente a mudan√ßas no ambiente.

### Compara√ß√£o e Considera√ß√µes Pr√°ticas

![Average performance of Œµ-greedy action-value methods on a 10-armed testbed, demonstrating the exploration-exploitation trade-off.](./../images/image6.png)

A efic√°cia relativa desses m√©todos depende das caracter√≠sticas do problema de bandido [^30].  Para tarefas *estacion√°rias*, UCB geralmente tem um desempenho melhor [^43]. Para tarefas *n√£o estacion√°rias*, uma variante de Œµ-greedy com um tamanho de passo constante ou algoritmos de gradiente de bandido podem ser mais apropriados [^30].

![Average performance comparison of UCB and Œµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

Cada algoritmo possui um par√¢metro crucial que precisa ser ajustado: $\epsilon$ para Œµ-greedy, *c* para UCB e $\alpha$ para algoritmos de gradiente de bandido [^42]. Um estudo de par√¢metros revela que todos os algoritmos t√™m melhor desempenho em um valor intermedi√°rio de seu par√¢metro [^42]. Isso demonstra a import√¢ncia de selecionar cuidadosamente os valores dos par√¢metros para um desempenho ideal [^42]. A sensibilidade do desempenho ao par√¢metro escolhido √© uma considera√ß√£o importante ao selecionar um algoritmo para um problema espec√≠fico [^43]. Al√©m disso, para problemas complexos, t√©cnicas de otimiza√ß√£o de hiperpar√¢metros, como busca em grade ou otimiza√ß√£o Bayesiana, podem ser empregadas para encontrar os valores √≥timos dos par√¢metros para cada algoritmo.

![Parameter study comparing bandit algorithms, showing average reward over 1000 steps as a function of algorithm-specific parameters.](./../images/image2.png)

> üí° **Exemplo Num√©rico:** Uma busca em grade para o par√¢metro $\epsilon$ no Œµ-greedy pode envolver testar valores como 0.01, 0.05, 0.1, 0.2 e 0.5. Para cada valor, o algoritmo √© executado por um certo n√∫mero de epis√≥dios e o desempenho (por exemplo, recompensa m√©dia) √© medido. O valor de $\epsilon$ que leva ao melhor desempenho √© ent√£o selecionado.

![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 bra√ßos.](./../images/image1.png)

![Comparative performance of optimistic vs. realistic action-value initialization in a 10-armed testbed, illustrating the impact on optimal action selection over time.](./../images/image3.png)

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

### Conclus√£o
Em resumo, este cap√≠tulo apresentou tr√™s abordagens fundamentais para lidar com o *trade-off* explora√ß√£o-explota√ß√£o no problema de *k-armed bandit* [^42]. M√©todos *Œµ-greedy* exploram aleatoriamente, UCB explora deterministicamente favorecendo a√ß√µes menos amostradas e algoritmos de gradiente de bandido estimam prefer√™ncias de a√ß√£o e favorecem a√ß√µes preferidas probabilisticamente [^42]. Compreender os pontos fortes e fracos de cada abordagem √© crucial para projetar agentes de aprendizado por refor√ßo eficazes [^43]. A escolha do m√©todo ideal depende da tarefa espec√≠fica e requer considera√ß√£o cuidadosa dos hiperpar√¢metros e caracter√≠sticas do ambiente [^42]. Futuras dire√ß√µes de pesquisa incluem o desenvolvimento de m√©todos que se adaptam automaticamente √†s caracter√≠sticas do ambiente e a combina√ß√£o de diferentes abordagens para obter um melhor desempenho.

### Refer√™ncias
[^26]: Cap√≠tulo 2, Se√ß√£o 2.1
[^27]: Cap√≠tulo 2, Se√ß√£o 2.2
[^28]: Cap√≠tulo 2, Se√ß√£o 2.3
[^30]: Cap√≠tulo 2, Se√ß√£o 2.3
[^31]: Cap√≠tulo 2, Se√ß√£o 2.4
[^35]: Cap√≠tulo 2, Se√ß√£o 2.7
[^36]: Cap√≠tulo 2, Se√ß√£o 2.7
[^37]: Cap√≠tulo 2, Se√ß√£o 2.8
[^42]: Cap√≠tulo 2, Se√ß√£o 2.10
[^43]: Cap√≠tulo 2, Se√ß√£o 2.10
<!-- END -->