## Optimistic Initial Values e o Vi√©s Inicial nos M√©todos de M√©dia Amostral

### Introdu√ß√£o

No contexto do aprendizado por refor√ßo, e especificamente nos problemas de *k-armed bandits*, os m√©todos de estimativa de valor de a√ß√£o desempenham um papel crucial na tomada de decis√µes. Estes m√©todos, como os m√©todos de m√©dia amostral, s√£o usados para estimar o valor esperado de recompensa de cada a√ß√£o. No entanto, como apontado [^12], todos os m√©todos discutidos at√© agora s√£o dependentes, em certa medida, das estimativas iniciais de valor de a√ß√£o, $Q_1(a)$. Em termos estat√≠sticos, esses m√©todos s√£o *tendenciosos* por suas estimativas iniciais. Este cap√≠tulo explorar√° como esse vi√©s surge, particularmente nos m√©todos de m√©dia amostral, e como ele pode ser tanto uma desvantagem quanto uma vantagem no contexto do aprendizado por refor√ßo [^12].

### Conceitos Fundamentais

Os m√©todos de *action-value* s√£o fundamentais para o aprendizado por refor√ßo, pois eles nos permitem estimar qu√£o boa √© cada a√ß√£o em termos de recompensa esperada. Uma forma natural de estimar o valor de uma a√ß√£o √© atrav√©s da m√©dia das recompensas recebidas, como expresso na equa√ß√£o (2.1) [^4]:
$$Q_t(a) = \frac{\text{soma das recompensas quando a foi tomada antes de t}}{\text{n√∫mero de vezes que a foi tomada antes de t}}$$
Esta abordagem, chamada *sample-average method*, converge para o verdadeiro valor da a√ß√£o $q_*(a)$ √† medida que o n√∫mero de sele√ß√µes da a√ß√£o tende ao infinito, de acordo com a lei dos grandes n√∫meros [^4]. No entanto, o comportamento inicial dessa estimativa √© influenciado pela inicializa√ß√£o $Q_1(a)$, e esse √© o ponto de partida da nossa discuss√£o sobre o vi√©s inicial.

**Vi√©s Inicial nos M√©todos de M√©dia Amostral**:
A inicializa√ß√£o das estimativas de valor de a√ß√£o $Q_1(a)$ introduz um vi√©s nos m√©todos de m√©dia amostral, ou seja, as estimativas iniciais influenciam as estimativas subsequentes. Se, por exemplo, todas as estimativas iniciais forem definidas como zero, a explora√ß√£o pode ser dificultada se algumas a√ß√µes produzirem recompensas consistentemente baixas [^12]. Embora o vi√©s desapare√ßa assim que cada a√ß√£o seja selecionada pelo menos uma vez [^12], no in√≠cio do aprendizado ele afeta significativamente o comportamento do algoritmo. Este vi√©s inicial √© tanto uma desvantagem quanto uma vantagem, uma vez que, se as estimativas iniciais forem bem escolhidas, o algoritmo pode convergir mais rapidamente para um comportamento pr√≥ximo do ideal. No entanto, se as estimativas iniciais forem mal escolhidas, elas podem levar a um desempenho inferior no in√≠cio do aprendizado.

> üí° **Exemplo Num√©rico:** Considere um problema de 3-armed bandit. Inicializamos $Q_1(a) = 0$ para todas as a√ß√µes. A√ß√µes $a_1$, $a_2$, e $a_3$ t√™m m√©dias de recompensa verdadeiras $q_*(a_1) = 1$, $q_*(a_2) = 2$ e $q_*(a_3) = 0.1$, respectivamente. Se nas primeiras intera√ß√µes o agente selecionar $a_3$ v√°rias vezes e receber recompensas pr√≥ximas a zero, seu valor estimado $Q_t(a_3)$ permanecer√° baixo, dificultando a explora√ß√£o de $a_2$ que tem o maior potencial de recompensa. Este √© um exemplo de como uma inicializa√ß√£o desfavor√°vel pode afetar a explora√ß√£o.

```mermaid
graph LR
    A[/"Inicializa√ß√£o Q_1(a) = 0 para todas as a√ß√µes"/] --> B("A√ß√£o a_3 selecionada repetidamente");
    B --> C("Recompensas ~ 0");
    C --> D("Q_t(a_3) permanece baixo");
    D --> E[/"Explora√ß√£o de a_2 dificultada (maior potencial de recompensa)"/];
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

**Lemma 1**: *A converg√™ncia de $Q_t(a)$ para $q_*(a)$ com o m√©todo de m√©dia amostral √© garantida pela lei dos grandes n√∫meros, mas o comportamento inicial √© influenciado pelas estimativas iniciais $Q_1(a)$.*

*Prova*: Conforme o n√∫mero de vezes que uma a√ß√£o $a$ √© tomada ($t$) cresce, o denominador da equa√ß√£o (2.1) tende ao infinito. Pela lei dos grandes n√∫meros, a m√©dia amostral de recompensas converge para o valor esperado da recompensa, que √© $q_*(a)$. No entanto, este processo de converg√™ncia √© afetado pelos valores iniciais atribu√≠dos a $Q_1(a)$, o que introduz um vi√©s tempor√°rio at√© que todas as a√ß√µes sejam selecionadas um n√∫mero suficiente de vezes. $\blacksquare$

**Lemma 1.1:** *Se $N_t(a)$ denota o n√∫mero de vezes que a a√ß√£o 'a' foi selecionada antes do instante 't', a equa√ß√£o (2.1) pode ser reescrita como $Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{N_t(a)} R_i(a)$, onde $R_i(a)$ s√£o as recompensas recebidas ao selecionar 'a'.*

*Prova:* Esta √© uma nota√ß√£o alternativa da equa√ß√£o (2.1) que torna expl√≠cito o n√∫mero de vezes que a a√ß√£o 'a' foi tomada e as recompensas associadas. $\blacksquare$

```mermaid
graph LR
    A["Equa√ß√£o (2.1)"] -- "Nota√ß√£o alternativa" --> B["$Q_t(a) =  (1/N_t(a)) * \\sum_{i=1}^{N_t(a)} R_i(a)$"];
    B -- "$N_t(a)$: n√∫mero de vezes que 'a' foi selecionada" --> C["$R_i(a)$: recompensas ao selecionar 'a'"];
    style A fill:#ccf,stroke:#333,stroke-width:2px
```

**O Impacto da Inicializa√ß√£o Otimista**:
Uma forma interessante de usar este vi√©s inicial √© a *inicializa√ß√£o otimista*, onde os valores iniciais $Q_1(a)$ s√£o definidos em n√≠veis mais altos do que as recompensas esperadas [^12]. Por exemplo, em um problema onde as recompensas est√£o distribu√≠das em torno de zero, pode-se inicializar $Q_1(a) = +5$. Este otimismo inicial for√ßa o algoritmo a explorar diferentes a√ß√µes, j√° que as recompensas obtidas inicialmente s√£o menores do que o esperado [^12]. Essa estrat√©gia incentiva a explora√ß√£o mesmo quando se usa uma pol√≠tica puramente gananciosa.

> üí° **Exemplo Num√©rico:** Usando o mesmo problema de 3-armed bandit, inicializamos $Q_1(a) = 5$ para todas as a√ß√µes. Na primeira intera√ß√£o, se a a√ß√£o $a_1$ √© selecionada e retorna uma recompensa de 1, $Q_2(a_1)$ ser√° calculado como $(5 + 1)/2 = 3$. O agente perceber√° que seu valor estimado era excessivamente otimista, motivando a explora√ß√£o de outras a√ß√µes. Isso acelera a descoberta de a√ß√µes mais recompensadoras, como $a_2$, que tem valor verdadeiro $q_*(a_2) = 2$.

```mermaid
graph LR
    A[/"Inicializa√ß√£o: Q_1(a) = 5 para todas as a√ß√µes"/] --> B("A√ß√£o a_1 selecionada");
    B --> C("Recompensa obtida: 1");
    C --> D("Q_2(a_1) = (5 + 1) / 2 = 3");
    D --> E("Agente percebe valor estimado otimista");
    E --> F[/"Explora√ß√£o de outras a√ß√µes (ex. a_2)"/];
    style A fill:#ffc,stroke:#333,stroke-width:2px
```

**Lemma 2**: *A inicializa√ß√£o otimista, com $Q_1(a)$ maior do que as recompensas esperadas, incentiva a explora√ß√£o mesmo com pol√≠ticas puramente gananciosas, melhorando o desempenho a longo prazo.*

*Prova*: Ao iniciar $Q_1(a)$ com um valor alto, as primeiras recompensas obtidas ser√£o menores do que a estimativa. Isso leva o algoritmo a explorar outras a√ß√µes em busca de recompensas melhores, pois os valores $Q_t(a)$ s√£o atualizados com cada recompensa. O algoritmo continuar√° explorando at√© que as recompensas obtidas se aproximem do valor $q_*(a)$, o que leva a uma maior explora√ß√£o inicial e um melhor desempenho a longo prazo. $\blacksquare$

**Lemma 2.1:** *A escolha de um valor $Q_1(a)$ suficientemente alto na inicializa√ß√£o otimista pode garantir que todas as a√ß√µes sejam exploradas pelo menos uma vez no in√≠cio do processo de aprendizado, mesmo com uma pol√≠tica gulosa.*

*Prova:*  Se $Q_1(a) > \max_a q_*(a)$, ent√£o a sele√ß√£o inicial da a√ß√£o com maior valor estimado for√ßar√° a explora√ß√£o de outras a√ß√µes, pois as recompensas reais s√£o menores do que a estimativa inicial. Isso garante que o algoritmo explore diferentes a√ß√µes pelo menos uma vez no in√≠cio do aprendizado. $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que a recompensa m√°xima poss√≠vel seja $R_{max} = 10$, e que todas as estimativas iniciais sejam inicializadas com $Q_1(a) = 15$. A primeira a√ß√£o selecionada, digamos $a_1$, produz uma recompensa de 7. Ent√£o, $Q_2(a_1) = (15 + 7) / 2 = 11$. Mesmo ap√≥s a primeira atualiza√ß√£o, $Q_2(a_1)$ ainda √© maior que $R_{max}$.  Isso incentiva a explora√ß√£o de outras a√ß√µes, j√° que o agente ainda espera recompensas maiores do que est√° obtendo.

```mermaid
graph LR
    A[/"$R_{max} = 10$, $Q_1(a) = 15$"/] --> B("A√ß√£o a_1 selecionada");
    B --> C("Recompensa = 7");
    C --> D("Q_2(a_1) = (15 + 7) / 2 = 11");
    D --> E["$Q_2(a_1) > R_{max}$"];
    E --> F[/"Incentivo para explorar outras a√ß√µes"/];
    style A fill:#aaf,stroke:#333,stroke-width:2px
```

**Proposi√ß√£o 1:** *Seja $R_{max}$ o valor m√°ximo de recompensa poss√≠vel. Se $Q_1(a) > R_{max}$, o m√©todo de m√©dia amostral com inicializa√ß√£o otimista garante que a a√ß√£o ser√° escolhida no m√≠nimo uma vez antes que sua estimativa de valor $Q_t(a)$ seja atualizada para um valor menor ou igual a $R_{max}$.*

*Prova:* Inicialmente, todas as estimativas $Q_1(a)$ s√£o maiores que $R_{max}$. Quando uma a√ß√£o $a$ √© selecionada pela primeira vez e uma recompensa $r(a) \leq R_{max}$ √© recebida, a estimativa $Q_2(a)$ ser√° uma m√©dia entre $Q_1(a)$ e $r(a)$, resultando em $Q_2(a) \leq \frac{Q_1(a) + R_{max}}{2}$ ou $Q_2(a) = r(a)$ se $N_t(a) = 1$. De qualquer forma,  $Q_2(a)$ pode ser menor que $Q_1(a)$ mas maior que $R_{max}$ at√© que outras a√ß√µes sejam exploradas. Caso nenhuma outra a√ß√£o seja explorada, o valor  $Q_t(a)$ tender√° para $R_{max}$ √† medida que $t$ aumenta e o efeito do valor inicial for diminuindo. Caso outra a√ß√£o seja explorada, os valores de $Q_t(a)$ poder√£o variar mais rapidamente. Isso garante que, eventualmente, todas as a√ß√µes ser√£o exploradas. $\blacksquare$

**Desaparecimento do Vi√©s Inicial**:
√â crucial notar que, para os m√©todos de m√©dia amostral, o vi√©s inicial desaparece quando todas as a√ß√µes s√£o selecionadas pelo menos uma vez. Isso ocorre porque cada $Q_t(a)$ √©, eventualmente, baseado em todas as recompensas observadas, e assim, o efeito da inicializa√ß√£o inicial √© reduzido √† medida que mais dados s√£o coletados.

**Teorema 1:** *Se todas as a√ß√µes em um problema de k-armed bandits s√£o selecionadas um n√∫mero infinito de vezes, ent√£o o vi√©s inicial induzido pela escolha de $Q_1(a)$ desaparece para o m√©todo de m√©dia amostral, e $Q_t(a)$ converge para $q_*(a)$ para cada a√ß√£o $a$.*

*Prova:* Conforme demonstrado no Lemma 1, o m√©todo de m√©dia amostral garante a converg√™ncia para o valor real da a√ß√£o, $q_*(a)$, √† medida que o n√∫mero de amostras tende ao infinito. Se cada a√ß√£o √© selecionada um n√∫mero infinito de vezes, ent√£o $N_t(a)$ tende ao infinito para cada $a$. Assim, a influ√™ncia do valor inicial $Q_1(a)$ em $Q_t(a)$ desaparece, e  $Q_t(a)$ se aproxima de $q_*(a)$ pela Lei dos Grandes N√∫meros. $\blacksquare$

> üí° **Exemplo Num√©rico:** Continuando com o 3-armed bandit, ap√≥s 1000 itera√ß√µes onde cada a√ß√£o √© selecionada aproximadamente 300 vezes, os valores $Q_t(a)$ para cada a√ß√£o se aproximar√£o de seus valores verdadeiros $q_*(a)$. Por exemplo, se $q_*(a_1) = 1$, $q_*(a_2) = 2$ e $q_*(a_3) = 0.1$, teremos algo como $Q_t(a_1) \approx 1$, $Q_t(a_2) \approx 2$, e $Q_t(a_3) \approx 0.1$. O vi√©s inicial, devido a valores $Q_1(a)$ diferentes de zero ou otimistas, ter√° um efeito desprez√≠vel no c√°lculo de $Q_t(a)$, demonstrando o desaparecimento do vi√©s inicial.

```mermaid
graph LR
    A[/"A√ß√µes selecionadas muitas vezes"/] --> B("Itera√ß√µes >> 0");
    B --> C("$Q_t(a)$ se aproximam de $q_*(a)$");
    C --> D[/"Vi√©s inicial se torna desprez√≠vel"/];
    style A fill:#afa,stroke:#333,stroke-width:2px
```

**Observa√ß√£o 1:** *A converg√™ncia do Teorema 1 √© uma converg√™ncia assint√≥tica, ou seja, ela √© garantida apenas no limite quando o n√∫mero de intera√ß√µes tende ao infinito. Na pr√°tica, o vi√©s inicial pode ainda afetar o desempenho do aprendizado em um n√∫mero finito de passos.*

### Conclus√£o

Em suma, os m√©todos de m√©dia amostral s√£o inicialmente tendenciosos devido √† inicializa√ß√£o de $Q_1(a)$. Este vi√©s, embora tempor√°rio, desempenha um papel significativo na fase inicial de aprendizado. O uso da inicializa√ß√£o otimista, explorando esse vi√©s inicial, pode ser uma estrat√©gia eficaz para incentivar a explora√ß√£o e melhorar o desempenho a longo prazo. No entanto, √© importante notar que m√©todos que usam um passo de aprendizagem constante podem n√£o eliminar esse vi√©s completamente [^12]. O estudo e o entendimento de como a inicializa√ß√£o afeta o aprendizado √© fundamental para desenvolver algoritmos robustos e eficientes em aprendizado por refor√ßo.

### Refer√™ncias

[^12]: "All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, Q1(a). In the language of statistics, these methods are biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant a, the bias is permanent, though decreasing over time as given by (2.6). In practice, this kind of bias is usually not a problem and can sometimes be very helpful. The downside is that the initial estimates become, in effect, a set of parameters that must be picked by the user, if only to set them all to zero." *(Trecho de Multi-armed Bandits)*
[^4]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: Qt(a) = sum of rewards when a taken prior to t / number of times a taken prior to t" *(Trecho de Multi-armed Bandits)*
