## Optimistic Initial Values in Multi-armed Bandits

### Introdu√ß√£o
Em problemas de **multi-armed bandits**, a explora√ß√£o e a explota√ß√£o representam um dilema fundamental. M√©todos que aprendem os valores das a√ß√µes s√£o influenciados por suas estimativas iniciais, e essa influ√™ncia pode persistir ao longo do tempo, especialmente em m√©todos com um par√¢metro de taxa de aprendizado constante [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). M√©todos de m√©dia amostral acabam por remover esse vi√©s assim que todas as a√ß√µes s√£o selecionadas pelo menos uma vez, mas para m√©todos com taxa de aprendizado constante, o vi√©s se torna permanente, diminuindo ao longo do tempo como definido em [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). O uso de **valores iniciais otimistas** √© uma t√©cnica que se aproveita desse vi√©s para promover uma maior explora√ß√£o, direcionando o agente a testar diferentes a√ß√µes e, eventualmente, convergir para o comportamento √≥timo [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

**Proposi√ß√£o 1.** *A persist√™ncia do vi√©s inicial em m√©todos com taxa de aprendizado constante pode ser modelada como uma fun√ß√£o exponencial decrescente, onde o vi√©s inicial √© progressivamente reduzido a cada passo de aprendizado, mas nunca totalmente eliminado*. Esta persist√™ncia implica que o valor inicial, mesmo que gradualmente diminu√≠do, continuar√° a influenciar a estimativa de valor ao longo do tempo.

```mermaid
graph LR
    A["Valor Inicial Q_1(a)"] --> B("Passo de Aprendizado t")
    B --> C("Valor Estimado Q_t(a)")
    C --> D("Vi√©s Persistente")
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    linkStyle 0,1,2 stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Suponha que temos um m√©todo com taxa de aprendizado $\alpha = 0.1$ e um valor inicial $Q_1(a) = 5$. Se a recompensa real de uma a√ß√£o $a$ √© $q_*(a) = 1$, a atualiza√ß√£o do valor estimado $Q_t(a)$ no tempo $t$ √© dada por $Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]$.  Na primeira itera√ß√£o, onde $R_1$ √© amostrada de uma distribui√ß√£o normal com m√©dia 1 e vari√¢ncia 1, digamos que $R_1 = 1.2$. Ent√£o:
>
> $Q_2(a) = 5 + 0.1 * (1.2 - 5) = 5 + 0.1 * (-3.8) = 5 - 0.38 = 4.62$
>
> O vi√©s inicial de 5 diminuiu para 4.62. Ap√≥s v√°rias atualiza√ß√µes, esse valor de $Q(a)$ se aproximar√° do valor real, mas sem nunca eliminar o vi√©s inicial por completo. A persist√™ncia desse vi√©s inicial afeta a converg√™ncia, incentivando a explora√ß√£o.

### Conceitos Fundamentais
A ideia central por tr√°s do uso de **valores iniciais otimistas** √© iniciar as estimativas de valor de a√ß√£o, $Q_1(a)$, com um valor alto. Esse valor inicial alto cria uma esp√©cie de vi√©s que for√ßa o agente a explorar o espa√ßo de a√ß√µes para descobrir o que realmente acontece no ambiente [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). No contexto de um problema de bandit de $k$-bra√ßos, o valor real de cada a√ß√£o, $q_*(a)$, √© amostrado de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1 [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). Definir $Q_1(a)$ para um valor de +5 (ou seja, um valor "otimista") representa, portanto, um vi√©s significativo [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3).

**Lema 1.** *Dado um problema de bandit k-bra√ßos onde as recompensas seguem uma distribui√ß√£o normal com m√©dia $\mu_a$ e vari√¢ncia $\sigma^2_a$ para cada a√ß√£o $a$, e usando um valor inicial $Q_1(a) = V_0$, o vi√©s inicial introduz uma diferen√ßa entre a estimativa inicial e o valor esperado real, ou seja, $|Q_1(a) - \mu_a| = |V_0 - \mu_a|$.* Esta diferen√ßa √© o que impulsiona a explora√ß√£o inicial, especialmente quando $V_0$ √© maior do que o valor m√©dio de qualquer a√ß√£o.

```mermaid
graph LR
    A["Valor Inicial Q_1(a) = V_0"] --> B("Valor Esperado Real Œº_a")
    B --> C("Vi√©s Inicial |V_0 - Œº_a|")
    style A fill:#aaf,stroke:#333,stroke-width:2px
    style C fill:#bbf,stroke:#333,stroke-width:2px
    linkStyle 0,1 stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Considere um bandit de 3 bra√ßos (k=3), onde os valores reais de cada a√ß√£o s√£o $\mu_1 = -1$, $\mu_2 = 0$, e $\mu_3 = 1$, respectivamente, amostrados de distribui√ß√µes normais com vari√¢ncia $\sigma^2 = 1$. Definindo $V_0 = 5$, temos os seguintes vieses iniciais para cada bra√ßo:
>
>  *   $|Q_1(1) - \mu_1| = |5 - (-1)| = 6$
>  *   $|Q_1(2) - \mu_2| = |5 - 0| = 5$
>  *   $|Q_1(3) - \mu_3| = |5 - 1| = 4$
>
> O maior vi√©s inicial √© para o bra√ßo 1, que tem o menor valor real, enquanto o menor vi√©s √© para o bra√ßo 3, com o maior valor real. Esse vi√©s inicial for√ßa o agente a explorar todos os bra√ßos, pois os valores estimados iniciais s√£o maiores que todos os valores reais.

Quando o agente escolhe uma a√ß√£o inicial e recebe uma recompensa, essa recompensa √©, em geral, inferior ao valor inicial otimista, levando o agente a ficar "desapontado" com a recompensa recebida [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). Isso estimula o agente a escolher outras a√ß√µes, de forma que, eventualmente, todas as a√ß√µes ser√£o testadas v√°rias vezes antes que os valores estimados convirjam [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). Mesmo utilizando uma pol√≠tica puramente *greedy*, que sempre escolhe a a√ß√£o com o maior valor estimado, a explora√ß√£o acontece, embora essa explora√ß√£o seja tempor√°ria [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3).

**Lema 1.1** *A magnitude da "decep√ß√£o" sofrida pelo agente ap√≥s receber uma recompensa $R_t$ para uma a√ß√£o $a_t$ no tempo $t$, √© diretamente proporcional √† diferen√ßa entre o valor inicial otimista e o valor esperado da a√ß√£o: $|Q_1(a_t) - R_t|$.* Isso significa que quanto mais otimista for o valor inicial, maior ser√° a tend√™ncia do agente em experimentar outras a√ß√µes ap√≥s a primeira intera√ß√£o.

```mermaid
sequenceDiagram
    participant Agente
    participant Ambiente
    Agente->>Ambiente: Seleciona a√ß√£o a_t
    Ambiente-->>Agente: Recompensa R_t
    Agente-->>Agente: Calcula "Decep√ß√£o" |Q_1(a_t) - R_t|
    Agente-->>Agente: Decide por nova a√ß√£o
    style Agente fill:#ddf,stroke:#333,stroke-width:2px
    style Ambiente fill:#eee,stroke:#333,stroke-width:2px
    
```

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, onde $Q_1(a) = 5$ para todos os bra√ßos. Suponha que o agente selecione o bra√ßo 1 na primeira intera√ß√£o, onde o valor real $\mu_1 = -1$. Uma poss√≠vel recompensa amostrada da distribui√ß√£o normal com $\mu_1 = -1$ e $\sigma^2 = 1$ seria $R_1 = -0.5$. A "decep√ß√£o" seria $|5 - (-0.5)| = 5.5$.  Se o agente selecionar o bra√ßo 3, onde $\mu_3 = 1$, e receber uma recompensa $R_1 = 0.8$, a "decep√ß√£o" seria $|5 - 0.8| = 4.2$. A maior "decep√ß√£o" leva o agente a explorar a√ß√µes diferentes em busca de melhores recompensas.

Para entender o efeito, considere um agente que usa o m√©todo **Œµ-greedy**, que explora com uma probabilidade Œµ e explora com probabilidade (1-Œµ), e o m√©todo *greedy* com valores iniciais otimistas. A Figura 2.3 no texto [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3) ilustra uma compara√ß√£o. Inicialmente, o m√©todo otimista tem desempenho inferior ao m√©todo **Œµ-greedy** porque explora mais, mas acaba superando o m√©todo **Œµ-greedy** porque sua explora√ß√£o diminui com o tempo [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). O uso de valores iniciais otimistas fornece uma maneira simples e eficaz de promover a explora√ß√£o em ambientes estacion√°rios [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). √â importante notar, no entanto, que essa abordagem n√£o √© adequada para ambientes n√£o estacion√°rios, onde a necessidade de explora√ß√£o pode se renovar com o tempo [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3).

> üí° **Exemplo Num√©rico:** Vamos simular um cen√°rio com 10 bra√ßos (k=10), onde as recompensas reais $q_*(a)$ s√£o sorteadas de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1. Vamos comparar o desempenho de um m√©todo Œµ-greedy com Œµ = 0.1 e um m√©todo greedy com valores iniciais otimistas (Q1(a)=5).
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> def generate_rewards(k):
>     return np.random.normal(0, 1, k)
>
> def epsilon_greedy(q_values, epsilon):
>     if np.random.rand() < epsilon:
>         return np.random.choice(len(q_values))
>     else:
>         return np.argmax(q_values)
>
> def update_q(q_values, action, reward, alpha):
>     q_values[action] = q_values[action] + alpha * (reward - q_values[action])
>
> def run_experiment(k, steps, epsilon, initial_value = 0, alpha = 0.1):
>     true_rewards = generate_rewards(k)
>     q_values = np.full(k, initial_value, dtype = float)
>     rewards_per_step = []
>     for _ in range(steps):
>       action = epsilon_greedy(q_values, epsilon) if epsilon != None else np.argmax(q_values)
>       reward = np.random.normal(true_rewards[action], 1)
>       update_q(q_values, action, reward, alpha)
>       rewards_per_step.append(reward)
>
>     return rewards_per_step
>
> steps = 1000
> k = 10
>
> epsilon_rewards = run_experiment(k,steps, epsilon = 0.1)
> optimistic_rewards = run_experiment(k, steps, epsilon = None, initial_value = 5)
>
> plt.plot(epsilon_rewards, label = "Œµ-greedy (Œµ=0.1)")
> plt.plot(optimistic_rewards, label = "Greedy com valores iniciais otimistas")
> plt.xlabel("Steps")
> plt.ylabel("Reward")
> plt.legend()
> plt.show()
> ```
>
> Este c√≥digo simula os dois m√©todos e gera um gr√°fico mostrando o desempenho ao longo dos steps. Inicialmente, o m√©todo Œµ-greedy pode ter um desempenho melhor, mas o m√©todo greedy com valores otimistas ir√° super√°-lo com o tempo. Essa simula√ß√£o ilustra o comportamento descrito no texto.

**Teorema 1.** *Em ambientes estacion√°rios, um agente usando valores iniciais otimistas com uma pol√≠tica greedy ir√°, sob certas condi√ß√µes, convergir para a a√ß√£o √≥tima com mais rapidez do que um agente usando Œµ-greedy e valores iniciais n√£o otimistas, mas a velocidade exata de converg√™ncia depende da magnitude do valor inicial otimista e da vari√¢ncia das recompensas.* A prova desse teorema envolveria uma an√°lise da rela√ß√£o entre a explora√ß√£o inicial induzida pelos valores otimistas e a subsequente converg√™ncia das estimativas de a√ß√£o, podendo ser feita usando t√©cnicas de an√°lise de converg√™ncia de processos de aprendizado por refor√ßo.

```mermaid
graph LR
    subgraph "Agente com Valores Iniciais Otimistas (Greedy)"
      A["In√≠cio: Q_1(a) = Valor Otimista"] --> B("Explora√ß√£o Inicial")
      B --> C("Converg√™ncia R√°pida para A√ß√£o √ìtima")
    end
    subgraph "Agente Œµ-Greedy (Valores Iniciais Padr√£o)"
      D["In√≠cio: Q_1(a) = Valor Padr√£o"] --> E("Explora√ß√£o Regular com Œµ")
      E --> F("Converg√™ncia Lenta")
    end
     C --> G("A√ß√£o √ìtima")
     F --> G
     style A fill:#afa,stroke:#333,stroke-width:2px
     style D fill:#faa,stroke:#333,stroke-width:2px
     style G fill:#ccf,stroke:#333,stroke-width:2px
     linkStyle 0,1,2,3,4,5 stroke-width:2px
```

### Conclus√£o
Valores iniciais otimistas representam uma estrat√©gia simples mas eficaz para incentivar a explora√ß√£o em **multi-armed bandits**. Ao inicializar as estimativas de valor de a√ß√£o com valores altos, o agente √© inicialmente "desapontado" com as recompensas obtidas, levando a uma explora√ß√£o inicial robusta do espa√ßo de a√ß√µes. Embora essa t√©cnica tenha suas limita√ß√µes, principalmente em ambientes n√£o estacion√°rios, ela serve como um valioso exemplo de como o vi√©s inicial pode ser usado para melhorar o comportamento explorat√≥rio de agentes de aprendizado por refor√ßo [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

**Corol√°rio 1.** *O uso de valores iniciais otimistas, em combina√ß√£o com m√©todos que aprendem as taxas de aprendizagem ao longo do tempo, pode potencialmente mitigar os problemas encontrados em ambientes n√£o-estacion√°rios, ao permitir que a explora√ß√£o seja ajustada de forma din√¢mica com o tempo.* Isso sugere que o aprendizado adaptativo das taxas de aprendizagem combinadas com a explora√ß√£o inicial induzida por valores otimistas pode levar a uma melhor adapta√ß√£o em ambientes din√¢micos.

### Refer√™ncias
[^1]: "All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, Q1(a). In the language of statistics, these methods are biased by their initial estimates."
[^2]: "For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant a, the bias is permanent, though decreasing over time as given by (2.6)."
[^3]: "Suppose that instead of setting the initial action values to zero, as we did in the 10-armed testbed, we set them all to +5. Recall that the q*(a) in this problem are selected from a normal distribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic."
