## O Impacto da Inicializa√ß√£o Otimista no Aprendizado em Multi-Armed Bandits

### Introdu√ß√£o
O aprendizado por refor√ßo (RL) se distingue de outros tipos de aprendizado por utilizar informa√ß√µes de treinamento que *avaliam* as a√ß√µes tomadas, em vez de *instru√≠rem* por meio de a√ß√µes corretas. Isso cria a necessidade de **explora√ß√£o ativa**, uma busca expl√≠cita por um bom comportamento [1]. Dentro deste contexto, o problema do **k-armed bandit** oferece um cen√°rio simplificado para estudar o *aspecto avaliativo* do RL, onde um agente deve repetidamente escolher entre k op√ß√µes (a√ß√µes), cada uma oferecendo uma recompensa num√©rica de uma distribui√ß√£o de probabilidade estacion√°ria [1]. O objetivo √© maximizar a recompensa total esperada ao longo do tempo, um desafio que envolve o **trade-off explora√ß√£o-explota√ß√£o** [1].

### Conceitos Fundamentais
Os m√©todos de **action-value** estimam os valores das a√ß√µes com base nas recompensas obtidas. O valor verdadeiro de uma a√ß√£o √© a recompensa m√©dia quando essa a√ß√£o √© selecionada. Uma forma natural de estimar isso √© usar a m√©dia das recompensas recebidas, conhecida como **sample-average method**, dada por:
$$ Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}} $$ [1], onde $\mathbb{1}_{\text{predicate}}$ √© uma vari√°vel aleat√≥ria que assume 1 se o predicado for verdadeiro e 0 caso contr√°rio, $R_i$ √© a recompensa no passo *i*, e $A_i$ √© a a√ß√£o tomada no passo *i*. Em contrapartida,  $Q_t(a)$ representa o valor estimado da a√ß√£o *a* no instante *t*. A sele√ß√£o de a√ß√µes pode ser feita de forma **greedy**, escolhendo a a√ß√£o com o maior valor estimado, ou de forma **$\epsilon$-greedy**, explorando a√ß√µes n√£o-greedy com probabilidade $\epsilon$ e explorando a a√ß√£o com maior valor estimado com probabilidade $1-\epsilon$ [1].

```mermaid
flowchart LR
    A["A√ß√µes (a)"] --> B("Selecionar a√ß√£o A_t");
    B --> C{Recompensa R_t};
    C --> D["Atualizar Q_t(a) com sample-average"];
    D --> E{Nova itera√ß√£o t+1};
    E --> B;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de 3-armed bandit (k=3). As a√ß√µes s√£o A, B e C. Ap√≥s algumas itera√ß√µes, temos os seguintes resultados:
>
> - A√ß√£o A foi selecionada 3 vezes com recompensas: 2, 3, 4.
> - A√ß√£o B foi selecionada 2 vezes com recompensas: 1, 2.
> - A√ß√£o C foi selecionada 1 vez com recompensa: 5.
>
> Usando o sample-average method, calculamos os valores estimados:
>
>   $Q_t(A) = (2 + 3 + 4) / 3 = 3$
>
>   $Q_t(B) = (1 + 2) / 2 = 1.5$
>
>   $Q_t(C) = 5 / 1 = 5$
>
>  Se usarmos uma abordagem greedy, a pr√≥xima a√ß√£o selecionada seria C, pois tem o maior valor estimado. Se usarmos um $\epsilon$-greedy com $\epsilon=0.1$, haver√° uma probabilidade de 10% de escolhermos A ou B aleatoriamente, e 90% de escolher C.

Entretanto, a inicializa√ß√£o das estimativas de valor da a√ß√£o, $Q_1(a)$, desempenha um papel crucial no processo de aprendizado. Os m√©todos descritos at√© agora dependem, em alguma medida, dessas estimativas iniciais [1]. Em termos estat√≠sticos, os m√©todos s√£o *enviesados* por essas estimativas iniciais. M√©todos com a m√©dia de amostras eliminam esse vi√©s ap√≥s todas as a√ß√µes serem selecionadas pelo menos uma vez. Contudo, m√©todos com taxa de aprendizado constante,  $\alpha$, mant√©m o vi√©s, mas ele diminui ao longo do tempo [1].

A inicializa√ß√£o otimista dos valores de a√ß√£o √© uma t√©cnica que incentiva a explora√ß√£o [1]. Ao inv√©s de inicializar os valores de a√ß√£o em zero, eles s√£o inicializados em um valor alto, superestimando o potencial das a√ß√µes [1]. Por exemplo, no testbed de 10-armed bandits, onde os valores das a√ß√µes s√£o escolhidos de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1, inicializar todos os $Q_1(a)$ como +5 √© considerado otimista. Isso incentiva o agente a explorar, pois as recompensas obtidas inicialmente ser√£o menores do que as estimativas, levando o agente a tentar outras a√ß√µes [1].
```mermaid
flowchart LR
    subgraph "Inicializa√ß√£o Otimista"
        A["Inicializar Q1(a) >> recompensas esperadas"]
    end
        A --> B["Explora√ß√£o inicial"];
        B --> C["Q_t(a) se ajusta √†s recompensas reais"];
        C --> D["A√ß√µes exploradas"]
   style A fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Considere um 4-armed bandit. Se inicializarmos os valores de a√ß√£o como $Q_1(a) = 0$ para todas as a√ß√µes, e as recompensas verdadeiras para as a√ß√µes forem -1, 0, 1, e 2 respectivamente, uma a√ß√£o pode ser considerada boa prematuramente ap√≥s apenas uma ou duas recompensas aleat√≥rias. No entanto, se inicializarmos todos os $Q_1(a) = 5$, todas as a√ß√µes ser√£o inicialmente consideradas como tendo alto potencial. Ap√≥s a primeira a√ß√£o ser selecionada e sua recompensa ser por exemplo -1, a estimativa de valor vai diminuir (para valores pr√≥ximos a 5) fazendo com que o agente explore as outras op√ß√µes. Ap√≥s repetidas atualiza√ß√µes, os valores devem convergir para suas m√©dias reais.

Essa t√©cnica proporciona uma forma simples de fornecer um conhecimento pr√©vio sobre os n√≠veis de recompensa esperados. A inicializa√ß√£o otimista garante que todas as a√ß√µes sejam tentadas v√°rias vezes antes que os valores estimados convirjam [1]. No entanto, √© importante notar que, embora eficaz para problemas estacion√°rios, essa abordagem pode n√£o ser adequada para problemas n√£o-estacion√°rios, nos quais a necessidade de explora√ß√£o pode surgir ao longo do tempo, e a natureza tempor√°ria da explora√ß√£o induzida pela inicializa√ß√£o otimista torna-se limitante [1].

**Lemma 1.** *A inicializa√ß√£o otimista promove a explora√ß√£o inicial em problemas de bandit, mas sua efic√°cia diminui em ambientes n√£o estacion√°rios, onde a necessidade de explora√ß√£o surge dinamicamente.*

*Prova.* Em problemas estacion√°rios, a inicializa√ß√£o otimista direciona o agente a explorar outras a√ß√µes porque a recompensa inicial √© menor do que a estimativa de valor otimista, levando o agente a visitar todas as a√ß√µes. Contudo, em ambientes n√£o-estacion√°rios, onde a distribui√ß√£o de recompensas pode mudar, essa explora√ß√£o inicial torna-se insuficiente. O agente, ap√≥s explorar e convergir para um conjunto de a√ß√µes, n√£o consegue se adaptar √†s mudan√ßas, pois a inicializa√ß√£o √© uma condi√ß√£o inicial, e n√£o um mecanismo de explora√ß√£o cont√≠nuo. $\blacksquare$

> üí° **Exemplo Num√©rico:** Imagine um 2-armed bandit onde inicialmente a a√ß√£o A d√° uma recompensa m√©dia de 1 e a a√ß√£o B d√° uma recompensa m√©dia de 0. Com inicializa√ß√£o otimista (ex: $Q_1(A) = Q_1(B) = 5$), o agente explora ambas as a√ß√µes. Em um dado momento, as recompensas mudam; agora A d√° em m√©dia 0 e B d√° em m√©dia 1. Ap√≥s um per√≠odo de converg√™ncia, o agente tende a continuar explorando a a√ß√£o A. A inicializa√ß√£o otimista n√£o ajuda a descobrir a mudan√ßa na distribui√ß√£o de recompensas, uma vez que a explora√ß√£o inicial j√° foi feita. O agente continua a explorar A devido ao seu hist√≥rico de valores, e n√£o devido √†s mudan√ßas no ambiente.

**Lemma 1.1** *Em ambientes estacion√°rios, ap√≥s um per√≠odo inicial de explora√ß√£o, o uso da inicializa√ß√£o otimista pode ser desnecess√°rio, podendo-se utilizar um m√©todo de explora√ß√£o mais balanceado como o $\epsilon$-greedy.*

*Prova.* A inicializa√ß√£o otimista for√ßa a explora√ß√£o no in√≠cio, mas √† medida que as estimativas de valor se aproximam das recompensas reais, a necessidade de explora√ß√£o induzida pela inicializa√ß√£o diminui. Assim, ap√≥s todas as a√ß√µes serem exploradas, um m√©todo de explora√ß√£o mais controlada como o $\epsilon$-greedy com um valor $\epsilon$ adequado pode ser mais eficiente para encontrar o equil√≠brio entre explora√ß√£o e explota√ß√£o e, ao mesmo tempo, continuar a melhorar as estimativas de valor. $\blacksquare$

> üí° **Exemplo Num√©rico:** Num problema estacion√°rio com 3 a√ß√µes, ap√≥s um certo n√∫mero de itera√ß√µes com inicializa√ß√£o otimista, o agente j√° explorou todas as a√ß√µes e tem uma boa estimativa de seus valores: $Q_t(A) = 2.1$, $Q_t(B) = 0.5$, $Q_t(C) = 1.2$. Neste ponto, uma abordagem $\epsilon$-greedy com $\epsilon = 0.1$  seria eficiente; 90% das vezes o agente escolheria A (explora√ß√£o) e 10% das vezes escolheria B ou C, mantendo um bom equil√≠brio entre explora√ß√£o e explota√ß√£o, e melhorando sua estimativa dos valores das a√ß√µes. O uso da inicializa√ß√£o otimista j√° n√£o √© mais t√£o necess√°rio, pois j√° cumpriu seu papel inicial.

**Proposi√ß√£o 1.** *O valor da inicializa√ß√£o otimista depende da diferen√ßa entre a estimativa inicial e os valores de recompensa reais.*

*Prova.* Se a inicializa√ß√£o for muito otimista (muito maior que as recompensas esperadas), o agente explorar√° vigorosamente no in√≠cio. No entanto, se a inicializa√ß√£o for muito pr√≥xima ou abaixo dos valores de recompensa esperados, a fase de explora√ß√£o ser√° menor ou poder√° nem ocorrer. Assim, a diferen√ßa entre a inicializa√ß√£o e os valores de recompensa reais determina a intensidade e a dura√ß√£o da explora√ß√£o. $\blacksquare$
```mermaid
graph LR
    A["Inicializa√ß√£o Otimista Q1(a)"] --> B{"Recompensas Reais"};
    B -->|delta alta| C["Explora√ß√£o Vigorosa"];
    B -->|delta baixa| D["Pouca Explora√ß√£o"];
    style A fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Num 5-armed bandit com valores de recompensa reais entre -2 e 2, se inicializarmos os valores em $Q_1(a) = 10$, haver√° uma fase de explora√ß√£o inicial grande porque a diferen√ßa entre as estimativas iniciais e as recompensas reais √© grande. Se inicializarmos em $Q_1(a) = 1$, a explora√ß√£o inicial ser√° menor, e se inicializarmos em $Q_1(a) = -5$ n√£o haver√° explora√ß√£o inicial. A diferen√ßa entre a inicializa√ß√£o e o valor das recompensas reais controla a intensidade da explora√ß√£o.

Al√©m disso, para abordar as limita√ß√µes da inicializa√ß√£o otimista em ambientes n√£o-estacion√°rios, outras t√©cnicas podem ser empregadas. Uma dessas t√©cnicas √© o **Upper Confidence Bound (UCB)**, que mant√©m uma estimativa da incerteza associada a cada a√ß√£o e encoraja a explora√ß√£o de a√ß√µes com alta incerteza. Este m√©todo, ao contr√°rio da inicializa√ß√£o otimista, n√£o depende de uma condi√ß√£o inicial, mas sim do hist√≥rico de recompensas.
```mermaid
flowchart LR
    A["A√ß√µes"] --> B["Estimar incerteza com UCB"];
    B --> C["Escolher a√ß√£o com maior incerteza"];
    C --> D{"Recompensa R_t"};
    D --> E["Atualizar estimativa de valor e incerteza"];
    E --> A;
     style B fill:#ccf,stroke:#333,stroke-width:2px
     style E fill:#ccf,stroke:#333,stroke-width:2px
```

**Teorema 1.** *O algoritmo UCB, ao considerar a incerteza na estimativa das a√ß√µes, proporciona uma abordagem de explora√ß√£o mais adaptativa em ambientes n√£o-estacion√°rios, em compara√ß√£o com a inicializa√ß√£o otimista.*

*Prova (Esbo√ßo)*. O UCB utiliza um limite de confian√ßa superior para selecionar a√ß√µes, o que leva o algoritmo a explorar a√ß√µes com alta incerteza, mesmo que suas estimativas de valor sejam baixas no momento. Em ambientes n√£o-estacion√°rios, isso permite ao agente revisitar a√ß√µes que podem ter se tornado mais promissoras devido a mudan√ßas no ambiente. A inicializa√ß√£o otimista, por outro lado, s√≥ atua na explora√ß√£o inicial. Quando as recompensas mudam, ela n√£o prov√™ um mecanismo para adaptar a explora√ß√£o. Portanto, o UCB √© uma melhor abordagem para ambientes n√£o-estacion√°rios. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um cen√°rio n√£o-estacion√°rio de 2-armed bandit. Inicialmente, a a√ß√£o A tem uma recompensa m√©dia de 1 e a a√ß√£o B tem uma recompensa m√©dia de 0. Depois de um tempo, as recompensas se invertem (A passa a dar 0 e B passa a dar 1). Com UCB, as a√ß√µes menos exploradas (com maior incerteza) ser√£o selecionadas. No in√≠cio A √© selecionada mais vezes, mas quando B come√ßa a gerar mais recompensas, o UCB estimar√° uma incerteza maior para B, e a a√ß√£o ser√° selecionada. O UCB √© adaptativo, pois revisita as a√ß√µes que se tornaram promissoras. O UCB n√£o sofre das limita√ß√µes da inicializa√ß√£o otimista pois se adapta a mudan√ßas.

### Conclus√£o
As estimativas iniciais de valor da a√ß√£o, particularmente com inicializa√ß√£o otimista, podem influenciar o aprendizado em problemas de *k-armed bandit*. Inicializar os valores de a√ß√£o de forma otimista incentiva a explora√ß√£o, permitindo que todas as a√ß√µes sejam amostradas v√°rias vezes. Embora eficazes para problemas estacion√°rios, √© essencial considerar as limita√ß√µes desses m√©todos em ambientes n√£o-estacion√°rios, onde outras t√©cnicas de explora√ß√£o mais din√¢micas, como o UCB, podem ser mais apropriadas. Em cen√°rios mais complexos, explorar o espa√ßo de a√ß√µes por meio de inicializa√ß√µes otimistas n√£o √© suficiente, e outros m√©todos precisam ser considerados. A escolha da estrat√©gia de inicializa√ß√£o e explora√ß√£o deve ser cuidadosamente avaliada em fun√ß√£o da natureza do problema.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken.
In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback differs from, and yet can be combined with, instructive feedback.
The particular nonassociative, evaluative feedback problem that we explore is a simple version of the k-armed bandit problem. We use this problem to introduce a number of basic learning methods which we extend in later chapters to apply to the full rein-forcement learning problem. At the end of this chapter, we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when the best action depends on the situation." *(Trecho de Chapter 2: Multi-armed Bandits)*
