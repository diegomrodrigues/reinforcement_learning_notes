## Optimistic Initial Values e N√£o Estacionariedade

### Introdu√ß√£o
O conceito de **Optimistic Initial Values** (Valores Iniciais Otimistas) √© uma t√©cnica utilizada em problemas de *multi-armed bandits* para incentivar a explora√ß√£o. Em vez de inicializar as estimativas de valor da a√ß√£o com zero ou um valor baixo, inicializamos com um valor alto, incitando o agente a explorar novas a√ß√µes com esperan√ßa de obter recompensas maiores do que as estimativas iniciais [^1]. No entanto, como ser√° discutido, essa abordagem enfrenta limita√ß√µes, especialmente em ambientes n√£o estacion√°rios, onde as recompensas e a melhor a√ß√£o podem mudar ao longo do tempo [^1].

### Conceitos Fundamentais
**Valores Iniciais Otimistas** servem como um mecanismo para incentivar a explora√ß√£o. Se as estimativas iniciais das a√ß√µes s√£o altas, o agente se torna mais propenso a experimentar outras a√ß√µes que n√£o as consideradas gananciosas inicialmente, pois suas recompensas iniciais ser√£o, provavelmente, inferiores √†s estimativas iniciais, causando um "desapontamento" e motivando a explora√ß√£o [^1].

Para ilustrar, considere um problema de *k-armed bandit* onde as recompensas s√£o sorteadas de distribui√ß√µes normais com m√©dia e vari√¢ncia espec√≠ficas [^1]. Usando o *10-armed testbed*, um m√©todo *greedy* com estimativas iniciais de a√ß√µes fixadas em +5 incentiva a explora√ß√£o, resultando em uma performance inicial pior do que a de um m√©todo Œµ-greedy com estimativas iniciais em 0. Contudo, a performance do m√©todo otimista supera a do m√©todo Œµ-greedy com o tempo [^1].

> üí° **Exemplo Num√©rico:**
> Suponha um *3-armed bandit* onde as recompensas de cada bra√ßo s√£o sorteadas de distribui√ß√µes normais com as seguintes m√©dias e desvios padr√£o:
>
> - Bra√ßo 1: $\mu_1 = 1$, $\sigma_1 = 1$
> - Bra√ßo 2: $\mu_2 = 2$, $\sigma_2 = 1$
> - Bra√ßo 3: $\mu_3 = 3$, $\sigma_3 = 1$
>
> Inicializamos as estimativas de valor de cada bra√ßo ($\hat{q}(a)$) com:
>
> - M√©todo Otimista: $\hat{q}(a_1) = \hat{q}(a_2) = \hat{q}(a_3) = 5$
> - M√©todo Œµ-greedy: $\hat{q}(a_1) = \hat{q}(a_2) = \hat{q}(a_3) = 0$
>
> No primeiro passo, ambos os m√©todos escolhem uma a√ß√£o aleatoriamente. Vamos supor que o bra√ßo 1 √© selecionado. As recompensas obtidas para cada m√©todo ser√£o menores que a estimativa inicial do m√©todo otimista, induzindo √† explora√ß√£o, enquanto o m√©todo Œµ-greedy n√£o se sente "desapontado" com a recompensa. Ap√≥s algumas itera√ß√µes, o m√©todo otimista come√ßa a ter melhores resultados do que o m√©todo Œµ-greedy.

```mermaid
graph LR
    subgraph "Inicializa√ß√£o das Estimativas"
    A["M√©todo Otimista: q_hat(a) = 5"] --> B["Explora√ß√£o Inicial"]
    C["M√©todo Œµ-greedy: q_hat(a) = 0"] --> D["Explora√ß√£o/Explota√ß√£o"]
    end
    B --> E{"Recompensa < q_hat(a)?"}
    D --> F{"Recompensa"}
    E -- "Sim" --> G["Incentivo √† Explora√ß√£o"]
    E -- "N√£o" --> H["Continua"]
    F --> I["Ajuste q_hat(a)"]
    G --> I
    H --> I
    I --> J["Pr√≥xima Itera√ß√£o"]
    J --> B
     J --> D
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
```

**Lemma 1**. *A performance inicial do m√©todo otimista √© pior do que a do Œµ-greedy*.
**Prova**. Inicialmente, o m√©todo otimista prioriza a explora√ß√£o devido √†s suas altas estimativas iniciais. Isso faz com que as a√ß√µes iniciais selecionadas ofere√ßam recompensas inferiores √†s estimativas, o que impulsiona a explora√ß√£o. J√° o m√©todo Œµ-greedy, com estimativas iniciais menores, tende a fazer uma escolha inicial melhor, explorando com a probabilidade $\epsilon$. Em seguida, o m√©todo otimista tende a melhorar com a redu√ß√£o da explora√ß√£o ao longo do tempo [^1].  $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um cen√°rio onde o m√©todo otimista e o m√©todo Œµ-greedy interagem com um bandit por 5 passos. Suponha que a a√ß√£o √≥tima seja o bra√ßo 3.
>
> | Passo | M√©todo | A√ß√£o Escolhida | Recompensa |  $\hat{q}(a_1)$ | $\hat{q}(a_2)$ | $\hat{q}(a_3)$ | Explora√ß√£o  |
> |------|-----------------|-------------|-----------|----------|----------|----------|----------------|
> | 1    | Otimista        | 1           | 0.8       | 4.9      | 5        | 5        | Alta  |
> | 1    | Œµ-greedy       | 3           | 2.9       | 0        | 0        | 2.9      | Baixa         |
> | 2    | Otimista       | 2           | 1.2        | 4.9      | 4.8        | 5        | Alta   |
> | 2    | Œµ-greedy       | 1           | 0.5       | 0.5      | 0        | 2.9       | Baixa         |
> | 3    | Otimista        | 3           | 3.1       | 4.9      | 4.8      | 4.9      | Moderada        |
> | 3    | Œµ-greedy       | 3           | 3.2       | 0.5      | 0        | 3.05       | Baixa         |
> | 4    | Otimista       | 1           | 0.7       | 4.8      | 4.8      | 4.9      | Moderada         |
> | 4    | Œµ-greedy       | 3           | 2.8       | 0.5     | 0        | 3.08      | Baixa         |
> | 5    | Otimista        | 3           | 2.9       | 4.8      | 4.8     | 4.9       | Baixa        |
> | 5    | Œµ-greedy       | 3           | 3.1       | 0.5      | 0        | 3.12     | Baixa        |
>
> Neste exemplo simplificado, o m√©todo otimista explorou mais no in√≠cio, enquanto o Œµ-greedy se concentrou mais na a√ß√£o que gerou mais recompensa rapidamente. A recompensa m√©dia do m√©todo Œµ-greedy √© maior no come√ßo, mas o m√©todo otimista acabar√° convergindo para uma pol√≠tica melhor.

**Lemma 2**. *A performance do m√©todo otimista pode superar a do Œµ-greedy com o tempo.*
**Prova**. √Ä medida que o m√©todo otimista explora, as estimativas de valor das a√ß√µes se ajustam √†s recompensas obtidas, e a explora√ß√£o diminui gradualmente. No longo prazo, ap√≥s explorar v√°rias a√ß√µes, o m√©todo otimista pode convergir para uma pol√≠tica melhor do que a do m√©todo Œµ-greedy que continua explorando indefinidamente [^1].  $\blacksquare$

```mermaid
graph LR
    subgraph "Evolu√ß√£o do Desempenho"
    A["M√©todo Otimista"] --> B("Explora√ß√£o Inicial Alta")
    B --> C("Ajuste das Estimativas")
    C --> D("Explora√ß√£o Diminui")
    D --> E("Converg√™ncia para Pol√≠tica Melhor")
    
    F["M√©todo Œµ-greedy"] --> G("Explora√ß√£o Constante com Œµ")
    G --> H("Ajuste das Estimativas")
    H --> I("Pode n√£o Convergir para a Pol√≠tica √ìtima")
   end
     E --> J("Desempenho Superior (Longo Prazo)")
     I --> K("Desempenho Potencialmente Sub-√ìtimo")

    J --> L[Desempenho a Longo Prazo]
    K --> L

     style A fill:#f9f,stroke:#333,stroke-width:2px
     style F fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, vamos assumir que ap√≥s 100 itera√ß√µes, as estimativas dos valores das a√ß√µes convergem para o seguinte:
>
> **M√©todo Otimista:**
>
> - $\hat{q}(a_1) = 1.1$
> - $\hat{q}(a_2) = 1.9$
> - $\hat{q}(a_3) = 3.0$
>
> **M√©todo Œµ-greedy:**
>
> - $\hat{q}(a_1) = 1.0$
> - $\hat{q}(a_2) = 1.8$
> - $\hat{q}(a_3) = 2.8$
>
> O m√©todo otimista, ap√≥s explorar mais inicialmente, converge para um valor de $\hat{q}(a_3)$ mais pr√≥ximo do valor real (que no nosso exemplo seria 3), j√° o m√©todo Œµ-greedy ainda n√£o teve tempo suficiente para a explora√ß√£o e sua estimativa √© menor. Em um cen√°rio estacion√°rio, o m√©todo otimista tende a obter melhores recompensas no longo prazo.

**Lemma 2.1**. *O desempenho assint√≥tico do m√©todo otimista em um ambiente estacion√°rio com um n√∫mero finito de a√ß√µes √© limitado pela pol√≠tica √≥tima.*
**Prova**. Em um ambiente estacion√°rio, ap√≥s um per√≠odo de explora√ß√£o inicial, as estimativas de valor das a√ß√µes tendem a se aproximar dos seus valores reais. A explora√ß√£o do m√©todo otimista diminui com o tempo, levando-o a convergir para uma pol√≠tica que se aproxima da pol√≠tica √≥tima. Como a melhor pol√≠tica √© fixa e o m√©todo se torna menos explorador, o desempenho n√£o pode ser arbitrariamente melhor do que o desempenho da melhor pol√≠tica. $\blacksquare$

No entanto, essa abordagem tem s√©rias limita√ß√µes. O m√©todo *Optimistic Initial Values* √©, essencialmente, dependente das condi√ß√µes iniciais. Em ambientes estacion√°rios, essa estrat√©gia pode ser eficaz para uma fase inicial de explora√ß√£o. Mas, se o problema muda (ou seja, torna-se n√£o estacion√°rio), esse m√©todo n√£o consegue se adaptar bem, porque seu incentivo √† explora√ß√£o √© inerentemente tempor√°rio [^1]. O in√≠cio do processo √© um evento √∫nico e n√£o deve receber uma aten√ß√£o excessiva [^1]. Os m√©todos de sample-average, que tratam o in√≠cio do tempo como um evento especial, com todos os pesos iguais, tamb√©m s√£o afetados por esse problema [^1].

**Proposi√ß√£o 3.** *Em ambientes n√£o-estacion√°rios, o desempenho do m√©todo com valores iniciais otimistas pode degradar-se rapidamente.*
**Prova.** Em ambientes n√£o-estacion√°rios, os valores √≥timos das a√ß√µes variam ao longo do tempo. O m√©todo otimista, ao convergir para um conjunto de a√ß√µes que foram consideradas boas em um dado momento, pode n√£o ser capaz de acompanhar as mudan√ßas nas a√ß√µes √≥timas. Sua explora√ß√£o √© limitada a uma fase inicial, e quando as condi√ß√µes mudam, o m√©todo pode permanecer preso em a√ß√µes sub-√≥timas, levando √† degrada√ß√£o da performance. $\blacksquare$

```mermaid
graph LR
 subgraph "Ambientes N√£o-Estacion√°rios"
    A["Mudan√ßa nas A√ß√µes √ìtimas"] --> B("M√©todo Otimista Converte para A√ß√µes Sub-√ìtimas (Inicialmente)")
    B --> C("Explora√ß√£o Inicial Limitada")
    C --> D("Incapacidade de Adaptar")
    D --> E("Degrada√ß√£o da Performance")
 end
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Considere um *2-armed bandit* em um ambiente n√£o-estacion√°rio. As m√©dias das recompensas dos bra√ßos mudam a cada 100 passos:
>
> - Passos 1-100: Bra√ßo 1 $\mu_1 = 1$, Bra√ßo 2 $\mu_2 = 2$
> - Passos 101-200: Bra√ßo 1 $\mu_1 = 3$, Bra√ßo 2 $\mu_2 = 1$
>
> Inicialmente, o m√©todo otimista ir√° explorar e convergir para o bra√ßo 2. No entanto, ap√≥s 100 passos, o bra√ßo 1 se torna o √≥timo, mas o m√©todo otimista n√£o tem um mecanismo para revisitar sua explora√ß√£o. A performance do m√©todo otimista ir√° degradar-se, pois ele n√£o ir√° explorar a nova melhor a√ß√£o, ficando "preso" √† a√ß√£o √≥tima do primeiro per√≠odo.

Em um cen√°rio n√£o estacion√°rio, onde as melhores a√ß√µes podem mudar com o tempo, uma abordagem que incentiva a explora√ß√£o apenas no in√≠cio n√£o √© adequada. A necessidade de explorar e a natureza da explora√ß√£o precisam se adaptar √†s mudan√ßas no ambiente, exigindo uma abordagem mais flex√≠vel para o processo de aprendizagem [^1].

**Teorema 4.** *A efic√°cia da explora√ß√£o utilizando valores iniciais otimistas √© inversamente proporcional √† frequ√™ncia e magnitude das mudan√ßas no ambiente n√£o-estacion√°rio.*
**Prova (Esbo√ßo)**.  Em ambientes onde mudan√ßas ocorrem com alta frequ√™ncia e grande magnitude, a vantagem inicial da explora√ß√£o do m√©todo otimista se torna rapidamente obsoleta. As a√ß√µes que inicialmente levavam a recompensas altas podem rapidamente se tornar sub-√≥timas. O m√©todo, por n√£o ter um mecanismo de explora√ß√£o cont√≠nuo, ter√° dificuldades em se adaptar, resultando em um desempenho fraco. Quanto mais din√¢mico o ambiente, menos eficaz ser√° a explora√ß√£o baseada apenas em valores iniciais otimistas. $\blacksquare$

```mermaid
graph LR
    subgraph "Rela√ß√£o com N√£o-Estacionariedade"
    A["Frequ√™ncia e Magnitude das Mudan√ßas"] --> B("Vantagem Inicial da Explora√ß√£o Otimista se Torna Obsoleta")
    B --> C("A√ß√µes se Tornam Sub-√ìtimas Rapidamente")
    C --> D("Dificuldade de Adapta√ß√£o")
    D --> E("Desempenho Fraco")
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Imaginemos um cen√°rio onde a melhor a√ß√£o muda drasticamente a cada 20 passos, em vez de a cada 100 passos.
>
> - Passos 1-20: Bra√ßo 1 $\mu_1 = 1$, Bra√ßo 2 $\mu_2 = 2$
> - Passos 21-40: Bra√ßo 1 $\mu_1 = 4$, Bra√ßo 2 $\mu_2 = 0$
> - Passos 41-60: Bra√ßo 1 $\mu_1 = 0$, Bra√ßo 2 $\mu_2 = 5$
>
> Neste cen√°rio, a r√°pida mudan√ßa das m√©dias de recompensa dos bra√ßos faz com que o m√©todo com valores iniciais otimistas seja ainda menos eficaz. O tempo de converg√™ncia inicial do m√©todo ser√° desperdi√ßado devido √†s mudan√ßas frequentes, n√£o permitindo que ele explore adequadamente o ambiente em constante mudan√ßa, fazendo com que sua performance seja baixa. Em contraste, m√©todos que se adaptam ao ambiente din√¢mico apresentariam melhor desempenho.

### Conclus√£o
Embora a t√©cnica de **Optimistic Initial Values** possa ser √∫til em ambientes estacion√°rios, ela apresenta limita√ß√µes significativas em contextos n√£o estacion√°rios. O m√©todo de *Optimistic Initial Values* √© uma t√©cnica inicial para incentivar a explora√ß√£o, mas n√£o √© uma estrat√©gia abrangente para ambientes din√¢micos. A explora√ß√£o adequada nesses ambientes exige m√©todos que adaptam o processo de aprendizagem √†s mudan√ßas do cen√°rio, em vez de depender de configura√ß√µes iniciais espec√≠ficas, como ser√° explorado em outras se√ß√µes do livro [^1].

### Refer√™ncias
[^1]: "All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, Q1(a). In the language of statistics, these methods are biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant a, the bias is permanent, though decreasing over time as given by (2.6). In practice, this kind of bias is usually not a problem and can sometimes be very helpful. The downside is that the initial estimates become, in effect, a set of parameters that must be picked by the user, if only to set them all to zero. The upside is that they provide an easy way to supply some prior knowledge about what level of rewards can be expected. Initial action values can also be used as a simple way to encourage exploration. Suppose that instead of setting the initial action values to zero, as we did in the 10-armed testbed, we set them all to +5. Recall that the q*(a) in this problem are selected from a normal distribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic. But this optimism encourages action-value methods to explore. Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being ‚Äúdisappointed‚Äù with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time. Figure 2.3 shows the performance on the 10-armed bandit testbed of a greedy method using Q1(a) = +5, for all a. For comparison, also shown is an …õ-greedy method with Q1(a) = 0. Initially, the optimistic method performs worse because it explores more, but eventually it performs better because its exploration decreases with time. We call this technique for encouraging exploration optimistic initial values. We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial conditions in any special way is unlikely to help with the general nonstationary case. The beginning of time occurs only once, and thus we should not focus on it too much. This criticism applies as well to the sample-average methods, which also treat the beginning of time as a special event, averaging all subsequent rewards with equal weights. Nevertheless, all of these methods are very simple, and one of them ‚Äî or some simple combination of them ‚Äî is often adequate in practice. In the rest of this book we make frequent use of several of these simple exploration techniques." *(Trecho de Multi-armed Bandits)*
