## Optimistic Initial Values e o Vi√©s em M√©todos com $\alpha$ Constante

### Introdu√ß√£o

No contexto do aprendizado por refor√ßo, especialmente em problemas de **multi-armed bandits** (bandidos multi-bra√ßos), a maneira como os valores iniciais das a√ß√µes, $Q_1(a)$, s√£o configurados pode influenciar significativamente a explora√ß√£o e o desempenho do algoritmo [^1]. Os m√©todos de estima√ß√£o de valores de a√ß√£o s√£o muitas vezes influenciados por suas estimativas iniciais, gerando um **vi√©s**. M√©todos que usam a m√©dia amostral, como visto na equa√ß√£o $Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}$ [^3], gradualmente eliminam esse vi√©s √† medida que todas as a√ß√µes s√£o selecionadas pelo menos uma vez. Entretanto, para m√©todos com um par√¢metro $\alpha$ constante, esse vi√©s persiste, embora diminua com o tempo, conforme descrito em [^1]. Este cap√≠tulo se aprofundar√° na an√°lise do impacto do vi√©s em m√©todos com $\alpha$ constante, particularmente dentro do contexto de **optimistic initial values** como uma t√©cnica para incentivar a explora√ß√£o, e discutir√° como esse vi√©s afeta o comportamento do aprendizado por refor√ßo.

### Conceitos Fundamentais

**Vi√©s Inicial e M√©todos de M√©dia Amostral:** M√©todos baseados em **sample averages**, como definido na equa√ß√£o (2.1) [^3], calculam a estimativa do valor de uma a√ß√£o $Q_t(a)$ como a m√©dia dos recompensas obtidas ao executar essa a√ß√£o at√© o tempo $t$. Formalmente, essa rela√ß√£o √© dada por [^3]:
$$
Q_t(a) = \frac{\text{soma das recompensas quando a foi escolhida antes de t}}{\text{n√∫mero de vezes que a foi escolhida antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$
onde $\mathbb{1}_{A_i=a}$ √© uma fun√ß√£o indicadora que √© igual a 1 se a a√ß√£o $a$ foi escolhida no tempo $i$, e 0 caso contr√°rio. Conforme o denominador da fra√ß√£o tende ao infinito, a lei dos grandes n√∫meros garante que $Q_t(a)$ converge para $q_*(a)$, o valor verdadeiro da a√ß√£o [^3]. Este m√©todo √© livre de vi√©s, desde que todas as a√ß√µes sejam selecionadas pelo menos uma vez, conforme mencionado no texto [^1].

> üí° **Exemplo Num√©rico:** Imagine um problema de multi-armed bandit com duas a√ß√µes, A e B. As recompensas obtidas s√£o: A = \[1, 2, 1, 3] e B = \[4, 5, 4]. Usando o m√©todo de m√©dia amostral:
>
> *   $Q_4(A) = (1 + 2 + 1 + 3) / 4 = 7 / 4 = 1.75$
> *   $Q_3(B) = (4 + 5 + 4) / 3 = 13 / 3 \approx 4.33$
>
>   Se a a√ß√£o A fosse selecionada mais uma vez e gerasse uma recompensa de 2, ent√£o $Q_5(A)$ seria $(1+2+1+3+2)/5 = 9/5 = 1.8$. Observe como cada nova recompensa impacta a m√©dia, eventualmente convergindo para o valor real da a√ß√£o.

```mermaid
flowchart LR
    subgraph "M√©dia Amostral"
      A["Recompensas da A√ß√£o 'a'"]
      B["N√∫mero de vezes que 'a' foi escolhida"]
      C["Qt(a)"]
      A -->|soma| D
      B -->|conta| E
      D --> F["Divis√£o"]
      E --> F
      F --> C
    end
     style A fill:#f9f,stroke:#333,stroke-width:2px
     style B fill:#f9f,stroke:#333,stroke-width:2px
     style C fill:#ccf,stroke:#333,stroke-width:2px
```

**Lema 1.1:** *Em m√©todos de m√©dia amostral, se uma a√ß√£o n√£o for selecionada, seu valor estimado permanece indefinido, n√£o sendo necessariamente um problema de vi√©s.*

*Prova:* A equa√ß√£o da m√©dia amostral exige que a a√ß√£o *a* seja selecionada pelo menos uma vez para que o denominador $\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}$ seja diferente de zero. Se uma a√ß√£o n√£o for selecionada, o denominador ser√° zero, e $Q_t(a)$ n√£o √© definido. Embora isso possa levar a problemas na pr√°tica, a falta de defini√ß√£o n√£o √© um vi√©s no sentido tradicional, mas uma consequ√™ncia da falta de informa√ß√µes sobre a a√ß√£o. $\blacksquare$

**M√©todos com $\alpha$ Constante e Vi√©s Persistente:** Em contraste, m√©todos que utilizam um par√¢metro de tamanho do passo $\alpha$ constante para atualizar as estimativas dos valores de a√ß√£o, como mostrado na equa√ß√£o (2.5) [^8]:
$$
Q_{n+1} = Q_n + \alpha[R_n - Q_n]
$$
introduzem um vi√©s que persiste ao longo do tempo.  Esta equa√ß√£o atualiza a estimativa $Q_n$ para $Q_{n+1}$ com base na diferen√ßa entre a recompensa recebida $R_n$ e a estimativa atual $Q_n$, ponderada pelo par√¢metro $\alpha$. Esta atualiza√ß√£o iterativa resulta em um vi√©s permanente, que diminui gradualmente conforme indicado pela equa√ß√£o (2.6) [^8]:

$$
Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^n \alpha (1-\alpha)^{n-i} R_i
$$

Esta f√≥rmula mostra que $Q_{n+1}$ √© uma m√©dia ponderada das recompensas passadas $R_i$ e da estimativa inicial $Q_1$, com pesos que decaem exponencialmente com o tempo, como descrito em [^9]. O vi√©s inicial, embora diminua, n√£o desaparece completamente [^1], diferentemente do que acontece com a m√©dia amostral.

> üí° **Exemplo Num√©rico:** Suponha que tenhamos uma a√ß√£o com $Q_1 = 0$ e $\alpha = 0.1$. As recompensas obtidas s√£o $R = [1, 2, 1, 3]$. Vamos calcular $Q_n$ usando a atualiza√ß√£o incremental:
>
> *   $Q_2 = Q_1 + \alpha (R_1 - Q_1) = 0 + 0.1(1 - 0) = 0.1$
> *   $Q_3 = Q_2 + \alpha (R_2 - Q_2) = 0.1 + 0.1(2 - 0.1) = 0.1 + 0.1(1.9) = 0.29$
> *   $Q_4 = Q_3 + \alpha (R_3 - Q_3) = 0.29 + 0.1(1 - 0.29) = 0.29 + 0.1(0.71) = 0.361$
> *   $Q_5 = Q_4 + \alpha (R_4 - Q_4) = 0.361 + 0.1(3 - 0.361) = 0.361 + 0.1(2.639) = 0.6249$
>
> Agora, usando a equa√ß√£o (2.6):
> *   $Q_5 = (1-0.1)^4 * 0 + 0.1(1-0.1)^3 * 1 + 0.1(1-0.1)^2 * 2 + 0.1(1-0.1)^1 * 1 + 0.1(1-0.1)^0 * 3$
> *   $Q_5 = 0 + 0.1(0.729) + 0.1(0.81) * 2 + 0.1(0.9) * 1 + 0.1 * 3$
> *   $Q_5 = 0.0729 + 0.162 + 0.09 + 0.3 = 0.6249$
>
>   Observe como a influ√™ncia do valor inicial $Q_1 = 0$ diminui com o tempo, mas ainda tem um impacto na estimativa. Se $Q_1$ fosse diferente, essa diferen√ßa persistiria, embora com um peso decrescente.

```mermaid
flowchart LR
    subgraph "Atualiza√ß√£o com Œ± constante"
      Qn["Qn"]
      Rn["Rn"]
      alpha["Œ±"]
      Qn -->|"- Qn"| A
      Rn -->|"+"| B
      A & B --> C["Rn - Qn"]
       C-->|"* Œ±"| D
      Qn -->|"+"|E
      D --> E
       E --> Qn1["Qn+1"]
    end
    style Qn fill:#f9f,stroke:#333,stroke-width:2px
    style Qn1 fill:#ccf,stroke:#333,stroke-width:2px
    style Rn fill:#f9f,stroke:#333,stroke-width:2px
    style alpha fill:#f9f,stroke:#333,stroke-width:2px

```

```mermaid
flowchart LR
    subgraph "M√©dia ponderada com Œ± constante"
        Q1["Q1"]
        R["R1...Rn"]
        alpha["Œ±"]
        n["n"]
        Q1 -->|"(1-Œ±)^n"|A
        R -->|$\sum \alpha (1-\alpha)^{n-i}$| B
         A & B --> C["Soma"]
         C --> Qn1["Qn+1"]

    end
        style Q1 fill:#f9f,stroke:#333,stroke-width:2px
    style Qn1 fill:#ccf,stroke:#333,stroke-width:2px
    style R fill:#f9f,stroke:#333,stroke-width:2px
        style alpha fill:#f9f,stroke:#333,stroke-width:2px
        style n fill:#f9f,stroke:#333,stroke-width:2px
```

**Proposi√ß√£o 2:** *A escolha de $\alpha$ afeta a taxa de converg√™ncia e a persist√™ncia do vi√©s em m√©todos com $\alpha$ constante.*

*Prova:* Analisando a equa√ß√£o (2.6), o termo $(1-\alpha)^n$ determina a influ√™ncia do valor inicial $Q_1$. Quando $\alpha$ est√° pr√≥ximo de 1, este termo decai rapidamente para 0, o que significa que o vi√©s inicial diminui rapidamente, e as estimativas se tornam mais sens√≠veis √†s recompensas recentes. Quando $\alpha$ est√° pr√≥ximo de 0, o termo $(1-\alpha)^n$ decai lentamente, fazendo com que o vi√©s inicial persista por mais tempo e as recompensas iniciais tenham um peso maior nas estimativas. Portanto, $\alpha$ controla tanto a velocidade com que o vi√©s inicial desaparece quanto a sensibilidade do m√©todo a recompensas mais recentes. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere dois cen√°rios: $\alpha_1 = 0.1$ e $\alpha_2 = 0.9$, ambos com $Q_1 = 0$ e recompensas $R = [1, 1, 1]$.
>
> Para $\alpha_1 = 0.1$:
> *   $Q_2 = 0 + 0.1(1-0) = 0.1$
> *   $Q_3 = 0.1 + 0.1(1-0.1) = 0.19$
> *   $Q_4 = 0.19 + 0.1(1-0.19) = 0.271$
>
> Para $\alpha_2 = 0.9$:
> *   $Q_2 = 0 + 0.9(1-0) = 0.9$
> *   $Q_3 = 0.9 + 0.9(1-0.9) = 0.99$
> *   $Q_4 = 0.99 + 0.9(1-0.99) = 0.999$
>
> Vemos que com $\alpha_2 = 0.9$, a estimativa $Q_n$ se aproxima mais rapidamente do valor da recompensa (1), mas tamb√©m √© mais sens√≠vel √†s √∫ltimas recompensas. Com $\alpha_1 = 0.1$, a converg√™ncia √© mais lenta, e as recompensas iniciais t√™m um peso maior por mais tempo, resultando em um vi√©s persistente, demonstrando que $\alpha$ controla tanto a velocidade de converg√™ncia como a persist√™ncia do vi√©s.

**Optimistic Initial Values:** Um m√©todo simples, mas eficaz, para incentivar a explora√ß√£o em algoritmos de aprendizado por refor√ßo √© o uso de **optimistic initial values** [^10]. Em vez de inicializar os valores de a√ß√£o com zero, eles s√£o inicializados com um valor alto, digamos +5, em um problema em que as recompensas esperadas est√£o em torno de zero [^10]. Esta abordagem incentiva o algoritmo a explorar novas a√ß√µes, pois os retornos iniciais s√£o tipicamente menores que a estimativa inicial. Ao explorar, o algoritmo eventualmente descobre a√ß√µes melhores, superando o vi√©s inicial [^10].

> üí° **Exemplo Num√©rico:** Considere um problema com duas a√ß√µes, A e B. Inicializamos os valores com $Q_1(A) = 5$ e $Q_1(B) = 5$. Se a a√ß√£o A √© selecionada primeiro e gera uma recompensa $R_1 = 1$ com $\alpha = 0.1$, ent√£o:
> *   $Q_2(A) = 5 + 0.1(1 - 5) = 5 - 0.4 = 4.6$
>
> Se a a√ß√£o B √© selecionada em seguida e gera uma recompensa $R_2 = 2$:
>
> *   $Q_2(B) = 5 + 0.1(2-5) = 5 - 0.3 = 4.7$
>
> O valor alto inicial incentiva o algoritmo a explorar, j√° que as recompensas iniciais (1 e 2) s√£o menores do que as estimativas iniciais (5).

```mermaid
sequenceDiagram
    participant Agente
    participant A√ß√£o_A
    participant A√ß√£o_B
    Agente->>A√ß√£o_A: Seleciona A
    A√ß√£o_A-->>Agente: Recompensa R1=1
    Agente->>A√ß√£o_B: Seleciona B
    A√ß√£o_B-->>Agente: Recompensa R2=2
    Agente->>Agente: Atualiza Q(A) e Q(B) com valores iniciais otimistas
    Note over Agente: Q1(A)=5, Q1(B)=5
    Note over Agente: Q2(A) = 4.6, Q2(B) = 4.7
```

**Impacto do Vi√©s na Explora√ß√£o com Valores Iniciais Otimistas:** O vi√©s causado por $\alpha$ constante pode ser ben√©fico no caso dos **optimistic initial values**, j√° que garante que os valores iniciais continuem a ter alguma influ√™ncia na explora√ß√£o [^10]. No entanto, este vi√©s pode ser problem√°tico em cen√°rios n√£o estacion√°rios, onde a recompensa associada a uma a√ß√£o muda ao longo do tempo, pois o m√©todo tende a manter um peso maior nas recompensas iniciais, como indicado em [^9].

**Lema 1:** *O vi√©s introduzido por um $\alpha$ constante √© persistente, mas diminui com o tempo.*

*Prova:* A equa√ß√£o 2.6 [^8] demonstra que a estimativa $Q_{n+1}$ √© uma m√©dia ponderada da estimativa inicial $Q_1$ e as recompensas passadas $R_i$, onde o peso de $Q_1$ √© $(1-\alpha)^n$. Como $\alpha$ est√° entre (0,1], esse peso diminui com o tempo, mas nunca chega a zero. Isso implica que $Q_1$ sempre ter√° alguma influ√™ncia na estimativa de $Q_{n+1}$, demonstrando que o vi√©s permanece. No entanto, com $n \to \infty$, o peso de $Q_1$ tende a zero, resultando na diminui√ß√£o do vi√©s. $\blacksquare$

**Lema 1.2:** *O vi√©s persistente pode levar a uma converg√™ncia mais lenta, especialmente se os valores iniciais forem significativamente diferentes dos valores verdadeiros das a√ß√µes.*

*Prova:* O vi√©s introduzido por $\alpha$ constante, como demonstrado no Lema 1, significa que a estimativa inicial $Q_1$ tem uma influ√™ncia cont√≠nua nas estimativas. Se $Q_1$ estiver muito distante do valor real da a√ß√£o, $q_*(a)$, as estimativas podem demorar mais tempo para se aproximarem desse valor verdadeiro, resultando em uma converg√™ncia mais lenta. A persist√™ncia desse vi√©s impede uma corre√ß√£o r√°pida da estimativa, especialmente no in√≠cio do aprendizado. $\blacksquare$

**Corol√°rio 1:** *O uso de valores iniciais otimistas, combinado com o vi√©s de m√©todos com $\alpha$ constante, pode ser uma estrat√©gia eficaz de explora√ß√£o em ambientes estacion√°rios, pois promove a busca por novas a√ß√µes devido a estimativas iniciais altas.*

*Deriva√ß√£o:* Como Lemma 1 demonstra, os m√©todos com $\alpha$ constante mant√™m um vi√©s persistente, mas decrescente, atribuindo maior peso √†s recompensas mais recentes. Com valores iniciais otimistas, as a√ß√µes iniciais s√£o ‚Äúdesapontadoras‚Äù porque suas recompensas s√£o menores que as estimativas iniciais, incentivando o algoritmo a explorar outras a√ß√µes. Isso, combinado com o vi√©s dos m√©todos com $\alpha$ constante, garante que a explora√ß√£o inicial seja vigorosa e, ao mesmo tempo, os valores convergem com o tempo. $\blacksquare$

```mermaid
flowchart LR
    subgraph "Explora√ß√£o com valores otimistas e Œ± constante"
        A["Valores Iniciais Otimistas (Q1 alto)"]
        B["M√©todo Œ± Constante"]
        C["Explora√ß√£o Vigorosa"]
        D["Vi√©s Persistente (mas decrescente)"]
        E["Converg√™ncia para a√ß√µes √≥timas"]
        A --> C
        B --> D
        C & D --> E
    end
     style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

**Corol√°rio 1.1:** *Em ambientes n√£o estacion√°rios, a explora√ß√£o incentivada por valores iniciais otimistas pode ser prejudicada pelo vi√©s persistente, impedindo o algoritmo de se adaptar rapidamente a mudan√ßas nas recompensas.*

*Deriva√ß√£o:* Em ambientes n√£o estacion√°rios, os valores √≥timos das a√ß√µes mudam com o tempo. O vi√©s persistente nos m√©todos com $\alpha$ constante, especialmente quando combinado com valores iniciais otimistas, far√° com que o algoritmo continue a dar peso a recompensas iniciais, mesmo quando as recompensas atuais mudam. Essa influ√™ncia das recompensas iniciais pode retardar a converg√™ncia para os novos valores √≥timos, prejudicando a adapta√ß√£o do algoritmo ao ambiente din√¢mico. $\blacksquare$

### Conclus√£o

O vi√©s introduzido por m√©todos que utilizam um par√¢metro $\alpha$ constante para atualizar estimativas de valores de a√ß√£o √© uma caracter√≠stica importante a ser considerada no aprendizado por refor√ßo. Enquanto esse vi√©s √© indesej√°vel em alguns cen√°rios, ele pode ser aproveitado como um mecanismo de explora√ß√£o quando combinado com **optimistic initial values**. Em ambientes estacion√°rios, o vi√©s persistente, mas decrescente, da equa√ß√£o (2.6) [^8] garante que a influ√™ncia das estimativas iniciais otimistas diminua com o tempo, permitindo que o algoritmo eventualmente se concentre nas melhores a√ß√µes. No entanto, em ambientes n√£o estacion√°rios, esse vi√©s pode ser problem√°tico devido ao peso indevido dado a recompensas iniciais e desconsidera√ß√£o das mudan√ßas nas recompensas ao longo do tempo. A escolha entre m√©todos com $\alpha$ constante e **sample averages** deve considerar a natureza do ambiente, estacion√°rio ou n√£o, e a necessidade de manter o vi√©s como forma de explora√ß√£o.

### Refer√™ncias
[^1]: "All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, Q1(a). In the language of statistics, these methods are biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant a, the bias is permanent, though decreasing over time as given by (2.6)."
[^3]: "One natural way to estimate this is by averaging the rewards actually received: Qt(a) = (sum of rewards when a taken prior to t) / (number of times a taken prior to t) = ($\sum_{i=1}^{t-1} R_i1_{A_i=a}$) / ($\sum_{i=1}^{t-1} 1_{A_i=a}$)"
[^8]: "One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule (2.3) for updating an average Qn of the n ‚Äì 1 past rewards is modified to be Qn+1 = Qn + a[Rn - Qn]"
[^9]: "This results in Qn+1 being a weighted average of past rewards and the initial estimate Q1: Qn+1 = ...  = (1 ‚àí a)^nQ1 + $\sum_{i=1}^n a(1 - a)^{n-i}Ri$"
[^10]: "Initial action values can also be used as a simple way to encourage exploration. Suppose that instead of setting the initial action values to zero, as we did in the 10-armed testbed, we set them all to +5."
