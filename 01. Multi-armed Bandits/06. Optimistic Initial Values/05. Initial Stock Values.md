## Optimistic Initial Values para Incentivar a Explora√ß√£o em Multi-Armed Bandits

### Introdu√ß√£o
Em problemas de **reinforcement learning (RL)**, a explora√ß√£o do espa√ßo de a√ß√µes √© essencial para encontrar a pol√≠tica √≥tima. Em cen√°rios de **multi-armed bandits**, onde as a√ß√µes s√£o independentes, essa explora√ß√£o se torna crucial. M√©todos como o *Œµ-greedy* e *UCB (Upper Confidence Bound)* oferecem abordagens para equilibrar a explora√ß√£o e a explota√ß√£o, mas outra t√©cnica, baseada na inicializa√ß√£o dos valores das a√ß√µes, tamb√©m pode ser eficaz [1]. Este cap√≠tulo visa explorar como valores iniciais de a√ß√µes otimistas podem ser utilizados como uma forma de incentivar a explora√ß√£o em **k-armed bandit problems**. Ao longo do texto, veremos como essa abordagem funciona e suas limita√ß√µes.

### Conceitos Fundamentais
**Action-value methods** estimam os valores das a√ß√µes (q*(a)) com base nas recompensas recebidas ao selecion√°-las. Inicialmente, essas estimativas s√£o valores arbitr√°rios, como $Q_1(a)$, e podem influenciar fortemente o comportamento do agente. A inicializa√ß√£o desses valores √© um ponto crucial, e, segundo [1], os m√©todos discutidos s√£o dependentes, em alguma medida, das estimativas iniciais de **action-value**, $Q_1(a)$. No caso do m√©todo *sample-average*, esse vi√©s desaparece quando todas as a√ß√µes s√£o selecionadas ao menos uma vez, mas em m√©todos com passo de aprendizagem constante, como o definido em [2], a polariza√ß√£o √© permanente. Na pr√°tica, esse vi√©s geralmente n√£o √© um problema, e √†s vezes pode ser at√© √∫til. A desvantagem √© que as estimativas iniciais se tornam par√¢metros que precisam ser definidos pelo usu√°rio [1].

**Proposi√ß√£o 1:** *A escolha de valores iniciais $Q_1(a)$ afeta a velocidade e a trajet√≥ria de converg√™ncia do aprendizado em m√©todos de action-value. Valores muito baixos podem levar a uma explora√ß√£o lenta, enquanto valores otimistas podem acelerar a explora√ß√£o inicial.*

**Observa√ß√£o 1:** *A proposi√ß√£o acima destaca a import√¢ncia da inicializa√ß√£o. Uma escolha inadequada pode retardar o aprendizado. O ponto chave √© equilibrar uma explora√ß√£o suficiente no in√≠cio com a eventual converg√™ncia para a a√ß√£o √≥tima.*
```mermaid
flowchart LR
    A[/"Inicializa√ß√£o dos Valores de A√ß√£o "Q_1(a)"/"] --> B{/"Escolha de A√ß√£o"/}
    B -- "Recompensa" R_n --> C[/"Atualiza√ß√£o de "Q_n(a)"/"]
    C --> D{/"Converg√™ncia?"/}
    D -- "Sim" --> E[/"Fim do Aprendizado"/]
    D -- "N√£o" --> B
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

Um modo simples de incentivar a explora√ß√£o √© configurar os valores iniciais das a√ß√µes de forma otimista. Em vez de usar valores nulos ou arbitr√°rios, podemos inicializar $Q_1(a)$ com valores que representem recompensas muito altas, superiores √†s recompensas que o agente espera receber. Essa abordagem incentiva o agente a explorar a√ß√µes diferentes das iniciais por "decep√ß√£o", j√° que as recompensas obtidas, geralmente, ser√£o inferiores √†s estimativas iniciais. Por exemplo, em um cen√°rio em que as recompensas s√£o selecionadas a partir de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1, inicializar $Q_1(a) = +5$ para todas as a√ß√µes seria uma escolha otimista [1].

> üí° **Exemplo Num√©rico:** Considere um cen√°rio com 3 a√ß√µes (A, B, C). Inicializamos os valores das a√ß√µes como $Q_1(A) = 5$, $Q_1(B) = 5$, e $Q_1(C) = 5$. Suponha que ap√≥s a primeira rodada de cada a√ß√£o, o agente recebe as recompensas $R_1(A) = 1$, $R_1(B) = -0.5$, e $R_1(C) = 2$. Com uma taxa de aprendizado $\alpha = 0.1$, as estimativas de valor seriam atualizadas como:
>
> $Q_2(A) = 5 + 0.1*(1-5) = 4.6$
> $Q_2(B) = 5 + 0.1*(-0.5-5) = 4.45$
> $Q_2(C) = 5 + 0.1*(2-5) = 4.7$
>
> Veja que as estimativas diminu√≠ram, incentivando a explora√ß√£o de outras a√ß√µes no pr√≥ximo passo.

**Lema 1:** *Se as recompensas $R_n$ s√£o limitadas superiormente por um valor $R_{max}$, e as estimativas iniciais $Q_1(a)$ s√£o definidas como $Q_1(a) > R_{max}$ para todas as a√ß√µes *a*, ent√£o a primeira recompensa observada para cada a√ß√£o ser√° sempre menor que sua estimativa inicial, induzindo explora√ß√£o.*
*Proof:* Dada a condi√ß√£o $Q_1(a) > R_{max}$, e como $R_n \le R_{max}$, tem-se que $R_n < Q_1(a)$. Portanto, o termo $(R_n - Q_1(a))$ na atualiza√ß√£o do valor da a√ß√£o ser√° negativo, levando a uma redu√ß√£o no valor de $Q_n(a)$.
```mermaid
graph LR
    A[/"Defini√ß√£o:"/] --> B("Q_1(a) > R_{max}");
    B --> C("R_n <= R_{max}");
    C --> D("R_n < Q_1(a)");
    D --> E[/"Atualiza√ß√£o "Q_n(a)" negativa/"];
     style A fill:#fff,stroke:#333,stroke-width:2px
     style E fill:#ccf,stroke:#333,stroke-width:2px
```

Essa t√©cnica de inicializa√ß√£o otimista tem como objetivo induzir o agente a experimentar diversas a√ß√µes antes de convergir, como uma forma de incentivar a explora√ß√£o. De acordo com [1], mesmo se o agente tomar decis√µes *greedy*, essa estrat√©gia produz um montante consider√°vel de explora√ß√£o. O resultado dessa explora√ß√£o √© que todas as a√ß√µes s√£o tentadas algumas vezes, antes que as estimativas de valores convirjam. No entanto, como ressalta o texto, essa t√©cnica √© uma ‚Äútruque simples‚Äù que pode funcionar bem em problemas estacion√°rios, mas n√£o √© uma abordagem geral para incentivar a explora√ß√£o em todos os cen√°rios, principalmente naqueles em que a n√£o estacionariedade faz parte do problema [1]. O fato de focar as condi√ß√µes iniciais √© um problema, principalmente se o ambiente for n√£o estacion√°rio, dado que esse tipo de explora√ß√£o √© inerentemente tempor√°ria.

**Teorema 1:** *Em um problema de multi-armed bandit estacion√°rio com recompensas limitadas, uma inicializa√ß√£o otimista $Q_1(a) > R_{max}$ garante que cada a√ß√£o seja explorada pelo menos uma vez antes que todas as estimativas de a√ß√£o convirjam para seus valores verdadeiros.*
*Proof Strategy:* O lema 1 estabelece que a primeira recompensa observada ser√° menor que a estimativa inicial, induzindo a explora√ß√£o inicial. Em um ambiente estacion√°rio, com o tempo, as estimativas de a√ß√£o convergir√£o. A combina√ß√£o desses fatores garante que todas as a√ß√µes ser√£o experimentadas antes da converg√™ncia.
```mermaid
flowchart LR
    A[/"Lema 1: "R_n < Q_1(a)"/"] --> B[/"Ambiente Estacion√°rio"/]
    B --> C[/"Converg√™ncia de "Q_n(a)"/"]
    A -- "Induz" --> D[/"Explora√ß√£o Inicial"/]
    D --> E[/"Todas as A√ß√µes Exploradas"/]
    C--> E
     style A fill:#fff,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

**Lema 1.1** *A efic√°cia da inicializa√ß√£o otimista √© inversamente proporcional √† vari√¢ncia das recompensas. Quanto maior a vari√¢ncia, menor a previsibilidade das recompensas, tornando a "decep√ß√£o" inicial menos pronunciada e a explora√ß√£o menos direcionada.*
*Proof:* Uma alta vari√¢ncia nas recompensas significa que √© poss√≠vel que uma recompensa seja pr√≥xima ou maior do que os valores iniciais otimistas, diminuindo o efeito da "decep√ß√£o" e, consequentemente, o incentivo √† explora√ß√£o.
```mermaid
graph LR
    A[/"Alta Vari√¢ncia "var(R)"/"] --> B("Recompensas menos previs√≠veis");
    B --> C("Menor 'decep√ß√£o' inicial");
    C --> D("Explora√ß√£o menos direcionada");
     style A fill:#fff,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Vamos comparar dois cen√°rios. No cen√°rio 1, as recompensas seguem uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1. No cen√°rio 2, as recompensas seguem uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 5. Inicializamos $Q_1(a) = 5$ para todas as a√ß√µes em ambos os cen√°rios.
>
> No cen√°rio 1, as recompensas t√≠picas estar√£o na faixa de -3 a 3, significativamente abaixo de 5. A "decep√ß√£o" ser√° clara e o agente explorar√° outras a√ß√µes. No cen√°rio 2, com maior vari√¢ncia, √© mais prov√°vel que algumas recompensas sejam pr√≥ximas ou maiores que 5. Isso reduz a "decep√ß√£o" e torna a explora√ß√£o inicial menos eficaz, pois n√£o h√° uma clara motiva√ß√£o para mudar de a√ß√£o.

**Corol√°rio 1:** *Em ambientes n√£o estacion√°rios, onde os valores das recompensas mudam ao longo do tempo, a inicializa√ß√£o otimista pode n√£o ser suficiente para garantir uma explora√ß√£o adequada a longo prazo. A explora√ß√£o torna-se tempor√°ria, pois os valores otimistas iniciais s√£o eventualmente superados pela mudan√ßa na distribui√ß√£o de recompensas.*
```mermaid
graph LR
    A[/"Ambiente N√£o Estacion√°rio"/"] --> B("Valores das Recompensas Mudam");
    B --> C("Inicializa√ß√£o Otimista Superada");
    C --> D("Explora√ß√£o Tempor√°ria");
    style A fill:#fff,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Considere um cen√°rio n√£o-estacion√°rio onde inicialmente a a√ß√£o A tem uma recompensa m√©dia de 0, enquanto a a√ß√£o B tem uma recompensa m√©dia de -2. Inicializamos $Q_1(A) = Q_1(B) = 5$. Inicialmente, o agente explora as duas a√ß√µes. Ap√≥s 100 itera√ß√µes, a recompensa m√©dia da a√ß√£o B muda para 3. A inicializa√ß√£o otimista n√£o incentivar√° o agente a explorar a a√ß√£o B novamente, pois seu valor inicial (alto) n√£o indica a mudan√ßa na recompensa. Nesse caso, estrat√©gias como Œµ-greedy ou UCB, que continuam explorando, seriam mais adequadas.

**Demonstra√ß√£o Pr√°tica:**
Para demonstrar o efeito da inicializa√ß√£o otimista, considere o problema do *10-armed testbed*. Nesse problema, as recompensas das a√ß√µes s√£o selecionadas a partir de distribui√ß√µes normais com vari√¢ncia 1 e m√©dia q*(a), que por sua vez tamb√©m seguem distribui√ß√µes normais com m√©dia 0 e vari√¢ncia 1. Se inicializarmos $Q_1(a)$ com +5 para todos os *a*, o agente tender√° a explorar mais a√ß√µes no in√≠cio, pois as recompensas obtidas ser√£o inferiores √†s estimativas iniciais, resultando numa "decep√ß√£o" e numa busca por a√ß√µes melhores.

**A seguinte an√°lise mostra a evolu√ß√£o das estimativas em uma abordagem greedy com valores iniciais otimistas.**

Seja a atualiza√ß√£o do valor da a√ß√£o dada por:

$$ Q_{n+1} = Q_n + \alpha(R_n - Q_n) $$

Onde:

$Q_{n+1}$ √© a nova estimativa do valor da a√ß√£o ap√≥s a n-√©sima tentativa.
$Q_n$ √© a estimativa atual do valor da a√ß√£o.
$R_n$ √© a recompensa recebida ap√≥s a n-√©sima tentativa.
$\alpha$ √© a taxa de aprendizado.
```mermaid
flowchart LR
    A[/"Q_{n+1} = Q_n + \alpha(R_n - Q_n)"/"]
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

Considere que $Q_1(a) = 5$, e que as recompensas s√£o valores selecionados de uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1. As recompensas observadas tendem a ser menores que 5, dado que essa distribui√ß√£o tem m√©dia 0.

1. **Primeira tentativa:**
   - O agente toma a a√ß√£o $a_1$. Recebe uma recompensa $R_1$, que provavelmente ser√° menor do que $Q_1(a_1) = 5$.
   - A estimativa de valor para a a√ß√£o $a_1$ √© atualizada: $Q_2(a_1) = 5 + \alpha(R_1 - 5)$.

2. **Segunda tentativa:**
   - O agente toma a a√ß√£o $a_2$. Recebe uma recompensa $R_2$, que tamb√©m ser√° provavelmente menor do que $Q_1(a_2) = 5$.
   - A estimativa de valor para a a√ß√£o $a_2$ √© atualizada: $Q_2(a_2) = 5 + \alpha(R_2 - 5)$.

3. **Terceira tentativa:**
    - O agente pode escolher, novamente, qualquer a√ß√£o, ou seja,  $a_1$ ou $a_2$ ou outra.
   - A estimativa de valor da a√ß√£o selecionada √© atualizada.

Assim, o agente continua a explorar, a tomar a√ß√µes e atualizar os valores das a√ß√µes. O agente fica "desapontado" com as recompensas e troca a a√ß√£o. Com o tempo, essas estimativas convergem para os verdadeiros valores das a√ß√µes q*(a), garantindo que a explora√ß√£o diminua. Isso demonstra o funcionamento da inicializa√ß√£o otimista como um incentivo √† explora√ß√£o no in√≠cio do aprendizado.
```mermaid
sequenceDiagram
    participant Agente
    participant A√ß√£o_a1
    participant A√ß√£o_a2
    participant Q_n
    Agente->>A√ß√£o_a1: Escolhe A√ß√£o a1
    A√ß√£o_a1-->>Agente: Retorna R_1
    Agente->>Q_n: Atualiza Q_2(a1)
    Agente->>A√ß√£o_a2: Escolhe A√ß√£o a2
    A√ß√£o_a2-->>Agente: Retorna R_2
    Agente->>Q_n: Atualiza Q_2(a2)
    loop Continua explorando
        Agente->>Q_n: Escolhe a√ß√£o, recebe R e atualiza Q
    end
    Q_n-->>Agente: Converte para valores verdadeiros
```

> üí° **Exemplo Num√©rico:** Vamos simular as primeiras 5 itera√ß√µes de um agente com 3 a√ß√µes (A, B, C), $\alpha = 0.1$, e valores iniciais $Q_1(A)=Q_1(B)=Q_1(C) = 5$. As recompensas recebidas s√£o:
>
> Itera√ß√£o 1: Agente escolhe A, $R_1(A) = 1$, $Q_2(A) = 5 + 0.1*(1-5) = 4.6$
> Itera√ß√£o 2: Agente escolhe B, $R_2(B) = -0.5$, $Q_2(B) = 5 + 0.1*(-0.5-5) = 4.45$
> Itera√ß√£o 3: Agente escolhe C, $R_3(C) = 2.0$, $Q_2(C) = 5 + 0.1*(2-5) = 4.7$
> Itera√ß√£o 4: Agente escolhe B, $R_4(B) = 1.5$, $Q_3(B) = 4.45 + 0.1*(1.5-4.45) = 4.155$
> Itera√ß√£o 5: Agente escolhe A, $R_5(A) = 0.5$, $Q_3(A) = 4.6 + 0.1*(0.5-4.6) = 4.19$
>
> Note que todos os valores das a√ß√µes est√£o decrescendo e as a√ß√µes est√£o sendo exploradas. A "decep√ß√£o" inicial leva √† explora√ß√£o.

**An√°lise Comparativa:**
A Figura 2.3 [5] do contexto original demonstra que um m√©todo *greedy* com valores iniciais otimistas inicialmente tem um desempenho inferior, mas, eventualmente, alcan√ßa um desempenho melhor do que o m√©todo *Œµ-greedy* com $Q_1(a) = 0$. O m√©todo otimista explora mais no in√≠cio, mas sua explora√ß√£o diminui ao longo do tempo, ao contr√°rio do m√©todo *Œµ-greedy* que continua a explorar em um ritmo constante.
```mermaid
graph LR
    subgraph "Greedy com Inicializa√ß√£o Otimista"
        A[/"Explora√ß√£o Inicial Alta"/] --> B[/"Desempenho Inicial Inferior"/]
        B --> C[/"Explora√ß√£o Diminui com o Tempo"/]
        C --> D[/"Desempenho Superior na Converg√™ncia"/]
    end
    subgraph "Epsilon-Greedy"
        E[/"Explora√ß√£o Constante"/] --> F[/"Desempenho Mais Est√°vel"/]
        F --> G[/"Desempenho Inferior na Converg√™ncia"/]
    end
    A -- "vs" --> E
     style A fill:#fff,stroke:#333,stroke-width:2px
     style D fill:#ccf,stroke:#333,stroke-width:2px
     style E fill:#fff,stroke:#333,stroke-width:2px
      style G fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Vamos analisar a diferen√ßa entre inicializa√ß√£o otimista e Œµ-greedy usando um experimento hipot√©tico em um ambiente de 10-armed bandits.
>
> **Configura√ß√µes:**
> *   10 a√ß√µes, cada uma com recompensas seguindo uma distribui√ß√£o normal com m√©dia q*(a) e desvio padr√£o 1.
> *   q*(a) tamb√©m segue uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1.
> *   Inicializa√ß√£o otimista: $Q_1(a) = 5$ para todas as a√ß√µes.
> *   Œµ-greedy: $Q_1(a) = 0$ para todas as a√ß√µes e Œµ = 0.1
> *   Taxa de aprendizado (Œ±) = 0.1 para ambos os m√©todos.
>
> **Resultados Esperados (hipot√©ticos):**
>
> | Itera√ß√µes | Inicializa√ß√£o Otimista (Recompensa M√©dia) | Œµ-greedy (Recompensa M√©dia) |
> |----------|---------------------------------------|----------------------------|
> | 100      | 0.8                                   | 0.5                        |
> | 500      | 1.3                                   | 1.2                         |
> | 1000     | 1.5                                   | 1.4                        |
>
> Inicialmente, o m√©todo otimista tem uma recompensa m√©dia menor porque explora mais. Com o tempo, o m√©todo otimista se estabiliza e alcan√ßa uma recompensa m√©dia maior, enquanto o m√©todo Œµ-greedy continua explorando, mas pode ter um desempenho ligeiramente inferior.

**Teorema 1.1:** *A diferen√ßa no desempenho entre uma estrat√©gia greedy com inicializa√ß√£o otimista e uma estrat√©gia Œµ-greedy com inicializa√ß√£o nula reside na sua natureza de explora√ß√£o. A primeira concentra a explora√ß√£o no in√≠cio e a diminui com o tempo, enquanto a segunda mant√©m um n√≠vel constante de explora√ß√£o.*
```mermaid
graph LR
    A[/"Greedy com Inicializa√ß√£o Otimista"/] --> B("Explora√ß√£o inicial concentrada");
    B --> C("Explora√ß√£o diminui com o tempo");
    D[/"Epsilon-Greedy"/] --> E("Explora√ß√£o constante");
    A -- "Natureza da Explora√ß√£o" --> D
    style A fill:#fff,stroke:#333,stroke-width:2px
    style D fill:#fff,stroke:#333,stroke-width:2px
      style C fill:#ccf,stroke:#333,stroke-width:2px
        style E fill:#ccf,stroke:#333,stroke-width:2px
```

### Conclus√£o
Valores iniciais otimistas s√£o uma forma simples e eficaz de incentivar a explora√ß√£o em problemas de *multi-armed bandits* com ambientes estacion√°rios. Ao iniciar as estimativas de valor das a√ß√µes com valores muito altos, o agente se sente ‚Äúdesapontado‚Äù com as recompensas recebidas e explora outras a√ß√µes, ajudando a convergir para as a√ß√µes √≥timas. No entanto, essa t√©cnica tem suas limita√ß√µes, especialmente em ambientes n√£o estacion√°rios, onde a explora√ß√£o cont√≠nua √© necess√°ria. Em tais casos, outros m√©todos como *Œµ-greedy*, *UCB*, ou algoritmos de *gradient bandit* podem ser mais apropriados.

**Observa√ß√£o 2:** *Para mitigar a limita√ß√£o da explora√ß√£o tempor√°ria, uma combina√ß√£o de inicializa√ß√£o otimista com outras estrat√©gias de explora√ß√£o, como Œµ-greedy com um Œµ decrescente ao longo do tempo, poderia ser uma abordagem promissora.*
```mermaid
flowchart LR
    A[/"Inicializa√ß√£o Otimista"/] --> B[/"Limita√ß√£o: Explora√ß√£o Tempor√°ria"/]
    B --> C[/"Solu√ß√£o: Estrat√©gias Complementares"/]
    C --> D[/"Exemplo: Epsilon-Greedy com decaimento de Œµ"/]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```
### Refer√™ncias
[^1]: "All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, Q1(a). In the language of statistics, these methods are biased by their initial estimates. For the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant a, the bias is permanent, though decreasing over time as given by (2.6). In practice, this kind of bias is usually not a problem and can sometimes be very helpful. The downside is that the initial estimates become, in effect, a set of parameters that must be picked by the user, if only to set them all to zero. The upside is that they provide an easy way to supply some prior knowledge about what level of rewards can be expected."
[^2]: "The averaging methods discussed so far are appropriate for stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule (2.3) for updating an average Qn of the n ‚Äì 1 past rewards is modified to be
Qn+1 = Qn + a[Rn - Qn],"
[^3]: "Initial action values can also be used as a simple way to encourage exploration. Suppose that instead of setting the initial action values to zero, as we did in the 10-armed testbed, we set them all to +5. Recall that the q*(a) in this problem are selected from a normal distribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic. But this optimism encourages action-value methods to explore. Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being ‚Äúdisappointed‚Äù with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time."
[^4]: "We regard it as a simple trick that can be quite effective on stationary problems, but it is far from being a generally useful approach to encouraging exploration. For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary."
[^5]: "Figure 2.3 shows the performance on the 10-armed bandit testbed of a greedy method using Q1(a) = +5, for all a. For comparison, also shown is an …õ-greedy method with Q1(a) = 0. Initially, the optimistic method performs worse because it explores more, but eventually it performs better because its exploration decreases with time."
