### Associative Search (Contextual Bandits)

### IntroduÃ§Ã£o

Este capÃ­tulo explora o conceito de **busca associativa**, uma extensÃ£o das tarefas nÃ£o associativas apresentadas anteriormente, e como elas se relacionam com o aprendizado por reforÃ§o. Em problemas nÃ£o associativos, o objetivo Ã© encontrar uma Ãºnica aÃ§Ã£o Ã³tima em um ambiente estacionÃ¡rio ou rastrear a melhor aÃ§Ã£o em um ambiente nÃ£o estacionÃ¡rio [^1]. No entanto, em cenÃ¡rios de aprendizado por reforÃ§o mais complexos, o objetivo Ã© aprender uma polÃ­tica que mapeia diferentes situaÃ§Ãµes para as aÃ§Ãµes mais apropriadas [^1]. A **busca associativa** serve como uma ponte entre o problema do k-armed bandit e o problema completo do aprendizado por reforÃ§o, abordando situaÃ§Ãµes em que as aÃ§Ãµes precisam ser associadas a contextos especÃ­ficos [^1]. Este tipo de problema Ã© frequentemente referido como **contextual bandits** na literatura [^1].

**ProposiÃ§Ã£o 1:**  A busca associativa pode ser vista como uma generalizaÃ§Ã£o do problema do k-armed bandit. Formalmente, o problema do k-armed bandit Ã© um caso especÃ­fico de busca associativa onde o espaÃ§o de contextos Ã© um conjunto unitÃ¡rio.

*Proof Outline:*
No problema do k-armed bandit, o agente toma decisÃµes com base apenas na aÃ§Ã£o escolhida, sem levar em consideraÃ§Ã£o nenhum contexto.  Isso equivale a ter um Ãºnico contexto presente em todas as iteraÃ§Ãµes. Assim, pode-se considerar a busca associativa como um problema que estende o k-armed bandit ao incluir um conjunto de contextos.

### Conceitos Fundamentais

O conceito de **busca associativa** surge quando o agente aprende a associar aÃ§Ãµes a situaÃ§Ãµes especÃ­ficas [^1]. Para exemplificar, imagine que um agente se depare com diferentes instÃ¢ncias do problema do *k*-armed bandit. Em cada passo, uma dessas instÃ¢ncias Ã© selecionada aleatoriamente [^1]. Se a probabilidade de seleÃ§Ã£o de cada instÃ¢ncia for constante, o problema pode ser tratado como um Ãºnico problema estacionÃ¡rio de *k*-armed bandit [^1]. No entanto, em um cenÃ¡rio de busca associativa, o agente recebe uma **dica** sobre a identidade da instÃ¢ncia atual, mas nÃ£o sobre seus valores de aÃ§Ã£o [^1]. O agente deve entÃ£o aprender uma polÃ­tica que associe essa dica Ã  melhor aÃ§Ã£o para aquela instÃ¢ncia especÃ­fica [^1].

**Exemplo:** Considere uma mÃ¡quina caÃ§a-nÃ­queis que muda a cor de seu display ao mudar seus valores de aÃ§Ã£o. O agente deve aprender que, se o display estiver vermelho, deve escolher a alavanca 1, e se estiver verde, deve escolher a alavanca 2 [^1]. Essa associaÃ§Ã£o de aÃ§Ã£o com o contexto (cor do display) permite que o agente tenha um desempenho melhor do que teria sem essa informaÃ§Ã£o [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos supor que temos duas mÃ¡quinas caÃ§a-nÃ­queis (A e B), cada uma com duas alavancas (1 e 2).
>
>  - A mÃ¡quina A (contexto 1) paga em mÃ©dia:
>    - Alavanca 1: recompensa de 1 com probabilidade 0.8, e 0 com probabilidade 0.2 (recompensa esperada: 0.8)
>    - Alavanca 2: recompensa de 1 com probabilidade 0.3, e 0 com probabilidade 0.7 (recompensa esperada: 0.3)
> - A mÃ¡quina B (contexto 2) paga em mÃ©dia:
>    - Alavanca 1: recompensa de 1 com probabilidade 0.2, e 0 com probabilidade 0.8 (recompensa esperada: 0.2)
>    - Alavanca 2: recompensa de 1 com probabilidade 0.9, e 0 com probabilidade 0.1 (recompensa esperada: 0.9)
>
> Sem contexto, um agente exploraria as 4 combinaÃ§Ãµes de alavanca, mas ao reconhecer o contexto, o agente aprende a usar a alavanca 1 na mÃ¡quina A e a alavanca 2 na mÃ¡quina B.
>
> ```mermaid
> graph LR
>     subgraph "Contextual Bandit"
>         A("Contexto 1: MÃ¡quina A") -->| "Alavanca 1" | R1_A("Recompensa ~0.8");
>         A -->| "Alavanca 2" | R2_A("Recompensa ~0.3");
>         B("Contexto 2: MÃ¡quina B") -->| "Alavanca 1" | R1_B("Recompensa ~0.2");
>         B -->| "Alavanca 2" | R2_B("Recompensa ~0.9");
>     end
>
> ```
>
> Um agente que aprende essa associaÃ§Ã£o teria um ganho mÃ©dio de 0.8 quando a mÃ¡quina A Ã© selecionada e 0.9 quando a mÃ¡quina B Ã© selecionada, obtendo, em mÃ©dia, 0.85 por iteraÃ§Ã£o (assumindo que as mÃ¡quinas sÃ£o selecionadas aleatoriamente com igual probabilidade), o que seria muito melhor do que uma exploraÃ§Ã£o cega.
>

**Contextual Bandits**: As **tarefas de busca associativa** sÃ£o frequentemente chamadas de *contextual bandits* na literatura [^1]. Elas representam um nÃ­vel intermediÃ¡rio entre o problema do *k*-armed bandit e o problema de aprendizado por reforÃ§o completo, pois envolvem o aprendizado de uma polÃ­tica, mas cada aÃ§Ã£o afeta apenas a recompensa imediata, nÃ£o a situaÃ§Ã£o futura [^1].

**DefiniÃ§Ã£o Formal:** Um problema de *contextual bandit* pode ser formalizado como uma sequÃªncia de rodadas. Em cada rodada *t*, o agente recebe um contexto $s_t$ e deve escolher uma aÃ§Ã£o $a_t$ a partir de um conjunto de aÃ§Ãµes possÃ­veis. O ambiente entÃ£o fornece uma recompensa $r_t$ ao agente. O objetivo do agente Ã© aprender uma polÃ­tica $\pi$ que mapeie cada contexto para uma aÃ§Ã£o, de forma a maximizar a recompensa total esperada [^1].

**ComparaÃ§Ã£o com o k-armed Bandit:** O problema do *k*-armed bandit pode ser visto como um caso especial de *contextual bandit* onde o contexto Ã© sempre o mesmo (ou seja, nÃ£o hÃ¡ contextos distintos). Em contraste, no aprendizado por reforÃ§o completo, as aÃ§Ãµes nÃ£o sÃ³ afetam a recompensa imediata, mas tambÃ©m a situaÃ§Ã£o futura [^1].

**Lema 1:** Se o conjunto de contextos for unitÃ¡rio, um problema de *contextual bandit* se reduz a um problema de *k*-armed bandit.

*Proof Outline:*
Se o conjunto de contextos possui um Ãºnico elemento, entÃ£o o agente sempre receberÃ¡ o mesmo contexto. Neste caso, a polÃ­tica Ã³tima seria associar este contexto fixo Ã  aÃ§Ã£o de maior recompensa esperada, tornando o problema idÃªntico a um problema de *k*-armed bandit, onde cada aÃ§Ã£o possui uma recompensa esperada associada.

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um *contextual bandit* com um Ãºnico contexto. Suponha que temos 3 aÃ§Ãµes com recompensas esperadas de 0.2, 0.5, e 0.7. Como hÃ¡ apenas um contexto, o problema Ã© idÃªntico a um *k-armed bandit* com k=3. A melhor aÃ§Ã£o sempre seria a terceira, com recompensa esperada de 0.7, independentemente do "contexto" (que Ã© sempre o mesmo).

**ImportÃ¢ncia da ExploraÃ§Ã£o e ExplotaÃ§Ã£o**: O problema de *contextual bandits* tambÃ©m requer um equilÃ­brio entre exploraÃ§Ã£o e explotaÃ§Ã£o, similar aos problemas de *k*-armed bandit. O agente precisa explorar diferentes aÃ§Ãµes em cada contexto para encontrar a melhor, mas tambÃ©m deve usar seu conhecimento atual para escolher a aÃ§Ã£o que parece ser mais recompensadora [^1].

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um cenÃ¡rio com dois contextos e duas aÃ§Ãµes. Inicialmente, o agente nÃ£o tem conhecimento sobre as recompensas associadas a cada aÃ§Ã£o em cada contexto.
>
> - **Contexto 1:**
>    - AÃ§Ã£o 1: recompensa mÃ©dia = 0.2
>    - AÃ§Ã£o 2: recompensa mÃ©dia = 0.8
>
> - **Contexto 2:**
>    - AÃ§Ã£o 1: recompensa mÃ©dia = 0.7
>    - AÃ§Ã£o 2: recompensa mÃ©dia = 0.3
>
> O agente pode comeÃ§ar explorando (ex: usando uma estratÃ©gia $\epsilon$-greedy com $\epsilon = 0.1$) cada aÃ§Ã£o em cada contexto um certo nÃºmero de vezes. Por exemplo, depois de 20 iteraÃ§Ãµes, ele pode ter as seguintes estimativas:
>
> - **Contexto 1:**
>    - AÃ§Ã£o 1: recompensa mÃ©dia estimada = 0.25 (5 vezes de sucesso com recompensa 1 e 15 vezes com recompensa 0, e 20/20 total)
>    - AÃ§Ã£o 2: recompensa mÃ©dia estimada = 0.75 (15 vezes de sucesso com recompensa 1 e 5 vezes com recompensa 0, e 20/20 total)
>
> - **Contexto 2:**
>    - AÃ§Ã£o 1: recompensa mÃ©dia estimada = 0.6 (12 vezes de sucesso com recompensa 1 e 8 vezes com recompensa 0, e 20/20 total)
>    - AÃ§Ã£o 2: recompensa mÃ©dia estimada = 0.45 (9 vezes de sucesso com recompensa 1 e 11 vezes com recompensa 0, e 20/20 total)
>
> Inicialmente, em ambas os contextos, a aÃ§Ã£o com maior recompensa estimada Ã© a aÃ§Ã£o 1. Contudo, no contexto 1, a aÃ§Ã£o 2 tem uma recompensa mÃ©dia maior (0.8 vs 0.2), e o agente, ao explorar aleatoriamente, deve ir atualizando sua estimativa atÃ© ter uma aÃ§Ã£o com maior recompensa esperada no contexto 1. Similarmente para o contexto 2, a aÃ§Ã£o 1 Ã© melhor. ApÃ³s um nÃºmero maior de iteraÃ§Ãµes e exploraÃ§Ãµes, o agente deve convergir para a aÃ§Ã£o 2 no contexto 1, e aÃ§Ã£o 1 no contexto 2.
>

**Teorema 1:**  A dificuldade de um problema de *contextual bandit* depende criticamente da complexidade do espaÃ§o de contextos e da estrutura das recompensas associadas a cada contexto-aÃ§Ã£o.

*Proof Outline:*
A complexidade do espaÃ§o de contextos determina a necessidade de generalizaÃ§Ã£o da polÃ­tica. Um espaÃ§o de contextos muito grande e pouco estruturado exigirÃ¡ que o agente explore muito mais para aprender uma polÃ­tica eficaz. Da mesma forma, se a recompensa associada a cada contexto-aÃ§Ã£o for altamente nÃ£o-linear, o agente precisarÃ¡ de mais informaÃ§Ãµes para modelar a relaÃ§Ã£o entre o contexto, a aÃ§Ã£o e a recompensa. Portanto, a dificuldade do problema aumenta com a complexidade do espaÃ§o de contextos e da estrutura das recompensas.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos um problema de *contextual bandit* com duas aÃ§Ãµes, e a recompensa depende do contexto.
>
> **Caso 1: Contextos Simples**
>
> - Contexto A: AÃ§Ã£o 1 -> Recompensa 0.1; AÃ§Ã£o 2 -> Recompensa 0.9
> - Contexto B: AÃ§Ã£o 1 -> Recompensa 0.9; AÃ§Ã£o 2 -> Recompensa 0.1
>
> Neste caso, o problema Ã© fÃ¡cil, pois basta associar a aÃ§Ã£o 2 ao Contexto A e a aÃ§Ã£o 1 ao Contexto B.
>
> **Caso 2: Contextos Complexos**
>
> - Contexto (x,y) (onde x e y sÃ£o nÃºmeros reais):
>    - AÃ§Ã£o 1 -> Recompensa:  $0.2 + 0.8 * \text{sigmoid}(x + y)$
>    - AÃ§Ã£o 2 -> Recompensa:  $0.8 + 0.2 * \text{sigmoid}(x - y)$
>
>    onde $\text{sigmoid}(z) = \frac{1}{1 + e^{-z}}$
>
> Neste caso, o problema Ã© mais difÃ­cil pois requer que o agente aprenda a relaÃ§Ã£o complexa entre os valores de `x` e `y`, o contexto e a recompensa das aÃ§Ãµes.
>
> Por exemplo, se o contexto for (x=1, y=0) terÃ­amos:
>    - AÃ§Ã£o 1 -> Recompensa: $0.2 + 0.8 * \text{sigmoid}(1) \approx 0.2 + 0.8 * 0.73 \approx 0.78$
>    - AÃ§Ã£o 2 -> Recompensa: $0.8 + 0.2 * \text{sigmoid}(1) \approx 0.8 + 0.2 * 0.73 \approx 0.95$
>
> Se o contexto for (x=0, y=1) terÃ­amos:
>   - AÃ§Ã£o 1 -> Recompensa: $0.2 + 0.8 * \text{sigmoid}(1) \approx 0.2 + 0.8 * 0.73 \approx 0.78$
>    - AÃ§Ã£o 2 -> Recompensa: $0.8 + 0.2 * \text{sigmoid}(-1) \approx 0.8 + 0.2 * 0.27 \approx 0.85$
>
> A funÃ§Ã£o sigmÃ³ide torna a relaÃ§Ã£o nÃ£o-linear e, portanto, mais difÃ­cil de aprender, o que ilustra o Teorema 1.
>
> ```mermaid
>  graph LR
>      subgraph "Contextos Simples"
>         A("Contexto A") -->| "AÃ§Ã£o 1" | R1A("Recompensa 0.1");
>         A -->| "AÃ§Ã£o 2" | R2A("Recompensa 0.9");
>         B("Contexto B") -->| "AÃ§Ã£o 1" | R1B("Recompensa 0.9");
>         B -->| "AÃ§Ã£o 2" | R2B("Recompensa 0.1");
>      end
>
>   subgraph "Contextos Complexos"
>   direction TB
>     C("Contexto (x, y)")
>     C -->| "AÃ§Ã£o 1" | R1C("Recompensa: 0.2 + 0.8 * sigmoid(x+y)");
>     C -->| "AÃ§Ã£o 2" | R2C("Recompensa: 0.8 + 0.2 * sigmoid(x-y)");
>    end
>
> ```

**Lema 1.1**: Em um problema de contextual bandit, o desempenho de uma polÃ­tica exploratÃ³ria Ã©, em geral, inferior a um problema de k-armed bandit com as mesmas aÃ§Ãµes.

*Proof Outline:*
Em um problema contextual, a exploraÃ§Ã£o deve ser feita em cada contexto separadamente, o que requer um nÃºmero maior de amostras do ambiente para cada aÃ§Ã£o e contexto. Em contrapartida, em um k-armed bandit, a exploraÃ§Ã£o Ã© feita globalmente, o que demanda menos iteraÃ§Ãµes. Portanto, uma polÃ­tica exploratÃ³ria em um problema de contextual bandit tende a obter um desempenho pior em relaÃ§Ã£o a um k-armed bandit com as mesmas aÃ§Ãµes, dado o mesmo nÃºmero de iteraÃ§Ãµes.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere dois cenÃ¡rios:
>
> **CenÃ¡rio 1: k-armed bandit**
>   - 2 aÃ§Ãµes
>   - AÃ§Ã£o 1: Recompensa mÃ©dia 0.3
>   - AÃ§Ã£o 2: Recompensa mÃ©dia 0.7
>
> **CenÃ¡rio 2: Contextual bandit**
>   - 2 contextos (C1 e C2)
>   - 2 aÃ§Ãµes
>   - Contexto C1: AÃ§Ã£o 1 -> 0.3, AÃ§Ã£o 2 -> 0.7
>   - Contexto C2: AÃ§Ã£o 1 -> 0.7, AÃ§Ã£o 2 -> 0.3
>
> Um agente que explora usando um $\epsilon$-greedy com $\epsilon=0.1$ necessitarÃ¡ de mais iteraÃ§Ãµes para se aproximar do resultado Ã³timo (selecionar a aÃ§Ã£o 2 no CenÃ¡rio 1, selecionar aÃ§Ã£o 2 em C1 e aÃ§Ã£o 1 em C2) em comparaÃ§Ã£o com o problema de k-armed bandit.
>
> | Rodada | k-armed bandit (Recompensa) | Contextual Bandit (Recompensa) |
> | ----- | --------------------------- | ---------------------------- |
> | 1     | 0.3                          |  0.3 ou 0.7 (escolhido aleatoriamente)                             |
> | 2     | 0.7                          | 0.7 ou 0.3                                      |
> | 3     | 0.3                          | 0.3 ou 0.7                               |
> | ...   | ...                          | ...                                      |
> | 100    |  0.65 (Aprox.)                        |  0.5 (Aprox.)                                  |
>
> Como o *contextual bandit* necessita aprender a melhor aÃ§Ã£o para cada contexto, a fase de exploraÃ§Ã£o tende a ser mais longa, obtendo um desempenho inferior ao do k-armed bandit para o mesmo nÃºmero de iteraÃ§Ãµes, em geral. ApÃ³s muitas iteraÃ§Ãµes, ambos os algoritmos convergem para o resultado Ã³timo, mas em geral, a convergÃªncia do k-armed bandit Ã© mais rÃ¡pida.
>

### ConclusÃ£o

As **tarefas de busca associativa**, ou *contextual bandits*, representam um importante passo em direÃ§Ã£o ao aprendizado por reforÃ§o completo [^1]. Ao introduzir a necessidade de associar aÃ§Ãµes a contextos especÃ­ficos, esses problemas desafiam os mÃ©todos de aprendizado a generalizar seu comportamento com base em sinais contextuais, tornando-se um modelo Ãºtil para entender cenÃ¡rios mais complexos de tomada de decisÃ£o [^1]. Esta categoria de problemas, contextual bandits, estende o conceito de busca por melhores aÃ§Ãµes do k-armed bandit e nos aproxima do problema de aprendizado por reforÃ§o [^1].

**Teorema 1.1:** Se o nÃºmero de contextos em um problema de contextual bandit tende a infinito, o problema se aproxima da complexidade de um problema de aprendizado por reforÃ§o completo.

*Proof Outline:*
Com um nÃºmero infinito de contextos, o problema de contextual bandit comeÃ§a a se assemelhar aos desafios de aprendizado por reforÃ§o. As aÃ§Ãµes nÃ£o somente tÃªm um efeito imediato na recompensa, mas um nÃºmero grande de contextos implica que o agente precisa aprender a generalizar o comportamento em situaÃ§Ãµes semelhantes. AlÃ©m disso, em um cenÃ¡rio com infinitos contextos, Ã© possÃ­vel modelar que a aÃ§Ã£o pode levar a transiÃ§Ãµes entre os contextos e o agente comeÃ§a a aprender a ter um comportamento estratÃ©gico em relaÃ§Ã£o a sua tomada de aÃ§Ã£o. Portanto, o problema se aproxima de um problema de aprendizado por reforÃ§o completo com um nÃºmero infinito de estados.
```mermaid
graph LR
    subgraph "k-armed bandit"
        A("Agente") -->| "AÃ§Ãµes"| B("Ambiente (Ãºnico)")
        B -->| "Recompensa" | A
    end
    subgraph "Contextual bandit"
       C("Agente") -->| "AÃ§Ãµes" | D("Ambiente (com contexto)")
       D -->| "Recompensa, Contexto" | C
    end
    subgraph "Aprendizado por ReforÃ§o"
        E("Agente") -->| "AÃ§Ãµes" | F("Ambiente (com estado)")
         F -->| "Recompensa, Estado" | E
    end
    B -->| "Caso especial"| D
    D -->| "Caso especial"| F

```

### ReferÃªncias

[^1]: "So far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no need to associate different actions with different situations. In these tasks the learner either tries to find a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary. However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations. To set the stage for the full problem, we briefly discuss the simplest way in which nonassociative tasks extend to the associative setting. As an example, suppose there are several different k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. If the probabilities with which each task is selected for you do not change over time, this would appear as a single stationary k-armed bandit task, and you could use one of the methods described in this chapter. Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that taskâ€”for instance, if red, select arm 1; if green, select arm 2. With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another. This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature. Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem. We present this problem in the next chapter and consider its ramifications throughout the rest of the book." *(Trecho de /content/reinforcement_learning_notes/01. Multi-armed Bandits)*

**Summary of Changes:**

1.  **Proposition 1:** A formal statement that the k-armed bandit problem is a special case of the contextual bandit problem, which helps to clarify the relationship between these two types of problems.
2.  **Lemma 1:** Provides a more formal connection with an easily stated condition under which the contextual bandit reduces to the k-armed bandit.
3. **Teorema 1:** This theorem states the relationship between the complexity of the context space and the difficulty of the contextual bandit problem, highlighting a critical factor that influences learning, and connects to the previous discussion on how contextual bandits extend k-armed bandit
4. **Lemma 1.1:** Relates the performance of explorative policies in both settings, showing how the additional complexity of contextual bandits impact the performance of exploration.
5.  **Theorem 1.1**: Connects the concept of contextual bandits to reinforcement learning by showing a limiting case where the two converge, and connects to the conclusion which mentions the relationship between contextual bandits and reinforcement learning.

These additions aim to deepen the reader's understanding of contextual bandits and their relationship to both k-armed bandits and the broader field of reinforcement learning. They maintain the original context and add significant theoretical value to the discussion.
