### Associative Search (Contextual Bandits)

### Introdu√ß√£o

Este cap√≠tulo explora o conceito de **busca associativa**, uma extens√£o das tarefas n√£o associativas apresentadas anteriormente, e como elas se relacionam com o aprendizado por refor√ßo. Em problemas n√£o associativos, o objetivo √© encontrar uma √∫nica a√ß√£o √≥tima em um ambiente estacion√°rio ou rastrear a melhor a√ß√£o em um ambiente n√£o estacion√°rio [^1]. No entanto, em cen√°rios de aprendizado por refor√ßo mais complexos, o objetivo √© aprender uma pol√≠tica que mapeia diferentes situa√ß√µes para as a√ß√µes mais apropriadas [^1]. A **busca associativa** serve como uma ponte entre o problema do k-armed bandit e o problema completo do aprendizado por refor√ßo, abordando situa√ß√µes em que as a√ß√µes precisam ser associadas a contextos espec√≠ficos [^1]. Este tipo de problema √© frequentemente referido como **contextual bandits** na literatura [^1].

**Proposi√ß√£o 1:**  A busca associativa pode ser vista como uma generaliza√ß√£o do problema do k-armed bandit. Formalmente, o problema do k-armed bandit √© um caso espec√≠fico de busca associativa onde o espa√ßo de contextos √© um conjunto unit√°rio.

*Proof Outline:*
No problema do k-armed bandit, o agente toma decis√µes com base apenas na a√ß√£o escolhida, sem levar em considera√ß√£o nenhum contexto.  Isso equivale a ter um √∫nico contexto presente em todas as itera√ß√µes. Assim, pode-se considerar a busca associativa como um problema que estende o k-armed bandit ao incluir um conjunto de contextos.

### Conceitos Fundamentais

O conceito de **busca associativa** surge quando o agente aprende a associar a√ß√µes a situa√ß√µes espec√≠ficas [^1]. Para exemplificar, imagine que um agente se depare com diferentes inst√¢ncias do problema do *k*-armed bandit. Em cada passo, uma dessas inst√¢ncias √© selecionada aleatoriamente [^1]. Se a probabilidade de sele√ß√£o de cada inst√¢ncia for constante, o problema pode ser tratado como um √∫nico problema estacion√°rio de *k*-armed bandit [^1]. No entanto, em um cen√°rio de busca associativa, o agente recebe uma **dica** sobre a identidade da inst√¢ncia atual, mas n√£o sobre seus valores de a√ß√£o [^1]. O agente deve ent√£o aprender uma pol√≠tica que associe essa dica √† melhor a√ß√£o para aquela inst√¢ncia espec√≠fica [^1].

**Exemplo:** Considere uma m√°quina ca√ßa-n√≠queis que muda a cor de seu display ao mudar seus valores de a√ß√£o. O agente deve aprender que, se o display estiver vermelho, deve escolher a alavanca 1, e se estiver verde, deve escolher a alavanca 2 [^1]. Essa associa√ß√£o de a√ß√£o com o contexto (cor do display) permite que o agente tenha um desempenho melhor do que teria sem essa informa√ß√£o [^1].

> üí° **Exemplo Num√©rico:** Vamos supor que temos duas m√°quinas ca√ßa-n√≠queis (A e B), cada uma com duas alavancas (1 e 2).
>
>  - A m√°quina A (contexto 1) paga em m√©dia:
>    - Alavanca 1: recompensa de 1 com probabilidade 0.8, e 0 com probabilidade 0.2 (recompensa esperada: 0.8)
>    - Alavanca 2: recompensa de 1 com probabilidade 0.3, e 0 com probabilidade 0.7 (recompensa esperada: 0.3)
> - A m√°quina B (contexto 2) paga em m√©dia:
>    - Alavanca 1: recompensa de 1 com probabilidade 0.2, e 0 com probabilidade 0.8 (recompensa esperada: 0.2)
>    - Alavanca 2: recompensa de 1 com probabilidade 0.9, e 0 com probabilidade 0.1 (recompensa esperada: 0.9)
>
> Sem contexto, um agente exploraria as 4 combina√ß√µes de alavanca, mas ao reconhecer o contexto, o agente aprende a usar a alavanca 1 na m√°quina A e a alavanca 2 na m√°quina B.
>
> ```mermaid
> graph LR
>     subgraph "Contextual Bandit"
>         A("Contexto 1: M√°quina A") -->| "Alavanca 1" | R1_A("Recompensa ~0.8");
>         A -->| "Alavanca 2" | R2_A("Recompensa ~0.3");
>         B("Contexto 2: M√°quina B") -->| "Alavanca 1" | R1_B("Recompensa ~0.2");
>         B -->| "Alavanca 2" | R2_B("Recompensa ~0.9");
>     end
>
> ```
>
> Um agente que aprende essa associa√ß√£o teria um ganho m√©dio de 0.8 quando a m√°quina A √© selecionada e 0.9 quando a m√°quina B √© selecionada, obtendo, em m√©dia, 0.85 por itera√ß√£o (assumindo que as m√°quinas s√£o selecionadas aleatoriamente com igual probabilidade), o que seria muito melhor do que uma explora√ß√£o cega.
>

**Contextual Bandits**: As **tarefas de busca associativa** s√£o frequentemente chamadas de *contextual bandits* na literatura [^1]. Elas representam um n√≠vel intermedi√°rio entre o problema do *k*-armed bandit e o problema de aprendizado por refor√ßo completo, pois envolvem o aprendizado de uma pol√≠tica, mas cada a√ß√£o afeta apenas a recompensa imediata, n√£o a situa√ß√£o futura [^1].

**Defini√ß√£o Formal:** Um problema de *contextual bandit* pode ser formalizado como uma sequ√™ncia de rodadas. Em cada rodada *t*, o agente recebe um contexto $s_t$ e deve escolher uma a√ß√£o $a_t$ a partir de um conjunto de a√ß√µes poss√≠veis. O ambiente ent√£o fornece uma recompensa $r_t$ ao agente. O objetivo do agente √© aprender uma pol√≠tica $\pi$ que mapeie cada contexto para uma a√ß√£o, de forma a maximizar a recompensa total esperada [^1].

**Compara√ß√£o com o k-armed Bandit:** O problema do *k*-armed bandit pode ser visto como um caso especial de *contextual bandit* onde o contexto √© sempre o mesmo (ou seja, n√£o h√° contextos distintos). Em contraste, no aprendizado por refor√ßo completo, as a√ß√µes n√£o s√≥ afetam a recompensa imediata, mas tamb√©m a situa√ß√£o futura [^1].

**Lema 1:** Se o conjunto de contextos for unit√°rio, um problema de *contextual bandit* se reduz a um problema de *k*-armed bandit.

*Proof Outline:*
Se o conjunto de contextos possui um √∫nico elemento, ent√£o o agente sempre receber√° o mesmo contexto. Neste caso, a pol√≠tica √≥tima seria associar este contexto fixo √† a√ß√£o de maior recompensa esperada, tornando o problema id√™ntico a um problema de *k*-armed bandit, onde cada a√ß√£o possui uma recompensa esperada associada.

> üí° **Exemplo Num√©rico:** Considere um *contextual bandit* com um √∫nico contexto. Suponha que temos 3 a√ß√µes com recompensas esperadas de 0.2, 0.5, e 0.7. Como h√° apenas um contexto, o problema √© id√™ntico a um *k-armed bandit* com k=3. A melhor a√ß√£o sempre seria a terceira, com recompensa esperada de 0.7, independentemente do "contexto" (que √© sempre o mesmo).

**Import√¢ncia da Explora√ß√£o e Explota√ß√£o**: O problema de *contextual bandits* tamb√©m requer um equil√≠brio entre explora√ß√£o e explota√ß√£o, similar aos problemas de *k*-armed bandit. O agente precisa explorar diferentes a√ß√µes em cada contexto para encontrar a melhor, mas tamb√©m deve usar seu conhecimento atual para escolher a a√ß√£o que parece ser mais recompensadora [^1].

> üí° **Exemplo Num√©rico:**
>
> Considere um cen√°rio com dois contextos e duas a√ß√µes. Inicialmente, o agente n√£o tem conhecimento sobre as recompensas associadas a cada a√ß√£o em cada contexto.
>
> - **Contexto 1:**
>    - A√ß√£o 1: recompensa m√©dia = 0.2
>    - A√ß√£o 2: recompensa m√©dia = 0.8
>
> - **Contexto 2:**
>    - A√ß√£o 1: recompensa m√©dia = 0.7
>    - A√ß√£o 2: recompensa m√©dia = 0.3
>
> O agente pode come√ßar explorando (ex: usando uma estrat√©gia $\epsilon$-greedy com $\epsilon = 0.1$) cada a√ß√£o em cada contexto um certo n√∫mero de vezes. Por exemplo, depois de 20 itera√ß√µes, ele pode ter as seguintes estimativas:
>
> - **Contexto 1:**
>    - A√ß√£o 1: recompensa m√©dia estimada = 0.25 (5 vezes de sucesso com recompensa 1 e 15 vezes com recompensa 0, e 20/20 total)
>    - A√ß√£o 2: recompensa m√©dia estimada = 0.75 (15 vezes de sucesso com recompensa 1 e 5 vezes com recompensa 0, e 20/20 total)
>
> - **Contexto 2:**
>    - A√ß√£o 1: recompensa m√©dia estimada = 0.6 (12 vezes de sucesso com recompensa 1 e 8 vezes com recompensa 0, e 20/20 total)
>    - A√ß√£o 2: recompensa m√©dia estimada = 0.45 (9 vezes de sucesso com recompensa 1 e 11 vezes com recompensa 0, e 20/20 total)
>
> Inicialmente, em ambas os contextos, a a√ß√£o com maior recompensa estimada √© a a√ß√£o 1. Contudo, no contexto 1, a a√ß√£o 2 tem uma recompensa m√©dia maior (0.8 vs 0.2), e o agente, ao explorar aleatoriamente, deve ir atualizando sua estimativa at√© ter uma a√ß√£o com maior recompensa esperada no contexto 1. Similarmente para o contexto 2, a a√ß√£o 1 √© melhor. Ap√≥s um n√∫mero maior de itera√ß√µes e explora√ß√µes, o agente deve convergir para a a√ß√£o 2 no contexto 1, e a√ß√£o 1 no contexto 2.
>

**Teorema 1:**  A dificuldade de um problema de *contextual bandit* depende criticamente da complexidade do espa√ßo de contextos e da estrutura das recompensas associadas a cada contexto-a√ß√£o.

*Proof Outline:*
A complexidade do espa√ßo de contextos determina a necessidade de generaliza√ß√£o da pol√≠tica. Um espa√ßo de contextos muito grande e pouco estruturado exigir√° que o agente explore muito mais para aprender uma pol√≠tica eficaz. Da mesma forma, se a recompensa associada a cada contexto-a√ß√£o for altamente n√£o-linear, o agente precisar√° de mais informa√ß√µes para modelar a rela√ß√£o entre o contexto, a a√ß√£o e a recompensa. Portanto, a dificuldade do problema aumenta com a complexidade do espa√ßo de contextos e da estrutura das recompensas.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um problema de *contextual bandit* com duas a√ß√µes, e a recompensa depende do contexto.
>
> **Caso 1: Contextos Simples**
>
> - Contexto A: A√ß√£o 1 -> Recompensa 0.1; A√ß√£o 2 -> Recompensa 0.9
> - Contexto B: A√ß√£o 1 -> Recompensa 0.9; A√ß√£o 2 -> Recompensa 0.1
>
> Neste caso, o problema √© f√°cil, pois basta associar a a√ß√£o 2 ao Contexto A e a a√ß√£o 1 ao Contexto B.
>
> **Caso 2: Contextos Complexos**
>
> - Contexto (x,y) (onde x e y s√£o n√∫meros reais):
>    - A√ß√£o 1 -> Recompensa:  $0.2 + 0.8 * \text{sigmoid}(x + y)$
>    - A√ß√£o 2 -> Recompensa:  $0.8 + 0.2 * \text{sigmoid}(x - y)$
>
>    onde $\text{sigmoid}(z) = \frac{1}{1 + e^{-z}}$
>
> Neste caso, o problema √© mais dif√≠cil pois requer que o agente aprenda a rela√ß√£o complexa entre os valores de `x` e `y`, o contexto e a recompensa das a√ß√µes.
>
> Por exemplo, se o contexto for (x=1, y=0) ter√≠amos:
>    - A√ß√£o 1 -> Recompensa: $0.2 + 0.8 * \text{sigmoid}(1) \approx 0.2 + 0.8 * 0.73 \approx 0.78$
>    - A√ß√£o 2 -> Recompensa: $0.8 + 0.2 * \text{sigmoid}(1) \approx 0.8 + 0.2 * 0.73 \approx 0.95$
>
> Se o contexto for (x=0, y=1) ter√≠amos:
>   - A√ß√£o 1 -> Recompensa: $0.2 + 0.8 * \text{sigmoid}(1) \approx 0.2 + 0.8 * 0.73 \approx 0.78$
>    - A√ß√£o 2 -> Recompensa: $0.8 + 0.2 * \text{sigmoid}(-1) \approx 0.8 + 0.2 * 0.27 \approx 0.85$
>
> A fun√ß√£o sigm√≥ide torna a rela√ß√£o n√£o-linear e, portanto, mais dif√≠cil de aprender, o que ilustra o Teorema 1.
>
> ```mermaid
>  graph LR
>      subgraph "Contextos Simples"
>         A("Contexto A") -->| "A√ß√£o 1" | R1A("Recompensa 0.1");
>         A -->| "A√ß√£o 2" | R2A("Recompensa 0.9");
>         B("Contexto B") -->| "A√ß√£o 1" | R1B("Recompensa 0.9");
>         B -->| "A√ß√£o 2" | R2B("Recompensa 0.1");
>      end
>
>   subgraph "Contextos Complexos"
>   direction TB
>     C("Contexto (x, y)")
>     C -->| "A√ß√£o 1" | R1C("Recompensa: 0.2 + 0.8 * sigmoid(x+y)");
>     C -->| "A√ß√£o 2" | R2C("Recompensa: 0.8 + 0.2 * sigmoid(x-y)");
>    end
>
> ```

**Lema 1.1**: Em um problema de contextual bandit, o desempenho de uma pol√≠tica explorat√≥ria √©, em geral, inferior a um problema de k-armed bandit com as mesmas a√ß√µes.

*Proof Outline:*
Em um problema contextual, a explora√ß√£o deve ser feita em cada contexto separadamente, o que requer um n√∫mero maior de amostras do ambiente para cada a√ß√£o e contexto. Em contrapartida, em um k-armed bandit, a explora√ß√£o √© feita globalmente, o que demanda menos itera√ß√µes. Portanto, uma pol√≠tica explorat√≥ria em um problema de contextual bandit tende a obter um desempenho pior em rela√ß√£o a um k-armed bandit com as mesmas a√ß√µes, dado o mesmo n√∫mero de itera√ß√µes.

> üí° **Exemplo Num√©rico:**
>
> Considere dois cen√°rios:
>
> **Cen√°rio 1: k-armed bandit**
>   - 2 a√ß√µes
>   - A√ß√£o 1: Recompensa m√©dia 0.3
>   - A√ß√£o 2: Recompensa m√©dia 0.7
>
> **Cen√°rio 2: Contextual bandit**
>   - 2 contextos (C1 e C2)
>   - 2 a√ß√µes
>   - Contexto C1: A√ß√£o 1 -> 0.3, A√ß√£o 2 -> 0.7
>   - Contexto C2: A√ß√£o 1 -> 0.7, A√ß√£o 2 -> 0.3
>
> Um agente que explora usando um $\epsilon$-greedy com $\epsilon=0.1$ necessitar√° de mais itera√ß√µes para se aproximar do resultado √≥timo (selecionar a a√ß√£o 2 no Cen√°rio 1, selecionar a√ß√£o 2 em C1 e a√ß√£o 1 em C2) em compara√ß√£o com o problema de k-armed bandit.
>
> | Rodada | k-armed bandit (Recompensa) | Contextual Bandit (Recompensa) |
> | ----- | --------------------------- | ---------------------------- |
> | 1     | 0.3                          |  0.3 ou 0.7 (escolhido aleatoriamente)                             |
> | 2     | 0.7                          | 0.7 ou 0.3                                      |
> | 3     | 0.3                          | 0.3 ou 0.7                               |
> | ...   | ...                          | ...                                      |
> | 100    |  0.65 (Aprox.)                        |  0.5 (Aprox.)                                  |
>
> Como o *contextual bandit* necessita aprender a melhor a√ß√£o para cada contexto, a fase de explora√ß√£o tende a ser mais longa, obtendo um desempenho inferior ao do k-armed bandit para o mesmo n√∫mero de itera√ß√µes, em geral. Ap√≥s muitas itera√ß√µes, ambos os algoritmos convergem para o resultado √≥timo, mas em geral, a converg√™ncia do k-armed bandit √© mais r√°pida.
>

### Conclus√£o

As **tarefas de busca associativa**, ou *contextual bandits*, representam um importante passo em dire√ß√£o ao aprendizado por refor√ßo completo [^1]. Ao introduzir a necessidade de associar a√ß√µes a contextos espec√≠ficos, esses problemas desafiam os m√©todos de aprendizado a generalizar seu comportamento com base em sinais contextuais, tornando-se um modelo √∫til para entender cen√°rios mais complexos de tomada de decis√£o [^1]. Esta categoria de problemas, contextual bandits, estende o conceito de busca por melhores a√ß√µes do k-armed bandit e nos aproxima do problema de aprendizado por refor√ßo [^1].

**Teorema 1.1:** Se o n√∫mero de contextos em um problema de contextual bandit tende a infinito, o problema se aproxima da complexidade de um problema de aprendizado por refor√ßo completo.

*Proof Outline:*
Com um n√∫mero infinito de contextos, o problema de contextual bandit come√ßa a se assemelhar aos desafios de aprendizado por refor√ßo. As a√ß√µes n√£o somente t√™m um efeito imediato na recompensa, mas um n√∫mero grande de contextos implica que o agente precisa aprender a generalizar o comportamento em situa√ß√µes semelhantes. Al√©m disso, em um cen√°rio com infinitos contextos, √© poss√≠vel modelar que a a√ß√£o pode levar a transi√ß√µes entre os contextos e o agente come√ßa a aprender a ter um comportamento estrat√©gico em rela√ß√£o a sua tomada de a√ß√£o. Portanto, o problema se aproxima de um problema de aprendizado por refor√ßo completo com um n√∫mero infinito de estados.
```mermaid
graph LR
    subgraph "k-armed bandit"
        A("Agente") -->| "A√ß√µes"| B("Ambiente (√∫nico)")
        B -->| "Recompensa" | A
    end
    subgraph "Contextual bandit"
       C("Agente") -->| "A√ß√µes" | D("Ambiente (com contexto)")
       D -->| "Recompensa, Contexto" | C
    end
    subgraph "Aprendizado por Refor√ßo"
        E("Agente") -->| "A√ß√µes" | F("Ambiente (com estado)")
         F -->| "Recompensa, Estado" | E
    end
    B -->| "Caso especial"| D
    D -->| "Caso especial"| F

```

### Refer√™ncias

[^1]: "So far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no need to associate different actions with different situations. In these tasks the learner either tries to find a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary. However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations. To set the stage for the full problem, we briefly discuss the simplest way in which nonassociative tasks extend to the associative setting. As an example, suppose there are several different k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. If the probabilities with which each task is selected for you do not change over time, this would appear as a single stationary k-armed bandit task, and you could use one of the methods described in this chapter. Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that task‚Äîfor instance, if red, select arm 1; if green, select arm 2. With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another. This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature. Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem. We present this problem in the next chapter and consider its ramifications throughout the rest of the book." *(Trecho de /content/reinforcement_learning_notes/01. Multi-armed Bandits)*

**Summary of Changes:**

1.  **Proposition 1:** A formal statement that the k-armed bandit problem is a special case of the contextual bandit problem, which helps to clarify the relationship between these two types of problems.
2.  **Lemma 1:** Provides a more formal connection with an easily stated condition under which the contextual bandit reduces to the k-armed bandit.
3. **Teorema 1:** This theorem states the relationship between the complexity of the context space and the difficulty of the contextual bandit problem, highlighting a critical factor that influences learning, and connects to the previous discussion on how contextual bandits extend k-armed bandit
4. **Lemma 1.1:** Relates the performance of explorative policies in both settings, showing how the additional complexity of contextual bandits impact the performance of exploration.
5.  **Theorem 1.1**: Connects the concept of contextual bandits to reinforcement learning by showing a limiting case where the two converge, and connects to the conclusion which mentions the relationship between contextual bandits and reinforcement learning.

These additions aim to deepen the reader's understanding of contextual bandits and their relationship to both k-armed bandits and the broader field of reinforcement learning. They maintain the original context and add significant theoretical value to the discussion.
