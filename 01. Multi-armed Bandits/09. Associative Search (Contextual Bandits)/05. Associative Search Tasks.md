## Associative Search (Contextual Bandits)

### Introdu√ß√£o
Este cap√≠tulo aborda o tema de **Multi-armed Bandits**, um problema fundamental em *reinforcement learning* (RL), introduzindo o conceito de tarefas n√£o associativas, onde o aprendizado ocorre em um √∫nico contexto. O objetivo √© maximizar o total de recompensa esperada atrav√©s da sele√ß√£o de a√ß√µes, onde cada a√ß√£o resulta em uma recompensa num√©rica de uma distribui√ß√£o de probabilidade estacion√°ria [^1]. Este cap√≠tulo avan√ßa para um cen√°rio mais complexo, o de **busca associativa** (associative search), que representa um passo intermedi√°rio para o problema completo de RL. Em tarefas n√£o associativas, o foco √© encontrar uma √∫nica a√ß√£o ideal, enquanto em tarefas de busca associativa, as a√ß√µes devem ser associadas a situa√ß√µes espec√≠ficas. Em outras palavras, a tarefa associativa trata de mapear situa√ß√µes para a√ß√µes que s√£o ideais naquele contexto [^17].
```mermaid
graph LR
    A("Multi-armed Bandit (N√£o Associativo)") --> B("Busca Associativa (Contextual Bandit)");
    B --> C("Reinforcement Learning Completo");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
    subgraph "N√≠veis de Complexidade em RL"
       A
       B
       C
    end
    linkStyle 0,1,2 stroke:#333,stroke-width:1px;
```

### Conceitos Fundamentais
As tarefas de **busca associativa** (associative search), tamb√©m chamadas de *contextual bandits*, s√£o um avan√ßo das tarefas n√£o associativas do problema *k-armed bandit*. As tarefas n√£o associativas focam em encontrar uma √∫nica a√ß√£o √≥tima para todas as situa√ß√µes. Contudo, em cen√°rios mais realistas, diferentes a√ß√µes s√£o adequadas dependendo do contexto, o que leva √† necessidade de *aprender uma pol√≠tica* que mapeia situa√ß√µes para a√ß√µes apropriadas [^17].

Imagine um cen√°rio onde existem m√∫ltiplos problemas *k-armed bandit*, cada um com diferentes distribui√ß√µes de recompensa. A cada passo, um desses problemas √© selecionado aleatoriamente. Se a probabilidade de sele√ß√£o de cada problema for constante, o problema pode ser tratado como um √∫nico *k-armed bandit* estacion√°rio, aplicando os m√©todos abordados no cap√≠tulo [^17]. Agora, suponha que, ao selecionar um problema, o agente recebe uma dica distintiva sobre sua identidade, embora n√£o sobre seus valores de a√ß√£o. Isso pode ser ilustrado por uma m√°quina ca√ßa-n√≠queis, onde a cor da tela varia conforme os valores de a√ß√£o. Nesse contexto, o agente pode aprender uma *pol√≠tica* que associa cada tarefa (identificada pela cor) com a melhor a√ß√£o. Por exemplo, se a tela √© vermelha, o bra√ßo 1 √© selecionado; se a tela √© verde, o bra√ßo 2 √© selecionado. Essa *pol√≠tica* permite um desempenho muito superior se comparado a n√£o usar essa informa√ß√£o distintiva [^17].

> üí° **Exemplo Num√©rico:** Suponha que temos duas m√°quinas ca√ßa-n√≠queis (problemas *k-armed bandit*), uma com tela vermelha e outra com tela azul. Cada m√°quina tem dois bra√ßos (a√ß√µes). A m√°quina vermelha tem uma recompensa m√©dia de 1 ao puxar o bra√ßo 1 e 0.2 ao puxar o bra√ßo 2. A m√°quina azul tem uma recompensa m√©dia de 0.1 ao puxar o bra√ßo 1 e 0.9 ao puxar o bra√ßo 2. Inicialmente, o agente n√£o sabe qual bra√ßo √© melhor em cada m√°quina. Com um algoritmo de busca associativa, o agente aprender√° a associar a cor vermelha ao bra√ßo 1 e a cor azul ao bra√ßo 2.

Essa configura√ß√£o √© um exemplo de uma **tarefa de busca associativa**, combinando aprendizagem por tentativa e erro para identificar as melhores a√ß√µes com a associa√ß√£o dessas a√ß√µes com as situa√ß√µes em que s√£o mais eficazes [^17]. Essa classe de tarefas est√° entre o problema *k-armed bandit* e o problema de *reinforcement learning* completo. Assim como no *k-armed bandit*, cada a√ß√£o afeta apenas a recompensa imediata, mas, como no *reinforcement learning* completo, o problema envolve o aprendizado de uma *pol√≠tica* [^17].
```mermaid
graph LR
    A["k-armed bandit"] --> B["Busca Associativa"];
    B --> C["Reinforcement Learning Completo"];
    A -- "A√ß√µes afetam apenas recompensa imediata" --> A
    B -- "A√ß√µes afetam apenas recompensa imediata, mas aprende uma pol√≠tica" --> B
    C -- "A√ß√µes afetam recompensas e estados futuros" --> C
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
    linkStyle 0,1,2 stroke:#333,stroke-width:1px;
```

**Lemma 1: Equival√™ncia no Cen√°rio de Dois Bra√ßos com Soft-Max**
*Declara√ß√£o*: No cen√°rio de dois bra√ßos, a distribui√ß√£o *soft-max* para sele√ß√£o de a√ß√µes √© equivalente √† distribui√ß√£o dada pela fun√ß√£o *logistic* ou *sigmoid*.
*Prova*:
Sejam $H_t(1)$ e $H_t(2)$ as prefer√™ncias de a√ß√£o para os bra√ßos 1 e 2, respectivamente. A distribui√ß√£o *soft-max* para a escolha da a√ß√£o 1 √© dada por:

$$
\pi_t(1) = \frac{e^{H_t(1)}}{e^{H_t(1)} + e^{H_t(2)}}
$$

Vamos reescrever essa express√£o dividindo o numerador e o denominador por $e^{H_t(1)}$:
$$
\pi_t(1) = \frac{1}{1 + e^{H_t(2) - H_t(1)}}
$$
Agora, definindo $x = H_t(2) - H_t(1)$, temos
$$
\pi_t(1) = \frac{1}{1 + e^x}
$$
que √© a forma da fun√ß√£o log√≠stica ou sigmoide, mostrando a equival√™ncia entre as duas distribui√ß√µes [^13]. $\blacksquare$
```mermaid
graph LR
    subgraph "Distribui√ß√£o Softmax (2 a√ß√µes)"
    A["œÄ_t(1) = e^{H_t(1)} / (e^{H_t(1)} + e^{H_t(2)})"]
    B["Dividir por e^{H_t(1)}"]
    C["œÄ_t(1) = 1 / (1 + e^{H_t(2) - H_t(1)})"]
    D["x = H_t(2) - H_t(1)"]
    E["œÄ_t(1) = 1 / (1 + e^x)"]
    A --> B
    B --> C
    C --> D
    D --> E
    end
    style A fill:#ccf,stroke:#333,stroke-width:1px
    style B fill:#ccf,stroke:#333,stroke-width:1px
    style C fill:#ccf,stroke:#333,stroke-width:1px
    style D fill:#ccf,stroke:#333,stroke-width:1px
    style E fill:#ccf,stroke:#333,stroke-width:1px
```

> üí° **Exemplo Num√©rico:** Suponha que em um dado instante $t$, as prefer√™ncias de a√ß√£o s√£o $H_t(1) = 1$ e $H_t(2) = 0.5$. Ent√£o, a probabilidade de escolher a a√ß√£o 1 usando a fun√ß√£o *soft-max* √©:
>
> $\pi_t(1) = \frac{e^1}{e^1 + e^{0.5}} \approx \frac{2.718}{2.718 + 1.649} \approx \frac{2.718}{4.367} \approx 0.622$
>
> Usando a transforma√ß√£o para a forma sigmoide, $x = H_t(2) - H_t(1) = 0.5 - 1 = -0.5$.  A probabilidade de escolher a a√ß√£o 1 √©:
>
> $\pi_t(1) = \frac{1}{1 + e^{-0.5}} \approx \frac{1}{1 + 0.607} \approx \frac{1}{1.607} \approx 0.622$
>
> Este exemplo ilustra numericamente que a distribui√ß√£o softmax e a fun√ß√£o sigmoide levam √† mesma probabilidade de sele√ß√£o da a√ß√£o, dada a diferen√ßa nas prefer√™ncias.

**Lema 1.1: Generaliza√ß√£o da Equival√™ncia para $k$ A√ß√µes**
*Declara√ß√£o*: A distribui√ß√£o *soft-max* para a sele√ß√£o de a√ß√µes pode ser expressa em termos da fun√ß√£o exponencial e da soma das exponenciais das prefer√™ncias de todas as a√ß√µes.
*Prova*:
Sejam $H_t(a)$ as prefer√™ncias de a√ß√£o para cada a√ß√£o $a$, onde $a = 1, 2, \ldots, k$. A probabilidade de selecionar a a√ß√£o $a$ √© dada por:
$$
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}}
$$
Essa express√£o mostra como a probabilidade de escolher uma a√ß√£o $a$ √© proporcional √† exponencial da sua prefer√™ncia $H_t(a)$, normalizada pela soma das exponenciais das prefer√™ncias de todas as outras a√ß√µes. Isso generaliza a formula√ß√£o para $k$ a√ß√µes, mantendo a ideia de que a√ß√µes com prefer√™ncias maiores s√£o mais propensas a serem selecionadas. $\blacksquare$
```mermaid
graph LR
    A["œÄ_t(a) = e^{H_t(a)} / (Œ£_{b=1}^k e^{H_t(b)})"]
    subgraph "Distribui√ß√£o Softmax (k a√ß√µes)"
       A
    end
    style A fill:#ccf,stroke:#333,stroke-width:1px;
```

> üí° **Exemplo Num√©rico:**  Considere um cen√°rio com 3 a√ß√µes, e as prefer√™ncias em um dado instante $t$ s√£o $H_t(1) = 0.1$, $H_t(2) = 0.5$ e $H_t(3) = -0.2$. As probabilidades de selecionar cada a√ß√£o usando a *soft-max* s√£o calculadas como:
>
> $\pi_t(1) = \frac{e^{0.1}}{e^{0.1} + e^{0.5} + e^{-0.2}} \approx \frac{1.105}{1.105 + 1.649 + 0.819} \approx \frac{1.105}{3.573} \approx 0.309$
>
> $\pi_t(2) = \frac{e^{0.5}}{e^{0.1} + e^{0.5} + e^{-0.2}} \approx \frac{1.649}{3.573} \approx 0.461$
>
> $\pi_t(3) = \frac{e^{-0.2}}{e^{0.1} + e^{0.5} + e^{-0.2}} \approx \frac{0.819}{3.573} \approx 0.229$
>
> Como esperado, a a√ß√£o 2, com a maior prefer√™ncia (0.5), tem a maior probabilidade de ser selecionada (0.461).

A busca associativa √© um problema crucial no aprendizado por refor√ßo, pois permite que os agentes aprendam a tomar decis√µes √≥timas em diferentes contextos, aproximando-se dos desafios do problema de RL completo.

**Lema 2: Condi√ß√£o para Converg√™ncia em Busca Associativa**
*Declara√ß√£o*: Para garantir a converg√™ncia para a pol√≠tica √≥tima em tarefas de busca associativa, √© necess√°rio que o algoritmo de aprendizado explore suficientemente o espa√ßo de a√ß√µes em cada contexto e que os par√¢metros de aprendizado sejam adequadamente ajustados.
*Prova*:
A converg√™ncia para a pol√≠tica √≥tima implica que a probabilidade de selecionar a a√ß√£o correta em cada contexto se aproxima de 1 conforme o n√∫mero de itera√ß√µes aumenta. Para isso, o algoritmo precisa explorar a√ß√µes sub-√≥timas no in√≠cio para descobrir a a√ß√£o √≥tima em cada contexto. A explora√ß√£o √© geralmente controlada por um par√¢metro $\epsilon$ em algoritmos $\epsilon$-greedy, onde o agente age aleatoriamente com probabilidade $\epsilon$ e escolhe a a√ß√£o gulosa com probabilidade $1 - \epsilon$. A taxa de aprendizado $\alpha$ deve ser cuidadosamente ajustada para garantir que o algoritmo aprenda rapidamente, mas sem oscilar ou divergir. Uma taxa de aprendizado muito alta pode causar oscila√ß√µes, enquanto uma taxa muito baixa pode fazer o aprendizado ser muito lento. Portanto, um decaimento gradual da taxa de explora√ß√£o $\epsilon$ e da taxa de aprendizado $\alpha$ √© frequentemente necess√°rio. $\blacksquare$
```mermaid
graph LR
    subgraph "Converg√™ncia em Busca Associativa"
    A["Explora√ß√£o Suficiente (Œµ-greedy)"]
    B["Par√¢metros de Aprendizado Ajustados (Œ±)"]
    C["Decaimento Gradual de Œµ e Œ±"]
    D["Converg√™ncia para Pol√≠tica √ìtima"]
    A --> D
    B --> D
    C --> D
   end
   style A fill:#ccf,stroke:#333,stroke-width:1px;
   style B fill:#ccf,stroke:#333,stroke-width:1px;
    style C fill:#ccf,stroke:#333,stroke-width:1px;
   style D fill:#cfc,stroke:#333,stroke-width:2px;
```

> üí° **Exemplo Num√©rico:** Em um experimento de busca associativa com duas m√°quinas ca√ßa-n√≠queis (vermelha e azul) e dois bra√ßos cada, podemos usar um algoritmo $\epsilon$-greedy com $\epsilon = 0.2$ e uma taxa de aprendizado $\alpha = 0.1$. Inicialmente, todas as prefer√™ncias de a√ß√£o s√£o 0.
>
> - **Explora√ß√£o**: Com probabilidade 0.2, o agente escolher√° um bra√ßo aleatoriamente.
> - **Exploita√ß√£o**: Com probabilidade 0.8, o agente escolher√° o bra√ßo com a maior prefer√™ncia para a m√°quina atual.
>
>  Suponha que o agente esteja interagindo com a m√°quina vermelha. No primeiro passo, o agente escolhe aleatoriamente o bra√ßo 1 (explora√ß√£o), recebendo uma recompensa de 1. A prefer√™ncia de a√ß√£o para o bra√ßo 1 na m√°quina vermelha ser√° atualizada:
>
> $H_{t+1}(1) = H_t(1) + \alpha (R_t - H_t(1)) = 0 + 0.1(1 - 0) = 0.1$
>
>  Se a pr√≥xima intera√ß√£o tamb√©m for com a m√°quina vermelha e o agente escolher o bra√ßo 2 (explora√ß√£o), recebendo uma recompensa de 0.2, a prefer√™ncia do bra√ßo 2 ser√° atualizada:
>
> $H_{t+1}(2) = H_t(2) + \alpha (R_t - H_t(2)) = 0 + 0.1(0.2 - 0) = 0.02$
>
> O algoritmo continuar√° atualizando as prefer√™ncias. √Ä medida que o n√∫mero de intera√ß√µes aumenta, a probabilidade de explora√ß√£o $(\epsilon)$ pode ser reduzida gradualmente, concentrando-se mais na explora√ß√£o. O valor da taxa de aprendizado $(\alpha)$ tamb√©m pode ser reduzido para estabilizar o processo. Esta abordagem garante que o algoritmo explore suficientemente o espa√ßo de a√ß√µes e encontre a pol√≠tica √≥tima.

**Teorema 1: Rela√ß√£o entre Busca Associativa e Aprendizado por Refor√ßo**
*Declara√ß√£o*: Tarefas de busca associativa s√£o um caso especial de aprendizado por refor√ßo, onde o estado √© o contexto e n√£o h√° transi√ß√µes entre estados.
*Prova*:
Em tarefas de busca associativa, cada contexto pode ser visto como um estado espec√≠fico. O agente recebe um contexto (estado) $s_t$ e seleciona uma a√ß√£o $a_t$ com base na pol√≠tica atual. A recompensa $r_t$ √© recebida imediatamente ap√≥s a sele√ß√£o da a√ß√£o, dependendo do contexto. N√£o h√° transi√ß√µes de estado como no aprendizado por refor√ßo geral. Portanto, a busca associativa pode ser vista como um caso especial de aprendizado por refor√ßo onde a din√¢mica do ambiente √© limitada a uma correspond√™ncia entre contextos, a√ß√µes e recompensas imediatas. Em outras palavras, em tarefas de busca associativa o "estado" √© o contexto, e n√£o h√° transi√ß√£o para outros estados; uma a√ß√£o leva a uma recompensa imediata dentro do mesmo contexto. $\blacksquare$
```mermaid
graph LR
    A["Busca Associativa"] -- "Estado = Contexto" --> B
    B["A√ß√£o (a_t)"] --> C
    C["Recompensa (r_t)"]
    B -- "Sem transi√ß√µes de estado" --> B
    style A fill:#ccf,stroke:#333,stroke-width:1px
    style B fill:#ccf,stroke:#333,stroke-width:1px
     style C fill:#cfc,stroke:#333,stroke-width:2px
    subgraph "Rela√ß√£o com Aprendizado por Refor√ßo"
        A
        B
        C
    end
```

### Conclus√£o
As tarefas de busca associativa representam um passo fundamental na compreens√£o de como agentes de *reinforcement learning* podem lidar com m√∫ltiplos contextos e aprender *pol√≠ticas* que mapeiam situa√ß√µes para a√ß√µes ideais. Elas se posicionam como intermedi√°rias entre os problemas simples *k-armed bandit* e o problema completo de *reinforcement learning*, fornecendo um ambiente de estudo para estrat√©gias de aprendizado que consideram o contexto. A capacidade de aprender associa√ß√µes entre situa√ß√µes e a√ß√µes permite que os agentes tomem decis√µes mais eficazes em ambientes complexos e din√¢micos. Embora o problema completo de RL ainda esteja distante, as tarefas de busca associativa nos proporcionam uma base s√≥lida para explorar estrat√©gias de aprendizado por refor√ßo.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of
learning is that it uses training information that evaluates the actions taken rather
than instructs by giving correct actions. This is what creates the need for active
exploration, for an explicit search for good behavior." *(Trecho de Chapter 2)*
[^17]: "So far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no need to associate different actions with different situations...However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations." *(Trecho de Chapter 2)*
[^13]: "Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks." *(Trecho de Chapter 2)*
