## Associative Search (Contextual Bandits)

### Introdu√ß√£o

Neste cap√≠tulo, at√© agora, foram considerados somente problemas n√£o associativos, ou seja, problemas onde n√£o h√° necessidade de associar diferentes a√ß√µes com diferentes situa√ß√µes [1]. Nesses problemas, o agente tenta encontrar a melhor a√ß√£o quando o problema √© estacion√°rio, ou acompanha a melhor a√ß√£o √† medida que ela muda quando o problema √© n√£o-estacion√°rio [1]. No entanto, em um problema geral de **reinforcement learning**, existem m√∫ltiplas situa√ß√µes, e o objetivo √© aprender uma pol√≠tica: um mapeamento de situa√ß√µes para as a√ß√µes que s√£o melhores nessas situa√ß√µes [1]. Para preparar o terreno para o problema completo, ser√° discutida brevemente a forma mais simples pela qual os problemas n√£o associativos se estendem ao cen√°rio associativo [1].

### Conceitos Fundamentais

Imagine que existem v√°rios problemas diferentes de *k*-armed bandits, e em cada passo o agente se depara com um desses problemas, escolhidos aleatoriamente [1]. Assim, o problema de *bandit* muda aleatoriamente de passo a passo. Se as probabilidades com que cada problema √© selecionado n√£o mudam com o tempo, isso equivaleria a um √∫nico problema estacion√°rio de *k*-armed bandit, e um dos m√©todos descritos neste cap√≠tulo poderia ser usado [1].

Agora, suponha que quando um problema de *bandit* √© selecionado, o agente recebe uma pista distintiva sobre sua identidade (mas n√£o seus valores de a√ß√£o) [1]. Talvez o agente esteja diante de uma m√°quina ca√ßa-n√≠queis que muda a cor de sua tela conforme seus valores de a√ß√£o mudam. Agora √© poss√≠vel aprender uma pol√≠tica associando cada problema, sinalizado pela cor, com a melhor a√ß√£o para aquele problema. Por exemplo, se for vermelho, selecione a alavanca 1; se for verde, selecione a alavanca 2 [1]. Com a pol√≠tica correta, √© poss√≠vel obter resultados melhores do que na aus√™ncia de informa√ß√µes que distinguem um problema de *bandit* do outro [1].

Este √© um exemplo de **associative search task**, assim chamado porque envolve tanto aprendizado por tentativa e erro para encontrar as melhores a√ß√µes, quanto a associa√ß√£o dessas a√ß√µes com as situa√ß√µes em que s√£o melhores [1]. As **associative search tasks** s√£o frequentemente chamadas de **contextual bandits** na literatura [1]. **Contextual bandits** s√£o um meio termo entre o problema do *k*-armed bandit e o problema completo de *reinforcement learning*. S√£o semelhantes ao problema completo de *reinforcement learning* porque envolvem o aprendizado de uma pol√≠tica. No entanto, tamb√©m s√£o semelhantes √† vers√£o do problema do *k*-armed bandit porque cada a√ß√£o afeta apenas a recompensa imediata [1].

```mermaid
graph LR
    A["k-armed Bandit"] --> B("Contextual Bandit");
    B --> C("Full Reinforcement Learning");
    B --> D["A√ß√µes afetam apenas recompensa imediata"];
    C --> E["A√ß√µes afetam o pr√≥ximo estado e recompensa"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9ff,stroke:#333,stroke-width:2px
```

*A principal distin√ß√£o aqui √© que, em contextual bandits, as a√ß√µes n√£o influenciam o pr√≥ximo estado ou situa√ß√£o, apenas a recompensa obtida no momento atual.* Isto √©, a a√ß√£o n√£o tem influ√™ncia sobre a *pr√≥xima situa√ß√£o*, ou seja, qual bandit ser√° sorteado na pr√≥xima itera√ß√£o [1].

**Proposi√ß√£o 1:** Em um problema de *contextual bandit*, a sequ√™ncia de contextos √© independente da a√ß√£o tomada pelo agente, o que simplifica o processo de aprendizado da pol√≠tica comparado a problemas onde a a√ß√£o influencia a transi√ß√£o para o pr√≥ximo estado.
*Prova:* A defini√ß√£o do problema *contextual bandit* estabelece que a a√ß√£o escolhida afeta apenas a recompensa imediata e n√£o o pr√≥ximo contexto. Portanto, o processo de escolha do contexto √© ex√≥geno e n√£o influenciado pelas a√ß√µes do agente. $\blacksquare$

> üí° **Exemplo Num√©rico:** Imagine que temos dois contextos, $s_1$ (tela vermelha) e $s_2$ (tela verde), e tr√™s a√ß√µes poss√≠veis (alavancas), $a_1$, $a_2$, e $a_3$. As recompensas associadas a cada a√ß√£o em cada contexto s√£o as seguintes:
>
> *   Contexto $s_1$: $a_1 \rightarrow 10$, $a_2 \rightarrow 2$, $a_3 \rightarrow 5$
> *   Contexto $s_2$: $a_1 \rightarrow 1$, $a_2 \rightarrow 9$, $a_3 \rightarrow 3$
>
> Uma pol√≠tica √≥tima para este problema seria selecionar $a_1$ quando o contexto √© $s_1$ e $a_2$ quando o contexto √© $s_2$.  A sequ√™ncia de contextos √© aleat√≥ria e independe das a√ß√µes escolhidas. Por exemplo, a sequ√™ncia pode ser $s_1, s_2, s_1, s_1, s_2$, etc., e as a√ß√µes tomadas em cada etapa n√£o influenciam qual contexto ser√° o pr√≥ximo.
>
> Em um problema tradicional de *k*-armed bandits, sem contexto, o agente tentaria aprender qual das tr√™s a√ß√µes √© melhor *em m√©dia*, enquanto que no *contextual bandit* o agente aprende qual a√ß√£o √© melhor *dado um contexto*.

Se as a√ß√µes fossem permitidas para afetar a *pr√≥xima situa√ß√£o* bem como a recompensa, ter√≠amos o problema completo de *reinforcement learning* [1]. Este problema ser√° apresentado no pr√≥ximo cap√≠tulo e suas ramifica√ß√µes ser√£o consideradas ao longo do restante deste livro [1].

**Lemma 1:** A transi√ß√£o do problema de *k*-armed bandits para o **contextual bandit** adiciona a complexidade de aprendizado de uma pol√≠tica que associa contextos (ou situa√ß√µes) a a√ß√µes espec√≠ficas, enquanto o problema de *k*-armed bandits busca encontrar a melhor a√ß√£o em uma situa√ß√£o fixa.

*Prova:* No problema de *k*-armed bandits, a fun√ß√£o √≥tima √© expressa como $$a^* = \text{argmax}_a Q(a)$$, ou seja, seleciona a a√ß√£o que maximiza a recompensa esperada. Ao introduzir contextos (ou estados), passamos a ter $$a^* = \text{argmax}_a Q(s,a)$$, onde cada contexto $s$ tem sua pr√≥pria fun√ß√£o de valor, o que significa aprender um mapeamento (uma pol√≠tica) de contextos para a√ß√µes. $\blacksquare$

```mermaid
graph LR
    subgraph "k-armed Bandit"
        A["Fun√ß√£o de valor: Q(a)"]
        B["A√ß√£o √≥tima: a* = argmax Q(a)"]
    end
    subgraph "Contextual Bandit"
        C["Fun√ß√£o de valor: Q(s,a)"]
        D["A√ß√£o √≥tima: a* = argmax Q(s,a)"]
        E["Pol√≠tica: Mapeamento s -> a"]
    end
    A --> B
    C --> D
    D --> E
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

**Lemma 1.1:** Em *contextual bandits*, a complexidade de aprendizado reside na necessidade de estimar $Q(s,a)$ para cada par $(s, a)$, o que aumenta o espa√ßo de busca quando comparado ao problema de *k*-armed bandits.
*Prova:* No problema de *k*-armed bandits, temos que estimar $Q(a)$ para $k$ a√ß√µes. No problema de *contextual bandits*, temos que estimar $Q(s, a)$ para cada par de contextos $s$ e a√ß√µes $a$. Se temos $m$ contextos e $k$ a√ß√µes, o n√∫mero de par√¢metros a serem aprendidos √© $m \times k$, o que √© uma complexidade maior quando comparado a $k$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um problema com 2 contextos ($m=2$) e 3 a√ß√µes ($k=3$). No problema de *k*-armed bandits, precisar√≠amos estimar 3 valores de $Q(a)$, um para cada a√ß√£o. No *contextual bandit*, precisamos estimar $Q(s, a)$ para cada par $(s, a)$, ou seja, $2 \times 3 = 6$ valores: $Q(s_1, a_1), Q(s_1, a_2), Q(s_1, a_3), Q(s_2, a_1), Q(s_2, a_2), Q(s_2, a_3)$. Este aumento no n√∫mero de valores a serem estimados ilustra o aumento da complexidade do problema. Se tiv√©ssemos, por exemplo, 10 contextos e 5 a√ß√µes, precisar√≠amos de 50 valores a serem estimados, enquanto no *k*-armed bandit seriam apenas 5.
>
> O espa√ßo de busca √© o conjunto de todas as poss√≠veis pol√≠ticas. Em um problema *k*-armed bandit, o espa√ßo de busca √© o conjunto das poss√≠veis a√ß√µes. Em *contextual bandits*, o espa√ßo de busca √© o conjunto das poss√≠veis fun√ß√µes que mapeiam contextos para a√ß√µes, e esse espa√ßo cresce exponencialmente com o n√∫mero de contextos.
>
> Por exemplo, se tivermos 2 contextos e 2 a√ß√µes, temos 4 poss√≠veis pol√≠ticas:
>
> 1.  $s_1 \rightarrow a_1$, $s_2 \rightarrow a_1$
> 2.  $s_1 \rightarrow a_1$, $s_2 \rightarrow a_2$
> 3.  $s_1 \rightarrow a_2$, $s_2 \rightarrow a_1$
> 4.  $s_1 \rightarrow a_2$, $s_2 \rightarrow a_2$

**Corol√°rio 1:** No problema de *k*-armed bandits, se a recompensa esperada de cada a√ß√£o √© constante, existe uma pol√≠tica √≥tima √∫nica; por√©m, em **contextual bandits**, a pol√≠tica √≥tima pode mudar dependendo do contexto, necessitando um mecanismo de aprendizado que identifique e se adapte a diferentes contextos.

**Corol√°rio 1.1:** A necessidade de aprendizado adaptativo em *contextual bandits* significa que m√©todos de explora√ß√£o-explota√ß√£o s√£o importantes, pois o agente precisa explorar diferentes a√ß√µes em diferentes contextos para descobrir a pol√≠tica √≥tima.

```mermaid
sequenceDiagram
    participant Agente
    participant Ambiente
    Agente->>Ambiente: Escolhe a√ß√£o a
    Ambiente->>Agente: Retorna recompensa R e contexto s'
    loop Iterar
    Ambiente->>Agente: Apresenta contexto s
    Agente->>Ambiente: Escolhe a√ß√£o a' (Explora√ß√£o/Explota√ß√£o)
    Ambiente->>Agente: Retorna recompensa R' e contexto s''
    end
```

> üí° **Exemplo Num√©rico:** Vamos considerar novamente o exemplo de dois contextos e tr√™s a√ß√µes com as recompensas dadas anteriormente. Se usarmos um m√©todo *greedy* simples (explora√ß√£o = 0), o agente escolher√° sempre a a√ß√£o com maior recompensa estimada *at√© o momento* para cada contexto. Inicialmente, as estimativas das recompensas ser√£o iguais para todas as a√ß√µes, por exemplo, 0.
>
> *   **Itera√ß√£o 1:** Contexto $s_1$, escolhe aleatoriamente $a_1$, recebe recompensa 10. $Q(s_1, a_1) = 10$, outras recompensas $Q(s,a)=0$.
> *   **Itera√ß√£o 2:** Contexto $s_2$, escolhe aleatoriamente $a_2$, recebe recompensa 9. $Q(s_2, a_2) = 9$, outras recompensas $Q(s,a)=0$.
> *   **Itera√ß√£o 3:** Contexto $s_1$, escolhe $a_1$ (maior valor), recebe recompensa 10.  $Q(s_1, a_1)$ atualizado, outras recompensas mant√™m-se.
>
> Se o agente fosse *greedy*, ap√≥s as duas primeiras itera√ß√µes, ele nunca mais exploraria outras a√ß√µes e ficaria preso a uma pol√≠tica sub√≥tima no caso de que, por exemplo, $a_3$ fosse a a√ß√£o √≥tima em $s_1$.
>
> Um m√©todo $\epsilon$-greedy, por exemplo, permitiria ao agente explorar outras a√ß√µes com probabilidade $\epsilon$, o que poderia levar a descobrir que $a_3$ em $s_1$ ou $a_1$ em $s_2$ poderiam ser melhores a√ß√µes a longo prazo. Por exemplo, em um determinado passo onde $\epsilon$ √© acionado, o agente pode escolher a a√ß√£o $a_2$ em $s_1$, recebendo uma recompensa de 2, e atualizando $Q(s_1, a_2)$. Com o tempo, o agente adaptaria sua pol√≠tica de acordo com os valores atualizados.

### Conclus√£o

O **associative search** (ou **contextual bandits**) introduz a ideia de que as a√ß√µes devem ser selecionadas n√£o apenas com base em seus valores de recompensa, mas tamb√©m no contexto em que s√£o aplicadas [1]. Essa abordagem difere do problema de *k*-armed bandits ao exigir que o agente aprenda uma pol√≠tica, o que torna a tomada de decis√µes mais dependente da situa√ß√£o. Ao mesmo tempo, a natureza do problema **contextual bandit** ainda √© restrita se comparada a um problema de **reinforcement learning** completo, pois as a√ß√µes tomadas n√£o t√™m impacto nas situa√ß√µes futuras [1].

### Refer√™ncias
[^1]: "So far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no need to associate different actions with different situations. In these tasks the learner either tries to find a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary. However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations. To set the stage for the full problem, we briefly discuss the simplest way in which nonassociative tasks extend to the associative setting. As an example, suppose there are several different k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. If the probabilities with which each task is selected for you do not change over time, this would appear as a single stationary k-armed bandit task, and you could use one of the methods described in this chapter. Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that task‚Äîfor instance, if red, select arm 1; if green, select arm 2. With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another. This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature. Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem. We present this problem in the next chapter and consider its ramifications throughout the rest of the book." *(Trecho de Chapter 2: Multi-armed Bandits)*
