## Associative Search (Contextual Bandits) em Reinforcement Learning

### IntroduÃ§Ã£o
O aprendizado por reforÃ§o se distingue de outras formas de aprendizado por utilizar informaÃ§Ãµes de treinamento que avaliam as aÃ§Ãµes tomadas, em vez de instruir fornecendo as aÃ§Ãµes corretas [^1]. Este aspecto avaliativo Ã© fundamental para a necessidade de exploraÃ§Ã£o ativa e busca explÃ­cita por um bom comportamento. No contexto de **multi-armed bandits**, exploramos inicialmente cenÃ¡rios *nÃ£o-associativos*, onde o aprendizado nÃ£o envolve agir em mÃºltiplas situaÃ§Ãµes. Este capÃ­tulo aprofunda esse tema, introduzindo **cenÃ¡rios associativos** onde o aprendizado se estende para mapear aÃ§Ãµes a diferentes situaÃ§Ãµes, o que leva ao conceito de **contextual bandits** [^1]. Em contraste com o feedback *instrutivo*, que indica a aÃ§Ã£o correta independentemente da aÃ§Ã£o tomada, o feedback *avaliativo* depende inteiramente da aÃ§Ã£o tomada. O aprendizado por reforÃ§o foca no aspecto avaliativo, que possibilita a exploraÃ§Ã£o e a descoberta de quais aÃ§Ãµes sÃ£o melhores em cada situaÃ§Ã£o especÃ­fica.

**ProposiÃ§Ã£o 1:** *O feedback avaliativo, ao contrÃ¡rio do feedback instrutivo, nÃ£o fornece explicitamente a aÃ§Ã£o Ã³tima, mas sim uma recompensa associada Ã  aÃ§Ã£o tomada, o que exige exploraÃ§Ã£o para identificar as melhores aÃ§Ãµes em cada contexto. Essa caracterÃ­stica Ã© essencial para o aprendizado em ambientes complexos.*

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine um cenÃ¡rio com duas aÃ§Ãµes (AÃ§Ã£o 1 e AÃ§Ã£o 2). Em feedback instrutivo, o sistema te diria "AÃ§Ã£o 1 Ã© a correta". Em feedback avaliativo, vocÃª tenta AÃ§Ã£o 1 e recebe uma recompensa de 0.5; depois tenta AÃ§Ã£o 2 e recebe uma recompensa de 0.8. VocÃª nÃ£o soube *diretamente* qual a melhor, mas aprendeu pela experiÃªncia.  Este feedback avaliativo Ã© o cerne do aprendizado por reforÃ§o.

```mermaid
graph LR
    A[Feedback Instrutivo] -->|Informa "AÃ§Ã£o Correta"| B("AÃ§Ã£o Ã“tima Conhecida");
    C[Feedback Avaliativo] -->|Recompensa apÃ³s aÃ§Ã£o| D("AÃ§Ã£o â†’ Recompensa");
    D --> E("ExploraÃ§Ã£o NecessÃ¡ria");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
```

A necessidade de exploraÃ§Ã£o, decorrente da natureza avaliativa do feedback, impÃµe um *trade-off* entre explorar aÃ§Ãµes desconhecidas na esperanÃ§a de encontrar aÃ§Ãµes melhores, e explorar as aÃ§Ãµes conhecidas que proporcionam um bom resultado. Esse *trade-off* Ã© intrÃ­nseco ao aprendizado por reforÃ§o e serÃ¡ explorado com mais profundidade nas seÃ§Ãµes subsequentes.

### Conceitos Fundamentais
Nos problemas de **k-armed bandit**, o objetivo Ã© maximizar a recompensa total esperada ao longo do tempo, fazendo escolhas repetidas entre *k* opÃ§Ãµes ou aÃ§Ãµes, onde cada escolha leva a uma recompensa numÃ©rica selecionada a partir de uma distribuiÃ§Ã£o de probabilidade estacionÃ¡ria que depende da aÃ§Ã£o escolhida [^1]. Em cenÃ¡rios *nÃ£o-associativos*, buscamos uma Ãºnica melhor aÃ§Ã£o, seja ela fixa ou variÃ¡vel no tempo. No entanto, no aprendizado por reforÃ§o, a complexidade surge quando o ambiente apresenta mÃºltiplas situaÃ§Ãµes, exigindo que o agente aprenda uma *polÃ­tica* que mapeia situaÃ§Ãµes para aÃ§Ãµes apropriadas. Essa Ã© a essÃªncia dos problemas de **busca associativa**, tambÃ©m conhecidos como **contextual bandits** [^1].

**Lema 1:** *Em um problema de k-armed bandit nÃ£o-associativo, se as distribuiÃ§Ãµes de recompensa para cada aÃ§Ã£o forem estacionÃ¡rias, existe uma aÃ§Ã£o Ã³tima que maximiza a recompensa esperada a longo prazo.*

*Proof Strategy:* Isso segue diretamente da definiÃ§Ã£o de problemas de k-armed bandits nÃ£o-associativos e da existÃªncia de um valor esperado para cada distribuiÃ§Ã£o estacionÃ¡ria. A aÃ§Ã£o com o maior valor esperado Ã© a aÃ§Ã£o Ã³tima.

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um k-armed bandit com 3 aÃ§Ãµes (k=3). As recompensas de cada aÃ§Ã£o seguem uma distribuiÃ§Ã£o normal: AÃ§Ã£o 1 ~ N(1, 0.5), AÃ§Ã£o 2 ~ N(2, 0.5) e AÃ§Ã£o 3 ~ N(1.5, 0.5). A aÃ§Ã£o Ã³tima Ã© a AÃ§Ã£o 2, pois tem a maior recompensa esperada (mÃ©dia de 2). Em um cenÃ¡rio nÃ£o-associativo, o agente aprende a escolher essa aÃ§Ã£o repetidamente.

```mermaid
graph LR
    A[k-armed Bandit] --> B("AÃ§Ã£o 1 ~ N(1, 0.5)");
    A --> C("AÃ§Ã£o 2 ~ N(2, 0.5)");
    A --> D("AÃ§Ã£o 3 ~ N(1.5, 0.5)");
    B --> E("Recompensa Esperada = 1");
    C --> F("Recompensa Esperada = 2");
    D --> G("Recompensa Esperada = 1.5");
    F --> H("AÃ§Ã£o Ã“tima");
     style A fill:#fff,stroke:#333,stroke-width:2px
     style H fill:#ccf,stroke:#333,stroke-width:2px
```

A **busca associativa** envolve tanto a aprendizagem por tentativa e erro para encontrar as melhores aÃ§Ãµes, quanto a associaÃ§Ã£o dessas aÃ§Ãµes com as situaÃ§Ãµes em que elas sÃ£o mais eficazes. Este cenÃ¡rio Ã© uma ponte entre o problema do k-armed bandit e o problema completo de aprendizado por reforÃ§o, pois cada aÃ§Ã£o afeta apenas a recompensa imediata, e nÃ£o o estado futuro.

*O objetivo Ã© aprender uma polÃ­tica*: uma funÃ§Ã£o que mapeia cada situaÃ§Ã£o para a melhor aÃ§Ã£o a ser tomada naquela situaÃ§Ã£o. Em problemas **nÃ£o-associativos**, ou o agente tenta encontrar uma Ãºnica melhor aÃ§Ã£o (em um ambiente estacionÃ¡rio) ou acompanhar a melhor aÃ§Ã£o que muda ao longo do tempo (em um ambiente nÃ£o estacionÃ¡rio). No entanto, em problemas mais gerais, cada situaÃ§Ã£o pode exigir uma aÃ§Ã£o diferente. Para ilustrar, imagine um cenÃ¡rio onde se tem vÃ¡rios problemas de k-armed bandits e a cada passo um desses problemas Ã© selecionado aleatoriamente [^1]. Se a probabilidade de seleÃ§Ã£o de cada problema nÃ£o muda ao longo do tempo, o problema pode ser visto como um k-armed bandit estacionÃ¡rio. No entanto, se for oferecida uma pista distintiva sobre a identidade do problema (como uma mudanÃ§a na cor do display de uma slot machine), o agente pode aprender uma polÃ­tica para associar cada pista Ã  melhor aÃ§Ã£o para aquele problema. Por exemplo, "se vermelho, selecionar o braÃ§o 1; se verde, selecionar o braÃ§o 2" [^1]. Esta Ã© a essÃªncia de um **contextual bandit**, e Ã© um passo crucial em direÃ§Ã£o ao aprendizado por reforÃ§o.

**DefiniÃ§Ã£o 1:** *Uma polÃ­tica $\pi$ em um problema de contextual bandit Ã© uma funÃ§Ã£o $\pi: S \rightarrow A$, que mapeia cada estado (contexto) $s \in S$ para uma aÃ§Ã£o $a \in A$. O objetivo do agente Ã© aprender uma polÃ­tica $\pi^*$ que maximize a recompensa esperada em cada estado.*

> ðŸ’¡ **Exemplo NumÃ©rico:** Em um contexto de recomendaÃ§Ã£o de filmes, $S$ pode ser o perfil do usuÃ¡rio (idade, gÃªnero, histÃ³rico de visualizaÃ§Ãµes) e $A$ pode ser o conjunto de filmes disponÃ­veis. Uma polÃ­tica $\pi$ mapeia um perfil de usuÃ¡rio para um filme a ser recomendado. O objetivo Ã© aprender uma polÃ­tica $\pi^*$ que maximize a probabilidade de o usuÃ¡rio assistir e gostar do filme recomendado. Por exemplo, $\pi(\text{user\_age=25, genre='Action'}) = \text{movie\_id=123}$

```mermaid
flowchart LR
    subgraph "Contextual Bandit"
        S["Estado (Contexto) s âˆˆ S"]
        A["AÃ§Ã£o a âˆˆ A"]
        P["PolÃ­tica Ï€: S â†’ A"]
    end
    S --> P
    P --> A
    style S fill:#fff,stroke:#333,stroke-width:2px
     style A fill:#fff,stroke:#333,stroke-width:2px
     style P fill:#ccf,stroke:#333,stroke-width:2px
```

**Lema 1.1:** *Em um problema de contextual bandit com um conjunto finito de contextos $S$, a aprendizagem de uma polÃ­tica Ã³tima $\pi^*$ pode ser vista como a aprendizagem de mÃºltiplas polÃ­ticas Ã³timas, uma para cada contexto, mas com a dificuldade de que a exploraÃ§Ã£o deve considerar simultaneamente todos os contextos.*

*Proof Strategy:* Este lema demonstra que, embora a aprendizagem de polÃ­ticas por contexto se assemelhe Ã  resoluÃ§Ã£o de mÃºltiplos k-armed bandits nÃ£o-associativos, o problema se torna mais complexo porque o agente deve aprender qual aÃ§Ã£o Ã© Ã³tima para qual contexto, e isso requer uma exploraÃ§Ã£o combinada que nÃ£o Ã© necessÃ¡ria nos casos nÃ£o associativos.

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine dois contextos: $S = \{s_1, s_2\}$. Em $s_1$, a aÃ§Ã£o $a_1$ Ã© Ã³tima; em $s_2$, a aÃ§Ã£o $a_2$ Ã© Ã³tima. Resolver o problema de contextual bandit exige explorar aÃ§Ãµes em ambos os contextos para descobrir a aÃ§Ã£o Ã³tima em cada um. O agente deve aprender uma polÃ­tica: $\pi(s_1) = a_1$ e $\pi(s_2) = a_2$.

```mermaid
graph LR
    subgraph Contexto s1
        A("AÃ§Ã£o a1")
         style A fill:#ccf,stroke:#333,stroke-width:2px
    end
    subgraph Contexto s2
        B("AÃ§Ã£o a2")
         style B fill:#ccf,stroke:#333,stroke-width:2px
    end
    C["PolÃ­tica Ï€"] --> |"Ï€(s1) = a1"| A
    C --> |"Ï€(s2) = a2"| B
    style C fill:#fff,stroke:#333,stroke-width:2px
```

O aprendizado por reforÃ§o, em sua forma completa, permite que as aÃ§Ãµes influenciem tanto a recompensa imediata quanto a situaÃ§Ã£o futura, criando uma dinÃ¢mica complexa que exige uma polÃ­tica que lide com ambas [^1].

### ConclusÃ£o
A transiÃ§Ã£o dos problemas de *k*-armed bandits nÃ£o-associativos para o cenÃ¡rio de busca associativa representa um passo crucial na compreensÃ£o do aprendizado por reforÃ§o. O objetivo em **contextual bandits** nÃ£o Ã© apenas encontrar a melhor aÃ§Ã£o, mas sim *aprender uma polÃ­tica* que mapeia cada situaÃ§Ã£o para a aÃ§Ã£o apropriada. Esta capacidade de generalizaÃ§Ã£o para diferentes contextos Ã© uma das chaves para a resoluÃ§Ã£o de problemas de aprendizado por reforÃ§o mais complexos. A capacidade de associar aÃ§Ãµes a situaÃ§Ãµes permite que os agentes adaptem seu comportamento de maneira mais eficiente, o que Ã© fundamental para o sucesso em ambientes complexos e dinÃ¢micos. Os contextual bandits servem como um passo intermediÃ¡rio entre os problemas mais simples de *k*-armed bandits e os problemas de aprendizado por reforÃ§o completo, onde as aÃ§Ãµes influenciam o estado futuro.

**Teorema 1:** *Em problemas de aprendizado por reforÃ§o, a complexidade do problema aumenta Ã  medida que passamos de problemas nÃ£o-associativos (k-armed bandit) para problemas associativos (contextual bandit) e, finalmente, para problemas de aprendizado por reforÃ§o completo, onde as aÃ§Ãµes afetam nÃ£o apenas a recompensa imediata, mas tambÃ©m o estado futuro.*

*Proof Strategy:* Este teorema sintetiza os argumentos do texto, demonstrando uma hierarquia de complexidade e a necessidade de abordagens de aprendizado cada vez mais sofisticadas para lidar com ambientes mais complexos.

```mermaid
graph LR
    A["k-armed Bandit (NÃ£o-Associativo)"] -->|Simples| B("Contextual Bandit (Associativo)");
    B -->|Complexo| C("Aprendizado por ReforÃ§o Completo");
    A --> D("Recompensa Imediata");
     B --> E("Recompensa Imediata + Contexto");
    C --> F("Recompensa Imediata + Estado Futuro");
     style A fill:#fff,stroke:#333,stroke-width:2px
    style B fill:#fff,stroke:#333,stroke-width:2px
      style C fill:#ccf,stroke:#333,stroke-width:2px

```

**CorolÃ¡rio 1:** *Os algoritmos projetados para resolver problemas de k-armed bandit nÃ£o-associativos podem nÃ£o ser eficazes em problemas de contextual bandit, e algoritmos projetados para contextual bandit podem nÃ£o ser eficazes em problemas de aprendizado por reforÃ§o completo devido ao impacto das aÃ§Ãµes nos estados futuros.*

> ðŸ’¡ **Exemplo NumÃ©rico:** Um algoritmo simples para k-armed bandits, como epsilon-greedy, pode funcionar bem para escolher a melhor mÃ¡quina caÃ§a-nÃ­queis (nÃ£o-associativo). Mas, em um cenÃ¡rio de contextual bandit (como recomendar anÃºncios), usar epsilon-greedy diretamente sem levar em consideraÃ§Ã£o o contexto (usuÃ¡rio, hora, etc.) levarÃ¡ a escolhas subÃ³timas. Um algoritmo de RL completo, como Q-learning, Ã© necessÃ¡rio quando a aÃ§Ã£o de hoje afeta o estado de amanhÃ£, o que um contextual bandit nÃ£o cobre.

```mermaid
graph LR
    A["Epsilon-Greedy (k-armed)"] -->|Funciona| B("MÃ¡quina caÃ§a-nÃ­queis");
     A -->|Ineficaz| C("RecomendaÃ§Ã£o de anÃºncios (Contextual Bandit)");
    D["Q-learning (RL Completo)"] -->|NecessÃ¡rio| E("AÃ§Ã£o afeta estado futuro");
        style A fill:#fff,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

### ReferÃªncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions... The particular nonassociative, evaluative feedback problem that we explore is a simple version of the k-armed bandit problem... At the end of this chapter, we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when the best action depends on the situation." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
[^2]: "So far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no need to associate different actions with different situations... However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
[^3]: "This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
