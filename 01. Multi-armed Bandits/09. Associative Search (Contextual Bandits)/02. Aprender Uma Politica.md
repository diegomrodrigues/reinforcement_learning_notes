## Associative Search (Contextual Bandits) em Reinforcement Learning

### Introdu√ß√£o
O aprendizado por refor√ßo se distingue de outras formas de aprendizado por utilizar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de instruir fornecendo as a√ß√µes corretas [^1]. Este aspecto avaliativo √© fundamental para a necessidade de explora√ß√£o ativa e busca expl√≠cita por um bom comportamento. No contexto de **multi-armed bandits**, exploramos inicialmente cen√°rios *n√£o-associativos*, onde o aprendizado n√£o envolve agir em m√∫ltiplas situa√ß√µes. Este cap√≠tulo aprofunda esse tema, introduzindo **cen√°rios associativos** onde o aprendizado se estende para mapear a√ß√µes a diferentes situa√ß√µes, o que leva ao conceito de **contextual bandits** [^1]. Em contraste com o feedback *instrutivo*, que indica a a√ß√£o correta independentemente da a√ß√£o tomada, o feedback *avaliativo* depende inteiramente da a√ß√£o tomada. O aprendizado por refor√ßo foca no aspecto avaliativo, que possibilita a explora√ß√£o e a descoberta de quais a√ß√µes s√£o melhores em cada situa√ß√£o espec√≠fica.

**Proposi√ß√£o 1:** *O feedback avaliativo, ao contr√°rio do feedback instrutivo, n√£o fornece explicitamente a a√ß√£o √≥tima, mas sim uma recompensa associada √† a√ß√£o tomada, o que exige explora√ß√£o para identificar as melhores a√ß√µes em cada contexto. Essa caracter√≠stica √© essencial para o aprendizado em ambientes complexos.*

> üí° **Exemplo Num√©rico:** Imagine um cen√°rio com duas a√ß√µes (A√ß√£o 1 e A√ß√£o 2). Em feedback instrutivo, o sistema te diria "A√ß√£o 1 √© a correta". Em feedback avaliativo, voc√™ tenta A√ß√£o 1 e recebe uma recompensa de 0.5; depois tenta A√ß√£o 2 e recebe uma recompensa de 0.8. Voc√™ n√£o soube *diretamente* qual a melhor, mas aprendeu pela experi√™ncia.  Este feedback avaliativo √© o cerne do aprendizado por refor√ßo.

```mermaid
graph LR
    A[Feedback Instrutivo] -->|Informa "A√ß√£o Correta"| B("A√ß√£o √ìtima Conhecida");
    C[Feedback Avaliativo] -->|Recompensa ap√≥s a√ß√£o| D("A√ß√£o ‚Üí Recompensa");
    D --> E("Explora√ß√£o Necess√°ria");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
```

A necessidade de explora√ß√£o, decorrente da natureza avaliativa do feedback, imp√µe um *trade-off* entre explorar a√ß√µes desconhecidas na esperan√ßa de encontrar a√ß√µes melhores, e explorar as a√ß√µes conhecidas que proporcionam um bom resultado. Esse *trade-off* √© intr√≠nseco ao aprendizado por refor√ßo e ser√° explorado com mais profundidade nas se√ß√µes subsequentes.

### Conceitos Fundamentais
Nos problemas de **k-armed bandit**, o objetivo √© maximizar a recompensa total esperada ao longo do tempo, fazendo escolhas repetidas entre *k* op√ß√µes ou a√ß√µes, onde cada escolha leva a uma recompensa num√©rica selecionada a partir de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o escolhida [^1]. Em cen√°rios *n√£o-associativos*, buscamos uma √∫nica melhor a√ß√£o, seja ela fixa ou vari√°vel no tempo. No entanto, no aprendizado por refor√ßo, a complexidade surge quando o ambiente apresenta m√∫ltiplas situa√ß√µes, exigindo que o agente aprenda uma *pol√≠tica* que mapeia situa√ß√µes para a√ß√µes apropriadas. Essa √© a ess√™ncia dos problemas de **busca associativa**, tamb√©m conhecidos como **contextual bandits** [^1].

**Lema 1:** *Em um problema de k-armed bandit n√£o-associativo, se as distribui√ß√µes de recompensa para cada a√ß√£o forem estacion√°rias, existe uma a√ß√£o √≥tima que maximiza a recompensa esperada a longo prazo.*

*Proof Strategy:* Isso segue diretamente da defini√ß√£o de problemas de k-armed bandits n√£o-associativos e da exist√™ncia de um valor esperado para cada distribui√ß√£o estacion√°ria. A a√ß√£o com o maior valor esperado √© a a√ß√£o √≥tima.

> üí° **Exemplo Num√©rico:** Considere um k-armed bandit com 3 a√ß√µes (k=3). As recompensas de cada a√ß√£o seguem uma distribui√ß√£o normal: A√ß√£o 1 ~ N(1, 0.5), A√ß√£o 2 ~ N(2, 0.5) e A√ß√£o 3 ~ N(1.5, 0.5). A a√ß√£o √≥tima √© a A√ß√£o 2, pois tem a maior recompensa esperada (m√©dia de 2). Em um cen√°rio n√£o-associativo, o agente aprende a escolher essa a√ß√£o repetidamente.

```mermaid
graph LR
    A[k-armed Bandit] --> B("A√ß√£o 1 ~ N(1, 0.5)");
    A --> C("A√ß√£o 2 ~ N(2, 0.5)");
    A --> D("A√ß√£o 3 ~ N(1.5, 0.5)");
    B --> E("Recompensa Esperada = 1");
    C --> F("Recompensa Esperada = 2");
    D --> G("Recompensa Esperada = 1.5");
    F --> H("A√ß√£o √ìtima");
     style A fill:#fff,stroke:#333,stroke-width:2px
     style H fill:#ccf,stroke:#333,stroke-width:2px
```

A **busca associativa** envolve tanto a aprendizagem por tentativa e erro para encontrar as melhores a√ß√µes, quanto a associa√ß√£o dessas a√ß√µes com as situa√ß√µes em que elas s√£o mais eficazes. Este cen√°rio √© uma ponte entre o problema do k-armed bandit e o problema completo de aprendizado por refor√ßo, pois cada a√ß√£o afeta apenas a recompensa imediata, e n√£o o estado futuro.

*O objetivo √© aprender uma pol√≠tica*: uma fun√ß√£o que mapeia cada situa√ß√£o para a melhor a√ß√£o a ser tomada naquela situa√ß√£o. Em problemas **n√£o-associativos**, ou o agente tenta encontrar uma √∫nica melhor a√ß√£o (em um ambiente estacion√°rio) ou acompanhar a melhor a√ß√£o que muda ao longo do tempo (em um ambiente n√£o estacion√°rio). No entanto, em problemas mais gerais, cada situa√ß√£o pode exigir uma a√ß√£o diferente. Para ilustrar, imagine um cen√°rio onde se tem v√°rios problemas de k-armed bandits e a cada passo um desses problemas √© selecionado aleatoriamente [^1]. Se a probabilidade de sele√ß√£o de cada problema n√£o muda ao longo do tempo, o problema pode ser visto como um k-armed bandit estacion√°rio. No entanto, se for oferecida uma pista distintiva sobre a identidade do problema (como uma mudan√ßa na cor do display de uma slot machine), o agente pode aprender uma pol√≠tica para associar cada pista √† melhor a√ß√£o para aquele problema. Por exemplo, "se vermelho, selecionar o bra√ßo 1; se verde, selecionar o bra√ßo 2" [^1]. Esta √© a ess√™ncia de um **contextual bandit**, e √© um passo crucial em dire√ß√£o ao aprendizado por refor√ßo.

**Defini√ß√£o 1:** *Uma pol√≠tica $\pi$ em um problema de contextual bandit √© uma fun√ß√£o $\pi: S \rightarrow A$, que mapeia cada estado (contexto) $s \in S$ para uma a√ß√£o $a \in A$. O objetivo do agente √© aprender uma pol√≠tica $\pi^*$ que maximize a recompensa esperada em cada estado.*

> üí° **Exemplo Num√©rico:** Em um contexto de recomenda√ß√£o de filmes, $S$ pode ser o perfil do usu√°rio (idade, g√™nero, hist√≥rico de visualiza√ß√µes) e $A$ pode ser o conjunto de filmes dispon√≠veis. Uma pol√≠tica $\pi$ mapeia um perfil de usu√°rio para um filme a ser recomendado. O objetivo √© aprender uma pol√≠tica $\pi^*$ que maximize a probabilidade de o usu√°rio assistir e gostar do filme recomendado. Por exemplo, $\pi(\text{user\_age=25, genre='Action'}) = \text{movie\_id=123}$

```mermaid
flowchart LR
    subgraph "Contextual Bandit"
        S["Estado (Contexto) s ‚àà S"]
        A["A√ß√£o a ‚àà A"]
        P["Pol√≠tica œÄ: S ‚Üí A"]
    end
    S --> P
    P --> A
    style S fill:#fff,stroke:#333,stroke-width:2px
     style A fill:#fff,stroke:#333,stroke-width:2px
     style P fill:#ccf,stroke:#333,stroke-width:2px
```

**Lema 1.1:** *Em um problema de contextual bandit com um conjunto finito de contextos $S$, a aprendizagem de uma pol√≠tica √≥tima $\pi^*$ pode ser vista como a aprendizagem de m√∫ltiplas pol√≠ticas √≥timas, uma para cada contexto, mas com a dificuldade de que a explora√ß√£o deve considerar simultaneamente todos os contextos.*

*Proof Strategy:* Este lema demonstra que, embora a aprendizagem de pol√≠ticas por contexto se assemelhe √† resolu√ß√£o de m√∫ltiplos k-armed bandits n√£o-associativos, o problema se torna mais complexo porque o agente deve aprender qual a√ß√£o √© √≥tima para qual contexto, e isso requer uma explora√ß√£o combinada que n√£o √© necess√°ria nos casos n√£o associativos.

> üí° **Exemplo Num√©rico:** Imagine dois contextos: $S = \{s_1, s_2\}$. Em $s_1$, a a√ß√£o $a_1$ √© √≥tima; em $s_2$, a a√ß√£o $a_2$ √© √≥tima. Resolver o problema de contextual bandit exige explorar a√ß√µes em ambos os contextos para descobrir a a√ß√£o √≥tima em cada um. O agente deve aprender uma pol√≠tica: $\pi(s_1) = a_1$ e $\pi(s_2) = a_2$.

```mermaid
graph LR
    subgraph Contexto s1
        A("A√ß√£o a1")
         style A fill:#ccf,stroke:#333,stroke-width:2px
    end
    subgraph Contexto s2
        B("A√ß√£o a2")
         style B fill:#ccf,stroke:#333,stroke-width:2px
    end
    C["Pol√≠tica œÄ"] --> |"œÄ(s1) = a1"| A
    C --> |"œÄ(s2) = a2"| B
    style C fill:#fff,stroke:#333,stroke-width:2px
```

O aprendizado por refor√ßo, em sua forma completa, permite que as a√ß√µes influenciem tanto a recompensa imediata quanto a situa√ß√£o futura, criando uma din√¢mica complexa que exige uma pol√≠tica que lide com ambas [^1].

### Conclus√£o
A transi√ß√£o dos problemas de *k*-armed bandits n√£o-associativos para o cen√°rio de busca associativa representa um passo crucial na compreens√£o do aprendizado por refor√ßo. O objetivo em **contextual bandits** n√£o √© apenas encontrar a melhor a√ß√£o, mas sim *aprender uma pol√≠tica* que mapeia cada situa√ß√£o para a a√ß√£o apropriada. Esta capacidade de generaliza√ß√£o para diferentes contextos √© uma das chaves para a resolu√ß√£o de problemas de aprendizado por refor√ßo mais complexos. A capacidade de associar a√ß√µes a situa√ß√µes permite que os agentes adaptem seu comportamento de maneira mais eficiente, o que √© fundamental para o sucesso em ambientes complexos e din√¢micos. Os contextual bandits servem como um passo intermedi√°rio entre os problemas mais simples de *k*-armed bandits e os problemas de aprendizado por refor√ßo completo, onde as a√ß√µes influenciam o estado futuro.

**Teorema 1:** *Em problemas de aprendizado por refor√ßo, a complexidade do problema aumenta √† medida que passamos de problemas n√£o-associativos (k-armed bandit) para problemas associativos (contextual bandit) e, finalmente, para problemas de aprendizado por refor√ßo completo, onde as a√ß√µes afetam n√£o apenas a recompensa imediata, mas tamb√©m o estado futuro.*

*Proof Strategy:* Este teorema sintetiza os argumentos do texto, demonstrando uma hierarquia de complexidade e a necessidade de abordagens de aprendizado cada vez mais sofisticadas para lidar com ambientes mais complexos.

```mermaid
graph LR
    A["k-armed Bandit (N√£o-Associativo)"] -->|Simples| B("Contextual Bandit (Associativo)");
    B -->|Complexo| C("Aprendizado por Refor√ßo Completo");
    A --> D("Recompensa Imediata");
     B --> E("Recompensa Imediata + Contexto");
    C --> F("Recompensa Imediata + Estado Futuro");
     style A fill:#fff,stroke:#333,stroke-width:2px
    style B fill:#fff,stroke:#333,stroke-width:2px
      style C fill:#ccf,stroke:#333,stroke-width:2px

```

**Corol√°rio 1:** *Os algoritmos projetados para resolver problemas de k-armed bandit n√£o-associativos podem n√£o ser eficazes em problemas de contextual bandit, e algoritmos projetados para contextual bandit podem n√£o ser eficazes em problemas de aprendizado por refor√ßo completo devido ao impacto das a√ß√µes nos estados futuros.*

> üí° **Exemplo Num√©rico:** Um algoritmo simples para k-armed bandits, como epsilon-greedy, pode funcionar bem para escolher a melhor m√°quina ca√ßa-n√≠queis (n√£o-associativo). Mas, em um cen√°rio de contextual bandit (como recomendar an√∫ncios), usar epsilon-greedy diretamente sem levar em considera√ß√£o o contexto (usu√°rio, hora, etc.) levar√° a escolhas sub√≥timas. Um algoritmo de RL completo, como Q-learning, √© necess√°rio quando a a√ß√£o de hoje afeta o estado de amanh√£, o que um contextual bandit n√£o cobre.

```mermaid
graph LR
    A["Epsilon-Greedy (k-armed)"] -->|Funciona| B("M√°quina ca√ßa-n√≠queis");
     A -->|Ineficaz| C("Recomenda√ß√£o de an√∫ncios (Contextual Bandit)");
    D["Q-learning (RL Completo)"] -->|Necess√°rio| E("A√ß√£o afeta estado futuro");
        style A fill:#fff,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions... The particular nonassociative, evaluative feedback problem that we explore is a simple version of the k-armed bandit problem... At the end of this chapter, we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when the best action depends on the situation." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
[^2]: "So far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no need to associate different actions with different situations... However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
[^3]: "This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
