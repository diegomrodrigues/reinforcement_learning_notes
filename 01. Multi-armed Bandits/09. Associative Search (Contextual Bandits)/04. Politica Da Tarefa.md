## Associative Search (Contextual Bandits)
### Introdu√ß√£o
O cap√≠tulo anterior focou em tarefas n√£o associativas, onde o objetivo era encontrar uma √∫nica a√ß√£o √≥tima, seja num ambiente est√°tico ou din√¢mico. Contudo, em cen√°rios mais complexos de aprendizagem por refor√ßo, √© necess√°rio associar a√ß√µes espec√≠ficas a diferentes situa√ß√µes, aprendendo assim uma pol√≠tica que mapeia cada situa√ß√£o √† a√ß√£o mais adequada. Esta se√ß√£o do cap√≠tulo introduzir√° a forma mais simples de estender as tarefas n√£o associativas para o contexto associativo, explorando como diferentes situa√ß√µes influenciam as a√ß√µes tomadas [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

### Conceitos Fundamentais
A transi√ß√£o das tarefas n√£o associativas para as associativas √© fundamental para entender como a aprendizagem por refor√ßo se adapta a ambientes mais realistas. Imagine que, em vez de um √∫nico problema de bandit k-armado, o agente se depare com v√°rios desses problemas, cada um selecionado aleatoriamente a cada passo [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

Inicialmente, se a probabilidade de selecionar cada problema de bandit permanecer constante ao longo do tempo, o cen√°rio poderia ser tratado como um √∫nico problema de bandit k-armado estacion√°rio. No entanto, a introdu√ß√£o de um **identificador** que fornece uma pista sobre qual problema de bandit est√° ativo no momento, torna a abordagem anterior inadequada. Este identificador, que pode ser, por exemplo, uma mudan√ßa na cor de uma slot machine, permite que o agente aprenda uma pol√≠tica que associe cada problema de bandit (identificado pelo seu sinal) √† a√ß√£o mais apropriada [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).
```mermaid
graph LR
    A("Problema k-armado \n(N√£o-Associativo)") --> B("M√∫ltiplos Problemas k-armado");
    B --> C("Identificador \n(Contexto)");
    C --> D("Pol√≠tica Associativa \n(Contextual Bandit)");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```
A capacidade de associar a√ß√µes a contextos espec√≠ficos, impulsiona a necessidade do conceito de **associative search**.  A **associative search** envolve a aprendizagem por tentativa e erro para encontrar as melhores a√ß√µes, mas tamb√©m associa essas a√ß√µes aos contextos onde s√£o mais eficazes. Estas tarefas, tamb√©m conhecidas como **contextual bandits**, preenchem a lacuna entre o problema de bandit k-armado e o problema completo de aprendizagem por refor√ßo, introduzindo a ideia de aprender uma pol√≠tica, embora cada a√ß√£o continue a influenciar apenas a recompensa imediata [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

Em um cen√°rio de bandit contextual, o objetivo √© aprender uma pol√≠tica que, para cada situa√ß√£o espec√≠fica, determine a melhor a√ß√£o a ser tomada. Em contraste com as tarefas n√£o associativas, onde se procura uma √∫nica melhor a√ß√£o para todas as situa√ß√µes, o bandit contextual busca a melhor a√ß√£o para cada contexto poss√≠vel. A pol√≠tica √© uma fun√ß√£o que mapeia contextos para a√ß√µes, onde um contexto √© uma informa√ß√£o observ√°vel do ambiente que influencia a melhor a√ß√£o [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

Para ilustrar, suponha que o agente se depare com um problema de bandit k-armado, onde cada a√ß√£o oferece uma recompensa diferente. Sem qualquer informa√ß√£o adicional, o agente tentaria encontrar a a√ß√£o que maximiza a recompensa m√©dia. Agora, introduzindo a ideia de que cada problema tem uma "cor" diferente. O agente pode aprender que se o problema tem a cor vermelha, ele deve escolher a a√ß√£o 1, se a cor for verde, deve escolher a a√ß√£o 2, e assim por diante. Este aprendizado de pol√≠tica √© um exemplo de **associative search**.
```mermaid
graph LR
    A("Contexto (s)") --> B("Pol√≠tica (œÄ)");
    B --> C("A√ß√£o (a)");
    C --> D("Recompensa (R)");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
     style D fill:#afa,stroke:#333,stroke-width:2px
     linkStyle 0,1,2 stroke:#333,stroke-width:1px
```
> üí° **Exemplo Num√©rico:** Imagine um cen√°rio com duas "cores" (contextos): vermelho ($s_1$) e verde ($s_2$). Existem duas a√ß√µes poss√≠veis: $a_1$ e $a_2$. As recompensas esperadas para cada contexto e a√ß√£o s√£o as seguintes:
>
> *   Contexto Vermelho ($s_1$):
>     *   $E[R | s_1, a_1] = 10$
>     *   $E[R | s_1, a_2] = 2$
> *   Contexto Verde ($s_2$):
>     *   $E[R | s_2, a_1] = 3$
>     *   $E[R | s_2, a_2] = 12$
>
> A pol√≠tica √≥tima $\pi^*$ mapearia:
>
> *   $\pi^*(s_1) = a_1$ (escolher $a_1$ no contexto vermelho)
> *   $\pi^*(s_2) = a_2$ (escolher $a_2$ no contexto verde)
>
> Esta pol√≠tica garante a recompensa m√°xima para cada contexto. Uma pol√≠tica sub√≥tima, como escolher sempre $a_1$ resultaria em uma recompensa m√©dia de $(10 + 3)/2=6.5$ enquanto a pol√≠tica √≥tima alcan√ßaria $(10 + 12)/2 = 11$.

A formula√ß√£o matem√°tica pode ser descrita da seguinte forma: seja $s \in S$ o espa√ßo dos poss√≠veis contextos (identificadores das tarefas), e $a \in A$ o conjunto de poss√≠veis a√ß√µes. O objetivo √© aprender uma pol√≠tica $\pi: S \rightarrow A$ que mapeie cada contexto $s$ para uma a√ß√£o $a$ que maximiza a recompensa esperada $E[R|s,a]$. Ou seja, a pol√≠tica $\pi(s)$ indica qual a√ß√£o deve ser tomada quando o agente se encontra no contexto $s$.
```mermaid
graph LR
    subgraph "Espa√ßo de Contextos (S)"
        S("s ‚àà S")
    end
    subgraph "Espa√ßo de A√ß√µes (A)"
        A("a ‚àà A")
    end
    S -->|œÄ: S ‚Üí A| A
    A -->|E[R|s,a]| R("Recompensa Esperada")
    style S fill:#f9f,stroke:#333,stroke-width:2px
    style A fill:#ccf,stroke:#333,stroke-width:2px
     style R fill:#afa,stroke:#333,stroke-width:2px
    linkStyle 0,1 stroke:#333,stroke-width:1px
```
**Lemma 1.** *A pol√≠tica √≥tima para um problema de bandit contextual √© a fun√ß√£o que mapeia cada contexto √† a√ß√£o que maximiza a recompensa esperada para aquele contexto.*

*Prova*: Seja $\pi^*(s)$ a a√ß√£o que maximiza a recompensa esperada para o contexto $s$, isto √©, $\pi^*(s) = \underset{a}{\operatorname{argmax}} \, E[R|s, a]$. Qualquer outra pol√≠tica $\pi(s)$ que mapeia $s$ para uma a√ß√£o diferente de $\pi^*(s)$ resultar√° em uma recompensa esperada menor para aquele contexto. Como o objetivo √© maximizar a recompensa total esperada, selecionar a a√ß√£o que maximiza a recompensa em cada contexto garante a pol√≠tica √≥tima. $\blacksquare$

**Lemma 1.1.** *A pol√≠tica √≥tima $\pi^*$ √© √∫nica se e somente se, para cada contexto $s \in S$, existe uma a√ß√£o $a \in A$ tal que $E[R|s, a] > E[R|s, a']$ para todas as a√ß√µes $a' \in A$, com $a' \neq a$.*
```mermaid
graph LR
    A("Pol√≠tica √ìtima œÄ* √© √∫nica") -- "se e somente se" --> B("Para cada s ‚àà S, existe a ‚àà A")
    B --> C("E[R|s, a] > E[R|s, a'] para todas a' ‚â† a")
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#afa,stroke:#333,stroke-width:2px
     linkStyle 0,1 stroke:#333,stroke-width:1px
```
*Proof:* Se para cada contexto $s$ existe uma a√ß√£o √∫nica $a$ que maximiza a recompensa esperada, ent√£o a pol√≠tica que mapeia cada contexto para essa a√ß√£o √©, por defini√ß√£o, a √∫nica pol√≠tica √≥tima. Reciprocamente, se a pol√≠tica √≥tima $\pi^*$ √© √∫nica, isso significa que, em cada contexto $s$, qualquer desvio da a√ß√£o escolhida por $\pi^*$ resulta em uma recompensa esperada menor. Isso s√≥ √© poss√≠vel se a a√ß√£o escolhida por $\pi^*$ √© a √∫nica a√ß√£o que maximiza a recompensa esperada nesse contexto.  $\blacksquare$

> üí° **Exemplo Num√©rico (Unicidade):**
>
> Considere o exemplo anterior. Para o contexto vermelho ($s_1$), a a√ß√£o $a_1$ tem $E[R | s_1, a_1] = 10$, enquanto $a_2$ tem $E[R | s_1, a_2] = 2$. Como $10 > 2$, $a_1$ √© a √∫nica a√ß√£o √≥tima.  Para o contexto verde ($s_2$),  $E[R | s_2, a_1] = 3$ e $E[R | s_2, a_2] = 12$, ent√£o $a_2$ √© a √∫nica a√ß√£o √≥tima. Logo, a pol√≠tica que mapeia $s_1$ para $a_1$ e $s_2$ para $a_2$ √© a √∫nica pol√≠tica √≥tima.
>
> Agora, imagine que $E[R | s_1, a_1] = 10$ e $E[R | s_1, a_2] = 10$. Nesse caso, as a√ß√µes s√£o equivalentes no contexto vermelho e a pol√≠tica √≥tima n√£o √© √∫nica.

A diferen√ßa crucial em rela√ß√£o ao problema de bandit k-armado √© que, em vez de uma √∫nica a√ß√£o, o agente deve aprender uma fun√ß√£o que associe cada poss√≠vel contexto a uma a√ß√£o que maximize a recompensa esperada para aquele contexto. No contexto da modelagem financeira, isso pode ser aplicado para determinar estrat√©gias de investimento √≥timas com base em condi√ß√µes de mercado.

**Proposi√ß√£o 2.** *Em um problema de bandit contextual, se o n√∫mero de contextos $|S|$ for finito, ent√£o o aprendizado de uma pol√≠tica √≥tima pode ser abordado como um conjunto de problemas de bandit k-armado independentes.*
```mermaid
graph LR
    A("Problema de Bandit Contextual \n(|S| finito)") --> B("Conjunto de |S| Problemas de Bandit k-armado \nIndependentes");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
```
*Proof:* Se o conjunto de contextos $S$ √© finito, podemos considerar cada contexto $s \in S$ individualmente. Para cada contexto fixo $s$, o problema de encontrar a a√ß√£o que maximiza a recompensa esperada $E[R|s,a]$ √© exatamente um problema de bandit k-armado com a√ß√µes $a \in A$. Portanto, o aprendizado da pol√≠tica √≥tima em um problema de bandit contextual com $|S|$ contextos finitos pode ser visto como a solu√ß√£o de $|S|$ problemas independentes de bandit k-armado. $\blacksquare$

> üí° **Exemplo Num√©rico (Contextos Finitos):**
>
> Suponha que temos 3 contextos: $s_1, s_2,$ e $s_3$, e 2 a√ß√µes: $a_1$ e $a_2$. Podemos tratar cada contexto como um problema de bandit k-armado separado:
>
> *   **Contexto $s_1$**:  $E[R | s_1, a_1] = 5$, $E[R | s_1, a_2] = 2$. O melhor seria escolher $a_1$.
> *   **Contexto $s_2$**:  $E[R | s_2, a_1] = 1$, $E[R | s_2, a_2] = 8$. O melhor seria escolher $a_2$.
> *   **Contexto $s_3$**:  $E[R | s_3, a_1] = 9$, $E[R | s_3, a_2] = 4$. O melhor seria escolher $a_1$.
>
> Para resolver o problema geral, resolvemos tr√™s problemas de bandit k-armado independentes, um para cada contexto. Assim, a pol√≠tica √≥tima √© $\pi^*(s_1) = a_1$, $\pi^*(s_2) = a_2$, and $\pi^*(s_3) = a_1$.

O conceito de **associative search** destaca a import√¢ncia de aprender pol√≠ticas em vez de a√ß√µes isoladas. Em cen√°rios mais complexos, como em jogos ou rob√≥tica, o ambiente pode apresentar um n√∫mero muito grande de contextos diferentes, tornando invi√°vel explorar exaustivamente todas as combina√ß√µes de a√ß√£o e contexto. Assim, o aprendizado de uma pol√≠tica que generaliza o conhecimento de a√ß√µes para contextos diferentes √© uma solu√ß√£o mais eficiente e robusta. O problema de bandit contextual √© um ponto de partida para a abordagem do problema de aprendizagem por refor√ßo, onde as a√ß√µes podem afetar tanto a recompensa imediata quanto o contexto futuro [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

**Teorema 3.** *O problema de bandit contextual pode ser interpretado como um caso especial do problema de aprendizagem por refor√ßo, onde o estado √© equivalente ao contexto e as a√ß√µes n√£o afetam o estado futuro.*
```mermaid
graph LR
    A("Bandit Contextual") --> B("Caso Especial de Aprendizagem por Refor√ßo");
    B --> C("Estado = Contexto");
    C --> D("A√ß√µes n√£o afetam o estado futuro");
     style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#afa,stroke:#333,stroke-width:2px
     linkStyle 0,1,2 stroke:#333,stroke-width:1px
```
*Proof:* No problema de bandit contextual, cada contexto $s \in S$ pode ser visto como um estado do ambiente. A escolha de uma a√ß√£o $a \in A$ em um contexto $s$ produz uma recompensa imediata $R$, mas n√£o altera o contexto futuro, uma vez que,  por defini√ß√£o, o contexto √© selecionado aleatoriamente a cada passo. Isso corresponde exatamente ao caso especial do problema de aprendizagem por refor√ßo em que o pr√≥ximo estado √© sempre independente da a√ß√£o atual. Portanto, o problema de bandit contextual √© um caso particular da aprendizagem por refor√ßo. $\blacksquare$

> üí° **Exemplo Num√©rico (RL Connection):**
>
> Em um jogo simples onde um agente deve escolher entre duas a√ß√µes ($a_1$ e $a_2$) em dois "estados" (contextos) diferentes ($s_1$ e $s_2$), o agente recebe recompensas imediatas, mas o "estado" seguinte √© escolhido aleatoriamente, independente da a√ß√£o.
>
> *   Em $s_1$, $a_1$ d√° uma recompensa m√©dia de 5, e $a_2$ d√° 2.
> *   Em $s_2$, $a_1$ d√° uma recompensa m√©dia de 1 e $a_2$ d√° 8.
>
> Mesmo que o agente esteja sempre em $s_1$ (contexto 1), ele n√£o pode mudar para $s_2$ por suas a√ß√µes. Cada escolha √© independente. Isso √© o mesmo que um bandit contextual, onde cada contexto ($s_1$ ou $s_2$) equivale a um estado da aprendizagem por refor√ßo, e a a√ß√£o n√£o afeta o estado futuro.

**Corol√°rio 3.1.** *Os algoritmos de aprendizagem por refor√ßo que n√£o dependem da transi√ß√£o de estados (model-free) s√£o aplic√°veis a problemas de bandit contextual.*
```mermaid
graph LR
    A("Aprendizagem por Refor√ßo \n(Model-Free)") --> B("Aplic√°vel a Bandit Contextual");
    B --> C("A√ß√µes n√£o influenciam o estado futuro");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#afa,stroke:#333,stroke-width:2px
    linkStyle 0,1 stroke:#333,stroke-width:1px
```
*Proof:* Dado que no problema de bandit contextual, a a√ß√£o n√£o influencia o estado futuro, algoritmos como Q-learning e SARSA, que aprendem a pol√≠tica diretamente atrav√©s das recompensas observadas sem conhecer o modelo do ambiente, s√£o adequados para este tipo de problema.  $\blacksquare$

> üí° **Exemplo Num√©rico (Model-Free RL):**
>
> Imagine usar Q-learning para o exemplo das cores (vermelho e verde). Inicialmente, o agente n√£o sabe as recompensas associadas a cada a√ß√£o em cada contexto.
>
> O agente mant√©m uma tabela Q com dimens√µes |S| x |A|, onde S √© o conjunto de contextos e A o conjunto de a√ß√µes.
>
> *   $Q(s_1, a_1)$ , $Q(s_1, a_2)$, $Q(s_2, a_1)$, $Q(s_2, a_2)$
>
> O algoritmo atualiza esses valores iterativamente:
>
> $\text{Nova } Q(s, a) = \text{Antiga } Q(s, a) + \alpha [R + \gamma \max_{a'} Q(s', a') - \text{Antiga } Q(s, a)]$
>
> onde $\alpha$ √© a taxa de aprendizado e $\gamma$ o fator de desconto (que pode ser 0, pois n√£o h√° estados futuros). Como n√£o existe transi√ß√£o de estado, o $s'$ ser√° o pr√≥ximo contexto selecionado aleatoriamente, e portanto a atualiza√ß√£o se simplifica para
>
> $\text{Nova } Q(s, a) = \text{Antiga } Q(s, a) + \alpha [R  - \text{Antiga } Q(s, a)]$
>
> A atualiza√ß√£o √© feita com base na recompensa obtida e no valor existente, sem depender de um modelo de transi√ß√£o.
>
> Com o tempo e explora√ß√£o, Q-learning convergeria para os valores esperados, e o agente aprenderia a pol√≠tica √≥tima.

### Conclus√£o
Em resumo, o problema de **associative search** ou **contextual bandit** introduz a ideia de que a melhor a√ß√£o pode depender do contexto em que ela √© realizada. Ao aprender uma pol√≠tica que associa cada contexto √† sua a√ß√£o mais apropriada, um agente pode otimizar o seu desempenho de forma mais eficiente. Este conceito √© uma pe√ßa chave para entender a transi√ß√£o para problemas de aprendizagem por refor√ßo mais complexos e est√° diretamente relacionado com a necessidade de aprender pol√≠ticas, em vez de a√ß√µes isoladas [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).
### Refer√™ncias
[^1]: "So far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no need to associate different actions with different situations. In these tasks the learner either tries to find a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary. However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations. To set the stage for the full problem, we briefly discuss the simplest way in which nonassociative tasks extend to the associative setting. As an example, suppose there are several different k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. If the probabilities with which each task is selected for you do not change over time, this would appear as a single stationary k-armed bandit task, and you could use one of the methods described in this chapter. Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that task‚Äîfor instance, if red, select arm 1; if green, select arm 2. With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another. This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature. Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem. We present this problem in the next chapter and consider its ramifications throughout the rest of the book." *(Trecho de Multi-armed Bandits)*
