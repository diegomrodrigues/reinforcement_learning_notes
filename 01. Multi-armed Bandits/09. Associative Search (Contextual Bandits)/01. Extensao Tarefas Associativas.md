## Multi-armed Bandits: Extens√£o para Tarefas Associativas
### Introdu√ß√£o
Neste cap√≠tulo, exploramos o conceito de **multi-armed bandits**, um problema fundamental no aprendizado por refor√ßo (Reinforcement Learning - RL) [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). At√© o momento, concentramo-nos em tarefas **n√£o associativas**, onde o objetivo √© encontrar a melhor a√ß√£o √∫nica em um ambiente estacion√°rio ou rastrear a melhor a√ß√£o quando o ambiente √© n√£o estacion√°rio. No entanto, em cen√°rios mais complexos de RL, as a√ß√µes precisam ser associadas a diferentes situa√ß√µes ou contextos. Esta se√ß√£o aprofunda essa extens√£o para tarefas **associativas**, que s√£o um passo em dire√ß√£o ao problema completo do aprendizado por refor√ßo [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). A transi√ß√£o para tarefas associativas √© crucial para entender como o aprendizado por refor√ßo pode ser aplicado em situa√ß√µes do mundo real, onde as decis√µes devem ser tomadas com base no contexto em que s√£o encontradas.

**Proposi√ß√£o 1.** *A necessidade de associar a√ß√µes a contextos em ambientes de RL surge da complexidade e variabilidade do mundo real. Enquanto tarefas n√£o associativas s√£o √∫teis para problemas simples, a capacidade de adaptar o comportamento com base no contexto √© essencial para a aplica√ß√£o do RL em cen√°rios pr√°ticos.*

### Conceitos Fundamentais
As tarefas n√£o associativas, discutidas anteriormente, envolvem a busca por uma √∫nica a√ß√£o √≥tima. J√° nas **tarefas associativas**, tamb√©m conhecidas como *contextual bandits*, o objetivo √© aprender uma **pol√≠tica**, ou seja, um mapeamento de situa√ß√µes para a√ß√µes [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Um exemplo pr√°tico pode ser ilustrado com uma m√°quina ca√ßa-n√≠queis onde a cor do display muda, alterando tamb√©m seus valores de a√ß√£o. Nesse cen√°rio, o aprendizado se concentraria em associar a cor do display (situa√ß√£o) com a melhor a√ß√£o a ser tomada. Por exemplo, se a cor for vermelha, selecionar o bra√ßo 1, e se a cor for verde, selecionar o bra√ßo 2.

> üí° **Exemplo Num√©rico:** Considere uma m√°quina ca√ßa-n√≠queis com dois bra√ßos. O contexto √© dado pela cor do display: vermelho (contexto 1) ou azul (contexto 2). As recompensas esperadas para cada bra√ßo em cada contexto s√£o:
>
> *   **Contexto 1 (Vermelho):**
>     *   Bra√ßo 1: Recompensa esperada de 10
>     *   Bra√ßo 2: Recompensa esperada de 5
> *   **Contexto 2 (Azul):**
>     *   Bra√ßo 1: Recompensa esperada de 2
>     *   Bra√ßo 2: Recompensa esperada de 15
>
> Um agente que ignora o contexto e escolhe sempre o bra√ßo 1 teria uma recompensa esperada de $(10 + 2)/2 = 6$. Um agente que escolhe sempre o bra√ßo 2 teria uma recompensa esperada de $(5 + 15)/2 = 10$. J√° um agente que aprende a pol√≠tica ideal (bra√ßo 1 no contexto vermelho e bra√ßo 2 no contexto azul) teria uma recompensa m√©dia de $(10+15)/2 = 12.5$. Este exemplo num√©rico ilustra a vantagem de se adaptar ao contexto.

A distin√ß√£o crucial entre tarefas n√£o associativas e associativas reside na complexidade da tomada de decis√µes. Nas tarefas n√£o associativas, o agente aprende a agir em um ambiente uniforme, enquanto nas associativas o agente aprende a adaptar seu comportamento a diferentes contextos. Esta adaptabilidade √© essencial em ambientes mais complexos e realistas, onde as a√ß√µes mais eficazes dependem das circunst√¢ncias [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

**Lema 1.** *A adaptabilidade do agente em tarefas associativas √© uma fun√ß√£o da complexidade do espa√ßo de contextos. Quanto mais diverso o conjunto de situa√ß√µes, maior √© a necessidade de um mapeamento preciso entre contextos e a√ß√µes.*

> üí° **Exemplo Num√©rico:** Vamos aumentar a complexidade do espa√ßo de contextos. Agora, vamos supor que temos 3 contextos (vermelho, azul e verde) com as seguintes recompensas esperadas:
>
> *   **Contexto 1 (Vermelho):**
>     *   Bra√ßo 1: 10
>     *   Bra√ßo 2: 5
> *   **Contexto 2 (Azul):**
>     *   Bra√ßo 1: 2
>     *   Bra√ßo 2: 15
> *   **Contexto 3 (Verde):**
>     *   Bra√ßo 1: 18
>     *   Bra√ßo 2: 1
>
>  Neste caso, a pol√≠tica √≥tima seria escolher o bra√ßo 1 no contexto vermelho, o bra√ßo 2 no contexto azul e o bra√ßo 1 no contexto verde. Um agente que n√£o considera o contexto teria um desempenho pior.
>
> ```mermaid
> graph LR
>     subgraph "Contextos"
>     A("Contexto Vermelho")
>     D("Contexto Azul")
>     G("Contexto Verde")
>     end
>     subgraph "A√ß√µes"
>     B("Bra√ßo 1")
>     C("Bra√ßo 2")
>     E("Bra√ßo 1")
>     F("Bra√ßo 2")
>     H("Bra√ßo 1")
>     I("Bra√ßo 2")
>     end
>     A --> B
>     A --> C
>     D --> E
>     D --> F
>     G --> H
>     G --> I
>     B -->|Recompensa| J("10")
>     C -->|Recompensa| K("5")
>     E -->|Recompensa| L("2")
>     F -->|Recompensa| M("15")
>     H -->|Recompensa| N("18")
>     I -->|Recompensa| O("1")
>
> ```
>Este diagrama mostra os fluxos de recompensa para cada bra√ßo em cada contexto. Note como a recompensa m√°xima depende do contexto.

A ideia central por tr√°s das tarefas associativas √© a combina√ß√£o de *trial-and-error learning* com a *associa√ß√£o de a√ß√µes* a seus contextos ideais. A tomada de decis√£o √© guiada pela necessidade de explorar diferentes a√ß√µes em diversos contextos para otimizar o desempenho geral. Essas tarefas servem como um elo entre o problema do multi-armed bandit e o problema completo de aprendizado por refor√ßo. Elas compartilham caracter√≠sticas com ambos, como o aprendizado de pol√≠ticas do problema de RL, e a limita√ß√£o de recompensas imediatas do multi-armed bandit, onde as a√ß√µes n√£o afetam diretamente o ambiente futuro [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

**Teorema 1.** *Em tarefas associativas, o objetivo √© aprender uma pol√≠tica $\pi(a|s)$ que mapeia cada contexto $s$ para uma distribui√ß√£o de probabilidade sobre as a√ß√µes $a$. A pol√≠tica √≥tima $\pi^*(a|s)$ maximiza o valor esperado da recompensa em cada contexto, ou seja, $\pi^* = \arg \max_{\pi} \mathbb{E}[R|s, \pi]$, onde $R$ √© a recompensa obtida.*

*Proof Strategy:* This theorem formally states the goal of learning in associative tasks, which is to maximize the expected reward for each context. It is fundamental to the understanding of contextual bandits and provides the basis for designing algorithms to achieve optimal policies. The proof of the existence of an optimal policy is beyond this discussion but it is common in RL literature.

Para ilustrar, considere um cen√°rio de *bandit* com 2 bra√ßos, onde os valores reais das a√ß√µes mudam aleatoriamente a cada etapa de tempo. Especificamente, as a√ß√µes 1 e 2 t√™m os valores 10 e 20 com probabilidade de 0,5 (caso A) e 90 e 80 com probabilidade de 0,5 (caso B) [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Se o agente n√£o souber qual caso est√° em vigor, a melhor a√ß√£o ter√° um valor esperado. Entretanto, em um problema de *contextual bandit*, o agente recebe informa√ß√µes sobre qual caso est√° em vigor (A ou B), embora n√£o saiba os valores reais das a√ß√µes. Nesta situa√ß√£o, o agente deve aprender a escolher as a√ß√µes corretas dependendo do contexto fornecido, maximizando o valor esperado.

> üí° **Exemplo Num√©rico:**
>
> *   **Caso A:**
>     *   Bra√ßo 1: Recompensa esperada de 10
>     *   Bra√ßo 2: Recompensa esperada de 20
> *   **Caso B:**
>     *   Bra√ßo 1: Recompensa esperada de 90
>     *   Bra√ßo 2: Recompensa esperada de 80
>
> Se o agente n√£o considerar o contexto, a a√ß√£o com maior valor esperado seria o bra√ßo 2, com $(20 + 80)/2 = 50$. No entanto, se o agente souber o contexto, ele escolheria o bra√ßo 2 no caso A (recompensa 20) e o bra√ßo 1 no caso B (recompensa 90), obtendo uma recompensa esperada de $(20+90)/2 = 55$. A informa√ß√£o contextual permite que o agente ajuste suas a√ß√µes para otimizar o resultado.
>
> Vamos simular um pequeno experimento para ilustrar a diferen√ßa entre ignorar o contexto e usar o contexto.
>
> ```python
> import numpy as np
>
> np.random.seed(42)
>
> # Par√¢metros do problema
> n_steps = 1000
> probs = [0.5, 0.5] # Probabilidade de cada caso
>
> # Recompensas dos bra√ßos em cada contexto
> rewards_case_a = [10, 20]
> rewards_case_b = [90, 80]
>
> # Simula√ß√£o sem contexto
> rewards_no_context = []
> for _ in range(n_steps):
>     case = np.random.choice([0,1], p=probs)
>     if case == 0:
>       reward = np.random.choice(rewards_case_a)
>     else:
>       reward = np.random.choice(rewards_case_b)
>     rewards_no_context.append(reward)
> avg_reward_no_context = np.mean(rewards_no_context)
> print(f'Recompensa m√©dia sem contexto: {avg_reward_no_context:.2f}')
>
>
> # Simula√ß√£o com contexto
> rewards_with_context = []
> for _ in range(n_steps):
>    case = np.random.choice([0,1], p=probs)
>    if case == 0:
>      rewards_with_context.append(rewards_case_a[1]) # Escolhe o melhor bra√ßo (2)
>    else:
>      rewards_with_context.append(rewards_case_b[0]) # Escolhe o melhor bra√ßo (1)
>
> avg_reward_with_context = np.mean(rewards_with_context)
> print(f'Recompensa m√©dia com contexto: {avg_reward_with_context:.2f}')
> ```
>
>  A simula√ß√£o acima exemplifica a diferen√ßa em recompensa ao considerar o contexto, mostrando que um agente com informa√ß√£o contextual tem melhor desempenho.

**Lema 1.1** *A informa√ß√£o do contexto possibilita que o agente aprenda uma pol√≠tica adaptativa, em vez de uma √∫nica a√ß√£o √≥tima. A diferen√ßa de desempenho entre um agente que ignora o contexto e um que o utiliza pode ser significativa em ambientes onde as recompensas s√£o fortemente influenciadas pelo contexto.*

### Conclus√£o
A transi√ß√£o das tarefas n√£o associativas para as associativas representa um passo crucial no desenvolvimento de sistemas de aprendizado por refor√ßo que podem operar em ambientes complexos e vari√°veis. Os *contextual bandits* exigem que os agentes aprendam pol√≠ticas que associem a√ß√µes a situa√ß√µes espec√≠ficas, um conceito fundamental para o aprendizado em ambientes do mundo real [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Ao introduzir o conceito de contexto, o problema do multi-armed bandit se aproxima do desafio geral do aprendizado por refor√ßo, onde o ambiente interage com o agente, afetando as a√ß√µes e recompensas.

**Corol√°rio 1.** *Tarefas associativas representam uma ponte fundamental entre o problema simples do multi-armed bandit e o problema complexo do aprendizado por refor√ßo completo. Elas ilustram a necessidade de aprender pol√≠ticas dependentes do contexto e, ao mesmo tempo, mant√™m a caracter√≠stica de recompensas imediatas.*

### Refer√™ncias
[^1]: "So far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no need to associate different actions with different situations. In these tasks the learner either tries to find a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary. However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations. To set the stage for the full problem, we briefly discuss the simplest way in which nonassociative tasks extend to the associative setting. As an example, suppose there are several different k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. If the probabilities with which each task is selected for you do not change over time, this would appear as a single stationary k-armed bandit task, and you could use one of the methods described in this chapter. Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that task‚Äîfor instance, if red, select arm 1; if green, select arm 2. With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another. This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature. Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem. We present this problem in the next chapter and consider its ramifications throughout the rest of the book." *(Trecho de <Multi-armed Bandits>)*
