## Associative Search (Contextual Bandits)

### Introdu√ß√£o

Neste cap√≠tulo, exploramos inicialmente tarefas n√£o associativas, onde o objetivo √© encontrar uma √∫nica a√ß√£o ideal, seja em um ambiente estacion√°rio ou rastrear uma a√ß√£o ideal em um ambiente n√£o estacion√°rio. Contudo, a realidade do aprendizado por refor√ßo (reinforcement learning) frequentemente exige lidar com m√∫ltiplas situa√ß√µes, cada uma demandando uma a√ß√£o espec√≠fica. O aprendizado de uma pol√≠tica, ou seja, o mapeamento de situa√ß√µes para a√ß√µes, torna-se, portanto, essencial. Para construir uma base para o problema mais geral, vamos discutir como tarefas n√£o associativas se estendem ao contexto associativo.

### Conceitos Fundamentais

A forma mais simples de estender tarefas n√£o associativas para o cen√°rio associativo √© considerar diversas tarefas **k-armed bandit**, cada uma apresentando uma caracter√≠stica √∫nica. Imagine que a cada passo voc√™ enfrenta uma dessas tarefas, selecionada aleatoriamente. Se as probabilidades de sele√ß√£o das tarefas permanecem constantes, a situa√ß√£o se resume a uma √∫nica tarefa **k-armed bandit** estacion√°ria, pass√≠vel de solu√ß√£o com os m√©todos discutidos. No entanto, se cada tarefa for acompanhada de uma *clue* distintiva, como uma mudan√ßa na cor da m√°quina ca√ßa-n√≠queis, o problema se torna mais interessante.

Neste contexto, podemos aprender uma pol√≠tica que associe cada tarefa, sinalizada pela cor, √† a√ß√£o √≥tima para aquela tarefa. Por exemplo, "se a cor for vermelha, selecione o bra√ßo 1; se a cor for verde, selecione o bra√ßo 2". Uma pol√≠tica bem definida permite um desempenho superior em compara√ß√£o com uma abordagem que ignora as informa√ß√µes distintivas das tarefas.

Este cen√°rio exemplifica uma **tarefa de busca associativa**, que envolve tanto a explora√ß√£o por tentativa e erro para encontrar as melhores a√ß√µes, quanto a associa√ß√£o dessas a√ß√µes com as situa√ß√µes em que elas s√£o mais eficazes. Em muitos casos, essas tarefas s√£o chamadas de **contextual bandits** na literatura. As tarefas de busca associativa atuam como intermedi√°rias entre o problema **k-armed bandit** e o problema completo de **reinforcement learning**. Compartilham com o problema completo a necessidade de aprender uma pol√≠tica, enquanto mant√™m a caracter√≠stica do **k-armed bandit**, onde cada a√ß√£o afeta apenas a recompensa imediata.

Um problema completo de **reinforcement learning** surge quando as a√ß√µes podem influenciar tanto a recompensa imediata quanto a pr√≥xima situa√ß√£o.
```mermaid
graph LR
    A["k-armed Bandit"] -->|A√ß√µes afetam apenas recompensa imediata| B("Busca Associativa (Contextual Bandits)")
    B -->|A√ß√µes afetam recompensa imediata e s√£o condicionadas ao contexto| C("Reinforcement Learning Completo")
    C -->|A√ß√µes afetam recompensa e pr√≥ximo estado| D("Problema Completo de RL")
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9cf,stroke:#333,stroke-width:2px
    style D fill:#9cf,stroke:#333,stroke-width:2px
```

**Lemma 1**: *O aprendizado em tarefas associativas requer a explora√ß√£o das a√ß√µes e a associa√ß√£o dessas a√ß√µes aos contextos em que elas proporcionam o melhor resultado.*

*Prova:*
Em tarefas associativas, o desempenho √≥timo depende de uma pol√≠tica que mapeie contextos (identificadores de tarefa) para a√ß√µes. Para encontrar tal pol√≠tica, o agente deve:

1.  **Explorar:** Realizar a√ß√µes diferentes em v√°rios contextos para avaliar suas recompensas.
2.  **Associar:** Aprender a rela√ß√£o entre os contextos e as a√ß√µes que maximizam as recompensas.

Se o agente n√£o explorar, ele pode ficar preso em a√ß√µes sub√≥timas, mesmo que encontre uma recompensa consider√°vel no in√≠cio. Se n√£o associar as a√ß√µes com os contextos, ele n√£o saber√° qual a√ß√£o tomar em cada contexto. Assim, ambos os processos s√£o necess√°rios para o sucesso em tarefas associativas. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um problema com dois contextos (cores: vermelha e azul) e dois bra√ßos (A√ß√£o 1 e A√ß√£o 2). As recompensas m√©dias para cada contexto-a√ß√£o s√£o as seguintes:
>   - Contexto Vermelho: A√ß√£o 1 -> 10, A√ß√£o 2 -> 2
>   - Contexto Azul: A√ß√£o 1 -> 1, A√ß√£o 2 -> 9
>
>   Inicialmente, o agente pode escolher a√ß√µes aleatoriamente em cada contexto. Ap√≥s algumas itera√ß√µes, um agente que explora e associa aprender√°:
>   - Se o contexto for vermelho, a A√ß√£o 1 √© melhor (recompensa m√©dia 10)
>   - Se o contexto for azul, a A√ß√£o 2 √© melhor (recompensa m√©dia 9)
>   Um agente que n√£o explora ou n√£o associa corretamente os contextos √†s a√ß√µes n√£o conseguir√° obter o m√°ximo de recompensa poss√≠vel, por exemplo, poderia ficar preso na a√ß√£o 1 em ambos os contextos, com recompensas m√©dias de 10 e 1, respectivamente.
```mermaid
sequenceDiagram
    participant Agente
    participant Ambiente
    Agente ->> Ambiente: Escolhe a√ß√£o aleat√≥ria
    Ambiente -->> Agente: Retorna recompensa e contexto
    Agente ->> Agente: Avalia recompensa no contexto
    alt Contexto √© vermelho
        Agente ->> Agente: Associa contexto vermelho √† A√ß√£o 1
    else Contexto √© azul
        Agente ->> Agente: Associa contexto azul √† A√ß√£o 2
    end
    loop At√© Converg√™ncia
        Agente ->> Ambiente: Escolhe a√ß√£o baseado na associa√ß√£o
        Ambiente -->> Agente: Retorna recompensa e contexto
        Agente ->> Agente: Ajusta associa√ß√£o (se necess√°rio)
    end
```
**Corol√°rio 1**: *A escolha de uma a√ß√£o em uma tarefa associativa n√£o pode ser feita sem levar em conta o contexto atual, pois a mesma a√ß√£o pode ter resultados diferentes em diferentes contextos.*

*Prova:*
A partir do **Lemma 1**, inferimos que a√ß√µes devem ser associadas a contextos. Isso implica que, para tomar a melhor decis√£o em qualquer momento, o agente precisa considerar o contexto atual. Ignorar o contexto levaria a uma aplica√ß√£o de uma pol√≠tica inapropriada para aquela situa√ß√£o, resultando em um desempenho sub√≥ptimo. $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo anterior, se um agente escolhe a A√ß√£o 1 independentemente do contexto, ele receber√° 10 quando o contexto for vermelho e apenas 1 quando o contexto for azul. Um agente que associa a a√ß√£o ao contexto, escolhe a a√ß√£o 1 quando o contexto √© vermelho, e a a√ß√£o 2 quando o contexto √© azul,  conseguir√° um resultado √≥timo. Isso mostra a necessidade de se levar em conta o contexto na escolha da a√ß√£o.

**Proposi√ß√£o 1**: *Em uma tarefa de busca associativa com um n√∫mero finito de contextos e a√ß√µes, existe uma pol√≠tica √≥tima que associa cada contexto √† a√ß√£o que produz a maior recompensa esperada para aquele contexto.*

*Prova:*
Considere um conjunto finito de contextos $C = \{c_1, c_2, \ldots, c_n\}$ e um conjunto finito de a√ß√µes $A = \{a_1, a_2, \ldots, a_m\}$. Para cada contexto $c_i$, existe uma recompensa esperada $Q(c_i, a_j)$ para cada a√ß√£o $a_j$. Uma pol√≠tica $\pi$ √© um mapeamento de contextos para a√ß√µes, ou seja, $\pi: C \rightarrow A$.  Definimos a pol√≠tica √≥tima $\pi^*$ como aquela que para cada contexto $c_i$ escolhe a a√ß√£o $a_j$ que maximiza a recompensa esperada, ou seja, $\pi^*(c_i) = \arg\max_{a_j} Q(c_i, a_j)$. Como o n√∫mero de contextos e a√ß√µes √© finito, e a recompensa esperada para cada par (contexto, a√ß√£o) √© bem definida, ent√£o essa pol√≠tica √≥tima existe. $\blacksquare$
```mermaid
graph LR
    subgraph "Conjunto de Contextos (C)"
        c1["c1"]
        cn["cn"]
        c1 -.-> cn
    end
    subgraph "Conjunto de A√ß√µes (A)"
        a1["a1"]
        am["am"]
        a1 -.-> am
    end
    C -->|mapeamento œÄ| A
    subgraph "Pol√≠tica √ìtima (œÄ*)"
        c1 --> a1
        cn --> am
        c1 -.-> cn
        a1 -.-> am
    end
     style c1 fill:#f9f,stroke:#333,stroke-width:2px
    style cn fill:#f9f,stroke:#333,stroke-width:2px
    style a1 fill:#ccf,stroke:#333,stroke-width:2px
    style am fill:#ccf,stroke:#333,stroke-width:2px
    style c1 fill:#9cf,stroke:#333,stroke-width:2px
    style cn fill:#9cf,stroke:#333,stroke-width:2px
        style a1 fill:#9cf,stroke:#333,stroke-width:2px
    style am fill:#9cf,stroke:#333,stroke-width:2px
    
```

> üí° **Exemplo Num√©rico:** Seja um problema com 3 contextos (c1, c2, c3) e 2 a√ß√µes (a1, a2). As recompensas esperadas para cada par contexto-a√ß√£o s√£o:
>   - Q(c1, a1) = 5, Q(c1, a2) = 2
>   - Q(c2, a1) = 1, Q(c2, a2) = 8
>   - Q(c3, a1) = 9, Q(c3, a2) = 3
>
>   A pol√≠tica √≥tima $\pi^*$ seria:
>   - $\pi^*(c1) = a1$ (pois Q(c1, a1) > Q(c1, a2))
>   - $\pi^*(c2) = a2$ (pois Q(c2, a2) > Q(c2, a1))
>   - $\pi^*(c3) = a1$ (pois Q(c3, a1) > Q(c3, a2))
>   Essa pol√≠tica $\pi^*$ √© a pol√≠tica que maximiza as recompensas esperadas em todos os contextos.

A **Proposi√ß√£o 1** estabelece a exist√™ncia de uma pol√≠tica √≥tima em tarefas de busca associativa com conjuntos finitos de contextos e a√ß√µes. Contudo, o desafio reside em *encontrar* esta pol√≠tica √≥tima. Para isso, devemos explorar algoritmos que aprendem a associar contextos a a√ß√µes, como veremos a seguir.

**Lemma 1.1**: *A efici√™ncia de um algoritmo para tarefas associativas depende tanto da sua capacidade de explorar a√ß√µes quanto da sua capacidade de generalizar entre contextos similares.*

*Prova:*
A efici√™ncia da explora√ß√£o j√° foi abordada no **Lemma 1**. A generaliza√ß√£o entre contextos similares √© crucial para acelerar o aprendizado. Em alguns cen√°rios, contextos podem compartilhar caracter√≠sticas que indicam a√ß√µes √≥timas semelhantes. Se o algoritmo for capaz de reconhecer e explorar essas semelhan√ßas, ele pode aprender de forma mais eficiente, evitando a necessidade de explorar totalmente cada contexto individualmente.  Por exemplo, se a m√°quina ca√ßa-n√≠queis exibe tons de vermelho em vez de apenas vermelho, um algoritmo eficaz n√£o precisa aprender a a√ß√£o √≥tima para cada tom de vermelho separadamente, mas pode generalizar a partir de um subconjunto de amostras. Caso contr√°rio, para cada novo contexto o agente teria que aprender do zero, ignorando experi√™ncias passadas. $\blacksquare$

> üí° **Exemplo Num√©rico:** Imagine um cen√°rio onde as m√°quinas ca√ßa-n√≠queis t√™m v√°rias tonalidades de cor vermelha (vermelho-claro, vermelho-m√©dio, vermelho-escuro) e a a√ß√£o √≥tima para todas elas √© a mesma (A√ß√£o 1). Um algoritmo que n√£o generaliza trataria cada tom de vermelho como um contexto diferente e teria que explorar a a√ß√£o para cada tom separadamente.  Um algoritmo que generaliza reconheceria que esses contextos s√£o similares (todos s√£o vermelhos) e aprenderia a a√ß√£o √≥tima para qualquer tom de vermelho com base na sua experi√™ncia, acelerando o aprendizado.
```mermaid
graph LR
    subgraph "Algoritmo Sem Generaliza√ß√£o"
        C1["Contexto 1"]
        C2["Contexto 2"]
        C3["Contexto 3"]
        C1 --> A1["Explora A√ß√µes"]
        C2 --> A2["Explora A√ß√µes"]
        C3 --> A3["Explora A√ß√µes"]
    end
    subgraph "Algoritmo Com Generaliza√ß√£o"
      subgraph "Representa√ß√£o de Contexto"
          C1'("Contexto 1'")
          C2'("Contexto 2'")
          C3'("Contexto 3'")
           C1' -.-> C2'
           C2' -.-> C3'
      end
        C1' --> AG["Generaliza√ß√£o"]
        C2' --> AG
        C3' --> AG
        AG --> A4["Explora A√ß√µes"]
    end
    style C1 fill:#f9f,stroke:#333,stroke-width:2px
    style C2 fill:#f9f,stroke:#333,stroke-width:2px
    style C3 fill:#f9f,stroke:#333,stroke-width:2px
    style C1' fill:#ccf,stroke:#333,stroke-width:2px
    style C2' fill:#ccf,stroke:#333,stroke-width:2px
    style C3' fill:#ccf,stroke:#333,stroke-width:2px
```
**Corol√°rio 1.1**: *Algoritmos que combinam estrat√©gias de explora√ß√£o com m√©todos de generaliza√ß√£o entre contextos tendem a apresentar melhor desempenho em tarefas associativas complexas.*

*Prova:*
Este corol√°rio decorre diretamente do **Lemma 1.1**. A necessidade de explorar e generalizar evidencia que um algoritmo que combine esses dois elementos tender√° a performar melhor, pois o agente precisa tanto descobrir as melhores a√ß√µes (explora√ß√£o) quanto aprender a associar contextos semelhantes a a√ß√µes similares (generaliza√ß√£o). $\blacksquare$

> üí° **Exemplo Num√©rico:** Um algoritmo como o *Epsilon-Greedy* realiza uma explora√ß√£o simples (escolher aleatoriamente uma a√ß√£o com probabilidade Œµ), por√©m, combinado com t√©cnicas de aprendizado de similaridade de contexto (como o uso de *embeddings*) pode ter um desempenho superior. Por exemplo, um algoritmo com Œµ-greedy e embeddings pode aprender que contextos com cores semelhantes indicam a√ß√µes √≥timas similares. Essa generaliza√ß√£o, combinada com a explora√ß√£o, permite que o algoritmo descubra a√ß√µes √≥timas mais rapidamente e se adapte a novos contextos com base em experi√™ncias passadas.

**Proposi√ß√£o 1.1**: *Em tarefas associativas com um grande n√∫mero de contextos, algoritmos que utilizam alguma forma de representa√ß√£o de contexto (ex: embeddings) podem ser mais eficientes que algoritmos que tratam cada contexto como independente.*

*Prova:*
Quando o n√∫mero de contextos √© muito grande, memorizar um valor de a√ß√£o para cada combina√ß√£o contexto-a√ß√£o torna-se invi√°vel. A utiliza√ß√£o de representa√ß√µes de contexto, como *embeddings*, permite que o algoritmo aprenda a generalizar sobre grupos de contextos similares, reduzindo a necessidade de amostragem exaustiva de cada contexto individualmente. O algoritmo pode projetar contextos para um espa√ßo de representa√ß√£o e usar essa representa√ß√£o para aprender uma pol√≠tica. Algoritmos que tratam cada contexto como independente n√£o conseguem generalizar entre contextos, o que resulta em uma complexidade de aprendizado muito alta para muitos contextos. $\blacksquare$
```mermaid
graph LR
    subgraph "Algoritmo Sem Embeddings"
        C1["Contexto 1"]
        C2["Contexto 2"]
        Cn["Contexto N"]
        C1 --> A1["A√ß√£o"]
        C2 --> A2["A√ß√£o"]
        Cn --> An["A√ß√£o"]
    end
    subgraph "Algoritmo Com Embeddings"
        C1'["Contexto 1"]
        C2'["Contexto 2"]
        Cn'["Contexto N"]
        C1' --> E["Embedding"]
        C2' --> E
        Cn' --> E
        E --> P["Pol√≠tica"]
        P --> A1'["A√ß√£o"]
        P --> A2'["A√ß√£o"]
          P --> An'["A√ß√£o"]
    end
        style C1 fill:#f9f,stroke:#333,stroke-width:2px
    style C2 fill:#f9f,stroke:#333,stroke-width:2px
    style Cn fill:#f9f,stroke:#333,stroke-width:2px
     style C1' fill:#ccf,stroke:#333,stroke-width:2px
    style C2' fill:#ccf,stroke:#333,stroke-width:2px
    style Cn' fill:#ccf,stroke:#333,stroke-width:2px
     style E fill:#9cf,stroke:#333,stroke-width:2px
    style P fill:#9cf,stroke:#333,stroke-width:2px
```
> üí° **Exemplo Num√©rico:** Considere um problema com 1000 contextos, onde cada contexto √© descrito por um vetor de 10 dimens√µes. Um algoritmo que trata cada contexto como independente teria que aprender a a√ß√£o √≥tima para cada um dos 1000 contextos individualmente. Em contraste, um algoritmo que usa embeddings pode projetar esses vetores de 10 dimens√µes em um espa√ßo de dimens√£o menor (por exemplo, 5 dimens√µes) e aprender uma pol√≠tica sobre esse espa√ßo. Se contextos similares tiverem embeddings similares, o algoritmo generaliza o aprendizado, conseguindo um desempenho muito melhor. Por exemplo, usando um modelo linear para associar embeddings a a√ß√µes, os pesos aprendidos generalizam para contextos n√£o vistos, desde que seus embeddings estejam pr√≥ximos.

### Conclus√£o

A introdu√ß√£o das tarefas de busca associativa ou **contextual bandits** expande a complexidade dos problemas de aprendizado por refor√ßo, marcando a transi√ß√£o de problemas onde uma √∫nica a√ß√£o √≥tima √© procurada para cen√°rios onde a√ß√µes devem ser escolhidas baseadas em contextos espec√≠ficos. Este conceito prepara o terreno para o tratamento de problemas mais complexos em **reinforcement learning**, que ser√£o abordados nos pr√≥ximos cap√≠tulos, onde a√ß√µes podem influenciar n√£o s√≥ a recompensa imediata, mas tamb√©m o pr√≥ximo estado, dando origem ao problema completo de **reinforcement learning**.

### Refer√™ncias

[^1]: "So far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no need to associate different actions with different situations. In these tasks the learner either tries to find a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary. However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations. To set the stage for the full problem, we briefly discuss the simplest way in which nonassociative tasks extend to the associative setting. As an example, suppose there are several different k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. If the probabilities with which each task is selected for you do not change over time, this would appear as a single stationary k-armed bandit task, and you could use one of the methods described in this chapter. Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that task‚Äîfor instance, if red, select arm 1; if green, select arm 2. With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another. This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature. Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but they are also like our version of the k-armed bandit problem in that each action affects only the immediate reward. If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem. We present this problem in the next chapter and consider its ramifications throughout the rest of the book." *(Trecho de Multi-armed Bandits)*
