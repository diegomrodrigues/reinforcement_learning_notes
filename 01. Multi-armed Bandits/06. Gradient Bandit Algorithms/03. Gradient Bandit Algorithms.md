## Gradient Bandit Algorithms e Stochastic Gradient Ascent

### Introdu√ß√£o
Este cap√≠tulo explora o algoritmo de **Gradient Bandit**, uma alternativa aos m√©todos de estima√ß√£o de *action values* para a resolu√ß√£o do problema de *k-armed bandit* [^36]. Diferentemente dos m√©todos que estimam o valor das a√ß√µes, o algoritmo de Gradient Bandit aprende uma prefer√™ncia num√©rica para cada a√ß√£o, denotada por $H_t(a) \in \mathbb{R}$ [^37]. Essas prefer√™ncias influenciam a probabilidade de sele√ß√£o de cada a√ß√£o atrav√©s de uma distribui√ß√£o *softmax*. Esta se√ß√£o detalha como esse algoritmo pode ser visto como uma aproxima√ß√£o de *stochastic gradient ascent* e analisa as equa√ß√µes de atualiza√ß√£o das prefer√™ncias de a√ß√£o.

### Conceitos Fundamentais

O algoritmo de **Gradient Bandit** utiliza uma abordagem diferente para selecionar a√ß√µes em compara√ß√£o com os m√©todos baseados em *action-value* [^37]. Em vez de estimar os valores das a√ß√µes, ele mant√©m uma prefer√™ncia num√©rica $H_t(a)$ para cada a√ß√£o $a$. A probabilidade de selecionar uma a√ß√£o $a$ no tempo $t$, denotada por $\pi_t(a)$, √© determinada atrav√©s de uma fun√ß√£o *softmax* [^37]:

$$
Pr\{A_t=a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} = \pi_t(a) \quad \text{[2.11]}
$$

onde $k$ √© o n√∫mero total de a√ß√µes. Inicialmente, todas as prefer√™ncias de a√ß√£o s√£o iguais, resultando em uma probabilidade uniforme de sele√ß√£o [^37].

> üí° **Exemplo Num√©rico:** Considere um problema de 3-armed bandit (k=3) onde inicialmente as prefer√™ncias de a√ß√£o s√£o $H_1(1) = 0.0$, $H_1(2) = 0.0$, e $H_1(3) = 0.0$.  A probabilidade inicial de selecionar cada a√ß√£o seria:
>
> $\pi_1(1) = \frac{e^{0.0}}{e^{0.0} + e^{0.0} + e^{0.0}} = \frac{1}{3} \approx 0.333$
>
> $\pi_1(2) = \frac{e^{0.0}}{e^{0.0} + e^{0.0} + e^{0.0}} = \frac{1}{3} \approx 0.333$
>
> $\pi_1(3) = \frac{e^{0.0}}{e^{0.0} + e^{0.0} + e^{0.0}} = \frac{1}{3} \approx 0.333$
>
> Isso demonstra que, no in√≠cio, todas as a√ß√µes s√£o selecionadas com igual probabilidade.

O algoritmo atualiza as prefer√™ncias de a√ß√£o ap√≥s selecionar a a√ß√£o $A_t$ e receber a recompensa $R_t$ [^37]. A atualiza√ß√£o √© realizada utilizando as seguintes equa√ß√µes:

$$
H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t)) \quad \text{[2.12]}
$$

$$
H_{t+1}(a) = H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a), \quad \text{para todo } a \neq A_t \quad \text{[2.12]}
$$

onde $\alpha > 0$ √© o par√¢metro de *step-size* e $\bar{R}_t$ √© a recompensa m√©dia at√© o momento $t$ [^37]. O termo $\bar{R}_t$ serve como uma *baseline* para comparar a recompensa recebida com o desempenho m√©dio [^37]. Se a recompensa for maior que a *baseline*, a probabilidade de selecionar a a√ß√£o $A_t$ no futuro aumenta; caso contr√°rio, diminui. As a√ß√µes n√£o selecionadas movem-se na dire√ß√£o oposta [^37].

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, suponha que a a√ß√£o 2 ($A_t = 2$) seja selecionada no tempo t=1 e a recompensa recebida seja $R_t = 1.0$. Suponha tamb√©m que a recompensa m√©dia at√© o momento seja $\bar{R}_t = 0.5$ e o *step-size* seja $\alpha = 0.1$. A atualiza√ß√£o das prefer√™ncias de a√ß√£o seria:
>
> $H_{2}(2) = H_1(2) + \alpha(R_t - \bar{R}_t)(1 - \pi_1(2)) = 0.0 + 0.1(1.0 - 0.5)(1 - 0.333) = 0.0 + 0.1(0.5)(0.667) = 0.03335$
>
> $H_{2}(1) = H_1(1) - \alpha(R_t - \bar{R}_t)\pi_1(1) = 0.0 - 0.1(1.0 - 0.5)(0.333) = 0.0 - 0.1(0.5)(0.333) = -0.01665$
>
> $H_{2}(3) = H_1(3) - \alpha(R_t - \bar{R}_t)\pi_1(3) = 0.0 - 0.1(1.0 - 0.5)(0.333) = 0.0 - 0.1(0.5)(0.333) = -0.01665$
>
> As novas probabilidades de sele√ß√£o das a√ß√µes seriam:
>
> $\pi_2(1) = \frac{e^{-0.01665}}{e^{-0.01665} + e^{0.03335} + e^{-0.01665}} \approx \frac{0.9835}{0.9835 + 1.0340 + 0.9835} \approx 0.327$
>
> $\pi_2(2) = \frac{e^{0.03335}}{e^{-0.01665} + e^{0.03335} + e^{-0.01665}} \approx \frac{1.0340}{0.9835 + 1.0340 + 0.9835} \approx 0.345$
>
> $\pi_2(3) = \frac{e^{-0.01665}}{e^{-0.01665} + e^{0.03335} + e^{-0.01665}} \approx \frac{0.9835}{0.9835 + 1.0340 + 0.9835} \approx 0.327$
>
> Observe que a probabilidade de selecionar a a√ß√£o 2 aumentou (de 0.333 para 0.345) porque ela forneceu uma recompensa acima da m√©dia. As probabilidades das outras a√ß√µes diminu√≠ram.

**Proposi√ß√£o 1.** A soma das prefer√™ncias de a√ß√£o n√£o √© invariante sob as atualiza√ß√µes descritas em (2.12).

*Prova.* Seja $S_t = \sum_{a=1}^{k} H_t(a)$. Ent√£o,

$S_{t+1} = H_{t+1}(A_t) + \sum_{a \neq A_t} H_{t+1}(a) = H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t)) + \sum_{a \neq A_t} [H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a)] = \sum_{a=1}^{k} H_t(a) + \alpha(R_t - \bar{R}_t) [1 - \pi_t(A_t) - \sum_{a \neq A_t} \pi_t(a)] = S_t + \alpha(R_t - \bar{R}_t) [1 - \sum_{a=1}^{k} \pi_t(a)] = S_t + \alpha(R_t - \bar{R}_t) [1 - 1] = S_t$.

Portanto, $S_{t+1} = S_t$.  Isto significa que a soma das prefer√™ncias se mant√©m constante a cada passo. $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando os valores do exemplo anterior, vamos verificar que a soma das prefer√™ncias permanece constante.
>
> $S_1 = H_1(1) + H_1(2) + H_1(3) = 0.0 + 0.0 + 0.0 = 0.0$
>
> $S_2 = H_2(1) + H_2(2) + H_2(3) = -0.01665 + 0.03335 - 0.01665 = 0.0$
>
> Como esperado, a soma das prefer√™ncias permaneceu constante.

**Proposi√ß√£o 2.** A soma das probabilidades $\pi_t(a)$ sobre todas as a√ß√µes $a$ √© sempre igual a 1.

*Prova.*
I.  Pela defini√ß√£o da fun√ß√£o softmax, temos:
    $$\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}}$$

II. A soma das probabilidades sobre todas as a√ß√µes √©:
    $$\sum_{a=1}^{k} \pi_t(a) = \sum_{a=1}^{k} \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}}$$

III. Como o denominador n√£o depende do √≠ndice de somat√≥rio $a$, podemos retir√°-lo da soma:
    $$\sum_{a=1}^{k} \pi_t(a) = \frac{\sum_{a=1}^{k} e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}}$$

IV. O numerador e o denominador s√£o agora id√™nticos:
    $$\sum_{a=1}^{k} \pi_t(a) = \frac{\sum_{a=1}^{k} e^{H_t(a)}}{\sum_{a=1}^{k} e^{H_t(a)}}$$

V. Portanto:
    $$\sum_{a=1}^{k} \pi_t(a) = 1$$  $\blacksquare$

> üí° **Exemplo Num√©rico:** Novamente, usando os valores do exemplo anterior:
>
> $\pi_1(1) + \pi_1(2) + \pi_1(3) = 0.333 + 0.333 + 0.333 = 0.999 \approx 1.0$
>
> $\pi_2(1) + \pi_2(2) + \pi_2(3) = 0.327 + 0.345 + 0.327 = 0.999 \approx 1.0$
>
> A soma das probabilidades √© sempre (aproximadamente) igual a 1, devido √† fun√ß√£o softmax.

**Stochastic Gradient Ascent**

O algoritmo de **Gradient Bandit** pode ser interpretado como uma aproxima√ß√£o de *stochastic gradient ascent* [^37]. Em *gradient ascent* exato, a prefer√™ncia de cada a√ß√£o $H_t(a)$ seria incrementada proporcionalmente ao efeito do incremento no desempenho [^38]:

$$
H_{t+1}(a) \doteq H_t(a) + \alpha \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} \quad \text{[2.13]}
$$

onde $\mathbb{E}[R_t]$ √© a recompensa esperada [^38]:

$$
\mathbb{E}[R_t] = \sum_x \pi_t(x)q_*(x)
$$

e $q_*(x)$ √© o valor real da a√ß√£o $x$ [^38].

No entanto, como n√£o conhecemos os valores reais das a√ß√µes $q_*(x)$, n√£o podemos implementar o *gradient ascent* exatamente [^38]. Em vez disso, as atualiza√ß√µes do algoritmo (2.12) s√£o iguais a (2.13) em valor esperado, tornando o algoritmo uma inst√¢ncia de *stochastic gradient ascent* [^38].

Para demonstrar essa equival√™ncia, derivamos o gradiente de desempenho exato [^39]:
$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)} \left[ \sum_x \pi_t(x) q_*(x) \right] = \sum_x q_*(x) \frac{\partial \pi_t(x)}{\partial H_t(a)}
$$

Introduzimos uma *baseline* $B_t$, que pode ser qualquer escalar independente de $x$, sem alterar a igualdade, pois o gradiente soma zero sobre todas as a√ß√µes:
$$
\sum_a \frac{\partial \pi_t(x)}{\partial H_t(a)} = 0
$$

*Prova.*

I.  Come√ßamos diferenciando a fun√ß√£o softmax em rela√ß√£o a $H_t(a)$:
    $$
    \pi_t(x) = \frac{e^{H_t(x)}}{\sum_{b=1}^{k} e^{H_t(b)}}
    $$
    Ent√£o:
    $$
    \frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)} \left( \frac{e^{H_t(x)}}{\sum_{b=1}^{k} e^{H_t(b)}} \right)
    $$

II.  Usando a regra do quociente, obtemos:
    $$
    \frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\frac{\partial e^{H_t(x)}}{\partial H_t(a)} \sum_{b=1}^{k} e^{H_t(b)} - e^{H_t(x)} \frac{\partial}{\partial H_t(a)} \sum_{b=1}^{k} e^{H_t(b)}}{\left( \sum_{b=1}^{k} e^{H_t(b)} \right)^2}
    $$

III. Calculando as derivadas parciais:
    $$
    \frac{\partial e^{H_t(x)}}{\partial H_t(a)} = \begin{cases} e^{H_t(x)} & \text{se } x = a \\ 0 & \text{se } x \neq a \end{cases}
    $$
    $$
    \frac{\partial}{\partial H_t(a)} \sum_{b=1}^{k} e^{H_t(b)} = e^{H_t(a)}
    $$

IV. Substituindo as derivadas parciais na equa√ß√£o do passo II:
    $$
    \frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\mathbb{1}_{x=a} e^{H_t(x)} \sum_{b=1}^{k} e^{H_t(b)} - e^{H_t(x)} e^{H_t(a)}}{\left( \sum_{b=1}^{k} e^{H_t(b)} \right)^2}
    $$
    onde $\mathbb{1}_{x=a}$ √© a fun√ß√£o indicadora que vale 1 se $x=a$ e 0 caso contr√°rio.

V. Agora, somamos sobre todas as a√ß√µes $x$:
    $$
    \sum_{x=1}^{k} \frac{\partial \pi_t(x)}{\partial H_t(a)} = \sum_{x=1}^{k} \frac{\mathbb{1}_{x=a} e^{H_t(x)} \sum_{b=1}^{k} e^{H_t(b)} - e^{H_t(x)} e^{H_t(a)}}{\left( \sum_{b=1}^{k} e^{H_t(b)} \right)^2}
    $$

VI. Simplificando a soma:
    $$
    \sum_{x=1}^{k} \frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{e^{H_t(a)} \sum_{b=1}^{k} e^{H_t(b)} - e^{H_t(a)} \sum_{x=1}^{k} e^{H_t(x)}}{\left( \sum_{b=1}^{k} e^{H_t(b)} \right)^2}
    $$

VII. Como $\sum_{x=1}^{k} e^{H_t(x)} = \sum_{b=1}^{k} e^{H_t(b)}$, a equa√ß√£o se simplifica para:
    $$
    \sum_{x=1}^{k} \frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{e^{H_t(a)} \sum_{b=1}^{k} e^{H_t(b)} - e^{H_t(a)} \sum_{b=1}^{k} e^{H_t(b)}}{\left( \sum_{b=1}^{k} e^{H_t(b)} \right)^2} = 0
    $$

VIII. Portanto, demonstramos que:
     $$
     \sum_{x=1}^{k} \frac{\partial \pi_t(x)}{\partial H_t(a)} = 0
     $$
$\blacksquare$

Assim:
$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \sum_x (q_*(x) - B_t) \frac{\partial \pi_t(x)}{\partial H_t(a)}
$$

Multiplicando cada termo da soma por $\pi_t(x) / \pi_t(x)$, obtemos:
$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \sum_x \pi_t(x) (q_*(x) - B_t) \frac{\partial \pi_t(x)}{\partial H_t(a)} / \pi_t(x)
$$

Essa equa√ß√£o est√° agora na forma de uma expectativa:
$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \mathbb{E} \left[ (q_*(A_t) - B_t) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} / \pi_t(A_t) \right]
$$

Escolhendo a *baseline* $B_t = \bar{R}_t$ e substituindo $R_t$ por $q_*(A_t)$ (permitido porque $\mathbb{E}[R_t | A_t] = q_*(A_t)$), temos:
$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \mathbb{E} \left[ (R_t - \bar{R}_t) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} / \pi_t(A_t) \right]
$$

Usando o resultado de que $\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) (\mathbb{1}_{a=x} - \pi_t(a))$ [^40], onde $\mathbb{1}_{a=x}$ √© 1 se $a=x$ e 0 caso contr√°rio, obtemos:

*Prova.*

I. Come√ßamos com a defini√ß√£o da fun√ß√£o softmax:
    $$
    \pi_t(x) = \frac{e^{H_t(x)}}{\sum_{b=1}^{k} e^{H_t(b)}}
    $$

II. Queremos calcular a derivada parcial de $\pi_t(x)$ em rela√ß√£o a $H_t(a)$:
    $$
    \frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)} \left( \frac{e^{H_t(x)}}{\sum_{b=1}^{k} e^{H_t(b)}} \right)
    $$

III. Aplicamos a regra do quociente:
    $$
    \frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\frac{\partial}{\partial H_t(a)} \left( e^{H_t(x)} \right) \cdot \sum_{b=1}^{k} e^{H_t(b)} - e^{H_t(x)} \cdot \frac{\partial}{\partial H_t(a)} \left( \sum_{b=1}^{k} e^{H_t(b)} \right)}{\left( \sum_{b=1}^{k} e^{H_t(b)} \right)^2}
    $$

IV. Calculamos as derivadas parciais:
    $$
    \frac{\partial}{\partial H_t(a)} e^{H_t(x)} = \begin{cases} e^{H_t(x)}, & \text{se } a = x \\ 0, & \text{se } a \neq x \end{cases} = \mathbb{1}_{a=x} e^{H_t(x)}
    $$
    $$
    \frac{\partial}{\partial H_t(a)} \sum_{b=1}^{k} e^{H_t(b)} = e^{H_t(a)}
    $$

V. Substitu√≠mos as derivadas parciais na equa√ß√£o do passo III:
    $$
    \frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\mathbb{1}_{a=x} e^{H_t(x)} \sum_{b=1}^{k} e^{H_t(b)} - e^{H_t(x)} e^{H_t(a)}}{\left( \sum_{b=1}^{k} e^{H_t(b)} \right)^2}
    $$

VI. Simplificamos a express√£o:
    $$
    \frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{e^{H_t(x)}}{\sum_{b=1}^{k} e^{H_t(b)}} \cdot \frac{\mathbb{1}_{a=x} \sum_{b=1}^{k} e^{H_t(b)} - e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}}
    $$
    $$
     = \pi_t(x) \cdot \left( \mathbb{1}_{a=x} - \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} \right)
    $$
    $$
     = \pi_t(x) \cdot \left( \mathbb{1}_{a=x} - \pi_t(a) \right)
    $$

VII. Portanto, demonstramos que:
    $$
    \frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) (\mathbb{1}_{a=x} - \pi_t(a))
    $$
$\blacksquare$

$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \mathbb{E} \left[ (R_t - \bar{R}_t) (\mathbb{1}_{a=A_t} - \pi_t(a)) \right]
$$
Finalmente, substituindo essa expectativa na equa√ß√£o (2.13) para *stochastic gradient ascent*, obtemos:
$$
H_{t+1}(a) = H_t(a) + \alpha (R_t - \bar{R}_t) (\mathbb{1}_{a=A_t} - \pi_t(a))
$$
que √© equivalente ao algoritmo de **Gradient Bandit** original (2.12) [^39].  $\blacksquare$





Um estudo comparativo do algoritmo *gradient bandit* com e sem *baseline* de recompensa pode ilustrar o impacto desta √∫ltima.

![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 bra√ßos.](./../images/image1.png)

**Teorema 1.** O algoritmo Gradient Bandit converge para uma pol√≠tica √≥tima sob certas condi√ß√µes de *step-size* e explora√ß√£o.

*Prova (Esbo√ßo).* A converg√™ncia do algoritmo Gradient Bandit pode ser analisada utilizando a teoria de converg√™ncia para algoritmos de *stochastic gradient ascent*. Como demonstrado, o algoritmo Gradient Bandit √© uma inst√¢ncia de *stochastic gradient ascent*. As condi√ß√µes para converg√™ncia incluem um *step-size* $\alpha$ que satisfaz as condi√ß√µes de Robbins-Monro (i.e., $\sum_{t=1}^{\infty} \alpha_t = \infty$ e $\sum_{t=1}^{\infty} \alpha_t^2 < \infty$), e uma explora√ß√£o persistente, garantindo que todas as a√ß√µes s√£o suficientemente amostradas. Sob estas condi√ß√µes, as prefer√™ncias de a√ß√£o $H_t(a)$ convergem para valores que refletem a otimalidade das a√ß√µes, e consequentemente, a pol√≠tica $\pi_t(a)$ converge para uma pol√≠tica √≥tima. Uma an√°lise detalhada exigiria a demonstra√ß√£o de que a vari√¢ncia do gradiente estoc√°stico √© limitada e a fun√ß√£o objetivo (recompensa esperada) satisfaz certas condi√ß√µes de suavidade. $\blacksquare$

### Conclus√£o

O algoritmo de **Gradient Bandit** oferece uma abordagem alternativa e eficaz para o problema de *k-armed bandit* [^36]. Ao aprender prefer√™ncias de a√ß√£o em vez de valores de a√ß√£o, ele adapta-se naturalmente a ambientes n√£o estacion√°rios [^37]. A interpreta√ß√£o do algoritmo como uma aproxima√ß√£o de *stochastic gradient ascent* fornece uma base te√≥rica s√≥lida e garante propriedades de converg√™ncia robustas [^40]. A escolha da *baseline* $\bar{R}_t$ afeta a vari√¢ncia das atualiza√ß√µes e, portanto, a taxa de converg√™ncia, mas n√£o afeta o valor esperado da atualiza√ß√£o [^40].

### Refer√™ncias
[^36]: Cap√≠tulo 2: Multi-armed Bandits.
[^37]: Se√ß√£o 2.8: Gradient Bandit Algorithms.
[^38]: O Algoritmo Gradient Bandit como Stochastic Gradient Ascent.
[^39]: Se√ß√£o 2.8. Gradient Bandit Algorithms
[^40]: Cap√≠tulo 2: Multi-armed Bandits
<!-- END -->