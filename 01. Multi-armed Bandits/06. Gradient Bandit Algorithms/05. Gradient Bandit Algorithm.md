## Gradient Bandit Algorithms as Stochastic Gradient Ascent

### Introdu√ß√£o
O algoritmo *gradient bandit* pode ser visto como uma inst√¢ncia de **stochastic gradient ascent**. Esta interpreta√ß√£o fornece uma vis√£o mais profunda sobre o funcionamento do algoritmo e suas propriedades de converg√™ncia [^38]. Em particular, cada prefer√™ncia de a√ß√£o $H_t(a)$ √© incrementada proporcionalmente ao efeito do incremento no desempenho. Esta se√ß√£o detalha a deriva√ß√£o matem√°tica que demonstra esta equival√™ncia.

### Stochastic Gradient Ascent
No *gradient ascent* exato, cada prefer√™ncia de a√ß√£o $H_t(a)$ seria incrementada em propor√ß√£o ao efeito do incremento no desempenho, ou seja [^38]:

$$
H_{t+1}(a) = H_t(a) + \alpha \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} \quad (2.13)
$$

onde $\mathbb{E}[R_t]$ √© a medida de desempenho, sendo a recompensa esperada [^38]:

$$
\mathbb{E}[R_t] = \sum_x \pi_t(x) q_*(x)
$$

Aqui, $\pi_t(x)$ representa a probabilidade de selecionar a a√ß√£o *x* no tempo *t*, e $q_*(x)$ √© o valor verdadeiro (esperado) da a√ß√£o *x* [^38].

**Observa√ß√£o:** A equa√ß√£o acima assume que o espa√ßo de a√ß√µes √© discreto, o que simplifica a nota√ß√£o usando a soma. No entanto, o conceito pode ser estendido para espa√ßos de a√ß√µes cont√≠nuos, substituindo a soma por uma integral.

> üí° **Exemplo Num√©rico:** Suponha que temos 3 a√ß√µes com valores verdadeiros $q_*(1) = 2$, $q_*(2) = 4$, e $q_*(3) = 1$. No tempo $t$, a pol√≠tica $\pi_t$ √© $\pi_t(1) = 0.3$, $\pi_t(2) = 0.5$, e $\pi_t(3) = 0.2$. Ent√£o, a recompensa esperada $\mathbb{E}[R_t]$ √© calculada como:
>
> $\mathbb{E}[R_t] = (0.3 \times 2) + (0.5 \times 4) + (0.2 \times 1) = 0.6 + 2.0 + 0.2 = 2.8$

### Deriva√ß√£o do Gradiente
A medida do efeito do incremento √© a *derivada parcial* desta medida de desempenho com respeito √† prefer√™ncia de a√ß√£o. Calcular esta derivada requer algumas etapas. Come√ßamos expandindo a derivada [^39]:

$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)} \sum_x \pi_t(x) q_*(x) = \sum_x q_*(x) \frac{\partial \pi_t(x)}{\partial H_t(a)}
$$

Subtraindo uma linha de base $B_t$ (que pode ser qualquer escalar que n√£o dependa de *x*) de $q_*(x)$ n√£o altera a igualdade, pois o gradiente soma zero sobre todas as a√ß√µes [^39]:
$$\sum_x \frac{\partial \pi_t(x)}{\partial H_t(a)} = 0$$. Assim [^39]:

$$
\sum_x q_*(x) \frac{\partial \pi_t(x)}{\partial H_t(a)} = \sum_x (q_*(x) - B_t) \frac{\partial \pi_t(x)}{\partial H_t(a)}
$$

**Prova:**
Provaremos que $\sum_x \frac{\partial \pi_t(x)}{\partial H_t(a)} = 0$.

I. Pela defini√ß√£o de $\pi_t(x)$ (usando softmax):
    $$\pi_t(x) = \frac{e^{H_t(x)}}{\sum_y e^{H_t(y)}}$$

II. Considere a soma das probabilidades sobre todas as a√ß√µes *x*:
    $$\sum_x \pi_t(x) = \sum_x \frac{e^{H_t(x)}}{\sum_y e^{H_t(y)}}$$

III. Como a soma das probabilidades deve ser igual a 1:
    $$\sum_x \pi_t(x) = 1$$

IV. Diferenciando ambos os lados com respeito a $H_t(a)$:
    $$\frac{\partial}{\partial H_t(a)} \sum_x \pi_t(x) = \frac{\partial}{\partial H_t(a)} (1)$$

V. Portanto:
    $$\sum_x \frac{\partial \pi_t(x)}{\partial H_t(a)} = 0$$ ‚ñ†

Multiplicamos cada termo da soma por $\pi_t(x) / \pi_t(x)$ para obter uma forma de expectativa [^39]:

$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \sum_x (q_*(x) - B_t) \frac{\partial \pi_t(x)}{\partial H_t(a)} \frac{\pi_t(x)}{\pi_t(x)}
$$

Esta equa√ß√£o est√° agora na forma de uma expectativa, somando sobre todos os poss√≠veis valores *x* da vari√°vel aleat√≥ria $A_t$, ent√£o multiplicando pela probabilidade de tomar esses valores [^39]:

$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \mathbb{E} \left[ (q_*(A_t) - B_t) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} / \pi_t(A_t) \right]
$$

Escolhendo a linha de base $B_t = \bar{R}_t$ (a recompensa m√©dia) e substituindo $R_t$ por $q_*(A_t)$, o que √© permitido porque $\mathbb{E}[R_t|A_t] = q_*(A_t)$, temos [^39]:

$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \mathbb{E} \left[ (R_t - \bar{R}_t) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} / \pi_t(A_t) \right]
$$

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, suponha que $\bar{R}_t = 2.5$, e que a a√ß√£o selecionada $A_t$ foi a a√ß√£o 2 com recompensa $R_t = 4$. Ent√£o, para calcular $\frac{\partial \mathbb{E}[R_t]}{\partial H_t(1)}$, $\frac{\partial \mathbb{E}[R_t]}{\partial H_t(2)}$, e $\frac{\partial \mathbb{E}[R_t]}{\partial H_t(3)}$:
>
> Para $a = 1$: $\mathbb{E} \left[ (R_t - \bar{R}_t) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} / \pi_t(A_t) \right] = (4 - 2.5) \frac{\partial \pi_t(2)}{\partial H_t(1)} / \pi_t(2)$
>
> Para $a = 2$: $\mathbb{E} \left[ (R_t - \bar{R}_t) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} / \pi_t(A_t) \right] = (4 - 2.5) \frac{\partial \pi_t(2)}{\partial H_t(2)} / \pi_t(2)$
>
> Para $a = 3$: $\mathbb{E} \left[ (R_t - \bar{R}_t) \frac{\partial \pi_t(A_t)}{\partial H_t(a)} / \pi_t(A_t) \right] = (4 - 2.5) \frac{\partial \pi_t(2)}{\partial H_t(3)} / \pi_t(2)$

Usando o resultado derivado de que $\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) (\mathbb{1}_{a=x} - \pi_t(a))$, onde $\mathbb{1}_{a=x}$ √© 1 se $a = x$ e 0 caso contr√°rio, obtemos [^39]:

$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \mathbb{E} \left[ (R_t - \bar{R}_t) (\mathbb{1}_{a=A_t} - \pi_t(a)) \right]
$$

**Prova:** Provaremos que $\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) (\mathbb{1}_{a=x} - \pi_t(a))$.

I.  Come√ßamos com a defini√ß√£o da pol√≠tica softmax:
    $$\pi_t(x) = \frac{e^{H_t(x)}}{\sum_y e^{H_t(y)}}$$

II. Precisamos calcular a derivada parcial de $\pi_t(x)$ em rela√ß√£o a $H_t(a)$. Consideraremos dois casos: $a = x$ e $a \neq x$.

III. Caso 1: $a = x$
    $$\frac{\partial \pi_t(x)}{\partial H_t(x)} = \frac{\partial}{\partial H_t(x)} \left( \frac{e^{H_t(x)}}{\sum_y e^{H_t(y)}} \right)$$
    Usando a regra do quociente: $\frac{\partial}{\partial x} \left(\frac{u}{v}\right) = \frac{v\frac{\partial u}{\partial x} - u\frac{\partial v}{\partial x}}{v^2}$, onde $u = e^{H_t(x)}$ e $v = \sum_y e^{H_t(y)}$:
    $$\frac{\partial \pi_t(x)}{\partial H_t(x)} = \frac{\left(\sum_y e^{H_t(y)}\right) e^{H_t(x)} - e^{H_t(x)} e^{H_t(x)}}{\left(\sum_y e^{H_t(y)}\right)^2}$$
    $$\frac{\partial \pi_t(x)}{\partial H_t(x)} = \frac{e^{H_t(x)}}{\sum_y e^{H_t(y)}} \cdot \frac{\sum_y e^{H_t(y)} - e^{H_t(x)}}{\sum_y e^{H_t(y)}} = \pi_t(x) \left(1 - \pi_t(x)\right)$$

IV. Caso 2: $a \neq x$
    $$\frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)} \left( \frac{e^{H_t(x)}}{\sum_y e^{H_t(y)}} \right)$$
    Aqui, $e^{H_t(x)}$ n√£o depende de $H_t(a)$, ent√£o $\frac{\partial e^{H_t(x)}}{\partial H_t(a)} = 0$. Aplicando novamente a regra do quociente:
    $$\frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{0 - e^{H_t(x)} e^{H_t(a)}}{\left(\sum_y e^{H_t(y)}\right)^2} = - \frac{e^{H_t(x)}}{\sum_y e^{H_t(y)}} \cdot \frac{e^{H_t(a)}}{\sum_y e^{H_t(y)}} = - \pi_t(x) \pi_t(a)$$

V. Combinando os dois casos usando a fun√ß√£o indicadora $\mathbb{1}_{a=x}$:
    $$\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) (\mathbb{1}_{a=x} - \pi_t(a))$$

Portanto, $\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) (\mathbb{1}_{a=x} - \pi_t(a))$ ‚ñ†

> üí° **Exemplo Num√©rico:** Usando a derivada que acabamos de provar, e os valores do exemplo anterior ($A_t = 2$, $R_t = 4$, $\bar{R}_t = 2.5$, $\pi_t(1) = 0.3$, $\pi_t(2) = 0.5$, e $\pi_t(3) = 0.2$), podemos calcular $\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)}$ para cada a√ß√£o:
>
> Para $a = 1$: $\frac{\partial \mathbb{E}[R_t]}{\partial H_t(1)} = (4 - 2.5) (\mathbb{1}_{1=2} - 0.3) = 1.5 (0 - 0.3) = -0.45$
>
> Para $a = 2$: $\frac{\partial \mathbb{E}[R_t]}{\partial H_t(2)} = (4 - 2.5) (\mathbb{1}_{2=2} - 0.5) = 1.5 (1 - 0.5) = 0.75$
>
> Para $a = 3$: $\frac{\partial \mathbb{E}[R_t]}{\partial H_t(3)} = (4 - 2.5) (\mathbb{1}_{3=2} - 0.2) = 1.5 (0 - 0.2) = -0.3$

**Proposi√ß√£o 1:** A escolha da linha de base $B_t$ afeta a vari√¢ncia do gradiente estimado, mas n√£o afeta o valor esperado do gradiente.

*Prova:* Conforme mencionado no texto, qualquer linha de base que n√£o dependa de *x* √© v√°lida. A vari√¢ncia do gradiente √© dada por $Var[(R_t - B_t) (\mathbb{1}_{a=A_t} - \pi_t(a))]$. Escolher diferentes valores para $B_t$ alterar√° essa vari√¢ncia. No entanto, o valor esperado do gradiente permanece o mesmo, pois a subtra√ß√£o da linha de base √© anulada quando a expectativa √© calculada.

**Prova formal:**
Provaremos que $\mathbb{E}[(R_t - B_t) (\mathbb{1}_{a=A_t} - \pi_t(a))] = \mathbb{E}[R_t (\mathbb{1}_{a=A_t} - \pi_t(a))] - B_t \mathbb{E}[(\mathbb{1}_{a=A_t} - \pi_t(a))]$ e que o segundo termo √© zero.

I. Expandindo a expectativa:
    $$\mathbb{E}[(R_t - B_t) (\mathbb{1}_{a=A_t} - \pi_t(a))] = \mathbb{E}[R_t (\mathbb{1}_{a=A_t} - \pi_t(a)) - B_t (\mathbb{1}_{a=A_t} - \pi_t(a))]$$

II. Usando a linearidade da expectativa:
    $$\mathbb{E}[(R_t - B_t) (\mathbb{1}_{a=A_t} - \pi_t(a))] = \mathbb{E}[R_t (\mathbb{1}_{a=A_t} - \pi_t(a))] - B_t \mathbb{E}[(\mathbb{1}_{a=A_t} - \pi_t(a))]$$

III. Agora, mostre que $\mathbb{E}[(\mathbb{1}_{a=A_t} - \pi_t(a))] = 0$:
    $$\mathbb{E}[\mathbb{1}_{a=A_t}] = \sum_x \pi_t(x) \mathbb{1}_{a=x} = \pi_t(a)$$

IV. Portanto:
     $$\mathbb{E}[(\mathbb{1}_{a=A_t} - \pi_t(a))] = \mathbb{E}[\mathbb{1}_{a=A_t}] - \pi_t(a) = \pi_t(a) - \pi_t(a) = 0$$

V. Substituindo de volta na equa√ß√£o original:
    $$\mathbb{E}[(R_t - B_t) (\mathbb{1}_{a=A_t} - \pi_t(a))] = \mathbb{E}[R_t (\mathbb{1}_{a=A_t} - \pi_t(a))] - B_t \cdot 0 = \mathbb{E}[R_t (\mathbb{1}_{a=A_t} - \pi_t(a))]$$

Isso mostra que a escolha de $B_t$ n√£o afeta o valor esperado do gradiente. ‚ñ†

### Conex√£o com o Algoritmo Gradient Bandit
Nosso objetivo era escrever o gradiente de desempenho como uma expectativa de algo que podemos amostrar em cada etapa. Substituindo uma amostra da expectativa acima para o gradiente de desempenho na equa√ß√£o (2.13), obtemos [^39]:

$$
H_{t+1}(a) = H_t(a) + \alpha (R_t - \bar{R}_t) (\mathbb{1}_{a=A_t} - \pi_t(a)), \text{ para todo } a
$$

que √© equivalente ao algoritmo *gradient bandit* original (2.12) [^39].

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, suponha que a taxa de aprendizado $\alpha = 0.1$, e as prefer√™ncias iniciais s√£o $H_t(1) = 0.1$, $H_t(2) = 0.2$, e $H_t(3) = 0.3$. Com os valores calculados anteriormente, a atualiza√ß√£o das prefer√™ncias seria:
>
> $H_{t+1}(1) = 0.1 + 0.1 \times (-0.45) = 0.055$
>
> $H_{t+1}(2) = 0.2 + 0.1 \times (0.75) = 0.275$
>
> $H_{t+1}(3) = 0.3 + 0.1 \times (-0.3) = 0.27$

**Teorema 1:** O algoritmo Gradient Bandit converge para uma pol√≠tica √≥tima sob certas condi√ß√µes de regularidade e escolha apropriada do par√¢metro $\alpha$.

*Prova (Esbo√ßo):* Como o Gradient Bandit √© uma inst√¢ncia do Stochastic Gradient Ascent, os resultados de converg√™ncia bem estabelecidos para Stochastic Gradient Ascent podem ser aplicados. Especificamente, se a fun√ß√£o objetivo (neste caso, a recompensa esperada) √© suave e c√¥ncava, e se o tamanho do passo $\alpha$ satisfaz as condi√ß√µes de Robbins-Monro ($\sum_{t=1}^{\infty} \alpha_t = \infty$ e $\sum_{t=1}^{\infty} \alpha_t^2 < \infty$), ent√£o o algoritmo converge para um √≥timo local. Em muitos casos pr√°ticos, o √≥timo local tamb√©m √© o √≥timo global.

### Conclus√£o
Esta deriva√ß√£o demonstra que o algoritmo *gradient bandit* √© uma inst√¢ncia do *stochastic gradient ascent*. Portanto, o algoritmo tem propriedades de converg√™ncia robustas [^40]. √â importante notar que a deriva√ß√£o n√£o imp√¥s nenhuma restri√ß√£o √† linha de base da recompensa, al√©m de sua independ√™ncia da a√ß√£o selecionada. A escolha da linha de base afeta a vari√¢ncia da atualiza√ß√£o e, portanto, a taxa de converg√™ncia, como mostrado na Figura 2.5 [^40].





![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 bra√ßos.](./../images/image1.png)

**Corol√°rio 1:** Uma linha de base que reduz a vari√¢ncia da estimativa do gradiente resultar√° em uma converg√™ncia mais r√°pida do algoritmo Gradient Bandit.

Este corol√°rio segue diretamente da Proposi√ß√£o 1 e do Teorema 1. Reduzir a vari√¢ncia da estimativa do gradiente permite que o algoritmo fa√ßa atualiza√ß√µes mais precisas, levando a uma converg√™ncia mais r√°pida. T√©cnicas de redu√ß√£o de vari√¢ncia, como o uso de uma linha de base mais precisa (por exemplo, uma estimativa mais precisa da recompensa m√©dia), podem melhorar o desempenho do algoritmo.
<!-- END -->