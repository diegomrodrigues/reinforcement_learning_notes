## Gradient Bandit Algorithms: Softmax Action Probabilities and Preference Invariance

### IntroduÃ§Ã£o
O algoritmo de gradiente bandit introduz uma abordagem alternativa para o problema de *k-armed bandit*, onde em vez de estimar os valores das aÃ§Ãµes diretamente, o algoritmo aprende uma preferÃªncia numÃ©rica para cada aÃ§Ã£o [^37]. As probabilidades de selecionar cada aÃ§Ã£o sÃ£o entÃ£o determinadas atravÃ©s de uma distribuiÃ§Ã£o *softmax*, tambÃ©m conhecida como distribuiÃ§Ã£o de Gibbs ou Boltzmann. Uma propriedade fundamental desta distribuiÃ§Ã£o Ã© que adicionar uma constante a todas as preferÃªncias de aÃ§Ã£o nÃ£o altera as probabilidades de seleÃ§Ã£o das aÃ§Ãµes [^37].

### Conceitos Fundamentais

No algoritmo *gradient bandit*, cada aÃ§Ã£o *a* tem uma preferÃªncia $H_t(a) \in \mathbb{R}$ [^37]. Essas preferÃªncias nÃ£o tÃªm interpretaÃ§Ã£o direta em termos de recompensa, mas sim a preferÃªncia relativa entre as aÃ§Ãµes que importa [^37]. A probabilidade de selecionar uma aÃ§Ã£o *a* no tempo *t*, denotada como $\pi_t(a)$, Ã© dada pela distribuiÃ§Ã£o *softmax*:

$$
\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} \quad (2.11)
$$

onde *k* Ã© o nÃºmero total de aÃ§Ãµes [^37]. Essa equaÃ§Ã£o define como as preferÃªncias numÃ©ricas sÃ£o transformadas em probabilidades de aÃ§Ã£o.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que temos 3 aÃ§Ãµes (k=3) e as preferÃªncias atuais sÃ£o H_t(1) = 1.0, H_t(2) = 0.5, e H_t(3) = 0.0. Vamos calcular as probabilidades usando a equaÃ§Ã£o softmax:
>
> $\pi_t(1) = \frac{e^{1.0}}{e^{1.0} + e^{0.5} + e^{0.0}} = \frac{2.718}{2.718 + 1.649 + 1.0} = \frac{2.718}{5.367} \approx 0.506$
>
> $\pi_t(2) = \frac{e^{0.5}}{e^{1.0} + e^{0.5} + e^{0.0}} = \frac{1.649}{5.367} \approx 0.307$
>
> $\pi_t(3) = \frac{e^{0.0}}{e^{1.0} + e^{0.5} + e^{0.0}} = \frac{1.0}{5.367} \approx 0.186$
>
> Observe que $\pi_t(1) + \pi_t(2) + \pi_t(3) = 0.506 + 0.307 + 0.186 = 0.999 \approx 1.0$.  A aÃ§Ã£o 1 tem a maior probabilidade de ser selecionada, seguida pela aÃ§Ã£o 2 e, finalmente, pela aÃ§Ã£o 3.
>
> ```python
> import numpy as np
>
> # PreferÃªncias das aÃ§Ãµes
> H = np.array([1.0, 0.5, 0.0])
>
> # Calcula as probabilidades usando softmax
> pi = np.exp(H) / np.sum(np.exp(H))
>
> print(f"Probabilidades das aÃ§Ãµes: {pi}")
> ```

**Lema 1.** *A funÃ§Ã£o softmax Ã© diferenciÃ¡vel em relaÃ§Ã£o a cada preferÃªncia $H_t(a)$.*

*DemonstraÃ§Ã£o:* A funÃ§Ã£o exponencial e a soma sÃ£o diferenciÃ¡veis. Portanto, a funÃ§Ã£o softmax, sendo uma razÃ£o de funÃ§Ãµes diferenciÃ¡veis (com denominador diferente de zero), Ã© diferenciÃ¡vel. $\blacksquare$

Este resultado Ã© importante porque o algoritmo de gradiente bandit usa derivadas para otimizar as preferÃªncias.

**InvariÃ¢ncia Ã  AdiÃ§Ã£o de uma Constante:**
Uma caracterÃ­stica crucial da distribuiÃ§Ã£o *softmax* Ã© sua invariÃ¢ncia em relaÃ§Ã£o Ã  adiÃ§Ã£o de uma constante a todas as preferÃªncias [^37]. Isto significa que se adicionarmos uma constante *C* a todas as preferÃªncias $H_t(a)$, a probabilidade de selecionar qualquer aÃ§Ã£o *a* permanece inalterada. Para demonstrar isto, considere as preferÃªncias modificadas $H'_t(a) = H_t(a) + C$ e a probabilidade de aÃ§Ã£o correspondente $\pi'_t(a)$:

$$
\pi'_t(a) = \frac{e^{H'_t(a)}}{\sum_{b=1}^{k} e^{H'_t(b)}} = \frac{e^{H_t(a) + C}}{\sum_{b=1}^{k} e^{H_t(b) + C}}
$$

Podemos fatorar $e^C$ tanto do numerador quanto do denominador:

$$
\pi'_t(a) = \frac{e^C e^{H_t(a)}}{e^C \sum_{b=1}^{k} e^{H_t(b)}} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}} = \pi_t(a)
$$

Esta derivaÃ§Ã£o demonstra que $\pi'_t(a) = \pi_t(a)$, confirmando que adicionar uma constante a todas as preferÃªncias nÃ£o altera as probabilidades de aÃ§Ã£o. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:** Usando as preferÃªncias do exemplo anterior (H_t(1) = 1.0, H_t(2) = 0.5, H_t(3) = 0.0), vamos adicionar uma constante C = 2.0 a todas elas. As novas preferÃªncias sÃ£o H'_t(1) = 3.0, H'_t(2) = 2.5, H'_t(3) = 2.0. Vamos calcular as novas probabilidades:
>
> $\pi'_t(1) = \frac{e^{3.0}}{e^{3.0} + e^{2.5} + e^{2.0}} = \frac{20.086}{20.086 + 12.182 + 7.389} = \frac{20.086}{39.657} \approx 0.506$
>
> $\pi'_t(2) = \frac{e^{2.5}}{e^{3.0} + e^{2.5} + e^{2.0}} = \frac{12.182}{39.657} \approx 0.307$
>
> $\pi'_t(3) = \frac{e^{2.0}}{e^{3.0} + e^{2.5} + e^{2.0}} = \frac{7.389}{39.657} \approx 0.186$
>
> Como esperado, as probabilidades sÃ£o as mesmas de antes (0.506, 0.307, 0.186). Isso demonstra a invariÃ¢ncia Ã  adiÃ§Ã£o de uma constante.
>
> ```python
> import numpy as np
>
> # PreferÃªncias originais
> H = np.array([1.0, 0.5, 0.0])
>
> # Constante a ser adicionada
> C = 2.0
>
> # Novas preferÃªncias
> H_prime = H + C
>
> # Calcula as probabilidades usando softmax
> pi_prime = np.exp(H_prime) / np.sum(np.exp(H_prime))
>
> print(f"Probabilidades das aÃ§Ãµes com a constante adicionada: {pi_prime}")
> ```

**Teorema 1.** *A distribuiÃ§Ã£o softmax maximiza a entropia para um dado vetor de preferÃªncias H, sujeito Ã  restriÃ§Ã£o de que a soma das probabilidades seja igual a 1.*

*DemonstraÃ§Ã£o (EsboÃ§o):* Podemos usar o mÃ©todo dos multiplicadores de Lagrange para maximizar a entropia $S = -\sum_{a=1}^{k} \pi_t(a) \log \pi_t(a)$ sujeito Ã  restriÃ§Ã£o $\sum_{a=1}^{k} \pi_t(a) = 1$. Ao derivar a Lagrangiana em relaÃ§Ã£o a $\pi_t(a)$ e igualar a zero, obtemos uma relaÃ§Ã£o que leva Ã  forma da distribuiÃ§Ã£o softmax, onde as preferÃªncias $H_t(a)$ atuam como os multiplicadores de Lagrange relacionados a cada aÃ§Ã£o. $\blacksquare$

Este teorema conecta a distribuiÃ§Ã£o softmax ao princÃ­pio da mÃ¡xima entropia, justificando seu uso em situaÃ§Ãµes onde queremos modelar incerteza.

**Prova Detalhada do Teorema 1:**
Aqui apresentamos uma prova mais detalhada do Teorema 1 usando o mÃ©todo dos multiplicadores de Lagrange.

I. **Definir a Entropia e a RestriÃ§Ã£o:**
   Queremos maximizar a entropia $S$ dada por:
   $$S = -\sum_{a=1}^{k} \pi_t(a) \log \pi_t(a)$$
   Sujeito Ã  restriÃ§Ã£o:
   $$\sum_{a=1}^{k} \pi_t(a) = 1$$

II. **Formar a Lagrangiana:**
   Introduzimos um multiplicador de Lagrange, $\lambda$, e formamos a funÃ§Ã£o Lagrangiana $L$:
   $$L(\pi_t(1), \ldots, \pi_t(k), \lambda) = -\sum_{a=1}^{k} \pi_t(a) \log \pi_t(a) - \lambda \left(\sum_{a=1}^{k} \pi_t(a) - 1\right)$$

III. **Calcular as Derivadas Parciais:**
   Calculamos as derivadas parciais da Lagrangiana em relaÃ§Ã£o a cada $\pi_t(a)$ e a $\lambda$:
   $$\frac{\partial L}{\partial \pi_t(a)} = -\log \pi_t(a) - 1 - \lambda$$
   $$\frac{\partial L}{\partial \lambda} = -\left(\sum_{a=1}^{k} \pi_t(a) - 1\right)$$

IV. **Igualar as Derivadas a Zero:**
   Para encontrar os pontos crÃ­ticos, igualamos as derivadas parciais a zero:
   $$-\log \pi_t(a) - 1 - \lambda = 0 \quad \text{para todo } a$$
   $$\sum_{a=1}^{k} \pi_t(a) = 1$$

V. **Resolver para $\pi_t(a)$:**
   Da primeira equaÃ§Ã£o, temos:
   $$\log \pi_t(a) = -1 - \lambda$$
   $$\pi_t(a) = e^{-1-\lambda}$$
   Observe que $\pi_t(a)$ Ã© independente de $a$, significando que todas as aÃ§Ãµes tÃªm a mesma probabilidade sob a distribuiÃ§Ã£o de mÃ¡xima entropia sem preferÃªncias adicionais.

VI. **Determinar o Multiplicador de Lagrange:**
   Usamos a restriÃ§Ã£o $\sum_{a=1}^{k} \pi_t(a) = 1$:
   $$\sum_{a=1}^{k} e^{-1-\lambda} = 1$$
   $$k e^{-1-\lambda} = 1$$
   $$e^{-1-\lambda} = \frac{1}{k}$$

VII. **Introduzir as PreferÃªncias:**
   Para introduzir as preferÃªncias $H_t(a)$, modificamos a Lagrangiana para incorporar essas preferÃªncias como restriÃ§Ãµes adicionais.  Isso Ã© mais complexo e geralmente envolve a maximizaÃ§Ã£o de:
      $$L = -\sum_{a=1}^{k} \pi_t(a) \log \pi_t(a) - \lambda \left(\sum_{a=1}^{k} \pi_t(a) - 1\right) + \sum_{a=1}^{k} H_t(a) \pi_t(a)$$
   Onde $H_t(a)$ atuam como multiplicadores de Lagrange associados a cada aÃ§Ã£o.

VIII. **Resolver com as PreferÃªncias:**
   Tomando a derivada com respeito a $\pi_t(a)$ e igualando a zero, obtemos:
      $$\frac{\partial L}{\partial \pi_t(a)} = -\log(\pi_t(a)) - 1 - \lambda + H_t(a) = 0$$
      $$\log(\pi_t(a)) = H_t(a) - 1 - \lambda$$
      $$\pi_t(a) = e^{H_t(a) - 1 - \lambda} = e^{H_t(a)} e^{-1 - \lambda}$$

IX. **NormalizaÃ§Ã£o:**
   Para satisfazer a restriÃ§Ã£o $\sum_{a=1}^{k} \pi_t(a) = 1$, normalizamos as probabilidades:
      $$\sum_{a=1}^{k} \pi_t(a) = \sum_{a=1}^{k} e^{H_t(a)} e^{-1 - \lambda} = 1$$
      $$e^{-1 - \lambda} = \frac{1}{\sum_{b=1}^{k} e^{H_t(b)}}$$

X. **DistribuiÃ§Ã£o Softmax Final:**
    Substituindo de volta na expressÃ£o para $\pi_t(a)$:
      $$\pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b=1}^{k} e^{H_t(b)}}$$

Assim, mostramos que a distribuiÃ§Ã£o softmax surge da maximizaÃ§Ã£o da entropia sujeito Ã  restriÃ§Ã£o de que as probabilidades somem 1 e incorporando as preferÃªncias $H_t(a)$. â– 

Este teorema conecta a distribuiÃ§Ã£o softmax ao princÃ­pio da mÃ¡xima entropia, justificando seu uso em situaÃ§Ãµes onde queremos modelar incerteza.

**ImplicaÃ§Ãµes da InvariÃ¢ncia:**

Esta propriedade de invariÃ¢ncia tem implicaÃ§Ãµes importantes para o algoritmo *gradient bandit*:
*   **Estabilidade:** A estabilidade numÃ©rica Ã© aumentada, pois os valores absolutos das preferÃªncias $H_t(a)$ nÃ£o afetam diretamente as probabilidades. Podemos adicionar grandes constantes a todas as preferÃªncias sem alterar o comportamento do algoritmo [^37].
*   **Flexibilidade:** O algoritmo Ã© inerentemente flexÃ­vel em relaÃ§Ã£o Ã  escala das preferÃªncias. Isso permite que o algoritmo se adapte a diferentes escalas de recompensa sem necessidade de ajustes finos significativos [^38].

Para complementar a discussÃ£o sobre flexibilidade, podemos analisar como a escolha da taxa de aprendizado interage com a escala das recompensas.

> ğŸ’¡ **Exemplo NumÃ©rico:** Imagine que vocÃª estÃ¡ jogando um jogo de k-armed bandit onde as recompensas sÃ£o sempre nÃºmeros pequenos, como 0.01, 0.02, 0.03. Nesse caso, se a sua taxa de aprendizado ($\alpha$) for muito pequena (e.g., 0.001), levarÃ¡ muito tempo para que as preferÃªncias H_t(a) se ajustem e reflitam as recompensas reais. Por outro lado, se as recompensas fossem muito maiores (e.g., 10, 20, 30), uma taxa de aprendizado grande (e.g., 0.1) poderia causar oscilaÃ§Ãµes nas preferÃªncias, pois a cada passo as preferÃªncias seriam atualizadas de forma muito brusca.
>Este cÃ³digo simula um cenÃ¡rio simples de 2-armed bandit, comparando uma taxa de aprendizado pequena com uma grande. O grÃ¡fico gerado mostra como uma taxa de aprendizado maior pode convergir mais rapidamente para a recompensa mÃ©dia, mas tambÃ©m pode apresentar mais variabilidade.

**ObservaÃ§Ã£o:** A taxa de aprendizado ($\alpha$) no algoritmo de gradiente bandit desempenha um papel crucial na adaptaÃ§Ã£o Ã s escalas de recompensa. Se as recompensas sÃ£o consistentemente grandes, uma taxa de aprendizado menor pode ser apropriada para evitar oscilaÃ§Ãµes excessivas nas preferÃªncias. Por outro lado, se as recompensas sÃ£o pequenas, uma taxa de aprendizado maior pode ser necessÃ¡ria para garantir que as preferÃªncias convirjam em um tempo razoÃ¡vel.

### ConclusÃ£o
A distribuiÃ§Ã£o *softmax* desempenha um papel fundamental no algoritmo *gradient bandit*, mapeando as preferÃªncias de aÃ§Ã£o em probabilidades de seleÃ§Ã£o [^37]. A propriedade de invariÃ¢ncia Ã  adiÃ§Ã£o de uma constante demonstra que as probabilidades de aÃ§Ã£o dependem apenas das preferÃªncias relativas entre as aÃ§Ãµes, e nÃ£o de seus valores absolutos [^37]. Esta caracterÃ­stica confere estabilidade e flexibilidade ao algoritmo, tornando-o robusto a diferentes escalas de recompensa e simplificando sua implementaÃ§Ã£o e ajuste.

### ReferÃªncias
[^37]: CapÃ­tulo 2 do texto original.
[^38]: CapÃ­tulo 2, Figura 2.5 do texto original.
<!-- END -->