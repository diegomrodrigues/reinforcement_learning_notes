## O Papel da Baseline na Adapta√ß√£o e Estabilidade do Algoritmo Gradient Bandit

### Introdu√ß√£o
No contexto dos algoritmos Gradient Bandit, o termo *baseline* ($ \bar{R_t} $) desempenha um papel crucial na estabilidade e adaptabilidade do aprendizado [^37]. Este cap√≠tulo aprofunda a import√¢ncia desse componente, explorando como sua inclus√£o permite que o algoritmo se ajuste a mudan√ßas nas distribui√ß√µes de recompensa, enquanto sua omiss√£o pode levar a uma degrada√ß√£o significativa do desempenho. Analisaremos como a baseline atua na redu√ß√£o da vari√¢ncia das atualiza√ß√µes, facilitando uma converg√™ncia mais r√°pida e robusta, sem afetar a expectativa do aprendizado.

### Conceitos Fundamentais

Como vimos anteriormente [^37], o algoritmo Gradient Bandit atualiza as prefer√™ncias de a√ß√£o ($ H_t(a) $) com base na diferen√ßa entre a recompensa recebida ($ R_t $) e a recompensa m√©dia ou *baseline* ($ \bar{R_t} $). Matematicamente, a atualiza√ß√£o √© dada por:

$$
\begin{aligned}
H_{t+1}(A_t) &= H_t(A_t) + \alpha(R_t - \bar{R_t})(1 - \pi_t(A_t)), \\
H_{t+1}(a) &= H_t(a) - \alpha(R_t - \bar{R_t})\pi_t(a), \quad \text{para todo } a \neq A_t,
\end{aligned}
$$

onde $ \alpha > 0 $ √© o par√¢metro de tamanho do passo, e $ \pi_t(a) $ √© a probabilidade de selecionar a a√ß√£o $ a $ no tempo $ t $.

**O Papel da Baseline:** A baseline ($ \bar{R_t} $) serve como um ponto de refer√™ncia em rela√ß√£o ao qual a recompensa recebida √© avaliada [^37]. Em outras palavras, a atualiza√ß√£o das prefer√™ncias n√£o se baseia no valor absoluto da recompensa, mas sim em qu√£o melhor ou pior a recompensa √© em compara√ß√£o com o valor m√©dio esperado. Conforme mencionado em [^37], $ \bar{R_t} $ √© a m√©dia das recompensas at√© o tempo $t$, excluindo o tempo $t$: $ \bar{R_1} = R_1 $.

> üí° **Exemplo Num√©rico:**
> Suponha que temos duas a√ß√µes, A e B, e no tempo t=1, escolhemos a a√ß√£o A e recebemos uma recompensa de R_1 = 10. Inicialmente, H_1(A) = H_1(B) = 0 e œÄ_1(A) = œÄ_1(B) = 0.5. Œ± = 0.1.
>
>  *  Nesse caso, $\bar{R_1} = 0$ (inicializa√ß√£o).
>  *  $H_{2}(A) = H_1(A) + \alpha(R_1 - \bar{R_1})(1 - \pi_1(A)) = 0 + 0.1 * (10 - 0) * (1 - 0.5) = 0.5$
>  *  $H_{2}(B) = H_1(B) - \alpha(R_1 - \bar{R_1})\pi_1(B) = 0 - 0.1 * (10 - 0) * 0.5 = -0.5$
>
> Agora, as prefer√™ncias de a√ß√£o foram atualizadas com base na recompensa recebida e na baseline inicial.

**Adapta√ß√£o a Mudan√ßas nas Recompensas:** Imagine um cen√°rio em que as recompensas m√©dias de todas as a√ß√µes aumentam repentinamente. Sem uma baseline, o algoritmo interpretaria esse aumento como um sinal de que todas as a√ß√µes s√£o melhores do que o esperado, levando a um aumento generalizado nas prefer√™ncias de a√ß√£o. No entanto, com a baseline, o algoritmo compara cada recompensa com a m√©dia atual. Se todas as recompensas aumentarem na mesma propor√ß√£o, a diferen√ßa entre cada recompensa e a baseline permanecer√° constante, evitando um ajuste excessivo nas prefer√™ncias [^37].

> üí° **Exemplo Num√©rico:**
>
> Considere que as recompensas das a√ß√µes A e B eram inicialmente em torno de 5. De repente, ambas as recompensas aumentam para 15.
>
> *   **Sem Baseline:** O algoritmo aumentaria as prefer√™ncias de ambas as a√ß√µes, pois as recompensas agora s√£o muito maiores do que antes.
> *   **Com Baseline:** Se a baseline anterior era pr√≥xima de 5, a diferen√ßa (R_t - baseline) seria aproximadamente 10. Ap√≥s algumas itera√ß√µes, a baseline se ajustaria para perto de 15. Assim, a diferen√ßa diminuiria para perto de 0, impedindo um aumento excessivo nas prefer√™ncias de ambas as a√ß√µes, pois o algoritmo percebe que o aumento √© generalizado e n√£o espec√≠fico de uma a√ß√£o.
>
> Isso demonstra como a baseline permite que o algoritmo se adapte a mudan√ßas gerais no ambiente de recompensa sem reagir excessivamente.

**Redu√ß√£o da Vari√¢ncia:** A inclus√£o da baseline pode ser entendida como uma t√©cnica de redu√ß√£o de vari√¢ncia [^37]. Ao subtrair a baseline da recompensa, estamos centrando os valores em torno de zero, o que reduz a magnitude das atualiza√ß√µes e, consequentemente, a vari√¢ncia do aprendizado. Isto √© particularmente √∫til em ambientes ruidosos, onde as recompensas podem flutuar consideravelmente.

> üí° **Exemplo Num√©rico:**
>
> Suponha que as recompensas da a√ß√£o A tenham uma m√©dia de 10, mas com um desvio padr√£o de 5 (alta vari√¢ncia).
>
> *   **Sem Baseline:** As atualiza√ß√µes nas prefer√™ncias de a√ß√£o ser√£o muito vari√°veis, pois R_t pode variar significativamente (ex: 5 a 15).
> *   **Com Baseline:** Se a baseline estiver pr√≥xima de 10, ent√£o (R_t - Baseline) estar√° mais pr√≥ximo de 0, com uma varia√ß√£o menor (ex: -5 a 5). Isso resulta em atualiza√ß√µes mais suaves e est√°veis nas prefer√™ncias, reduzindo a vari√¢ncia do aprendizado.
>
> A baseline atua como um "amortecedor", reduzindo o impacto de recompensas altamente vari√°veis nas atualiza√ß√µes das prefer√™ncias.

**Converg√™ncia Acelerada:** A redu√ß√£o da vari√¢ncia, proporcionada pela baseline, pode acelerar a converg√™ncia do algoritmo. Com atualiza√ß√µes mais est√°veis, o algoritmo consegue identificar mais rapidamente as a√ß√µes √≥timas e ajustar suas prefer√™ncias de acordo.

**Inalterabilidade da Expectativa:** √â crucial notar que a inclus√£o da baseline n√£o afeta a expectativa do aprendizado [^37]. Em outras palavras, a longo prazo, o algoritmo convergir√° para a mesma solu√ß√£o, independentemente de usar ou n√£o uma baseline. No entanto, com a baseline, a trajet√≥ria de aprendizado √© tipicamente mais suave e eficiente.

Para complementar esta discuss√£o, podemos formalizar a defini√ß√£o de baseline utilizada at√© agora:

**Defini√ß√£o:** A *baseline* $ \bar{R_t} $ no tempo *t* √© a m√©dia das recompensas obtidas at√© o instante *t-1*:

$$
\bar{R_t} = \frac{1}{t-1}\sum_{i=1}^{t-1} R_i, \quad \text{para } t > 1, \quad \bar{R_1} = 0
$$

Observe que inicializamos $ \bar{R_1} = 0 $ para evitar divis√£o por zero. Alternativamente, a baseline pode ser definida de forma recursiva:

$$
\bar{R}_{t+1} = \bar{R_t} + \frac{1}{t}(R_t - \bar{R_t}), \quad \text{com } \bar{R_1} = 0.
$$

> üí° **Exemplo Num√©rico:**
>
> Suponha que as recompensas recebidas ao longo de 5 itera√ß√µes sejam: R = [2, 4, 6, 8, 10].
>
> *   **Defini√ß√£o n√£o-recursiva:**
>     *   $\bar{R_1} = 0$
>     *   $\bar{R_2} = \frac{1}{1} * 2 = 2$
>     *   $\bar{R_3} = \frac{1}{2} * (2 + 4) = 3$
>     *   $\bar{R_4} = \frac{1}{3} * (2 + 4 + 6) = 4$
>     *   $\bar{R_5} = \frac{1}{4} * (2 + 4 + 6 + 8) = 5$
> *   **Defini√ß√£o recursiva:**
>     *   $\bar{R_1} = 0$
>     *   $\bar{R_2} = 0 + \frac{1}{1} * (2 - 0) = 2$
>     *   $\bar{R_3} = 2 + \frac{1}{2} * (4 - 2) = 3$
>     *   $\bar{R_4} = 3 + \frac{1}{3} * (6 - 3) = 4$
>     *   $\bar{R_5} = 4 + \frac{1}{4} * (8 - 4) = 5$
>     *   $\bar{R_6} = 5 + \frac{1}{5} * (10 - 5) = 6$
>
> Ambas as defini√ß√µes fornecem os mesmos resultados para a baseline em cada itera√ß√£o, demonstrando a equival√™ncia.

**Prova da Equival√™ncia entre as Defini√ß√µes de Baseline:**

Provaremos que a defini√ß√£o recursiva da baseline √© equivalente √† defini√ß√£o n√£o-recursiva.

I. **Defini√ß√£o n√£o-recursiva:** $\bar{R_t} = \frac{1}{t-1}\sum_{i=1}^{t-1} R_i$ para $t > 1$, e $\bar{R_1} = 0$.

II. **Defini√ß√£o recursiva:** $\bar{R}_{t+1} = \bar{R_t} + \frac{1}{t}(R_t - \bar{R_t})$ com $\bar{R_1} = 0$.

III. Vamos analisar a defini√ß√£o recursiva para um passo gen√©rico $t+1$:
   $$\bar{R}_{t+1} = \bar{R_t} + \frac{1}{t}(R_t - \bar{R_t})$$

IV. Substituindo $\bar{R_t}$ pela sua defini√ß√£o n√£o-recursiva:
   $$\bar{R}_{t+1} = \frac{1}{t-1}\sum_{i=1}^{t-1} R_i + \frac{1}{t}\left(R_t - \frac{1}{t-1}\sum_{i=1}^{t-1} R_i\right)$$

V. Simplificando a express√£o:
   $$\bar{R}_{t+1} = \frac{1}{t-1}\sum_{i=1}^{t-1} R_i + \frac{1}{t}R_t - \frac{1}{t(t-1)}\sum_{i=1}^{t-1} R_i$$

VI. Combinando os termos com a somat√≥ria:
   $$\bar{R}_{t+1} = \frac{t}{t(t-1)}\sum_{i=1}^{t-1} R_i - \frac{1}{t(t-1)}\sum_{i=1}^{t-1} R_i + \frac{1}{t}R_t$$

VII. Simplificando ainda mais:
   $$\bar{R}_{t+1} = \frac{t-1}{t(t-1)}\sum_{i=1}^{t-1} R_i + \frac{1}{t}R_t$$
   $$\bar{R}_{t+1} = \frac{1}{t}\sum_{i=1}^{t-1} R_i + \frac{1}{t}R_t$$

VIII. Finalmente, combinando as somat√≥rias:
   $$\bar{R}_{t+1} = \frac{1}{t}\sum_{i=1}^{t} R_i$$

IX.  Este resultado corresponde √† defini√ß√£o n√£o-recursiva da baseline para o instante $t+1$. Portanto, as duas defini√ß√µes s√£o equivalentes. ‚ñ†

### Demonstra√ß√£o Matem√°tica

Para formalizar a influ√™ncia da baseline, podemos analisar o gradiente estoc√°stico ascendente, conforme derivado em [^39]. A atualiza√ß√£o das prefer√™ncias de a√ß√£o, em termos de gradiente ascendente, √© dada por [^39]:

$$
H_{t+1}(a) = H_t(a) + \alpha (R_t - \bar{R_t}) (1_{A_t=a} - \pi_t(a)),
$$

onde $1_{A_t=a}$ √© uma fun√ß√£o indicadora que vale 1 se $A_t=a$ e 0 caso contr√°rio.

Conforme demonstrado em [^39], a atualiza√ß√£o acima √© equivalente a uma aproxima√ß√£o estoc√°stica do gradiente da recompensa esperada com respeito √†s prefer√™ncias de a√ß√£o:

$$
\frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \mathbb{E}[(R_t - \bar{R_t}) (1_{A_t=a} - \pi_t(a))].
$$

A chave aqui √© que a escolha da baseline $ \bar{R_t} $ n√£o afeta a expectativa do gradiente, uma vez que:

$$
\mathbb{E}[\bar{R_t} (1_{A_t=a} - \pi_t(a))] = \bar{R_t} (\pi_t(a) - \pi_t(a)) = 0.
$$

**Prova de que a Baseline N√£o Afeta a Expectativa do Gradiente:**

Provaremos que  $ \mathbb{E}[\bar{R_t} (1_{A_t=a} - \pi_t(a))] = 0 $.

I. Sabemos que $ \mathbb{E}[\bar{R_t} (1_{A_t=a} - \pi_t(a))] = \mathbb{E}[\bar{R_t} 1_{A_t=a}] - \mathbb{E}[\bar{R_t} \pi_t(a)] $.

II. Dado que $ \bar{R_t} $ √© determin√≠stico dado o hist√≥rico at√© *t*, podemos retirar $ \bar{R_t} $ da esperan√ßa condicional na a√ß√£o $A_t$:
    $ \mathbb{E}[\bar{R_t} 1_{A_t=a} | \mathcal{H}_{t-1}] = \bar{R_t} \mathbb{E}[1_{A_t=a} | \mathcal{H}_{t-1}] $.

III. Sabemos que $ \mathbb{E}[1_{A_t=a} | \mathcal{H}_{t-1}] = P(A_t = a | \mathcal{H}_{t-1}) = \pi_t(a) $, onde $ \mathcal{H}_{t-1} $ representa o hist√≥rico at√© o tempo $t-1$.

IV. Portanto, $ \mathbb{E}[\bar{R_t} 1_{A_t=a} | \mathcal{H}_{t-1}] = \bar{R_t} \pi_t(a) $.

V. Tomando a esperan√ßa sobre o hist√≥rico, temos:
    $ \mathbb{E}[\bar{R_t} 1_{A_t=a}] = \mathbb{E}[\bar{R_t} \pi_t(a)] $.

VI. Substituindo de volta na equa√ß√£o original:
    $ \mathbb{E}[\bar{R_t} (1_{A_t=a} - \pi_t(a))] = \mathbb{E}[\bar{R_t} \pi_t(a)] - \mathbb{E}[\bar{R_t} \pi_t(a)] = 0 $.

VII. Assim, demonstramos que a escolha da baseline $ \bar{R_t} $ n√£o afeta a expectativa do gradiente. ‚ñ†

No entanto, a vari√¢ncia do gradiente estoc√°stico √© afetada pela escolha da baseline. Uma boa escolha de baseline pode reduzir a vari√¢ncia, levando a uma converg√™ncia mais r√°pida e est√°vel.

Al√©m da m√©dia das recompensas, outras op√ß√µes para a baseline podem ser consideradas. Uma escolha comum √© utilizar uma m√©dia ponderada exponencialmente das recompensas passadas:

**Defini√ß√£o:** A *baseline* $ \bar{R_t} $ no tempo *t* pode ser definida como uma m√©dia ponderada exponencialmente das recompensas passadas:

$$
\bar{R}_{t+1} = (1 - \beta) \bar{R_t} + \beta R_t, \quad \text{com } 0 < \beta \leq 1 \text{ e } \bar{R_1} = 0.
$$

onde $ \beta $ √© a taxa de aprendizado para a baseline. Esta abordagem d√° maior peso √†s recompensas mais recentes, permitindo que a baseline se adapte mais rapidamente a mudan√ßas nas recompensas.

> üí° **Exemplo Num√©rico:**
>
> Sejam $ \beta = 0.1 $ e as recompensas R = [2, 4, 6, 8, 10].
>
> *   $\bar{R_1} = 0$
> *   $\bar{R_2} = (1 - 0.1) * 0 + 0.1 * 2 = 0.2$
> *   $\bar{R_3} = (1 - 0.1) * 0.2 + 0.1 * 4 = 0.58$
> *   $\bar{R_4} = (1 - 0.1) * 0.58 + 0.1 * 6 = 1.122$
> *   $\bar{R_5} = (1 - 0.1) * 1.122 + 0.1 * 8 = 1.8098$
> *   $\bar{R_6} = (1 - 0.1) * 1.8098 + 0.1 * 10 = 2.62882$
>
> Comparando com a m√©dia simples, a m√©dia ponderada exponencialmente d√° mais peso √†s recompensas mais recentes. Se em $ t = 6 $ a recompensa ca√≠sse para 1, a m√©dia ponderada se ajustaria mais rapidamente do que a m√©dia simples.

**Teorema 1:** A utiliza√ß√£o da baseline definida como a m√©dia ponderada exponencialmente das recompensas passadas n√£o afeta a expectativa do gradiente.

*Proof:*
Similarmente ao caso da m√©dia simples, precisamos mostrar que:
$$
\mathbb{E}[\bar{R_t} (1_{A_t=a} - \pi_t(a))] = 0.
$$
Como $ \bar{R_t} $ √© independente da a√ß√£o escolhida no tempo *t* dado o hist√≥rico at√© *t-1*, e dado que $ \mathbb{E}[1_{A_t=a}] = \pi_t(a) $, temos:

$$
\mathbb{E}[\bar{R_t} (1_{A_t=a} - \pi_t(a))] = \mathbb{E}[\bar{R_t}] \mathbb{E}[(1_{A_t=a} - \pi_t(a))] = \mathbb{E}[\bar{R_t}] (\pi_t(a) - \pi_t(a)) = 0.
$$

Portanto, a utiliza√ß√£o da m√©dia ponderada exponencialmente como baseline n√£o introduz vi√©s no gradiente.

**Prova Detalhada do Teorema 1:**

I. Precisamos mostrar que $ \mathbb{E}[\bar{R_t} (1_{A_t=a} - \pi_t(a))] = 0 $.

II. Podemos reescrever a express√£o como $ \mathbb{E}[\bar{R_t} 1_{A_t=a}] - \mathbb{E}[\bar{R_t} \pi_t(a)] $.

III. Condicionando na hist√≥ria at√© o tempo $t-1$, denotada por $\mathcal{H}_{t-1}$, temos:
    $ \mathbb{E}[\bar{R_t} 1_{A_t=a} | \mathcal{H}_{t-1}] = \bar{R_t} \mathbb{E}[1_{A_t=a} | \mathcal{H}_{t-1}] $ e $ \mathbb{E}[\bar{R_t} \pi_t(a) | \mathcal{H}_{t-1}] = \bar{R_t} \pi_t(a) $, pois $\bar{R_t}$ √© determin√≠stico dado $\mathcal{H}_{t-1}$ e $\pi_t(a)$ √© a probabilidade da a√ß√£o $a$ ser escolhida, tamb√©m dado $\mathcal{H}_{t-1}$.

IV. Sabemos que $ \mathbb{E}[1_{A_t=a} | \mathcal{H}_{t-1}] = P(A_t = a | \mathcal{H}_{t-1}) = \pi_t(a) $.

V. Portanto, $ \mathbb{E}[\bar{R_t} 1_{A_t=a} | \mathcal{H}_{t-1}] = \bar{R_t} \pi_t(a) $.

VI. Removendo o condicionamento, temos $ \mathbb{E}[\bar{R_t} 1_{A_t=a}] = \mathbb{E}[\bar{R_t} \pi_t(a)] $.

VII. Substituindo de volta na equa√ß√£o original, $ \mathbb{E}[\bar{R_t} (1_{A_t=a} - \pi_t(a))] = \mathbb{E}[\bar{R_t} \pi_t(a)] - \mathbb{E}[\bar{R_t} \pi_t(a)] = 0 $.

VIII. Conclu√≠mos que a utiliza√ß√£o da m√©dia ponderada exponencialmente como baseline n√£o introduz vi√©s no gradiente. ‚ñ†

### Evid√™ncias Emp√≠ricas

A Figura 2.5 [^38] ilustra claramente o impacto da baseline no desempenho do algoritmo Gradient Bandit. Observa-se que, quando a baseline √© omitida, o algoritmo apresenta um desempenho significativamente inferior, especialmente em compara√ß√£o com o cen√°rio em que a baseline √© inclu√≠da [^38].



![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 bra√ßos.](./../images/image1.png)

### Conclus√£o

A baseline no algoritmo Gradient Bandit √© um componente essencial para garantir a adaptabilidade e estabilidade do aprendizado [^37]. Ao atuar como um ponto de refer√™ncia para avaliar as recompensas, a baseline permite que o algoritmo se ajuste a mudan√ßas nas distribui√ß√µes de recompensa, reduza a vari√¢ncia das atualiza√ß√µes e, consequentemente, acelere a converg√™ncia. A omiss√£o da baseline pode levar a uma degrada√ß√£o significativa do desempenho, conforme demonstrado por evid√™ncias te√≥ricas e emp√≠ricas [^37, 38, 39]. Em resumo, a baseline √© um mecanismo sutil, mas poderoso, que contribui para a robustez e efici√™ncia do algoritmo.

### Refer√™ncias
[^37]: Cap√≠tulo 2, Multi-armed Bandits, se√ß√£o 2.8. Gradient Bandit Algorithms.
[^38]: Cap√≠tulo 2, Multi-armed Bandits, Figura 2.5.
[^39]: Cap√≠tulo 2, Multi-armed Bandits, se√ß√£o 2.8. Gradient Bandit Algorithms, passos da deriva√ß√£o do gradiente estoc√°stico.
<!-- END -->