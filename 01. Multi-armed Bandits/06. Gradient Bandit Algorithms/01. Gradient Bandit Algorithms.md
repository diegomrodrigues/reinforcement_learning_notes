## Gradient Bandit Algorithms: Learning Preferences for Action Selection

### Introdu√ß√£o
No cap√≠tulo anterior, exploramos m√©todos para estimar os valores das a√ß√µes e usar essas estimativas para selecionar a√ß√µes. Nesta se√ß√£o, abordaremos uma abordagem alternativa: aprender uma prefer√™ncia num√©rica para cada a√ß√£o, denotada por $H_t(a) \in \mathbb{R}$ [^37]. Diferentemente dos m√©todos baseados em valores, as prefer√™ncias n√£o t√™m uma interpreta√ß√£o direta em termos de recompensa. Em vez disso, a import√¢ncia reside na prefer√™ncia relativa entre as a√ß√µes, onde prefer√™ncias mais altas levam a uma sele√ß√£o mais frequente da a√ß√£o correspondente [^37].

### Conceitos Fundamentais

O **algoritmo gradient bandit** difere das abordagens anteriores ao aprender uma prefer√™ncia num√©rica, $H_t(a)$, para cada a√ß√£o *a*, em vez de estimar os valores das a√ß√µes [^37]. A magnitude da prefer√™ncia influencia a frequ√™ncia com que uma a√ß√£o √© selecionada, mas n√£o possui uma interpreta√ß√£o direta em termos de recompensa [^37]. A escolha da a√ß√£o √© governada por uma *soft-max distribution*, tamb√©m conhecida como distribui√ß√£o de Gibbs ou Boltzmann, que transforma as prefer√™ncias em probabilidades de a√ß√£o [^37]:

$$
Pr\{A_t=a\} = \frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}} = \pi_t(a),
$$

onde $\pi_t(a)$ denota a probabilidade de selecionar a a√ß√£o *a* no instante *t* [^37]. Inicialmente, todas as prefer√™ncias de a√ß√£o s√£o iguais, resultando em uma probabilidade uniforme de sele√ß√£o para todas as a√ß√µes (por exemplo, $H_1(a) = 0$ para todo *a*) [^37].

> üí° **Exemplo Num√©rico:** Considere um problema de bandit com 3 a√ß√µes (k=3). Inicialmente, $H_1(1) = H_1(2) = H_1(3) = 0$.  Ent√£o, $\pi_1(1) = \frac{e^0}{e^0 + e^0 + e^0} = \frac{1}{3}$. Similarmente, $\pi_1(2) = \pi_1(3) = \frac{1}{3}$. Isso significa que cada a√ß√£o tem uma probabilidade igual de ser selecionada no in√≠cio.

O aprendizado das prefer√™ncias de a√ß√£o √© realizado por meio de um algoritmo de *stochastic gradient ascent* [^37]. Ap√≥s selecionar a a√ß√£o $A_t$ e receber a recompensa $R_t$, as prefer√™ncias de a√ß√£o s√£o atualizadas da seguinte forma [^37]:

$$
\begin{aligned}
H_{t+1}(A_t) &= H_t(A_t) + \alpha(R_t - \bar{R}_t)(1 - \pi_t(A_t)), \\
H_{t+1}(a) &= H_t(a) - \alpha(R_t - \bar{R}_t)\pi_t(a), \quad \text{para todo } a \neq A_t,
\end{aligned}
$$

onde $\alpha > 0$ √© um par√¢metro de *step-size* e $\bar{R}_t \in \mathbb{R}$ √© a m√©dia das recompensas at√© o instante *t* (excluindo *t*) e serve como uma *baseline* [^37]. A *baseline* √© crucial para o desempenho do algoritmo. Ela fornece um ponto de refer√™ncia para comparar a recompensa recebida. Se a recompensa for maior que a *baseline*, a probabilidade de selecionar $A_t$ no futuro aumenta; caso contr√°rio, diminui [^37]. As a√ß√µes n√£o selecionadas movem-se na dire√ß√£o oposta [^37]. A m√©dia das recompensas, $\bar{R}_t$, pode ser calculada incrementalmente como descrito na Se√ß√£o 2.4 ou 2.5 [^37].

> üí° **Exemplo Num√©rico:** Suponha que $\alpha = 0.1$, no instante *t* a a√ß√£o $A_t = 2$ √© selecionada, a recompensa $R_t = 1$, e a m√©dia das recompensas at√© o instante *t* √© $\bar{R}_t = 0.5$.  A probabilidade de selecionar a a√ß√£o 2 no instante *t* √© $\pi_t(2) = 0.4$.  Ent√£o, a atualiza√ß√£o das prefer√™ncias ser√°:
>
> $H_{t+1}(2) = H_t(2) + 0.1(1 - 0.5)(1 - 0.4) = H_t(2) + 0.03$
>
> Para as outras a√ß√µes, $a \neq 2$:
>
> $H_{t+1}(a) = H_t(a) - 0.1(1 - 0.5)(0.4) = H_t(a) - 0.02$
>
> Se $H_t(2)$ era inicialmente 1, ent√£o $H_{t+1}(2) = 1.03$. Se $H_t(1)$ e $H_t(3)$ eram inicialmente 0, ent√£o $H_{t+1}(1) = H_{t+1}(3) = -0.02$.  As prefer√™ncias da a√ß√£o selecionada (a√ß√£o 2) aumentaram, enquanto as outras diminu√≠ram.

**A import√¢ncia da *baseline***: A inclus√£o da *baseline* $\bar{R}_t$ √© crucial para o bom funcionamento do algoritmo [^37]. Sem ela, o desempenho pode ser significativamente degradado. A *baseline* serve para reduzir a vari√¢ncia das atualiza√ß√µes e estabilizar o aprendizado [^37].

> üí° **Exemplo Num√©rico:** Vamos simular o efeito da baseline em um cen√°rio simples. Considere duas a√ß√µes, A e B. A recompensa esperada para A √© 1, e para B √© 0. Sem baseline, se recebermos uma recompensa de 1 para A, simplesmente aumentamos a prefer√™ncia por A. No entanto, se *todas* as recompensas fossem sistematicamente altas (digamos, adicionar 10 a cada recompensa), sem a baseline, o algoritmo n√£o conseguiria distinguir que A √© *relativamente* melhor que B, pois ambas teriam suas prefer√™ncias aumentadas similarmente. Com a baseline, que rastreia a m√©dia das recompensas, o algoritmo consegue isolar a recompensa *relativa* (acima ou abaixo da m√©dia) e ajustar as prefer√™ncias de forma mais apropriada.

**Observa√ß√£o:** Uma alternativa para calcular a m√©dia das recompensas $\bar{R}_t$ incrementalmente, como mencionado na Se√ß√£o 2.4 ou 2.5, √© utilizar uma m√©dia ponderada exponencialmente (Exponential Weighted Average). Isso permite dar mais peso √†s recompensas mais recentes, o que pode ser vantajoso em ambientes n√£o-estacion√°rios.

**Teorema 1** (Exponential Weighted Average Baseline). *Seja $\bar{R}_{t+1} = (1-\beta)\bar{R}_t + \beta R_t$, com $0 < \beta \leq 1$ sendo o fator de decaimento, ent√£o utilizar $\bar{R}_{t+1}$ como baseline no algoritmo gradient bandit garante converg√™ncia para a a√ß√£o √≥tima sob certas condi√ß√µes de regularidade do ambiente.*

*Proof Sketch.* A prova envolve mostrar que, sob condi√ß√µes de regularidade (por exemplo, recompensas limitadas e uma taxa de aprendizado $\alpha$ suficientemente pequena), o uso da m√©dia ponderada exponencialmente como linha de base ainda permite que o algoritmo aprenda as prefer√™ncias de a√ß√£o corretas. A escolha de $\beta$ afeta a rapidez com que o algoritmo se adapta a mudan√ßas no ambiente. Um $\beta$ maior d√° mais peso √†s recompensas recentes, permitindo uma adapta√ß√£o mais r√°pida, enquanto um $\beta$ menor torna a m√©dia mais est√°vel.

> üí° **Exemplo Num√©rico:** Considere um cen√°rio n√£o-estacion√°rio onde a recompensa esperada da a√ß√£o A muda ao longo do tempo. Inicialmente, A oferece uma recompensa m√©dia de 1, mas ap√≥s 100 itera√ß√µes, a recompensa m√©dia de A muda para 0. Usando uma m√©dia ponderada exponencialmente com $\beta = 0.1$ permite que a baseline se adapte mais rapidamente a essa mudan√ßa em compara√ß√£o com uma m√©dia simples.  Se a recompensa m√©dia inicial $\bar{R}_0 = 0.5$, ap√≥s a mudan√ßa, $\bar{R}_{101} = (1-0.1)\bar{R}_{100} + 0.1 * R_{100}$.  Se $R_{100} = 0$, ent√£o $\bar{R}_{101} = 0.9\bar{R}_{100} + 0$.  A m√©dia ponderada exponencialmente converge gradualmente para a nova m√©dia.

Al√©m disso, podemos analisar a sensibilidade do algoritmo em rela√ß√£o aos par√¢metros $\alpha$ e $\beta$.

**Lema 1** (Sensibilidade aos par√¢metros). *A escolha dos par√¢metros $\alpha$ e $\beta$ afeta a velocidade de converg√™ncia e a estabilidade do algoritmo gradient bandit. Um valor muito alto de $\alpha$ pode levar a oscila√ß√µes e instabilidade, enquanto um valor muito baixo pode tornar o aprendizado lento. Similarmente, um $\beta$ muito alto torna o algoritmo sens√≠vel a flutua√ß√µes de curto prazo, enquanto um $\beta$ muito baixo pode impedir que o algoritmo se adapte a mudan√ßas no ambiente.*

A demonstra√ß√£o deste lema requer uma an√°lise da vari√¢ncia e do bias introduzidos pelos diferentes valores de $\alpha$ e $\beta$.

> üí° **Exemplo Num√©rico:** Para ilustrar a sensibilidade a $\alpha$, imagine que voc√™ est√° ajustando os pesos de uma rede neural (analogamente, as prefer√™ncias das a√ß√µes). Se $\alpha$ for muito alto (e.g., 0.9), cada atualiza√ß√£o de peso ser√° muito grande, e voc√™ pode sobrepassar o m√≠nimo √≥timo, levando a oscila√ß√µes. Se $\alpha$ for muito baixo (e.g., 0.001), cada atualiza√ß√£o ser√° min√∫scula, e o aprendizado ser√° excessivamente lento. De maneira similar, um $\beta$ alto (e.g., 0.5 com m√©dia ponderada exponencialmente) far√° com que a baseline se adapte rapidamente a cada nova recompensa, mas pode torn√°-la muito ruidosa. Um $\beta$ baixo (e.g., 0.01) suavizar√° muito a baseline, mas pode impedir que o algoritmo reaja a mudan√ßas r√°pidas no ambiente.  Uma an√°lise da vari√¢ncia mostraria que um $\alpha$ alto aumenta a vari√¢ncia nas estimativas, enquanto um $\beta$ alto aumenta a vari√¢ncia da baseline.

**Rela√ß√£o com Stochastic Gradient Ascent**: O algoritmo gradient bandit pode ser interpretado como uma aproxima√ß√£o *stochastic* ao *gradient ascent* [^38]. No *gradient ascent* exato, cada prefer√™ncia de a√ß√£o $H_t(a)$ seria incrementada proporcionalmente ao efeito do incremento no desempenho [^38]:

$$
H_{t+1}(a) \doteq H_t(a) + \alpha \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)},
$$

onde $\mathbb{E}[R_t]$ √© a recompensa esperada [^38].

Embora n√£o seja poss√≠vel implementar o *gradient ascent* exato porque desconhecemos $q_*(x)$, as atualiza√ß√µes do algoritmo gradient bandit s√£o iguais ao *gradient ascent* em valor esperado, tornando o algoritmo uma inst√¢ncia de *stochastic gradient ascent* [^38].

**Teorema 1.1** (Converg√™ncia do Stochastic Gradient Ascent). *Sob condi√ß√µes apropriadas (e.g., a fun√ß√£o objetivo √© suave e convexa, e o tamanho do passo $\alpha$ satisfaz certas condi√ß√µes de decaimento), o algoritmo stochastic gradient ascent converge para um √≥timo local.*

Este teorema, um resultado cl√°ssico em otimiza√ß√£o, fornece uma garantia te√≥rica para a converg√™ncia do algoritmo gradient bandit, desde que as condi√ß√µes necess√°rias sejam satisfeitas. Para ilustrar essa rela√ß√£o, podemos fornecer uma prova de que a atualiza√ß√£o do gradiente estoc√°stico √© uma estimativa n√£o viesada do verdadeiro gradiente.

**Prova da n√£o-tendenciosidade da atualiza√ß√£o do gradiente:**

O objetivo √© mostrar que a atualiza√ß√£o da prefer√™ncia no algoritmo gradient bandit √© uma estimativa n√£o-tendenciosa do gradiente do desempenho esperado em rela√ß√£o √†s prefer√™ncias. Formalmente, queremos mostrar que:

$$
\mathbb{E}[H_{t+1}(a) - H_t(a)] = \alpha \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)}
$$

I. **Defini√ß√£o do Desempenho Esperado:**
   O desempenho esperado no instante *t* √© dado por:
$$
   \mathbb{E}[R_t] = \sum_{b=1}^{k} \pi_t(b) q_*(b),
$$
   onde $\pi_t(b)$ √© a probabilidade de selecionar a a√ß√£o *b* no instante *t*, e $q_*(b)$ √© o valor verdadeiro da a√ß√£o *b*.

II. **C√°lculo da Derivada:**
    Tomamos a derivada do desempenho esperado em rela√ß√£o √† prefer√™ncia da a√ß√£o *a*:
    $$
    \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)} \sum_{b=1}^{k} \pi_t(b) q_*(b) = \sum_{b=1}^{k} q_*(b) \frac{\partial \pi_t(b)}{\partial H_t(a)}.
    $$

III. **Derivada da Softmax:**
     A derivada da fun√ß√£o softmax √© dada por:
     $$
     \frac{\partial \pi_t(b)}{\partial H_t(a)} = \frac{\partial}{\partial H_t(a)} \frac{e^{H_t(b)}}{\sum_{c=1}^{k} e^{H_t(c)}} = \pi_t(b) (\delta_{ab} - \pi_t(a)),
     $$
     onde $\delta_{ab}$ √© o delta de Kronecker (1 se a=b, 0 caso contr√°rio).

IV. **Substitui√ß√£o e Simplifica√ß√£o:**
    Substitu√≠mos a derivada da softmax na express√£o para a derivada do desempenho esperado:
    $$
    \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \sum_{b=1}^{k} q_*(b) \pi_t(b) (\delta_{ab} - \pi_t(a)) = q_*(a) \pi_t(a) - \pi_t(a) \sum_{b=1}^{k} \pi_t(b) q_*(b).
    $$
    Como $\mathbb{E}[R_t] = \sum_{b=1}^{k} \pi_t(b) q_*(b)$, temos:
    $$
    \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)} = \pi_t(a) (q_*(a) - \mathbb{E}[R_t]).
    $$

V. **Valor Esperado da Atualiza√ß√£o:**
   Calculamos o valor esperado da atualiza√ß√£o da prefer√™ncia:
$$
   \mathbb{E}[H_{t+1}(a) - H_t(a)] = \mathbb{E}[\alpha (R_t - \bar{R}_t) (\delta_{A_t, a} - \pi_t(a))].
$$
   Separamos a esperan√ßa:
$$
    \mathbb{E}[H_{t+1}(a) - H_t(a)] = \alpha \mathbb{E}[(R_t - \bar{R}_t) (\delta_{A_t, a} - \pi_t(a))] = \alpha \mathbb{E}[R_t - \bar{R}_t]\mathbb{E}[\delta_{A_t, a} - \pi_t(a)].
$$
   Usando a defini√ß√£o da probabilidade de selecionar a a√ß√£o *a*:
$$
   \mathbb{E}[R_t | A_t = a] = q_*(a).
$$
   Ent√£o,
$$
   \mathbb{E}[H_{t+1}(a) - H_t(a)] = \alpha (q_*(a) - \bar{R}_t) (\pi_t(a) - \pi_t(a)) = \alpha \pi_t(a) (q_*(a) - \bar{R}_t).
$$

VI. **Conclus√£o:**
   Se a *baseline* $\bar{R}_t$ √© uma estimativa de $\mathbb{E}[R_t]$, ent√£o:
$$
   \mathbb{E}[H_{t+1}(a) - H_t(a)] \approx \alpha \pi_t(a) (q_*(a) - \mathbb{E}[R_t]) = \alpha \frac{\partial \mathbb{E}[R_t]}{\partial H_t(a)}.
$$
   Portanto, a atualiza√ß√£o do gradiente estoc√°stico √© uma estimativa n√£o viesada do verdadeiro gradiente. ‚ñ†

### Conclus√£o

O algoritmo gradient bandit oferece uma abordagem alternativa para o problema do multi-armed bandit, aprendendo prefer√™ncias de a√ß√£o em vez de estimar valores. A utiliza√ß√£o de uma *soft-max distribution* para sele√ß√£o de a√ß√µes e a inclus√£o de uma *baseline* nas atualiza√ß√µes s√£o elementos-chave para o seu bom funcionamento. A interpreta√ß√£o como *stochastic gradient ascent* fornece uma base te√≥rica s√≥lida para suas propriedades de converg√™ncia.

![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 bra√ßos.](./../images/image1.png)

### Refer√™ncias
[^37]: Sutton, Richard S.; Barto, Andrew G.. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: The MIT Press, 2018.
[^38]: Sutton, Richard S.; Barto, Andrew G.. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: The MIT Press, 2018.
<!-- END -->