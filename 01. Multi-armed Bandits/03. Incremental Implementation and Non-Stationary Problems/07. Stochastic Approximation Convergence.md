## Stochastic Approximation and Convergence in Non-Stationary Bandit Problems

### Introdu√ß√£o
No contexto de **multi-armed bandits** [^1], um dos desafios cruciais reside na adapta√ß√£o a ambientes n√£o-estacion√°rios. Como visto anteriormente [^8], m√©todos de m√©dia amostral (sample-average) s√£o adequados para problemas estacion√°rios, nos quais as probabilidades de recompensa das a√ß√µes n√£o mudam ao longo do tempo. No entanto, em cen√°rios n√£o-estacion√°rios, √© fundamental ponderar mais as recompensas recentes para se adaptar √†s mudan√ßas nas distribui√ß√µes de recompensa. Uma abordagem comum para lidar com essa n√£o-estacionariedade √© o uso de um **step-size** constante. Contudo, a escolha do **step-size** e suas implica√ß√µes na converg√™ncia do algoritmo s√£o aspectos que merecem uma an√°lise mais aprofundada. Este cap√≠tulo explora as condi√ß√µes te√≥ricas para garantir a converg√™ncia dos algoritmos de **aprendizado por refor√ßo** no contexto de **aproxima√ß√£o estoc√°stica** e discute as nuances relacionadas ao uso de **step-sizes** constantes em ambientes n√£o-estacion√°rios.

### Condi√ß√µes de Aproxima√ß√£o Estoc√°stica para Converg√™ncia
A atualiza√ß√£o incremental do valor de uma a√ß√£o, $Q_{n+1}$, utilizando um **step-size** constante $\alpha$, √© dada por [^8]:

$$Q_{n+1} = Q_n + \alpha[R_n - Q_n] \quad \text{onde} \quad \alpha \in (0,1]$$

> üí° **Exemplo Num√©rico:**
>
> Suponha que tenhamos uma a√ß√£o com um valor inicial estimado $Q_n = 10$. Recebemos uma recompensa $R_n = 12$ ao selecionar esta a√ß√£o. Se usarmos um **step-size** constante $\alpha = 0.1$, a atualiza√ß√£o do valor da a√ß√£o seria:
>
> $Q_{n+1} = 10 + 0.1[12 - 10] = 10 + 0.1[2] = 10 + 0.2 = 10.2$
>
> Este exemplo mostra como o valor da a√ß√£o √© atualizado em dire√ß√£o √† recompensa recebida, com o **step-size** controlando a magnitude da atualiza√ß√£o.

Embora essa abordagem seja eficaz para rastrear mudan√ßas em ambientes n√£o-estacion√°rios, ela contrasta com o m√©todo de m√©dia amostral, $a_n(a) = \frac{1}{n}$ [^9], que garante a converg√™ncia para os verdadeiros valores das a√ß√µes em ambientes estacion√°rios. A converg√™ncia, neste contexto, refere-se √† capacidade do algoritmo de **aprendizado** de se aproximar, no limite, dos valores √≥timos das a√ß√µes, ou seja, $Q_t(a) \rightarrow q_*(a)$ quando $t \rightarrow \infty$.

As condi√ß√µes formais para garantir a converg√™ncia com probabilidade 1 em algoritmos de **aproxima√ß√£o estoc√°stica** s√£o expressas pelas seguintes rela√ß√µes [^9]:

$$\sum_{n=1}^{\infty} \alpha_n(a) = \infty \quad \text{e} \quad \sum_{n=1}^{\infty} \alpha_n(a)^2 < \infty$$

Essas condi√ß√µes estabelecem um compromisso entre a necessidade de dar passos suficientemente grandes para superar condi√ß√µes iniciais e flutua√ß√µes aleat√≥rias (primeira condi√ß√£o) e a necessidade de diminuir os passos ao longo do tempo para garantir que o **aprendizado** se estabilize e convirja para uma solu√ß√£o (segunda condi√ß√£o). A primeira condi√ß√£o implica que a soma dos **step-sizes** deve divergir, garantindo que o algoritmo continue a se mover em dire√ß√£o √† solu√ß√£o √≥tima. A segunda condi√ß√£o, por outro lado, exige que a soma dos quadrados dos **step-sizes** convirja, assegurando que os passos se tornem cada vez menores, evitando oscila√ß√µes excessivas em torno da solu√ß√£o.

A an√°lise dessas condi√ß√µes revela que, embora o m√©todo de m√©dia amostral ($a_n(a) = \frac{1}{n}$) satisfa√ßa ambas as condi√ß√µes [^9], o mesmo n√£o ocorre com o **step-size** constante ($a_n(a) = \alpha$) [^9]. De fato, para o **step-size** constante, $\sum_{n=1}^{\infty} \alpha = \infty$ √© satisfeita, mas $\sum_{n=1}^{\infty} \alpha^2 < \infty$ n√£o √©, indicando que as estimativas nunca convergem completamente, continuando a variar em resposta √†s recompensas mais recentemente recebidas [^9].

**Prova:** Para o m√©todo da m√©dia amostral, $\alpha_n(a) = \frac{1}{n}$:
I.  Verificar a primeira condi√ß√£o: $\sum_{n=1}^{\infty} \frac{1}{n}$. Esta √© a s√©rie harm√¥nica, que √© conhecida por divergir.

II. Verificar a segunda condi√ß√£o: $\sum_{n=1}^{\infty} \left(\frac{1}{n}\right)^2 = \sum_{n=1}^{\infty} \frac{1}{n^2}$. Esta √© a s√©rie p com $p = 2 > 1$, que converge.

III. Portanto, o m√©todo da m√©dia amostral satisfaz ambas as condi√ß√µes.

Para o **step-size** constante, $\alpha_n(a) = \alpha$:
I.  Verificar a primeira condi√ß√£o: $\sum_{n=1}^{\infty} \alpha = \alpha + \alpha + \alpha + \dots$. Como $\alpha > 0$, essa s√©rie diverge.

II. Verificar a segunda condi√ß√£o: $\sum_{n=1}^{\infty} \alpha^2 = \alpha^2 + \alpha^2 + \alpha^2 + \dots$. Como $\alpha > 0$, essa s√©rie tamb√©m diverge.

III. Portanto, o **step-size** constante satisfaz a primeira condi√ß√£o, mas n√£o a segunda. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos ilustrar as condi√ß√µes de converg√™ncia com valores num√©ricos. Considere $\alpha = 0.1$.
>
> Para a primeira condi√ß√£o, $\sum_{n=1}^{\infty} \alpha = \sum_{n=1}^{\infty} 0.1 = 0.1 + 0.1 + 0.1 + \dots$, que claramente diverge.
>
> Para a segunda condi√ß√£o, $\sum_{n=1}^{\infty} \alpha^2 = \sum_{n=1}^{\infty} (0.1)^2 = \sum_{n=1}^{\infty} 0.01 = 0.01 + 0.01 + 0.01 + \dots$, que tamb√©m diverge.
>
> Agora, compare com $\alpha_n = \frac{1}{n}$.
>
> Para a primeira condi√ß√£o, $\sum_{n=1}^{\infty} \frac{1}{n} = 1 + \frac{1}{2} + \frac{1}{3} + \dots$, que diverge (s√©rie harm√¥nica).
>
> Para a segunda condi√ß√£o, $\sum_{n=1}^{\infty} \left(\frac{1}{n}\right)^2 = \sum_{n=1}^{\infty} \frac{1}{n^2} = 1 + \frac{1}{4} + \frac{1}{9} + \dots$, que converge para $\frac{\pi^2}{6} \approx 1.645$.
>
> Este exemplo num√©rico ilustra como o m√©todo da m√©dia amostral satisfaz as condi√ß√µes de converg√™ncia, enquanto o **step-size** constante n√£o.

Para complementar a an√°lise das condi√ß√µes de converg√™ncia, √© √∫til considerar o conceito de **cadeias de Markov**.

**Teorema 1** *Converg√™ncia em Cadeias de Markov*. Se o processo de atualiza√ß√£o $Q_{n+1} = Q_n + \alpha_n(a)[R_n - Q_n]$ pode ser modelado como uma cadeia de Markov, e se as condi√ß√µes de **aproxima√ß√£o estoc√°stica** s√£o satisfeitas, ent√£o a cadeia de Markov converge para uma distribui√ß√£o estacion√°ria.

*Prova (Esbo√ßo)*: A prova envolve mostrar que, sob as condi√ß√µes dadas, a cadeia de Markov √© erg√≥dica e possui um √∫nico estado estacion√°rio. A condi√ß√£o $\sum_{n=1}^{\infty} \alpha_n(a) = \infty$ garante que todos os estados s√£o acess√≠veis, enquanto $\sum_{n=1}^{\infty} \alpha_n(a)^2 < \infty$ garante que a cadeia n√£o oscile indefinidamente e, portanto, convirja para um estado estacion√°rio.

### Implica√ß√µes da N√£o-Satisfa√ß√£o das Condi√ß√µes de Converg√™ncia
A n√£o satisfa√ß√£o da segunda condi√ß√£o de converg√™ncia pelo **step-size** constante n√£o √© necessariamente uma desvantagem em ambientes n√£o-estacion√°rios [^9]. A capacidade de se adaptar continuamente √†s mudan√ßas nas distribui√ß√µes de recompensa √© crucial para manter um bom desempenho ao longo do tempo. Em vez de convergir para um valor fixo, o valor estimado da a√ß√£o ($Q_t(a)$) rastreia a m√©dia das recompensas mais recentes, ponderando-as exponencialmente [^9].

Como visto anteriormente, a m√©dia ponderada resultante do uso de um **step-size** constante √© dada por [^8]:

$$Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i$$

Essa equa√ß√£o demonstra que as recompensas mais recentes t√™m um peso maior na determina√ß√£o do valor estimado da a√ß√£o, permitindo que o algoritmo se adapte rapidamente √†s mudan√ßas no ambiente.

> üí° **Exemplo Num√©rico:**
>
> Considere um cen√°rio onde o valor real da a√ß√£o muda ao longo do tempo. Inicialmente, a recompensa esperada √© $q_*(a) = 5$, mas ap√≥s 100 passos, muda para $q_*(a) = 15$. Usaremos $\alpha = 0.1$ e compararemos com $\alpha_n = \frac{1}{n}$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Configura√ß√£o
> alpha_constante = 0.1
> Q_inicial = 0
> num_passos = 200
>
> # Recompensas
> recompensas = np.zeros(num_passos)
> for i in range(num_passos):
>     if i < 100:
>         recompensas[i] = np.random.normal(5, 1) # M√©dia 5, desvio padr√£o 1
>     else:
>         recompensas[i] = np.random.normal(15, 1) # M√©dia 15, desvio padr√£o 1
>
> # Step-size constante
> Q_constante = np.zeros(num_passos)
> Q_constante[0] = Q_inicial
> for i in range(1, num_passos):
>     Q_constante[i] = Q_constante[i-1] + alpha_constante * (recompensas[i-1] - Q_constante[i-1])
>
> # Step-size decrescente
> Q_decrescente = np.zeros(num_passos)
> Q_decrescente[0] = Q_inicial
> for i in range(1, num_passos):
>     alpha_decrescente = 1 / i
>     Q_decrescente[i] = Q_decrescente[i-1] + alpha_decrescente * (recompensas[i-1] - Q_decrescente[i-1])
>
> # Plotagem
> plt.figure(figsize=(10, 6))
> plt.plot(Q_constante, label=f'Step-size constante (alpha={alpha_constante})')
> plt.plot(Q_decrescente, label='Step-size decrescente (1/n)')
> plt.axvline(x=100, color='red', linestyle='--', label='Mudan√ßa na recompensa esperada')
> plt.xlabel('Passo')
> plt.ylabel('Valor estimado da a√ß√£o (Q)')
> plt.title('Compara√ß√£o de Step-sizes em Ambiente N√£o-Estacion√°rio')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> Este c√≥digo simula um ambiente n√£o-estacion√°rio onde a recompensa esperada muda no passo 100. O gr√°fico mostra como o **step-size** constante se adapta mais rapidamente √† mudan√ßa, enquanto o **step-size** decrescente converge mais lentamente, mas potencialmente com menos vari√¢ncia ap√≥s a mudan√ßa.

Uma an√°lise mais detalhada da taxa de adapta√ß√£o pode ser feita considerando a seguinte proposi√ß√£o:

**Proposi√ß√£o 1** *Taxa de Adapta√ß√£o Exponencial*. O uso de um **step-size** constante $\alpha$ implica uma taxa de adapta√ß√£o exponencial, onde o peso das recompensas passadas decai exponencialmente com uma taxa de $(1 - \alpha)$.

*Prova*: Como demonstrado na equa√ß√£o $Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i$, o peso de uma recompensa $R_i$ decresce em $(1 - \alpha)^{n-i}$ com o passar do tempo. Portanto, a taxa de adapta√ß√£o √© exponencial, controlada por $\alpha$.

Para formalizar essa prova, podemos seguir os seguintes passos:

I.  Considere a equa√ß√£o de atualiza√ß√£o: $Q_{n+1} = Q_n + \alpha(R_n - Q_n)$.

II. Expanda recursivamente $Q_{n+1}$ em termos de $Q_1$ e $R_i$:

   $Q_2 = Q_1 + \alpha(R_1 - Q_1) = (1 - \alpha)Q_1 + \alpha R_1$

   $Q_3 = Q_2 + \alpha(R_2 - Q_2) = (1 - \alpha)Q_2 + \alpha R_2 = (1 - \alpha)((1 - \alpha)Q_1 + \alpha R_1) + \alpha R_2 = (1 - \alpha)^2 Q_1 + \alpha(1 - \alpha)R_1 + \alpha R_2$

   Continuando recursivamente, obtemos:

   $Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i$

III. Observe que o peso da recompensa $R_i$ √© $\alpha(1 - \alpha)^{n-i}$.

IV. Analise o termo $(1 - \alpha)^{n-i}$, que representa o fator de decaimento exponencial. √Ä medida que $n - i$ aumenta (ou seja, quanto mais antiga a recompensa), o peso da recompensa $R_i$ diminui exponencialmente.

V. A taxa de decaimento √© determinada por $(1 - \alpha)$. Portanto, o uso de um **step-size** constante $\alpha$ implica uma taxa de adapta√ß√£o exponencial, onde o peso das recompensas passadas decai exponencialmente com uma taxa de $(1 - \alpha)$. ‚ñ†

Al√©m disso, podemos relacionar o **step-size** constante com o conceito de **horizonte de tempo efetivo**.

**Teorema 1.1** *Horizonte de Tempo Efetivo*. Para um **step-size** constante $\alpha$, o horizonte de tempo efetivo, que representa o n√∫mero de recompensas passadas que influenciam significativamente a estimativa atual, √© proporcional a $\frac{1}{\alpha}$.

*Prova*: O horizonte de tempo efetivo pode ser definido como o tempo necess√°rio para que o peso de uma recompensa passada diminua para uma fra√ß√£o significativa de seu valor inicial. Usando a taxa de decaimento exponencial $(1 - \alpha)$, podemos aproximar o horizonte de tempo efetivo como $H \approx \frac{1}{\alpha}$. Isto porque, ap√≥s $\frac{1}{\alpha}$ passos, o peso de uma recompensa passada √© aproximadamente $e^{-1}$, indicando que seu impacto na estimativa atual √© significativamente reduzido.

Para formalizar essa prova, podemos seguir os seguintes passos:
I. Defina o horizonte de tempo efetivo ($H$) como o tempo necess√°rio para que o peso de uma recompensa passada diminua para uma fra√ß√£o $f$ de seu valor inicial, onde $f$ √© tipicamente uma pequena fra√ß√£o (e.g., $f = e^{-1} \approx 0.368$).

II. Considere o peso de uma recompensa $R_i$ ap√≥s $H$ passos: $\alpha(1 - \alpha)^{H}$.

III. Queremos encontrar $H$ tal que $(1 - \alpha)^{H}$ seja aproximadamente igual a $f$.  Assim, $(1 - \alpha)^H \approx f$.

IV. Tomando o logaritmo natural de ambos os lados: $H \ln(1 - \alpha) \approx \ln(f)$.

V. Se $\alpha$ √© pequeno (o que √© comum em muitas aplica√ß√µes de aprendizado por refor√ßo), podemos usar a aproxima√ß√£o $\ln(1 - \alpha) \approx -\alpha$.

VI. Substituindo na equa√ß√£o, temos: $H(-\alpha) \approx \ln(f)$.

VII. Resolvendo para $H$, obtemos: $H \approx -\frac{\ln(f)}{\alpha}$.

VIII. Se escolhermos $f = e^{-1}$, ent√£o $\ln(f) = -1$, e $H \approx \frac{1}{\alpha}$.

IX. Portanto, para um **step-size** constante $\alpha$, o horizonte de tempo efetivo √© aproximadamente proporcional a $\frac{1}{\alpha}$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Se $\alpha = 0.1$, o horizonte de tempo efetivo √© aproximadamente $\frac{1}{0.1} = 10$. Isso significa que as √∫ltimas 10 recompensas ter√£o um impacto significativo na estimativa atual do valor da a√ß√£o. Se $\alpha = 0.01$, o horizonte de tempo efetivo √© aproximadamente $\frac{1}{0.01} = 100$, o que significa que as √∫ltimas 100 recompensas influenciar√£o a estimativa. Isso demonstra como um $\alpha$ menor d√° mais peso √†s recompensas passadas, tornando o algoritmo mais est√°vel, mas menos adapt√°vel a mudan√ßas r√°pidas.
>
> Em contraste, um $\alpha$ maior como $\alpha = 0.5$ resulta em um horizonte de tempo efetivo de $\frac{1}{0.5} = 2$. Assim, apenas as duas recompensas mais recentes t√™m um impacto significativo. Isso torna o algoritmo muito responsivo a mudan√ßas, mas tamb√©m mais suscet√≠vel a flutua√ß√µes aleat√≥rias.

### Solu√ß√µes Alternativas e Considera√ß√µes Pr√°ticas
Apesar das vantagens do **step-size** constante em ambientes n√£o-estacion√°rios, existem abordagens que buscam combinar os benef√≠cios da converg√™ncia em ambientes estacion√°rios com a adaptabilidade em ambientes n√£o-estacion√°rios. Uma dessas abordagens envolve o uso de **step-sizes** que diminuem ao longo do tempo, mas de forma mais lenta do que $\frac{1}{n}$. No entanto, essas sequ√™ncias de **step-sizes** podem convergir muito lentamente ou necessitar de ajustes consider√°veis para obter uma taxa de converg√™ncia satisfat√≥ria [^9].

Outra alternativa interessante √© descrita no exerc√≠cio 2.7 [^11], que sugere o uso de um **step-size** adaptativo dado por $\beta_n = \frac{\alpha}{\bar{o}_n}$, onde $\alpha > 0$ √© uma constante e $\bar{o}_n$ √© um tra√ßo que come√ßa em 0 e √© atualizado iterativamente. Essa abordagem busca evitar o vi√©s inicial associado ao uso de **step-sizes** constantes, mantendo as vantagens de adaptabilidade em ambientes n√£o-estacion√°rios [^11].

**Lema 2** *Converg√™ncia do Step-size Adaptativo*. Se o tra√ßo $\bar{o}_n$ converge para um valor positivo $\bar{o}$, ent√£o o **step-size** adaptativo $\beta_n = \frac{\alpha}{\bar{o}_n}$ se comporta assintoticamente como um **step-size** constante.

*Prova*: Se $\bar{o}_n \rightarrow \bar{o} > 0$, ent√£o $\lim_{n \to \infty} \beta_n = \lim_{n \to \infty} \frac{\alpha}{\bar{o}_n} = \frac{\alpha}{\bar{o}}$. Portanto, para $n$ suficientemente grande, $\beta_n$ se aproxima de um valor constante $\frac{\alpha}{\bar{o}}$.

Para formalizar esta prova:

I.  Dado que $\bar{o}_n$ converge para $\bar{o} > 0$, podemos escrever:
    $$\lim_{n \to \infty} \bar{o}_n = \bar{o}$$

II. Considere o **step-size** adaptativo $\beta_n = \frac{\alpha}{\bar{o}_n}$, onde $\alpha > 0$.

III. Aplique o limite quando $n$ tende ao infinito:
     $$\lim_{n \to \infty} \beta_n = \lim_{n \to \infty} \frac{\alpha}{\bar{o}_n}$$

IV. Como $\alpha$ √© constante e $\lim_{n \to \infty} \bar{o}_n = \bar{o}$, podemos escrever:
    $$\lim_{n \to \infty} \beta_n = \frac{\alpha}{\lim_{n \to \infty} \bar{o}_n} = \frac{\alpha}{\bar{o}}$$

V. Portanto, $\lim_{n \to \infty} \beta_n = \frac{\alpha}{\bar{o}}$, que √© um valor constante. Isso significa que para $n$ suficientemente grande, $\beta_n$ se aproxima do valor constante $\frac{\alpha}{\bar{o}}$.

VI. Conclu√≠mos que o **step-size** adaptativo $\beta_n$ se comporta assintoticamente como um **step-size** constante. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Suponha que $\alpha = 0.1$ e que o tra√ßo $\bar{o}_n$ converge para $\bar{o} = 2$. Ent√£o, o **step-size** adaptativo se tornar√° $\beta_n = \frac{0.1}{\bar{o}_n}$. √Ä medida que $n$ aumenta, $\bar{o}_n$ se aproxima de 2, ent√£o $\beta_n$ se aproxima de $\frac{0.1}{2} = 0.05$. Isso significa que, a longo prazo, o **step-size** adaptativo se comporta como um **step-size** constante de 0.05.
>
> Considere a seguinte atualiza√ß√£o para o tra√ßo:
>
> $\bar{o}_{n+1} = \bar{o}_n + \gamma(R_n - Q_n - \bar{o}_n)$ com $\bar{o}_0 = 0$ e $\gamma = 0.01$.
>
> Este c√≥digo demonstra como o tra√ßo $\bar{o}_n$ se ajusta ao longo do tempo e como o **step-size** adaptativo $\beta_n$ se comporta de acordo.  A adi√ß√£o de um pequeno valor (0.0001) evita divis√£o por zero quando `o_n` est√° pr√≥ximo de zero. O gr√°fico resultante mostrar√° a evolu√ß√£o de `Q_adaptativo` e `o_n` ao longo do tempo.

Em termos pr√°ticos, a escolha do **step-size** ideal depende das caracter√≠sticas espec√≠ficas do problema em quest√£o. Em ambientes altamente n√£o-estacion√°rios, um **step-size** constante maior pode ser prefer√≠vel para garantir uma adapta√ß√£o r√°pida √†s mudan√ßas, mesmo que isso signifique uma maior variabilidade nas estimativas. Em ambientes mais estacion√°rios, um **step-size** menor ou uma abordagem adaptativa podem ser mais adequados para garantir uma converg√™ncia mais est√°vel.

### Conclus√£o
A an√°lise das condi√ß√µes de **aproxima√ß√£o estoc√°stica** para converg√™ncia revela um *trade-off* fundamental entre a estabilidade e a adaptabilidade em algoritmos de **aprendizado por refor√ßo**. Embora as condi√ß√µes $\sum_{n=1}^{\infty} \alpha_n(a) = \infty$ e $\sum_{n=1}^{\infty} \alpha_n(a)^2 < \infty$ garantam a converg√™ncia em ambientes estacion√°rios, o uso de **step-sizes** constantes, que n√£o satisfazem essas condi√ß√µes, pode ser mais apropriado em ambientes n√£o-estacion√°rios, permitindo uma adapta√ß√£o cont√≠nua √†s mudan√ßas nas distribui√ß√µes de recompensa. A escolha do **step-size** ideal depende das caracter√≠sticas espec√≠ficas do problema e requer uma considera√ß√£o cuidadosa das vantagens e desvantagens de diferentes abordagens.

### Refer√™ncias
[^1]: Cap√≠tulo 2: Multi-armed Bandits.
[^8]: Se√ß√£o 2.5: Tracking a Nonstationary Problem.
[^9]: Se√ß√£o 2.5: Tracking a Nonstationary Problem.
[^11]: Exerc√≠cio 2.7: Unbiased Constant-Step-Size Trick.
<!-- END -->