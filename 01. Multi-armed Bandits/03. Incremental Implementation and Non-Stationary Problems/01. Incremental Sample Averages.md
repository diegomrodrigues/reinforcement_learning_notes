## Implementa√ß√£o Incremental para C√°lculo de M√©dias Amostrais

### Introdu√ß√£o

No contexto dos *k-armed bandit problems*, uma das tarefas fundamentais √© estimar os **valores das a√ß√µes** com base nas recompensas observadas. M√©todos *action-value* usam essas estimativas para tomar decis√µes sobre qual a√ß√£o selecionar [^27]. Uma maneira intuitiva de estimar o valor de uma a√ß√£o √© calcular a **m√©dia amostral** das recompensas recebidas ap√≥s selecionar essa a√ß√£o [^27]. No entanto, calcular essa m√©dia diretamente armazenando todas as recompensas pode se tornar computacionalmente caro, especialmente com um grande n√∫mero de passos de tempo [^31]. Esta se√ß√£o explora uma **implementa√ß√£o incremental** que permite calcular m√©dias amostrais de forma eficiente, com **mem√≥ria constante** e **computa√ß√£o constante por passo de tempo** [^31].

### Conceitos Fundamentais

A **m√©dia amostral** $Q_n$ ap√≥s $n-1$ sele√ß√µes de uma a√ß√£o √© dada por [^31]:

$$
Q_n = \frac{R_1 + R_2 + \ldots + R_{n-1}}{n-1}
$$

Onde $R_i$ √© a recompensa recebida ap√≥s a $i$-√©sima sele√ß√£o da a√ß√£o [^31]. A implementa√ß√£o direta requer armazenar todas as recompensas $R_1, R_2, \ldots, R_{n-1}$ e recalcular a soma cada vez que uma nova recompensa √© recebida [^31].

> üí° **Exemplo Num√©rico:**
>
> Suponha que uma a√ß√£o foi selecionada 3 vezes e as recompensas obtidas foram: $R_1 = 2$, $R_2 = 4$, $R_3 = 6$. Usando a f√≥rmula direta, a m√©dia amostral $Q_4$ seria:
>
> $Q_4 = \frac{2 + 4 + 6}{3} = \frac{12}{3} = 4$.
>
> A implementa√ß√£o direta precisaria armazenar $R_1$, $R_2$ e $R_3$ para calcular $Q_4$.

A **implementa√ß√£o incremental** oferece uma alternativa mais eficiente. Dado $Q_n$ e a $n$-√©sima recompensa $R_n$, a nova m√©dia $Q_{n+1}$ pode ser calculada como [^31]:

$$
Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]
$$

Esta f√≥rmula elimina a necessidade de armazenar todas as recompensas anteriores. Apenas o valor atual $Q_n$ e o n√∫mero de vezes que a a√ß√£o foi selecionada, $n$, precisam ser armazenados [^31]. A computa√ß√£o envolvida em cada atualiza√ß√£o √© constante, independentemente do n√∫mero de recompensas observadas [^31].

> üí° **Exemplo Num√©rico:**
>
> Continuando o exemplo anterior, temos $Q_1$ (inicializado como 0 ou um valor arbitr√°rio), $R_1 = 2$. Vamos calcular $Q_2$:
>
> $Q_2 = Q_1 + \frac{1}{1}[R_1 - Q_1]$. Se $Q_1 = 0$, ent√£o $Q_2 = 0 + \frac{1}{1}[2 - 0] = 2$.
>
> Agora, $R_2 = 4$. Vamos calcular $Q_3$:
>
> $Q_3 = Q_2 + \frac{1}{2}[R_2 - Q_2] = 2 + \frac{1}{2}[4 - 2] = 2 + \frac{1}{2}[2] = 2 + 1 = 3$.
>
> Finalmente, $R_3 = 6$. Vamos calcular $Q_4$:
>
> $Q_4 = Q_3 + \frac{1}{3}[R_3 - Q_3] = 3 + \frac{1}{3}[6 - 3] = 3 + \frac{1}{3}[3] = 3 + 1 = 4$.
>
> Este resultado √© o mesmo obtido com a f√≥rmula direta, mas a implementa√ß√£o incremental apenas precisou armazenar o valor atual da m√©dia e o n√∫mero de vezes que a a√ß√£o foi selecionada.

**Deriva√ß√£o da F√≥rmula Incremental**:

A f√≥rmula incremental pode ser derivada da seguinte forma:

$$
Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i = \frac{1}{n} (R_n + \sum_{i=1}^{n-1} R_i)
$$

$$
Q_{n+1} = \frac{1}{n} (R_n + (n-1)Q_n) = \frac{R_n + nQ_n - Q_n}{n}
$$

$$
Q_{n+1} = Q_n + \frac{1}{n} (R_n - Q_n)
$$
$\blacksquare$

Para clareza e completude, aqui est√° uma prova formal da deriva√ß√£o da f√≥rmula incremental.

**Prova da F√≥rmula Incremental**
Provaremos que:
$$Q_{n+1} = Q_n + \frac{1}{n}(R_n - Q_n)$$

I. Definimos $Q_{n+1}$ como a m√©dia das primeiras $n$ recompensas:
$$Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i$$

II. Podemos separar a √∫ltima recompensa $R_n$ da soma:
$$Q_{n+1} = \frac{1}{n} \left( \sum_{i=1}^{n-1} R_i + R_n \right)$$

III. Reconhecemos que $\sum_{i=1}^{n-1} R_i$ √© igual a $(n-1)Q_n$, onde $Q_n$ √© a m√©dia das primeiras $n-1$ recompensas:
$$Q_{n+1} = \frac{1}{n} \left( (n-1)Q_n + R_n \right)$$

IV. Distribu√≠mos $\frac{1}{n}$ e rearranjamos os termos:
$$Q_{n+1} = \frac{nQ_n - Q_n + R_n}{n} = Q_n + \frac{R_n - Q_n}{n}$$

V. Portanto, provamos que:
$$Q_{n+1} = Q_n + \frac{1}{n} (R_n - Q_n)$$
$\blacksquare$

**Interpreta√ß√£o da F√≥rmula**:

A f√≥rmula incremental atualiza a estimativa anterior $Q_n$ na dire√ß√£o da nova recompensa $R_n$ [^31]. A magnitude da atualiza√ß√£o √© controlada pelo termo $\frac{1}{n}$, que funciona como um **step-size** [^31]. Quanto maior $n$, menor o step-size, o que significa que as novas recompensas t√™m menos influ√™ncia na estimativa [^31].

> üí° **Exemplo Num√©rico:**
>
> Considere $Q_n = 5$ e $R_n = 10$, com $n=10$. A atualiza√ß√£o seria:
>
> $Q_{n+1} = 5 + \frac{1}{10}[10 - 5] = 5 + \frac{1}{10}[5] = 5 + 0.5 = 5.5$.
>
> Agora, considere o mesmo cen√°rio com $n=100$:
>
> $Q_{n+1} = 5 + \frac{1}{100}[10 - 5] = 5 + \frac{1}{100}[5] = 5 + 0.05 = 5.05$.
>
> Observe como, com um $n$ maior, a recompensa $R_n$ tem menos impacto na atualiza√ß√£o da estimativa.

A forma geral desta regra de atualiza√ß√£o √© [^31]:

$$
\text{NewEstimate} \leftarrow \text{OldEstimate} + \text{StepSize} [\text{Target} - \text{OldEstimate}]
$$

Onde $[\text{Target} - \text{OldEstimate}]$ representa o **erro** na estimativa, e o objetivo √© reduzir esse erro tomando um passo na dire√ß√£o do "Target" [^31].

**Teorema 1** (Converg√™ncia da M√©dia Amostral)
Se a sequ√™ncia de recompensas $R_i$ √© i.i.d. (independentes e identicamente distribu√≠das) com m√©dia $\mu$, ent√£o a m√©dia amostral $Q_n$ converge para $\mu$ quase certamente quando $n \rightarrow \infty$.

*Proof:*
Esta √© uma consequ√™ncia direta da Lei Forte dos Grandes N√∫meros. Como $Q_n = \frac{1}{n-1}\sum_{i=1}^{n-1} R_i$, e as recompensas $R_i$ s√£o i.i.d. com m√©dia $\mu$, ent√£o $\lim_{n \to \infty} Q_n = \mu$ quase certamente. $\blacksquare$

**Observa√ß√£o:**
O Teorema 1 garante que, sob certas condi√ß√µes (recompensas i.i.d.), a m√©dia amostral converge para o valor verdadeiro da a√ß√£o. No entanto, na pr√°tica, as recompensas nem sempre s√£o i.i.d., especialmente em ambientes n√£o-estacion√°rios.

**Teorema 1.1** (Adaptando o Step-Size para Ambientes N√£o-Estacion√°rios)
Em ambientes n√£o-estacion√°rios, onde a distribui√ß√£o das recompensas muda ao longo do tempo, √© ben√©fico usar um step-size constante $\alpha \in (0, 1]$ em vez de $\frac{1}{n}$. A f√≥rmula de atualiza√ß√£o torna-se:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n]
$$

Esta abordagem permite que a estimativa responda mais rapidamente √†s mudan√ßas no ambiente.

> üí° **Exemplo Num√©rico:**
>
> Suponha $Q_n = 5$, $R_n = 10$ e $\alpha = 0.1$. Ent√£o:
>
> $Q_{n+1} = 5 + 0.1[10 - 5] = 5 + 0.1[5] = 5 + 0.5 = 5.5$.
>
> Agora, com $\alpha = 0.5$:
>
> $Q_{n+1} = 5 + 0.5[10 - 5] = 5 + 0.5[5] = 5 + 2.5 = 7.5$.
>
> Observe como um $\alpha$ maior faz com que a estimativa mude mais rapidamente em resposta √† recompensa.

*Proof:*
Com um step-size constante, a influ√™ncia das recompensas passadas diminui exponencialmente. Isso pode ser visto expandindo a f√≥rmula recursivamente:

$$
Q_{n+1} = \alpha R_n + (1 - \alpha) Q_n \\
= \alpha R_n + (1 - \alpha) (\alpha R_{n-1} + (1 - \alpha) Q_{n-1}) \\
= \alpha R_n + \alpha (1 - \alpha) R_{n-1} + (1 - \alpha)^2 Q_{n-1} \\
= \alpha \sum_{i=1}^{n} (1 - \alpha)^{n-i} R_{i-1} + (1-\alpha)^n Q_1
$$

Onde $Q_1$ √© a estimativa inicial. O peso de cada recompensa $R_i$ √© $\alpha(1 - \alpha)^{n-i}$, que diminui exponencialmente com a dist√¢ncia temporal de $R_n$. Isso significa que as recompensas mais recentes t√™m um impacto maior na estimativa atual do que as recompensas mais antigas.  Portanto, em um ambiente n√£o-estacion√°rio, onde a distribui√ß√£o das recompensas pode mudar, √© desej√°vel dar mais peso √†s recompensas recentes. $\blacksquare$

Para elucidar ainda mais o teorema 1.1, pode-se formalmente provar que a soma dos pesos geom√©tricos $\alpha(1 - \alpha)^{n-i}$ para todas as recompensas anteriores soma 1 (excluindo o termo inicial $(1-\alpha)^n Q_1$, que representa a influ√™ncia da estimativa inicial).

**Prova da Soma dos Pesos Exponenciais:**
Queremos provar que:
$$\sum_{i=1}^{n} \alpha(1 - \alpha)^{n-i} + (1-\alpha)^n = 1$$

I. Primeiro, focaremos na soma dos pesos exponenciais:
$$S = \sum_{i=1}^{n} \alpha(1 - \alpha)^{n-i} = \alpha \sum_{i=1}^{n} (1 - \alpha)^{n-i}$$

II. Fazemos uma mudan√ßa de vari√°vel $j = n - i$. Quando $i = 1$, $j = n - 1$. Quando $i = n$, $j = 0$. Ent√£o:
$$S = \alpha \sum_{j=0}^{n-1} (1 - \alpha)^{j}$$

III. Reconhecemos a soma como uma s√©rie geom√©trica finita com $n$ termos, primeiro termo $a = 1$, e raz√£o $r = (1 - \alpha)$. A soma de uma s√©rie geom√©trica finita √© dada por:
$$S_n = a \frac{1 - r^n}{1 - r}$$

IV. Aplicamos a f√≥rmula da s√©rie geom√©trica:
$$S = \alpha \frac{1 - (1 - \alpha)^n}{1 - (1 - \alpha)} = \alpha \frac{1 - (1 - \alpha)^n}{\alpha} = 1 - (1 - \alpha)^n$$

V. Agora, consideramos o termo $(1-\alpha)^n Q_1$ na expans√£o de $Q_{n+1}$.  A soma total dos pesos, incluindo a influ√™ncia da estimativa inicial, √©:

$$S_{\text{total}} =  \sum_{i=1}^{n} \alpha(1 - \alpha)^{n-i} + (1-\alpha)^n =  1 - (1 - \alpha)^n + (1-\alpha)^n = 1 $$

VI. Portanto, a soma dos pesos exponenciais, juntamente com o fator de decaimento da estimativa inicial, soma 1. Isto demonstra que os pesos s√£o normalizados e representam a influ√™ncia relativa de cada recompensa e da estimativa inicial na estimativa atual. $\blacksquare$

### Conclus√£o

A implementa√ß√£o incremental oferece um meio eficiente de calcular m√©dias amostrais em problemas *k-armed bandit* [^31]. Sua exig√™ncia de mem√≥ria constante e computa√ß√£o por passo de tempo a torna prefer√≠vel √† implementa√ß√£o direta quando se lida com grandes conjuntos de dados ou longos horizontes de tempo [^31]. Este m√©todo forma a base para muitos algoritmos de aprendizado por refor√ßo, e entender sua deriva√ß√£o e propriedades √© essencial para o desenvolvimento e aplica√ß√£o de t√©cnicas mais avan√ßadas [^31]. Al√©m disso, a adapta√ß√£o do step-size para ambientes n√£o-estacion√°rios √© crucial para garantir que as estimativas dos valores das a√ß√µes permane√ßam precisas e responsivas √†s mudan√ßas no ambiente.

### Refer√™ncias

[^27]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd Edition. The MIT Press, 2018.
[^31]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd Edition. The MIT Press, 2018.
<!-- END -->