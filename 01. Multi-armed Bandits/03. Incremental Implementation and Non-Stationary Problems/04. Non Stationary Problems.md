## Rastreamento de Problemas N√£o-Estacion√°rios em Bandidos Multi-armados

### Introdu√ß√£o
Nos cap√≠tulos anteriores, analisamos m√©todos de aprendizado por refor√ßo (RL) no contexto de problemas de *k-armed bandits* com distribui√ß√µes de recompensa *estacion√°rias*. No entanto, muitos problemas de RL s√£o efetivamente **n√£o-estacion√°rios**, o que significa que as probabilidades de recompensa das a√ß√µes mudam ao longo do tempo [^32]. Portanto, √© crucial adaptar as t√©cnicas de aprendizado para lidar com essas mudan√ßas, dando mais peso √†s recompensas recentes do que √†s recompensas passadas. Esta se√ß√£o detalha como adaptar m√©todos de valor de a√ß√£o para cen√°rios n√£o estacion√°rios, com foco no uso de um par√¢metro de tamanho de passo constante [^32].

### Implementa√ß√£o Incremental com Tamanho de Passo Constante
Para problemas *estacion√°rios*, a estimativa do valor da a√ß√£o √© uma m√©dia amostral das recompensas observadas [^27]. No entanto, essa abordagem atribui igual peso a todas as recompensas, o que n√£o √© ideal para problemas n√£o estacion√°rios. Em vez disso, podemos usar um **tamanho de passo constante** $\alpha \in (0, 1]$ para atualizar as estimativas de valor da a√ß√£o. A regra de atualiza√ß√£o incremental, apresentada anteriormente, √© modificada para [^32]:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n] \quad (2.5)
$$

onde $Q_{n+1}$ √© a nova estimativa de valor da a√ß√£o, $Q_n$ √© a estimativa anterior, $R_n$ √© a n-√©sima recompensa recebida, e $\alpha$ √© o tamanho de passo constante. Essa atualiza√ß√£o resulta em $Q_{n+1}$ sendo uma m√©dia ponderada das recompensas passadas e da estimativa inicial $Q_1$. A equa√ß√£o (2.6) mostra essa m√©dia ponderada [^32]:

$$
Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i \quad (2.6)
$$

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um problema de bandit com uma √∫nica a√ß√£o. Inicializamos o valor estimado da a√ß√£o $Q_1 = 0$. Escolhemos um tamanho de passo constante $\alpha = 0.1$. As recompensas que recebemos ao longo de 5 tentativas s√£o: $R_1 = 1, R_2 = 2, R_3 = 0, R_4 = 1, R_5 = 3$. Vamos calcular as estimativas de valor da a√ß√£o $Q_n$ para cada tentativa usando a Equa√ß√£o (2.5):
>
> *   Tentativa 1: $Q_2 = Q_1 + \alpha [R_1 - Q_1] = 0 + 0.1 [1 - 0] = 0.1$
> *   Tentativa 2: $Q_3 = Q_2 + \alpha [R_2 - Q_2] = 0.1 + 0.1 [2 - 0.1] = 0.1 + 0.1 * 1.9 = 0.29$
> *   Tentativa 3: $Q_4 = Q_3 + \alpha [R_3 - Q_3] = 0.29 + 0.1 [0 - 0.29] = 0.29 - 0.029 = 0.261$
> *   Tentativa 4: $Q_5 = Q_4 + \alpha [R_4 - Q_4] = 0.261 + 0.1 [1 - 0.261] = 0.261 + 0.1 * 0.739 = 0.3349$
> *   Tentativa 5: $Q_6 = Q_5 + \alpha [R_5 - Q_5] = 0.3349 + 0.1 [3 - 0.3349] = 0.3349 + 0.1 * 2.6651 = 0.60141$
>
> Observe como cada nova recompensa afeta a estimativa do valor da a√ß√£o, mas o impacto das recompensas mais antigas diminui exponencialmente. Isso torna o algoritmo sens√≠vel a mudan√ßas nas recompensas, o que √© crucial em ambientes n√£o estacion√°rios.

Para demonstrar que a Equa√ß√£o (2.5) leva √† Equa√ß√£o (2.6), podemos fornecer a seguinte prova:

*Prova:*
Provaremos que a aplica√ß√£o repetida da atualiza√ß√£o incremental com tamanho de passo constante (Equa√ß√£o 2.5) resulta na m√©dia ponderada das recompensas passadas e da estimativa inicial (Equa√ß√£o 2.6).

I.  Come√ßamos com a Equa√ß√£o (2.5):
    $Q_{n+1} = Q_n + \alpha[R_n - Q_n]$

II. Podemos reescrever essa equa√ß√£o como:
    $Q_{n+1} = (1 - \alpha)Q_n + \alpha R_n$

III. Agora, aplicamos iterativamente essa equa√ß√£o, expandindo $Q_n$ em termos de $Q_{n-1}$ e $R_{n-1}$:
     $Q_{n+1} = (1 - \alpha)[(1 - \alpha)Q_{n-1} + \alpha R_{n-1}] + \alpha R_n$
     $Q_{n+1} = (1 - \alpha)^2 Q_{n-1} + \alpha(1 - \alpha)R_{n-1} + \alpha R_n$

IV. Continuamos este processo iterativamente at√© chegarmos a $Q_1$:
    $Q_{n+1} = (1 - \alpha)^n Q_1 + \alpha(1 - \alpha)^{n-1}R_1 + \alpha(1 - \alpha)^{n-2}R_2 + \ldots + \alpha(1 - \alpha)R_{n-1} + \alpha R_n$

V.  Reorganizando os termos, obtemos:
    $Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i$

Assim, a aplica√ß√£o repetida da Equa√ß√£o (2.5) leva √† Equa√ß√£o (2.6). ‚ñ†

A equa√ß√£o (2.6) demonstra que o peso $ \alpha (1 - \alpha)^{n-i} $ dado √† recompensa $ R_i $ depende de qu√£o distante (em termos de passos) $ R_i $ foi observado no passado, com $n-i$ representando a dist√¢ncia. Quanto maior o valor de $ \alpha $, mais peso √© dado √†s recompensas recentes, tornando o algoritmo mais sens√≠vel √†s mudan√ßas no ambiente [^32].

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar o efeito do tamanho de passo $\alpha$, vamos considerar o exemplo anterior com $\alpha = 0.5$ em vez de $\alpha = 0.1$. Usando as mesmas recompensas ($R_1 = 1, R_2 = 2, R_3 = 0, R_4 = 1, R_5 = 3$) e $Q_1 = 0$, recalculemos as estimativas de valor da a√ß√£o:
>
> *   Tentativa 1: $Q_2 = Q_1 + \alpha [R_1 - Q_1] = 0 + 0.5 [1 - 0] = 0.5$
> *   Tentativa 2: $Q_3 = Q_2 + \alpha [R_2 - Q_2] = 0.5 + 0.5 [2 - 0.5] = 0.5 + 0.5 * 1.5 = 1.25$
> *   Tentativa 3: $Q_4 = Q_3 + \alpha [R_3 - Q_3] = 1.25 + 0.5 [0 - 1.25] = 1.25 - 0.625 = 0.625$
> *   Tentativa 4: $Q_5 = Q_4 + \alpha [R_4 - Q_4] = 0.625 + 0.5 [1 - 0.625] = 0.625 + 0.5 * 0.375 = 0.8125$
> *   Tentativa 5: $Q_6 = Q_5 + \alpha [R_5 - Q_5] = 0.8125 + 0.5 [3 - 0.8125] = 0.8125 + 0.5 * 2.1875 = 1.90625$
>
> Com $\alpha = 0.5$, as estimativas de valor da a√ß√£o mudam mais rapidamente em resposta √†s novas recompensas. Compare com o exemplo anterior onde $\alpha = 0.1$, as estimativas mudam mais lentamente. Um $\alpha$ maior torna o algoritmo mais adapt√°vel, por√©m mais sens√≠vel ao ru√≠do.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> alpha1 = 0.1
> alpha2 = 0.5
> rewards = [1, 2, 0, 1, 3]
> Q1 = 0
>
> # C√°lculo das estimativas de valor da a√ß√£o para alpha = 0.1
> Q_values_alpha1 = [Q1]
> for r in rewards:
>     Q1 = Q1 + alpha1 * (r - Q1)
>     Q_values_alpha1.append(Q1)
>
> # C√°lculo das estimativas de valor da a√ß√£o para alpha = 0.5
> Q1 = 0
> Q_values_alpha2 = [Q1]
> for r in rewards:
>     Q1 = Q1 + alpha2 * (r - Q1)
>     Q_values_alpha2.append(Q1)
>
> # Plotagem
> plt.figure(figsize=(10, 6))
> plt.plot(Q_values_alpha1, marker='o', label=f'Alpha = {alpha1}')
> plt.plot(Q_values_alpha2, marker='x', label=f'Alpha = {alpha2}')
> plt.xlabel('Tentativa')
> plt.ylabel('Estimativa de Valor da A√ß√£o (Q)')
> plt.title('Compara√ß√£o da Estimativa de Valor da A√ß√£o com Diferentes Tamanhos de Passo')
> plt.grid(True)
> plt.legend()
> plt.xticks(range(len(Q_values_alpha1))) # Garante que todos os valores de x sejam exibidos
> plt.show()
> ```

Para complementar essa an√°lise, podemos examinar o comportamento da m√©dia ponderada em rela√ß√£o a um valor alvo constante.

**Lema 1**
Se as recompensas $R_i$ forem identicamente iguais a um valor constante $R$, ent√£o $Q_{n+1}$ converge para $R$ quando $n$ tende ao infinito.

*Demonstra√ß√£o:*
Se $R_i = R$ para todo $i$, ent√£o a Equa√ß√£o (2.6) se torna:

$Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R$
$Q_{n+1} = (1 - \alpha)^n Q_1 + R \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i}$

A soma √© uma s√©rie geom√©trica: $\sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} = 1 - (1 - \alpha)^n$.

Para provar que $\sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} = 1 - (1 - \alpha)^n$, fornecemos a seguinte prova:

I. Seja $S = \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i}$.

II. Podemos reescrever a soma invertendo a ordem dos termos:
    $S = \alpha + \alpha(1 - \alpha) + \alpha(1 - \alpha)^2 + \ldots + \alpha(1 - \alpha)^{n-1}$

III. Multiplicamos ambos os lados por $(1 - \alpha)$:
     $(1 - \alpha)S = \alpha(1 - \alpha) + \alpha(1 - \alpha)^2 + \ldots + \alpha(1 - \alpha)^{n-1} + \alpha(1 - \alpha)^n$

IV. Subtra√≠mos a Equa√ß√£o III da Equa√ß√£o II:
    $S - (1 - \alpha)S = \alpha - \alpha(1 - \alpha)^n$
    $\alpha S = \alpha - \alpha(1 - \alpha)^n$

V. Dividimos ambos os lados por $\alpha$:
    $S = 1 - (1 - \alpha)^n$

Portanto, $\sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} = 1 - (1 - \alpha)^n$ ‚ñ†

Portanto, $Q_{n+1} = (1 - \alpha)^n Q_1 + R [1 - (1 - \alpha)^n]$.

Quando $n \to \infty$, $(1 - \alpha)^n \to 0$ dado que $0 < \alpha \leq 1$.

Assim, $\lim_{n \to \infty} Q_{n+1} = R$. ‚ñ†

### Considera√ß√µes de Converg√™ncia
A escolha de um tamanho de passo apropriado √© crucial. Para garantir a converg√™ncia em problemas *estacion√°rios*, as seguintes condi√ß√µes, derivadas da teoria de aproxima√ß√£o estoc√°stica, devem ser satisfeitas [^33]:

$$
\sum_{n=1}^{\infty} \alpha_n(a) = \infty \quad \text{e} \quad \sum_{n=1}^{\infty} \alpha_n^2(a) < \infty \quad (2.7)
$$

A primeira condi√ß√£o garante que os passos sejam grandes o suficiente para superar as condi√ß√µes iniciais ou flutua√ß√µes aleat√≥rias, enquanto a segunda condi√ß√£o garante que os passos eventualmente se tornem pequenos o suficiente para assegurar a converg√™ncia [^33]. No entanto, com um tamanho de passo constante $ \alpha $, a segunda condi√ß√£o n√£o √© satisfeita, indicando que as estimativas nunca convergem completamente, mas continuam a variar em resposta √†s recompensas mais recentemente recebidas. Essa falta de converg√™ncia completa √© desej√°vel em ambientes n√£o estacion√°rios [^33].

> üí° **Exemplo Num√©rico:**
>
> Consideremos o tamanho de passo constante $\alpha = 0.1$.  Vamos verificar se as condi√ß√µes de converg√™ncia da teoria de aproxima√ß√£o estoc√°stica s√£o satisfeitas:
>
> *   Condi√ß√£o 1: $\sum_{n=1}^{\infty} \alpha_n(a) = \sum_{n=1}^{\infty} 0.1$. Essa soma diverge porque estamos adicionando 0.1 infinitas vezes.
> *   Condi√ß√£o 2: $\sum_{n=1}^{\infty} \alpha_n^2(a) = \sum_{n=1}^{\infty} (0.1)^2 = \sum_{n=1}^{\infty} 0.01$. Essa soma tamb√©m diverge porque estamos adicionando 0.01 infinitas vezes.
>
> Como a segunda condi√ß√£o n√£o √© satisfeita, isso indica que o uso de um tamanho de passo constante n√£o garante a converg√™ncia em um ambiente estacion√°rio. No entanto, √© desej√°vel em ambientes n√£o estacion√°rios, pois permite que o algoritmo se adapte continuamente √†s mudan√ßas.
>
> Agora, considere um tamanho de passo decrescente $\alpha_n = 1/n$. Vamos verificar as condi√ß√µes de converg√™ncia:
>
> *   Condi√ß√£o 1: $\sum_{n=1}^{\infty} \alpha_n(a) = \sum_{n=1}^{\infty} 1/n$. Essa √© a s√©rie harm√¥nica, que diverge.
> *   Condi√ß√£o 2: $\sum_{n=1}^{\infty} \alpha_n^2(a) = \sum_{n=1}^{\infty} (1/n)^2 = \sum_{n=1}^{\infty} 1/n^2$. Essa √© a s√©rie p com p = 2, que converge para $\pi^2 / 6 \approx 1.645$.
>
> Como ambas as condi√ß√µes s√£o satisfeitas, o tamanho de passo decrescente garante a converg√™ncia em um ambiente estacion√°rio.

Para elucidar ainda mais as implica√ß√µes da n√£o-converg√™ncia em ambientes n√£o-estacion√°rios, considere a seguinte proposi√ß√£o:

**Proposi√ß√£o 2**
Em um ambiente onde a recompensa esperada de uma a√ß√£o muda abruptamente no tempo $t^*$, um tamanho de passo constante $\alpha$ permite que a estimativa $Q_n$ se adapte √† nova recompensa esperada, enquanto um tamanho de passo decrescente (satisfazendo as condi√ß√µes de converg√™ncia para ambientes estacion√°rios) faria com que a estimativa $Q_n$ convirja para um valor desatualizado.

*Demonstra√ß√£o (Esbo√ßo)*
Suponha que para $t < t^*$, $E[R_t] = \mu_1$, e para $t \geq t^*$, $E[R_t] = \mu_2$, onde $\mu_1 \neq \mu_2$. Com um tamanho de passo constante $\alpha$, $Q_n$ se aproximar√° de $\mu_2$ ap√≥s $t^*$. Com um tamanho de passo decrescente $\alpha_n$, o impacto das recompensas ap√≥s $t^*$ diminuir√° com o tempo, e $Q_n$ tender√° a um valor entre $\mu_1$ e $\mu_2$, influenciado principalmente por $\mu_1$ devido √†s primeiras itera√ß√µes terem maior peso.

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma √∫nica a√ß√£o. Inicialmente, a recompensa esperada √© $\mu_1 = 1$. No tempo $t^* = 100$, a recompensa esperada muda abruptamente para $\mu_2 = 5$. Vamos comparar o comportamento de $Q_n$ com um tamanho de passo constante $\alpha = 0.1$ e um tamanho de passo decrescente $\alpha_n = 1/n$. Inicializamos $Q_1 = 0$ em ambos os casos.
>
> Para simplificar, vamos simular as recompensas como sendo exatamente iguais √† recompensa esperada em cada per√≠odo (sem ru√≠do).
>
> *   **Tamanho de passo constante ($\alpha = 0.1$):**
>     *   Para $t < 100$, $R_t = 1$. Ap√≥s muitas itera√ß√µes, $Q_n$ se aproximar√° de 1.
>     *   Para $t \geq 100$, $R_t = 5$.  $Q_{100} \approx 1$.  Ent√£o, $Q_{101} = 1 + 0.1(5 - 1) = 1.4$, $Q_{102} = 1.4 + 0.1(5 - 1.4) = 1.76$, etc.  $Q_n$ eventualmente se aproximar√° de 5.
> *   **Tamanho de passo decrescente ($\alpha_n = 1/n$):**
>     *   Para $t < 100$, $R_t = 1$. Ap√≥s 100 itera√ß√µes, $Q_{100} \approx 1$ (devido √† propriedade de converg√™ncia de tamanhos de passo decrescentes em ambientes estacion√°rios).
>     *   Para $t \geq 100$, $R_t = 5$.  $Q_{101} = 1 + (1/101)(5 - 1) = 1 + 4/101 \approx 1.04$, $Q_{102} = 1.04 + (1/102)(5 - 1.04) \approx 1.079$, etc. Devido ao tamanho de passo decrescente, $Q_n$ se adaptar√° muito lentamente √† nova recompensa esperada de 5.  Ele convergir√° para um valor entre 1 e 5, mas muito mais pr√≥ximo de 1.
>
> Este exemplo ilustra como o tamanho de passo constante permite uma adapta√ß√£o mais r√°pida √†s mudan√ßas no ambiente, enquanto o tamanho de passo decrescente torna o algoritmo mais lento para se ajustar.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> alpha_constant = 0.1
> rewards1 = [1] * 100 + [5] * 100 # Recompensa muda em t=100
> rewards2 = [1] * 100 + [5] * 100 # Recompensa muda em t=100
> Q_initial = 0
>
> # C√°lculo das estimativas de valor da a√ß√£o com tamanho de passo constante
> Q_constant = [Q_initial]
> Q = Q_initial
> for i, r in enumerate(rewards1):
>     Q = Q + alpha_constant * (r - Q)
>     Q_constant.append(Q)
>
> # C√°lculo das estimativas de valor da a√ß√£o com tamanho de passo decrescente
> Q_decreasing = [Q_initial]
> Q = Q_initial
> for i, r in enumerate(rewards2):
>     alpha_decreasing = 1 / (i + 1)
>     Q = Q + alpha_decreasing * (r - Q)
>     Q_decreasing.append(Q)
>
> # Plotagem
> plt.figure(figsize=(12, 6))
> plt.plot(Q_constant, label='Tamanho de Passo Constante (alpha=0.1)')
> plt.plot(Q_decreasing, label='Tamanho de Passo Decrescente (alpha=1/n)')
> plt.xlabel('Tentativa')
> plt.ylabel('Estimativa de Valor da A√ß√£o (Q)')
> plt.title('Compara√ß√£o de Tamanhos de Passo em Ambiente N√£o Estacion√°rio')
> plt.axvline(x=100, color='r', linestyle='--', label='Mudan√ßa na Recompensa Esperada')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

### Abordagens alternativas para tamanho de passo n√£o constante

Uma forma de evitar o vi√©s de tamanhos de passo constantes, mantendo as vantagens para problemas n√£o-estacion√°rios √© usar o seguinte tamanho de passo [^35]:

$$
\beta_n = \alpha / \overline{o}_n \quad (2.8)
$$

onde $ \alpha > 0 $ √© um tamanho de passo constante convencional e $ \overline{o}_n $ √© um tra√ßo que come√ßa em 0 [^35]:

$$
\overline{o}_n = \overline{o}_{n-1} + \alpha(1 - \overline{o}_{n-1}), \text{ para } n > 0, \text{ com } \overline{o}_0 = 0 \quad (2.9)
$$

> üí° **Exemplo Num√©rico:**
>
> Seja $\alpha = 0.1$. Vamos calcular os primeiros valores de $\overline{o}_n$ usando a Equa√ß√£o (2.9):
>
> *   $\overline{o}_0 = 0$
> *   $\overline{o}_1 = \overline{o}_0 + \alpha(1 - \overline{o}_0) = 0 + 0.1(1 - 0) = 0.1$
> *   $\overline{o}_2 = \overline{o}_1 + \alpha(1 - \overline{o}_1) = 0.1 + 0.1(1 - 0.1) = 0.1 + 0.1 * 0.9 = 0.19$
> *   $\overline{o}_3 = \overline{o}_2 + \alpha(1 - \overline{o}_2) = 0.19 + 0.1(1 - 0.19) = 0.19 + 0.1 * 0.81 = 0.271$
> *   $\overline{o}_4 = \overline{o}_3 + \alpha(1 - \overline{o}_3) = 0.271 + 0.1(1 - 0.271) = 0.271 + 0.1 * 0.729 = 0.3439$
>
> Agora, vamos calcular os primeiros valores de $\beta_n$ usando a Equa√ß√£o (2.8):
>
> *   $\beta_1 = \alpha / \overline{o}_1 = 0.1 / 0.1 = 1$
> *   $\beta_2 = \alpha / \overline{o}_2 = 0.1 / 0.19 \approx 0.526$
> *   $\beta_3 = \alpha / \overline{o}_3 = 0.1 / 0.271 \approx 0.369$
> *   $\beta_4 = \alpha / \overline{o}_4 = 0.1 / 0.3439 \approx 0.291$
>
> Observe que $\beta_n$ diminui com o tempo, mas n√£o t√£o rapidamente quanto $1/n$. Isso permite uma adapta√ß√£o inicial r√°pida e, em seguida, uma converg√™ncia mais est√°vel.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metro
> alpha = 0.1
>
> # C√°lculo de o_n
> o_values = [0]
> o = 0
> for n in range(1, 101):
>     o = o + alpha * (1 - o)
>     o_values.append(o)
>
> # C√°lculo de beta_n
> beta_values = [alpha / o for o in o_values[1:]]
>
> # Plotagem de o_n
> plt.figure(figsize=(12, 6))
> plt.plot(o_values, label='overline{o}_n')
> plt.xlabel('n')
> plt.ylabel('overline{o}_n')
> plt.title('Comportamento de overline{o}_n')
> plt.grid(True)
> plt.legend()
> plt.show()
>
> # Plotagem de beta_n
> plt.figure(figsize=(12, 6))
> plt.plot(beta_values, label='beta_n = alpha / overline{o}_n')
> plt.xlabel('n')
> plt.ylabel('beta_n')
> plt.title('Comportamento de beta_n')
> plt.grid(True)
> plt.legend()
> plt.show()
> ```

Podemos ainda analisar o comportamento de $ \overline{o}_n $ √† medida que $ n $ aumenta.

**Lema 3**
A sequ√™ncia $ \overline{o}_n $ definida pela Equa√ß√£o (2.9) converge para 1 √† medida que $ n $ tende ao infinito.

*Demonstra√ß√£o:*
A Equa√ß√£o (2.9) pode ser reescrita como:

$ \overline{o}_n = \overline{o}_{n-1} + \alpha - \alpha\overline{o}_{n-1} = (1 - \alpha)\overline{o}_{n-1} + \alpha $

Esta √© uma rela√ß√£o de recorr√™ncia linear de primeira ordem.  Podemos encontrar o ponto fixo resolvendo $ x = (1 - \alpha)x + \alpha $, o que implica $ \alpha x = \alpha $, ent√£o $ x = 1 $.

Para demonstrar a converg√™ncia, podemos analisar a diferen√ßa entre $ \overline{o}_n $ e seu ponto fixo (1):

$ 1 - \overline{o}_n = 1 - [(1 - \alpha)\overline{o}_{n-1} + \alpha] = (1 - \alpha)(1 - \overline{o}_{n-1}) $

Aplicando recursivamente, temos:

$ 1 - \overline{o}_n = (1 - \alpha)^n (1 - \overline{o}_0) = (1 - \alpha)^n $

Como $ 0 < \alpha \leq 1 $, temos $ \lim_{n \to \infty} (1 - \alpha)^n = 0 $.

Portanto, $ \lim_{n \to \infty} \overline{o}_n = 1 $. ‚ñ†

### Valores Iniciais Otimistas
Como discutido anteriormente, as estimativas de valor de a√ß√£o iniciais podem influenciar o comportamento de explora√ß√£o [^34]. Definir valores iniciais otimistas, como definir $Q_1(a)$ para um valor significativamente alto (por exemplo, +5), incentiva a explora√ß√£o, pois as recompensas iniciais ser√£o provavelmente menores do que a estimativa inicial, levando o agente a tentar diferentes a√ß√µes. No entanto, essa abordagem √© mais eficaz em problemas *estacion√°rios* e pode n√£o ser t√£o ben√©fica em problemas *n√£o estacion√°rios*, pois o impulso de explora√ß√£o √© tempor√°rio [^34].

![Comparative performance of optimistic vs. realistic action-value initialization in a 10-armed testbed, illustrating the impact on optimal action selection over time.](./../images/image3.png)

> üí° **Exemplo Num√©rico:**
>
> Consideremos um problema com duas a√ß√µes. Inicializamos os valores de a√ß√£o para $Q_1(1) = 5$ e $Q_1(2) = 5$ (valores iniciais otimistas). Suponha que a recompensa real para a a√ß√£o 1 seja 1 e a recompensa real para a a√ß√£o 2 seja 0. Usando $\alpha = 0.1$:
>
> *   Ap√≥s selecionar a a√ß√£o 1: $Q_2(1) = 5 + 0.1(1 - 5) = 4.6$. O agente continua a explorar outras a√ß√µes porque $Q_2(1)$ ainda √© relativamente alto.
> *   Ap√≥s selecionar a a√ß√£o 2: $Q_2(2) = 5 + 0.1(0 - 5) = 4.5$.
>
> Em um problema *estacion√°rio*, isso levaria o agente a explorar ambas as a√ß√µes por um tempo consider√°vel at√© que suas estimativas de valor se aproximem de suas recompensas reais.
>
> Agora, suponha que, ap√≥s 100 etapas, a recompensa da a√ß√£o 1 mude para 6.  Como $Q(1)$ j√° est√° baixo (perto do valor real antigo), ele come√ßar√° a aumentar em dire√ß√£o ao novo valor de 6 rapidamente.
>
> Em contraste, se os valores iniciais fossem 0, a explora√ß√£o inicial seria menor, mas a adapta√ß√£o √† mudan√ßa na recompensa seria mais r√°pida.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> alpha = 0.1
> initial_value = 5
> rewards_action1 = [1] * 100 + [6] * 100  # Mudan√ßa na recompensa da a√ß√£o 1 em t=100
> rewards_action2 = [0] * 200
>
> # Inicializa√ß√£o otimista
> Q_action1_optimistic = [initial_value]
> Q = initial_value
> for i, r in enumerate(rewards_action1):
>     Q = Q + alpha * (r - Q)
>     Q_action1_optimistic.append(Q)
>
> # Inicializa√ß√£o neutra
> Q_action1_neutral = [0]
> Q = 0
> for i, r in enumerate(rewards_action1):
>     Q = Q + alpha * (r - Q)
>     Q_action1_neutral.append(Q)
>
> # Plotagem
> plt.figure(figsize=(12, 6))
> plt.plot(Q_action1_optimistic, label='Inicializa√ß√£o Otimista (Q=5)')
> plt.plot(Q_action1_neutral, label='Inicializa√ß√£o Neutra (Q=0)')
> plt.xlabel('Tentativa')
> plt.ylabel('Estimativa de Valor da A√ß√£o 1 (Q)')
> plt.title('Compara√ß√£o de Inicializa√ß√µes em Ambiente N√£o Estacion√°rio')
> plt.axvline(x=100, color='r', linestyle='--', label='Mudan√ßa na Recompensa da A√ß√£o 1')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

Para formalizar essa intui√ß√£o, considere o seguinte cen√°rio:

**Teorema 4**
Em um problema n√£o-estacion√°rio onde a recompensa m√©dia de uma a√ß√£o √≥tima diminui drasticamente ap√≥s um tempo $t^*$, o uso de valores iniciais otimistas pode levar a um desempenho sub√≥timo em compara√ß√£o com o uso de valores iniciais neutros (por exemplo, 0).

*Demonstra√ß√£o (Esbo√ßo)*
Suponha que inicialmente uma a√ß√£o $a^*$ tenha uma recompensa m√©dia alta, levando a uma estimativa de valor $Q(a^*)$ alta devido ao valor inicial otimista. Ap√≥s $t^*$, a recompensa m√©dia de $a^*$ diminui.  Devido √† estimativa inicial inflacionada, pode levar um tempo consider√°vel para $Q(a^*)$ se ajustar √† nova recompensa m√©dia, durante o qual o agente continua explorando outras a√ß√µes. No entanto, se o valor inicial fosse neutro, o agente se adaptaria mais rapidamente √† mudan√ßa na recompensa m√©dia de $a^*$, potencialmente convergindo para uma a√ß√£o diferente mais rapidamente e alcan√ßando um desempenho melhor no longo prazo.

### Conclus√£o
O rastreamento de problemas *n√£o estacion√°rios* requer uma adapta√ß√£o dos m√©todos de valor de a√ß√£o para dar mais peso √†s recompensas recentes. O uso de um tamanho de passo constante $ \alpha $ fornece uma maneira eficaz de alcan√ßar esse objetivo. Ao escolher cuidadosamente o valor de $ \alpha $, o algoritmo pode se adaptar √†s mudan√ßas no ambiente enquanto ainda mant√©m um aprendizado robusto. Embora os valores iniciais otimistas possam ajudar na explora√ß√£o, eles s√£o menos eficazes em problemas *n√£o estacion√°rios* em compara√ß√£o com os *estacion√°rios*. Portanto, a sele√ß√£o apropriada do tamanho de passo √© crucial para um desempenho bem-sucedido em problemas *n√£o estacion√°rios* de *k-armed bandit* [^33].

### Refer√™ncias
[^32]: Se√ß√£o 2.5 do texto original
[^33]: Se√ß√£o 2.5 do texto original
[^34]: Se√ß√£o 2.6 do texto original
[^35]: Se√ß√£o 2.7 do texto original
<!-- END -->