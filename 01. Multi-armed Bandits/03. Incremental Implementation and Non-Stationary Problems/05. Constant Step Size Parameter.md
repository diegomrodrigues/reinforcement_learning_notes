## Rastreamento de Problemas N√£o Estacion√°rios com Tamanho de Passo Constante

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre **m√©todos de valor de a√ß√£o** e suas implementa√ß√µes, este cap√≠tulo se aprofunda no cen√°rio de **problemas n√£o estacion√°rios**. Em particular, exploraremos o uso de um **tamanho de passo constante** ($\alpha \in (0, 1]$) para lidar com situa√ß√µes onde as propriedades de recompensa das a√ß√µes mudam ao longo do tempo [^32]. Essa abordagem, em contraste com o m√©todo de m√©dias amostrais, permite que o algoritmo d√™ maior peso √†s recompensas recentes, adaptando-se a mudan√ßas no ambiente.

### Tamanho de Passo Constante para Problemas N√£o Estacion√°rios

Nos cap√≠tulos anteriores, foram discutidos m√©todos de m√©dia para estimar os valores das a√ß√µes, adequados para problemas de *bandit* estacion√°rios. No entanto, em muitos cen√°rios de aprendizado por refor√ßo, nos deparamos com problemas que s√£o efetivamente **n√£o estacion√°rios**, ou seja, as probabilidades de recompensa das a√ß√µes mudam ao longo do tempo [^32].

Nessas situa√ß√µes, √© mais interessante dar maior peso √†s recompensas recentes em vez de recompensas passadas distantes. Uma das maneiras mais populares de fazer isso √© usar um **tamanho de passo constante**, denotado por $\alpha$ [^32]. A regra de atualiza√ß√£o incremental para a m√©dia $Q_n$ das recompensas passadas $n-1$ √© modificada para:

$$Q_{n+1} = Q_n + \alpha[R_n - Q_n] \qquad (2.5)$$

onde $\alpha \in (0, 1]$ √© constante [^32]. Isso resulta em $Q_{n+1}$ sendo uma m√©dia ponderada das recompensas passadas e da estimativa inicial $Q_1$:

$$Q_{n+1} = Q_n + \alpha[R_n - Q_n] = \alpha R_n + (1 - \alpha) Q_n$$

> üí° **Exemplo Num√©rico:**
>
> Suponha que $Q_1 = 10$ (nossa estimativa inicial do valor de uma a√ß√£o) e recebemos uma recompensa $R_1 = 15$ ap√≥s selecionar essa a√ß√£o. Se usarmos um tamanho de passo constante $\alpha = 0.1$, a atualiza√ß√£o seria:
>
> $Q_2 = Q_1 + \alpha[R_1 - Q_1] = 10 + 0.1[15 - 10] = 10 + 0.1[5] = 10 + 0.5 = 10.5$
>
> Agora, digamos que recebemos outra recompensa $R_2 = 12$. A atualiza√ß√£o seria:
>
> $Q_3 = Q_2 + \alpha[R_2 - Q_2] = 10.5 + 0.1[12 - 10.5] = 10.5 + 0.1[1.5] = 10.5 + 0.15 = 10.65$
>
> Observe como a estimativa $Q_n$ est√° se adaptando √†s recompensas recebidas, dando mais peso √†s recompensas recentes devido ao $\alpha$ constante.

Expandindo recursivamente a equa√ß√£o, obtemos:

$$Q_{n+1} = \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}]$$
$$Q_{n+1} = \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 Q_{n-1}$$

Continuando a expans√£o at√© $Q_1$, temos:

$$Q_{n+1} = \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 \alpha R_{n-2} + \dots + (1 - \alpha)^{n-1} \alpha R_1 + (1 - \alpha)^n Q_1$$

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior com $\alpha = 0.1$, vamos calcular $Q_3$ expandindo a equa√ß√£o at√© $Q_1$:
>
> $Q_3 = \alpha R_2 + (1 - \alpha) \alpha R_1 + (1 - \alpha)^2 Q_1 = 0.1(12) + (1 - 0.1)(0.1)(15) + (1 - 0.1)^2(10) = 0.1(12) + 0.9(0.1)(15) + 0.9^2(10) = 1.2 + 0.135 + 8.1 = 10.65$
>
> Este resultado coincide com o que obtivemos na atualiza√ß√£o incremental.  Podemos ver como cada recompensa √© ponderada de forma diferente com base em qu√£o recente ela √©.

Reescrevendo a equa√ß√£o na forma de somat√≥rio:

$$Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha(1 - \alpha)^{n-i} R_i \qquad (2.6)$$

Chamamos isso de uma **m√©dia ponderada** porque a soma dos pesos √© $(1 - \alpha)^n + \sum_{i=1}^{n} \alpha(1 - \alpha)^{n-i} = 1$ [^33]. O peso $\alpha(1 - \alpha)^{n-i}$ dado √† recompensa $R_i$ depende de quantos passos atr√°s, $n - i$, foi observada [^33]. A quantidade $1 - \alpha$ √© menor que 1, ent√£o o peso dado a $R_i$ diminui √† medida que o n√∫mero de recompensas intervenientes aumenta. De fato, o peso decai exponencialmente de acordo com o expoente em $1 - \alpha$ [^33]. Por conseguinte, isso √†s vezes √© chamado de **m√©dia ponderada exponencialmente**.

Vamos provar que a soma dos pesos √© de fato igual a 1.

**Proposi√ß√£o 0** A soma dos pesos na Equa√ß√£o 2.6 √© igual a 1.

*Prova:*
I. Precisamos provar que:
$$(1 - \alpha)^n + \sum_{i=1}^{n} \alpha(1 - \alpha)^{n-i} = 1$$
II. Vamos manipular a soma:
$$\sum_{i=1}^{n} \alpha(1 - \alpha)^{n-i} = \alpha \sum_{i=1}^{n} (1 - \alpha)^{n-i}$$
III. Fa√ßa uma mudan√ßa de vari√°vel: $j = n - i$. Quando $i = 1$, $j = n - 1$. Quando $i = n$, $j = 0$. Ent√£o:
$$\alpha \sum_{i=1}^{n} (1 - \alpha)^{n-i} = \alpha \sum_{j=0}^{n-1} (1 - \alpha)^{j}$$
IV. A soma $\sum_{j=0}^{n-1} (1 - \alpha)^{j}$ √© uma s√©rie geom√©trica com primeiro termo $a = 1$ e raz√£o $r = (1 - \alpha)$. A soma de uma s√©rie geom√©trica finita √© dada por: $S_n = \frac{a(1 - r^n)}{1 - r}$. Portanto:
$$\sum_{j=0}^{n-1} (1 - \alpha)^{j} = \frac{1 - (1 - \alpha)^n}{1 - (1 - \alpha)} = \frac{1 - (1 - \alpha)^n}{\alpha}$$
V. Substituindo isso de volta na express√£o original:
$$(1 - \alpha)^n + \sum_{i=1}^{n} \alpha(1 - \alpha)^{n-i} = (1 - \alpha)^n + \alpha \left(\frac{1 - (1 - \alpha)^n}{\alpha}\right)$$
VI. Simplificando:
$$(1 - \alpha)^n + 1 - (1 - \alpha)^n = 1$$
VII. Portanto, a soma dos pesos √© igual a 1. ‚ñ†

Al√©m da representa√ß√£o em somat√≥rio, podemos expressar $Q_{n+1}$ de forma compacta utilizando uma m√©dia m√≥vel exponencial (EMA).

**Proposi√ß√£o 1** A Equa√ß√£o 2.5 representa uma m√©dia m√≥vel exponencial (EMA) de primeira ordem com fator de suaviza√ß√£o $\alpha$.

*Prova:* A forma recursiva da Equa√ß√£o 2.5, $Q_{n+1} = Q_n + \alpha[R_n - Q_n]$, √© a defini√ß√£o padr√£o de uma EMA. Portanto, a Equa√ß√£o 2.5 representa uma m√©dia m√≥vel exponencial de primeira ordem.

### Compara√ß√£o com M√©dias Amostrais

Diferentemente do m√©todo de m√©dias amostrais, que atribui pesos iguais a todas as recompensas observadas, o tamanho de passo constante $\alpha$ d√° maior import√¢ncia √†s recompensas mais recentes. Essa caracter√≠stica √© crucial em ambientes n√£o estacion√°rios, pois permite que o algoritmo se adapte rapidamente a mudan√ßas nas distribui√ß√µes de recompensa.

No m√©todo de m√©dias amostrais, o tamanho do passo √© dado por $\frac{1}{n}$, onde $n$ √© o n√∫mero de vezes que uma a√ß√£o foi selecionada [^33]. Este tamanho de passo diminui com o tempo, garantindo que o algoritmo convirja para o valor verdadeiro da a√ß√£o em ambientes estacion√°rios, conforme garantido pela lei dos grandes n√∫meros [^33]. No entanto, essa converg√™ncia pode ser prejudicial em ambientes n√£o estacion√°rios, pois o algoritmo se torna cada vez menos sens√≠vel √†s mudan√ßas.

Para quantificar essa sensibilidade, podemos analisar a taxa de converg√™ncia do tamanho do passo constante em compara√ß√£o com a m√©dia amostral.

**Proposi√ß√£o 2** O tamanho de passo constante $\alpha$ resulta em uma taxa de converg√™ncia limitada, enquanto a m√©dia amostral converge para zero.

*Prova:* Por defini√ß√£o, $\alpha$ permanece constante e diferente de zero. A m√©dia amostral, $\frac{1}{n}$, tende a zero √† medida que $n$ tende ao infinito. Portanto, a taxa de converg√™ncia do tamanho de passo constante √© limitada inferiormente por $\alpha$, enquanto a taxa de converg√™ncia da m√©dia amostral tende a zero.

> üí° **Exemplo Num√©rico:**
>
> Considere uma a√ß√£o em um problema de bandit. Usando m√©dias amostrais, ap√≥s 10 sele√ß√µes da a√ß√£o, o tamanho do passo √© $\frac{1}{10} = 0.1$. Ap√≥s 100 sele√ß√µes, o tamanho do passo diminui para $\frac{1}{100} = 0.01$. Ap√≥s 1000 sele√ß√µes, torna-se $\frac{1}{1000} = 0.001$.  Vemos que a taxa de converg√™ncia est√° diminuindo.
>
> Agora, se usarmos um tamanho de passo constante de $\alpha = 0.1$, a taxa de converg√™ncia permanece constante em $0.1$, independentemente do n√∫mero de sele√ß√µes da a√ß√£o. Isso significa que, mesmo ap√≥s 1000 sele√ß√µes, o algoritmo ainda dar√° um peso significativo √†s recompensas recentes, permitindo que ele se adapte a mudan√ßas no ambiente.

Essa proposi√ß√£o refor√ßa a ideia de que o tamanho de passo constante mant√©m uma sensibilidade cont√≠nua √†s novas recompensas, enquanto a m√©dia amostral diminui progressivamente sua capacidade de adapta√ß√£o.

### Condi√ß√µes de Converg√™ncia

√Äs vezes, √© conveniente variar o par√¢metro de tamanho de passo de etapa para etapa. Seja $\alpha_n(a)$ denotando o par√¢metro de tamanho de passo usado para processar a recompensa recebida ap√≥s a *n*-√©sima sele√ß√£o da a√ß√£o *a* [^33]. Como observamos, a escolha $\alpha_n(a) = \frac{1}{n}$ resulta no m√©todo de m√©dia amostral, que tem garantia de convergir para os valores verdadeiros das a√ß√µes pela lei dos grandes n√∫meros [^33]. Mas, √© claro que a converg√™ncia n√£o √© garantida para todas as escolhas da sequ√™ncia $\{\alpha_n(a)\}$ [^33]. Um resultado bem conhecido na teoria de aproxima√ß√£o estoc√°stica nos d√° as condi√ß√µes necess√°rias para assegurar a converg√™ncia com probabilidade 1 [^33]:

$$\sum_{n=1}^{\infty} \alpha_n(a) = \infty \quad \text{e} \quad \sum_{n=1}^{\infty} \alpha_n^2(a) < \infty \qquad (2.7)$$

A primeira condi√ß√£o √© necess√°ria para garantir que os passos sejam grandes o suficiente para eventualmente superar quaisquer condi√ß√µes iniciais ou flutua√ß√µes aleat√≥rias. A segunda condi√ß√£o garante que, eventualmente, os passos se tornem pequenos o suficiente para assegurar a converg√™ncia [^33].

Note que ambas as condi√ß√µes de converg√™ncia s√£o satisfeitas para o caso da m√©dia amostral, $\alpha_n(a) = \frac{1}{n}$. Vamos provar isso.

**Proposi√ß√£o 2.1** A m√©dia amostral, $\alpha_n(a) = \frac{1}{n}$, satisfaz as condi√ß√µes de converg√™ncia dadas em (2.7).

*Prova:*
I. Precisamos provar que $\sum_{n=1}^{\infty} \frac{1}{n} = \infty$ e $\sum_{n=1}^{\infty} \frac{1}{n^2} < \infty$.
II. A s√©rie harm√¥nica $\sum_{n=1}^{\infty} \frac{1}{n}$ √© conhecida por divergir. Isso pode ser mostrado pelo teste integral ou pela compara√ß√£o com a integral $\int_{1}^{\infty} \frac{1}{x} dx$, que diverge. Portanto, $\sum_{n=1}^{\infty} \frac{1}{n} = \infty$.
III. A s√©rie $\sum_{n=1}^{\infty} \frac{1}{n^2}$ √© uma s√©rie p com $p = 2 > 1$, e √© conhecida por convergir. O valor da soma √© $\frac{\pi^2}{6}$, que √© finito. Portanto, $\sum_{n=1}^{\infty} \frac{1}{n^2} < \infty$.
IV. Como ambas as condi√ß√µes s√£o satisfeitas, a m√©dia amostral satisfaz as condi√ß√µes de converg√™ncia. ‚ñ†

Mas, √© claro que a converg√™ncia n√£o √© garantida para todas as escolhas da sequ√™ncia $\{\alpha_n(a)\}$ [^33]. Um resultado bem conhecido na teoria de aproxima√ß√£o estoc√°stica nos d√° as condi√ß√µes necess√°rias para assegurar a converg√™ncia com probabilidade 1 [^33]:

Note que ambas as condi√ß√µes de converg√™ncia s√£o satisfeitas para o caso da m√©dia amostral, $\alpha_n(a) = \frac{1}{n}$, mas n√£o para o caso do par√¢metro de tamanho de passo constante, $\alpha_n(a) = \alpha$ [^33]. Neste √∫ltimo caso, a segunda condi√ß√£o n√£o √© satisfeita, indicando que as estimativas nunca convergem completamente, mas continuam a variar em resposta √†s recompensas recebidas mais recentemente [^33]. Como mencionamos acima, isso √© realmente desej√°vel em um ambiente n√£o estacion√°rio, e os problemas que s√£o efetivamente n√£o estacion√°rios s√£o os mais comuns no aprendizado por refor√ßo [^33].

Vamos provar que o tamanho de passo constante n√£o satisfaz a segunda condi√ß√£o de converg√™ncia.

**Proposi√ß√£o 2.2** O tamanho de passo constante, $\alpha_n(a) = \alpha$, onde $\alpha \in (0, 1]$, n√£o satisfaz a condi√ß√£o $\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$.

*Prova:*
I. Precisamos provar que $\sum_{n=1}^{\infty} \alpha^2 = \infty$.
II. Como $\alpha$ √© uma constante, $\sum_{n=1}^{\infty} \alpha^2 = \alpha^2 \sum_{n=1}^{\infty} 1$.
III. A soma $\sum_{n=1}^{\infty} 1$ diverge para infinito.
IV. Portanto, $\sum_{n=1}^{\infty} \alpha^2 = \infty$, o que significa que o tamanho de passo constante n√£o satisfaz a condi√ß√£o $\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$. ‚ñ†

√â importante notar que, mesmo que o tamanho do passo constante n√£o satisfa√ßa as condi√ß√µes de converg√™ncia, ele ainda pode ser √∫til em ambientes n√£o estacion√°rios. A n√£o-converg√™ncia garante que o algoritmo continue a aprender e se adaptar a mudan√ßas no ambiente. No entanto, tamb√©m pode levar a flutua√ß√µes nas estimativas. Para mitigar essas flutua√ß√µes, podemos considerar uma abordagem que combina um tamanho de passo constante com um limite inferior para garantir uma certa estabilidade.

**Teorema 3 (Revisado)** Seja $\alpha_n(a) = \min(\alpha, \frac{1}{n})$. Ent√£o, a sequ√™ncia $\{\alpha_n(a)\}$ satisfaz a condi√ß√£o $\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$, mas n√£o satisfaz a condi√ß√£o $\sum_{n=1}^{\infty} \alpha_n(a) = \infty$.

*Prova:*
    I) $\sum_{n=1}^{\infty} \alpha_n(a) = \sum_{n=1}^{\lceil \frac{1}{\alpha} \rceil} \alpha + \sum_{n=\lceil \frac{1}{\alpha} \rceil + 1}^{\infty} \frac{1}{n} $ onde $\frac{1}{n} < \alpha$. Assim, $\sum_{n=1}^{\infty} \alpha_n(a) = \sum_{n=1}^{\infty} \frac{1}{n}$, o que diverge.
    II) $\sum_{n=1}^{\infty} \alpha_n^2(a) = \sum_{n=1}^{\lceil \frac{1}{\alpha} \rceil} \alpha^2 + \sum_{n=\lceil \frac{1}{\alpha} \rceil + 1}^{\infty} \frac{1}{n^2}$. A primeira soma √© finita. A segunda soma converge. Portanto, a soma converge.

VIII. A taxa de aprendizado √© limitada superiormente por $\alpha$.‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar o caso onde $\alpha = 0.2$.  Ent√£o, $N = \lceil \frac{1}{0.2} \rceil = 5$. Isso significa que para $n = 1, 2, 3, 4$, $\alpha_n(a) = 0.2$. Para $n \geq 5$, $\alpha_n(a) = \frac{1}{n}$.
>
> A sequ√™ncia de tamanhos de passo seria: $0.2, 0.2, 0.2, 0.2, 0.2, 0.2, \frac{1}{6}, \frac{1}{7}, \frac{1}{8}, \dots$
>
> Com este m√©todo, mantemos a taxa de aprendizado inicial de 0.2 at√© que tenhamos experimentado a a√ß√£o um certo n√∫mero de vezes (5 vezes neste caso) e, em seguida, come√ßamos a diminuir a taxa de aprendizado.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Define alpha
> alpha = 0.2
>
> # Calculate N
> N = int(np.ceil(1/alpha))
>
> # Generate sequence of step sizes
> step_sizes = [min(alpha, 1/n) for n in range(1, 21)]
>
> # Create the plot
> plt.figure(figsize=(10, 6))
> plt.plot(range(1, 21), step_sizes, marker='o')
> plt.xlabel("Iteration (n)")
> plt.ylabel("Step Size (alpha_n(a))")
> plt.title("Adaptive Step Size: alpha_n(a) = min(alpha, 1/n)")
> plt.grid(True)
> plt.xticks(range(1, 21))
> plt.show()
> ```
>

### Considera√ß√µes Pr√°ticas

Embora sequ√™ncias de par√¢metros de tamanho de passo que atendam √†s condi√ß√µes de converg√™ncia (2.7) sejam frequentemente usadas no trabalho te√≥rico, elas raramente s√£o usadas em aplica√ß√µes e pesquisas emp√≠ricas [^33]. Isso ocorre porque essas sequ√™ncias podem convergir muito lentamente ou precisam de ajuste consider√°vel para obter uma taxa de converg√™ncia satisfat√≥ria [^33].

Para abordar os problemas de converg√™ncia lenta e necessidade de ajuste fino, podemos considerar o uso de um tamanho de passo adaptativo que se ajuste automaticamente com base na variabilidade das recompensas.

**Teorema 4** Um tamanho de passo adaptativo que diminui quando a vari√¢ncia das recompensas aumenta e aumenta quando a vari√¢ncia diminui pode melhorar a estabilidade e a taxa de converg√™ncia em ambientes n√£o estacion√°rios.

*Prova (Esbo√ßo):* A prova envolve derivar uma atualiza√ß√£o para $\alpha_n(a)$ baseada em uma estimativa da vari√¢ncia das recompensas observadas para a a√ß√£o *a*. Um aumento na vari√¢ncia sugere que o ambiente est√° mudando rapidamente, justificando um aumento em $\alpha_n(a)$ para acompanhar as mudan√ßas. Uma diminui√ß√£o na vari√¢ncia sugere que o ambiente est√° se tornando mais est√°vel, justificando uma diminui√ß√£o em $\alpha_n(a)$ para evitar flutua√ß√µes excessivas. A prova formal exigiria mostrar que essa atualiza√ß√£o adaptativa leva a uma redu√ß√£o na vari√¢ncia das estimativas $Q_n(a)$ ao longo do tempo, em compara√ß√£o com um tamanho de passo constante.

> üí° **Exemplo Num√©rico:**
>
> Suponha que estejamos rastreando recompensas para uma determinada a√ß√£o. Inicialmente, as recompensas observadas s√£o $R_1 = 10, R_2 = 12, R_3 = 11, R_4 = 9$. Calculamos a vari√¢ncia amostral dessas recompensas:
>
> $\text{Vari√¢ncia} = \frac{\sum_{i=1}^{4}(R_i - \bar{R})^2}{4-1}$, onde $\bar{R} = \frac{10 + 12 + 11 + 9}{4} = 10.5$
>
> $\text{Vari√¢ncia} = \frac{(10-10.5)^2 + (12-10.5)^2 + (11-10.5)^2 + (9-10.5)^2}{3} = \frac{0.25 + 2.25 + 0.25 + 2.25}{3} = \frac{5}{3} \approx 1.67$
>
> Agora, suponha que, ap√≥s algumas itera√ß√µes, as recompensas se tornem $R_5 = 20, R_6 = 22, R_7 = 19, R_8 = 21$. A vari√¢ncia amostral agora √©:
>
> $\bar{R} = \frac{20 + 22 + 19 + 21}{4} = 20.5$
>
> $\text{Vari√¢ncia} = \frac{(20-20.5)^2 + (22-20.5)^2 + (19-20.5)^2 + (21-20.5)^2}{3} = \frac{0.25 + 2.25 + 2.25 + 0.25}{3} = \frac{5}{3} \approx 1.67$
>
> Um tamanho de passo adaptativo aumentaria $\alpha$ quando a vari√¢ncia fosse alta e diminuiria quando fosse baixa. Por exemplo, podemos usar a seguinte regra de atualiza√ß√£o:
>
> $\alpha_{n+1} = \alpha_n + \beta (\text{Vari√¢ncia}_n - \text{Vari√¢ncia}_{n-1})$
>
> Onde $\beta$ √© uma pequena taxa de aprendizado. Se a vari√¢ncia aumentou, $\alpha$ aumentar√°, e vice-versa.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Simulate rewards with varying variance
> np.random.seed(42)
> rewards1 = np.random.normal(10, 1, 100)  # Low variance
> rewards2 = np.random.normal(15, 5, 100)  # High variance
> rewards = np.concatenate([rewards1, rewards2])
>
> # Adaptive step size parameters
> alpha = 0.1
> beta = 0.01
> variance_history = []
> alpha_history = []
>
> # Calculate rolling variance and adjust alpha
> window_size = 10
> for i in range(window_size, len(rewards)):
>     window = rewards[i-window_size:i]
>     variance = np.var(window)
>     variance_history.append(variance)
>     if i > window_size:
>         alpha += beta * (variance - variance_history[-2])
>         alpha = np.clip(alpha, 0.01, 0.5)  # Clip alpha to a reasonable range
>     alpha_history.append(alpha)
>
> # Plotting
> fig, ax1 = plt.subplots(figsize=(12, 6))
>
> color = 'tab:red'
> ax1.set_xlabel('Iteration')
> ax1.set_ylabel('Variance', color=color)
> ax1.plot(range(window_size*2, len(rewards)), variance_history, color=color)
> ax1.tick_params(axis='y', labelcolor=color)
>
> ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
>
> color = 'tab:blue'
> ax2.set_ylabel('Alpha', color=color)  # we already handled the x-label with ax1
> ax2.plot(range(window_size*2, len(rewards)), alpha_history, color=color)
> ax2.tick_params(axis='y', labelcolor=color)
>
> fig.tight_layout()  # otherwise the right y-label is slightly clipped
> plt.title('Adaptive Step Size Based on Reward Variance')
> plt.show()
> ```

### Conclus√£o

O uso de um tamanho de passo constante $\alpha$ representa uma estrat√©gia eficaz para lidar com problemas de *bandit* n√£o estacion√°rios. Ao dar maior peso √†s recompensas recentes, o algoritmo pode se adaptar rapidamente a mudan√ßas no ambiente, mantendo uma estimativa precisa dos valores das a√ß√µes. Embora esse m√©todo n√£o garanta a converg√™ncia para um valor fixo, sua capacidade de adapta√ß√£o o torna uma ferramenta valiosa em cen√°rios din√¢micos de aprendizado por refor√ßo. Abordagens mais avan√ßadas, como tamanhos de passo adaptativos, podem aprimorar ainda mais a estabilidade e a taxa de converg√™ncia em ambientes complexos e n√£o estacion√°rios.

### Refer√™ncias
[^32]: Cap√≠tulo 2 do livro "Reinforcement Learning: An Introduction", Richard S. Sutton and Andrew G. Barto, 2nd edition.
[^33]: Cap√≠tulo 2 do livro "Reinforcement Learning: An Introduction", Richard S. Sutton and Andrew G. Barto, 2nd edition.
<!-- END -->