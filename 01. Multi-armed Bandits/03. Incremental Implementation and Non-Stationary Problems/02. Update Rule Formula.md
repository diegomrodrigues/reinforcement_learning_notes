## Implementa√ß√£o Incremental e a Regra de Atualiza√ß√£o Geral

### Introdu√ß√£o

O cap√≠tulo anterior introduziu o problema do *k-armed bandit* e explorou m√©todos para estimar os valores das a√ß√µes e selecionar a√ß√µes com base nessas estimativas. Uma abordagem direta para calcular as m√©dias amostrais dos valores das a√ß√µes pode ser computacionalmente dispendiosa, especialmente em cen√°rios de longo prazo. Esta se√ß√£o aborda a **implementa√ß√£o incremental**, uma t√©cnica eficiente para atualizar as estimativas dos valores das a√ß√µes [^31]. Al√©m disso, exploraremos como essa implementa√ß√£o incremental se encaixa em uma **regra de atualiza√ß√£o geral** que aparece frequentemente no contexto do aprendizado por refor√ßo [^31].

### Implementa√ß√£o Incremental

A **implementa√ß√£o incremental** √© uma t√©cnica computacionalmente eficiente para calcular m√©dias amostrais sem armazenar todos os recompensas observadas [^31]. Em vez de recalcular a m√©dia a cada passo, ela atualiza a m√©dia existente com base no novo recompensa recebido.

Seja $Q_n$ a estimativa do valor de uma a√ß√£o ap√≥s ela ter sido selecionada $n-1$ vezes e $R_i$ a recompensa recebida ap√≥s a $i$-√©sima sele√ß√£o dessa a√ß√£o. A m√©dia amostral pode ser expressa como:

$$
Q_n = \frac{R_1 + R_2 + \dots + R_{n-1}}{n-1}
$$

A forma incremental de atualizar esta m√©dia √© dada por [^31]:

$$
Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]
$$

Esta equa√ß√£o mostra que a nova estimativa $Q_{n+1}$ √© igual √† estimativa anterior $Q_n$ mais uma fra√ß√£o da diferen√ßa entre a nova recompensa $R_n$ e a estimativa anterior $Q_n$. A fra√ß√£o $\frac{1}{n}$ serve como um **step-size** ou taxa de aprendizado, determinando a magnitude da atualiza√ß√£o.

> üí° **Exemplo Num√©rico:** Suponha que temos uma a√ß√£o que foi selecionada 4 vezes. As recompensas obtidas foram $R_1 = 2$, $R_2 = 3$, $R_3 = 4$ e $R_4 = 5$. Inicialmente, $Q_1 = 0$. Vamos calcular $Q_5$ usando a implementa√ß√£o incremental.
>
> $\text{Passo 1: } Q_2 = Q_1 + \frac{1}{1}(R_1 - Q_1) = 0 + 1(2 - 0) = 2$
>
> $\text{Passo 2: } Q_3 = Q_2 + \frac{1}{2}(R_2 - Q_2) = 2 + \frac{1}{2}(3 - 2) = 2 + 0.5 = 2.5$
>
> $\text{Passo 3: } Q_4 = Q_3 + \frac{1}{3}(R_3 - Q_3) = 2.5 + \frac{1}{3}(4 - 2.5) = 2.5 + \frac{1}{3}(1.5) = 2.5 + 0.5 = 3$
>
> $\text{Passo 4: } Q_5 = Q_4 + \frac{1}{4}(R_4 - Q_4) = 3 + \frac{1}{4}(5 - 3) = 3 + \frac{1}{4}(2) = 3 + 0.5 = 3.5$
>
> Agora, vamos calcular a m√©dia diretamente: $Q_5 = \frac{2 + 3 + 4 + 5}{4} = \frac{14}{4} = 3.5$.
>
> Como podemos ver, a implementa√ß√£o incremental nos d√° o mesmo resultado que calcular a m√©dia diretamente, mas sem a necessidade de armazenar todas as recompensas.

**Proposi√ß√£o 1** A atualiza√ß√£o incremental √© equivalente a calcular a m√©dia amostral diretamente.

*Prova:* Podemos demonstrar a equival√™ncia por indu√ß√£o. Para $n=1$, $Q_2 = Q_1 + \frac{1}{1}(R_1 - Q_1) = R_1$, assumindo que $Q_1$ √© inicializado apropriadamente (e.g., $Q_1=0$ se nenhuma informa√ß√£o inicial estiver dispon√≠vel). Agora, assuma que $Q_n = \frac{R_1 + R_2 + \dots + R_{n-1}}{n-1}$ seja verdadeiro. Ent√£o,
$Q_{n+1} = Q_n + \frac{1}{n}(R_n - Q_n) = \frac{R_1 + \dots + R_{n-1}}{n-1} + \frac{1}{n}(R_n - \frac{R_1 + \dots + R_{n-1}}{n-1}) = \frac{n(R_1 + \dots + R_{n-1}) + (n-1)R_n - (R_1 + \dots + R_{n-1})}{n(n-1)} = \frac{(n-1)(R_1 + \dots + R_{n-1}) + (n-1)R_n}{n(n-1)} = \frac{R_1 + \dots + R_n}{n}$. Portanto, a equival√™ncia √© mantida. $\blacksquare$

### Regra de Atualiza√ß√£o Geral

A regra de atualiza√ß√£o incremental apresentada acima √© um caso espec√≠fico de uma **regra de atualiza√ß√£o geral** que aparece frequentemente em aprendizado por refor√ßo. Esta regra geral tem a seguinte forma [^31]:

```
NewEstimate ‚Üê OldEstimate + StepSize * [Target - OldEstimate]
```

Nesta regra:
*   `NewEstimate` √© a nova estimativa atualizada.
*   `OldEstimate` √© a estimativa anterior.
*   `StepSize` √© um par√¢metro que controla a magnitude da atualiza√ß√£o.
*   `Target` √© o valor para o qual a estimativa est√° sendo atualizada.
*   `[Target - OldEstimate]` representa o **erro** na estimativa, ou seja, a diferen√ßa entre o valor desejado e a estimativa atual [^31].

No contexto da implementa√ß√£o incremental, `OldEstimate` √© $Q_n$, `Target` √© $R_n$, e `StepSize` √© $\frac{1}{n}$. A express√£o `[Target - OldEstimate]` representa o erro na estimativa, e o `StepSize` controla a magnitude da corre√ß√£o aplicada √† estimativa anterior.

A beleza dessa regra geral √© sua aplicabilidade em uma variedade de contextos de aprendizado. Diferentes algoritmos e problemas podem ser expressos usando esta mesma estrutura, alterando apenas a defini√ß√£o de `StepSize` e `Target`.

> üí° **Exemplo Num√©rico:** Considere um problema onde estamos tentando prever a temperatura amanh√£. Nossa `OldEstimate` √© 25 graus Celsius. Recebemos a informa√ß√£o de que a temperatura real amanh√£ (`Target`) foi de 28 graus Celsius. Se usarmos um `StepSize` de 0.1, nossa `NewEstimate` ser√°:
>
> $\text{NewEstimate} = 25 + 0.1 * (28 - 25) = 25 + 0.1 * 3 = 25 + 0.3 = 25.3$
>
> Se usarmos um `StepSize` de 0.5:
>
> $\text{NewEstimate} = 25 + 0.5 * (28 - 25) = 25 + 0.5 * 3 = 25 + 1.5 = 26.5$
>
> Um `StepSize` maior (0.5) fez com que a estimativa mudasse mais em dire√ß√£o ao `Target` do que um `StepSize` menor (0.1).

### Significado do `StepSize`

O par√¢metro `StepSize` desempenha um papel crucial na regra de atualiza√ß√£o geral [^31]. Ele determina a magnitude da atualiza√ß√£o que √© aplicada √† `OldEstimate`. Um `StepSize` grande resulta em atualiza√ß√µes r√°pidas, mas tamb√©m pode levar a instabilidade e oscila√ß√µes. Um `StepSize` pequeno leva a atualiza√ß√µes mais lentas, mas pode resultar em uma converg√™ncia mais suave e est√°vel.

Na implementa√ß√£o incremental, o `StepSize` √© $\frac{1}{n}$, onde $n$ √© o n√∫mero de vezes que uma a√ß√£o foi selecionada. Isso significa que, √† medida que a a√ß√£o √© selecionada mais vezes, o `StepSize` diminui, dando menos peso √†s novas recompensas e mais peso √†s recompensas passadas. Esta abordagem √© apropriada para problemas **estacion√°rios**, onde a distribui√ß√£o de recompensas n√£o muda ao longo do tempo [^32].

Entretanto, para problemas **n√£o-estacion√°rios**, onde a distribui√ß√£o de recompensas pode mudar ao longo do tempo, √© prefer√≠vel utilizar um `StepSize` constante, denotado por $\alpha$, onde $0 < \alpha \leq 1$. Um `StepSize` constante permite que o algoritmo se adapte a mudan√ßas na distribui√ß√£o de recompensas, dando mais peso √†s recompensas mais recentes.

> üí° **Exemplo Num√©rico:** Suponha que estamos tentando estimar o valor de um an√∫ncio online (CTR - Click Through Rate). Inicialmente, a CTR √© de 0.01 (1%). De repente, a campanha publicit√°ria √© otimizada, e a CTR come√ßa a aumentar.
>
> **Cen√°rio 1: StepSize = 1/n**
>
> *   Ap√≥s 10 intera√ß√µes, a CTR observada √© 0.02. $Q_{11} = 0.01 + \frac{1}{10} (0.02 - 0.01) = 0.011$
> *   Ap√≥s 100 intera√ß√µes, a CTR observada √© 0.03. $Q_{101} = 0.011 + \frac{1}{100} (0.03 - 0.011) = 0.011 + 0.00019 = 0.01119$
>
> A estimativa est√° se movendo muito lentamente para capturar a mudan√ßa.
>
> **Cen√°rio 2: StepSize = alpha = 0.1**
>
> *   Ap√≥s 10 intera√ß√µes, a CTR observada √© 0.02. $Q_{11} = 0.01 + 0.1 (0.02 - 0.01) = 0.01 + 0.001 = 0.011$
> *   Ap√≥s 100 intera√ß√µes, a CTR observada √© 0.03. $Q_{101} = 0.011 + 0.1 (0.03 - 0.011) = 0.011 + 0.0019 = 0.0129$
>
> Embora a atualiza√ß√£o inicial seja semelhante, o `StepSize` constante continua a dar peso √†s novas informa√ß√µes, permitindo que o algoritmo rastreie a mudan√ßa na CTR de forma mais eficaz.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Simula√ß√£o para demonstrar a diferen√ßa entre StepSize = 1/n e StepSize = alpha
> np.random.seed(42)
>
> n_interactions = 200
> true_ctr = np.concatenate([np.full(100, 0.01), np.full(100, 0.03)]) # CTR muda ap√≥s 100 itera√ß√µes
>
> # StepSize = 1/n
> q_values_incremental = [0.01]
> for n in range(1, n_interactions + 1):
>     reward = true_ctr[n-1] + np.random.normal(0, 0.005) # Adiciona ru√≠do √† CTR real
>     q_values_incremental.append(q_values_incremental[-1] + (1/n) * (reward - q_values_incremental[-1]))
>
> # StepSize = alpha = 0.1
> q_values_alpha = [0.01]
> alpha = 0.1
> for n in range(1, n_interactions + 1):
>     reward = true_ctr[n-1] + np.random.normal(0, 0.005)
>     q_values_alpha.append(q_values_alpha[-1] + alpha * (reward - q_values_alpha[-1]))
>
> plt.figure(figsize=(10, 6))
> plt.plot(q_values_incremental[1:], label='StepSize = 1/n')
> plt.plot(q_values_alpha[1:], label='StepSize = alpha = 0.1')
> plt.plot(true_ctr, label='True CTR', linestyle='--')
> plt.xlabel('Intera√ß√£o')
> plt.ylabel('CTR Estimada')
> plt.title('Compara√ß√£o de StepSizes em um Ambiente N√£o-Estacion√°rio')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

**Prova (Converg√™ncia da M√©dia com StepSize Constante):** Vamos mostrar como a estimativa $Q_{n+1}$ pode ser expressa como uma m√©dia ponderada das recompensas passadas quando usamos um step size constante $\alpha$.

I. Come√ßamos com a regra de atualiza√ß√£o:
   $$Q_{n+1} = Q_n + \alpha[R_n - Q_n]$$

II. Podemos reorganizar isso para expressar $Q_{n+1}$ em termos de $Q_n$ e $R_n$:
    $$Q_{n+1} = (1 - \alpha)Q_n + \alpha R_n$$

III. Agora, expandimos $Q_n$ usando a mesma regra de atualiza√ß√£o, substituindo $n$ por $n-1$:
     $$Q_n = (1 - \alpha)Q_{n-1} + \alpha R_{n-1}$$

IV. Substitu√≠mos a express√£o de $Q_n$ na equa√ß√£o para $Q_{n+1}$:
    $$Q_{n+1} = (1 - \alpha)[(1 - \alpha)Q_{n-1} + \alpha R_{n-1}] + \alpha R_n$$
    $$Q_{n+1} = (1 - \alpha)^2 Q_{n-1} + \alpha (1 - \alpha) R_{n-1} + \alpha R_n$$

V. Continuando a expandir recursivamente $Q_{n-1}, Q_{n-2}, \dots, Q_1$:
    $$Q_{n+1} = (1 - \alpha)^n Q_1 + \alpha \sum_{i=1}^{n} (1 - \alpha)^{n-i} R_i$$

VI. Esta equa√ß√£o mostra que $Q_{n+1}$ √© uma m√©dia ponderada de todas as recompensas passadas $R_i$, onde o peso de cada recompensa diminui exponencialmente √† medida que a recompensa fica mais antiga. O peso da recompensa $R_i$ √© $\alpha (1 - \alpha)^{n-i}$, e o peso da estimativa inicial $Q_1$ diminui exponencialmente com $n$. Observe que a soma dos pesos das recompensas √©:
    $$\sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} = \alpha \sum_{k=0}^{n-1} (1 - \alpha)^k = \alpha \frac{1 - (1 - \alpha)^n}{1 - (1 - \alpha)} = 1 - (1 - \alpha)^n$$
    Assim, o peso da estimativa inicial $Q_1$ √© $(1 - \alpha)^n$, que tende a 0 √† medida que $n$ aumenta, desde que $0 < \alpha \leq 1$.

VII. Portanto, $Q_{n+1}$ converge para uma m√©dia ponderada das recompensas passadas, com recompensas mais recentes tendo maior peso, tornando-se adequada para ambientes n√£o estacion√°rios. $\blacksquare$

### Conclus√£o

A implementa√ß√£o incremental fornece uma maneira eficiente de atualizar as estimativas dos valores das a√ß√µes sem exigir um armazenamento excessivo e computa√ß√£o. Ele tamb√©m ilustra a utilidade da regra de atualiza√ß√£o geral. O `StepSize` tem um impacto profundo no comportamento do algoritmo de aprendizagem, e a escolha apropriada de `StepSize` depende das caracter√≠sticas do problema sendo resolvido. Nos cap√≠tulos seguintes, exploraremos outros m√©todos para escolher o `StepSize`, incluindo abordagens que s√£o adequadas para problemas **n√£o-estacion√°rios**. $\blacksquare$

**Teorema 1** Para um `StepSize` constante $\alpha$, a atualiza√ß√£o incremental converge para a m√©dia das recompensas se e somente se a sequ√™ncia de recompensas √© estacion√°ria e limitada.

*Prova:* (Esbo√ßo) A prova envolve mostrar que a vari√¢ncia da estimativa $Q_n$ diminui ao longo do tempo quando a sequ√™ncia de recompensas √© estacion√°ria e limitada. A converg√™ncia pode ser analisada usando resultados da teoria da converg√™ncia estoc√°stica. Se a sequ√™ncia n√£o for estacion√°ria, a estimativa continuar√° a flutuar em resposta √†s mudan√ßas nas recompensas. $\blacksquare$

### Refer√™ncias
[^31]: Section 2.4 of the provided text.
[^32]: Section 2.5 of the provided text.
<!-- END -->