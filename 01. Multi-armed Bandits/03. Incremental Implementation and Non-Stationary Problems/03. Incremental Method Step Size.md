## Implementa√ß√£o Incremental com Tamanho de Passo Vari√°vel para Ambientes Estacion√°rios

### Introdu√ß√£o
Este cap√≠tulo explora a implementa√ß√£o incremental de m√©todos de **action-value** para o problema do *k-armed bandit*, com foco especial no papel do **tamanho do passo** no aprendizado em ambientes estacion√°rios. Conforme introduzido anteriormente [^1], os m√©todos de **action-value** estimam os valores das a√ß√µes com base nas recompensas observadas, e a implementa√ß√£o incremental oferece uma maneira computacionalmente eficiente de atualizar essas estimativas.

### Conceitos Fundamentais

A implementa√ß√£o incremental oferece uma maneira eficiente de calcular a m√©dia das recompensas observadas para estimar o valor das a√ß√µes [^6]. Em vez de armazenar todas as recompensas e recalcul√°-las a cada passo, a atualiza√ß√£o incremental utiliza apenas a estimativa anterior, o n√∫mero de vezes que a a√ß√£o foi selecionada e a nova recompensa.

A regra de atualiza√ß√£o incremental √© dada por [^7]:

$$Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]$$

onde:

*   $Q_{n+1}$ √© a nova estimativa do valor da a√ß√£o ap√≥s observar a *n*-√©sima recompensa.
*   $Q_n$ √© a estimativa anterior do valor da a√ß√£o.
*   $R_n$ √© a *n*-√©sima recompensa observada ap√≥s selecionar a a√ß√£o.
*   $n$ √© o n√∫mero de vezes que a a√ß√£o foi selecionada at√© o momento.

> üí° **Exemplo Num√©rico:** Imagine que a estimativa inicial do valor de uma a√ß√£o ($Q_1$) √© 5. Ap√≥s selecionar essa a√ß√£o uma vez ($n=1$), voc√™ recebe uma recompensa de 10 ($R_1$). Usando a regra de atualiza√ß√£o incremental:
>
> $Q_{2} = 5 + \frac{1}{1} [10 - 5] = 5 + 5 = 10$.
>
> Agora, a estimativa do valor da a√ß√£o foi atualizada para 10. Se na segunda vez que voc√™ selecionar essa a√ß√£o ($n=2$), voc√™ receber uma recompensa de 6 ($R_2$), a atualiza√ß√£o seria:
>
> $Q_{3} = 10 + \frac{1}{2} [6 - 10] = 10 + \frac{1}{2} [-4] = 10 - 2 = 8$.
>
> A estimativa do valor da a√ß√£o agora √© 8, refletindo a m√©dia das recompensas observadas at√© o momento.

A equa√ß√£o acima pode ser generalizada para:

$$NewEstimate \leftarrow OldEstimate + StepSize[Target - OldEstimate]$$ [^7]

onde *StepSize* = $\frac{1}{n}$, *Target* = $R_n$, *OldEstimate* = $Q_n$ e *NewEstimate* = $Q_{n+1}$ [^7].

> üí° **Exemplo Num√©rico:**  Se a estimativa anterior (*OldEstimate*) for 7, a recompensa observada (*Target*) for 9 e o tamanho do passo (*StepSize*) for 0.2, a nova estimativa ser√°:
>
> $NewEstimate = 7 + 0.2[9 - 7] = 7 + 0.2[2] = 7 + 0.4 = 7.4$.

O termo $[R_n - Q_n]$ representa o **erro na estimativa**, que √© a diferen√ßa entre a recompensa observada e a estimativa atual do valor da a√ß√£o [^7]. O **tamanho do passo** ($\frac{1}{n}$) controla a magnitude da atualiza√ß√£o, determinando o quanto a nova recompensa influencia a estimativa.

**Lemma:** A escolha do tamanho do passo $\frac{1}{n}$ garante a converg√™ncia para o valor verdadeiro da a√ß√£o em ambientes estacion√°rios.

*Prova:*
Em um ambiente estacion√°rio, a distribui√ß√£o de probabilidade das recompensas para cada a√ß√£o √© constante ao longo do tempo [^1]. Isso significa que o valor esperado de cada a√ß√£o, $q_*(a) = E[R_t | A_t = a]$, permanece constante.

A regra de atualiza√ß√£o incremental com tamanho do passo $\frac{1}{n}$ calcula uma m√©dia amostral das recompensas. Pela lei dos grandes n√∫meros, a m√©dia amostral converge para o valor esperado verdadeiro quando o n√∫mero de amostras tende ao infinito [^3].

Formalmente, seja $Q_n$ a estimativa do valor da a√ß√£o ap√≥s $n$ sele√ß√µes, e $q_*(a)$ o valor verdadeiro da a√ß√£o. Queremos mostrar que $Q_n \rightarrow q_*(a)$ quando $n \rightarrow \infty$.

A regra de atualiza√ß√£o √©:

$$Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]$$

Reorganizando, temos:

$$Q_{n+1} = \frac{1}{n}R_n + \frac{n-1}{n}Q_n$$

Expandindo recursivamente, obtemos:

$$Q_{n+1} = \frac{1}{n}R_n + \frac{n-1}{n}\left(\frac{1}{n-1}R_{n-1} + \frac{n-2}{n-1}Q_{n-1}\right)$$
$$Q_{n+1} = \frac{1}{n}R_n + \frac{1}{n}R_{n-1} + \frac{n-2}{n}Q_{n-1}$$

Continuando a expans√£o, chegamos a:

$$Q_{n+1} = \frac{1}{n}R_n + \frac{1}{n}R_{n-1} + \ldots + \frac{1}{n}R_1 = \frac{1}{n} \sum_{i=1}^{n} R_i$$

Portanto, $Q_{n+1}$ √© a m√©dia amostral das primeiras $n$ recompensas. Pela lei dos grandes n√∫meros, quando $n \rightarrow \infty$:

$$Q_{n+1} \rightarrow E[R_t | A_t = a] = q_*(a)$$

Isso demonstra que a estimativa do valor da a√ß√£o converge para o valor verdadeiro da a√ß√£o em ambientes estacion√°rios quando o tamanho do passo √© $\frac{1}{n}$. $\blacksquare$

A propriedade fundamental desse tamanho do passo √© que ele diminui com o aumento do n√∫mero de recompensas observadas [^7]. Isso significa que as primeiras recompensas t√™m um impacto maior na estimativa inicial, enquanto as recompensas subsequentes t√™m um impacto gradualmente menor [^7].

> üí° **Exemplo Num√©rico:** Considere uma a√ß√£o que foi selecionada 100 vezes. O tamanho do passo seria $\frac{1}{100} = 0.01$. Se a recompensa observada fosse significativamente diferente da estimativa atual, o impacto na atualiza√ß√£o seria pequeno devido ao pequeno tamanho do passo. Por outro lado, se a a√ß√£o foi selecionada apenas 2 vezes, o tamanho do passo seria $\frac{1}{2} = 0.5$, e a nova recompensa teria um impacto muito maior na estimativa.

Em um ambiente estacion√°rio, essa diminui√ß√£o gradual do tamanho do passo √© desej√°vel, pois permite que o algoritmo "esque√ßa" flutua√ß√µes aleat√≥rias nas recompensas e se aproxime do valor verdadeiro da a√ß√£o [^3]. Contudo, em ambientes n√£o estacion√°rios, essa abordagem pode ser prejudicial, como discutiremos mais adiante.

**Teorema 1:** Para qualquer tamanho de passo $\alpha \in (0, 1]$, o m√©todo incremental converge para a m√©dia amostral se $\sum_{n=1}^{\infty} \alpha_n = \infty$ e $\sum_{n=1}^{\infty} \alpha_n^2 < \infty$.

*Prova:*
Este √© um resultado cl√°ssico da teoria da aproxima√ß√£o estoc√°stica. As condi√ß√µes $\sum_{n=1}^{\infty} \alpha_n = \infty$ garante que o algoritmo eventualmente explore todo o espa√ßo de estados, e $\sum_{n=1}^{\infty} \alpha_n^2 < \infty$ garante que as flutua√ß√µes diminuam ao longo do tempo, permitindo a converg√™ncia. A prova formal envolve t√©cnicas de an√°lise estoc√°stica e est√° al√©m do escopo deste cap√≠tulo, mas pode ser encontrada em [cite uma refer√™ncia apropriada]. Para o caso espec√≠fico de $\alpha_n = \frac{1}{n}$, ambas as condi√ß√µes s√£o satisfeitas, j√° que $\sum_{n=1}^{\infty} \frac{1}{n}$ diverge (s√©rie harm√¥nica) e $\sum_{n=1}^{\infty} \frac{1}{n^2}$ converge (para $\frac{\pi^2}{6}$). $\blacksquare$

Considerando o contexto da converg√™ncia, surge a quest√£o da taxa de converg√™ncia. Uma an√°lise mais detalhada permite entender a efici√™ncia da converg√™ncia.

**Teorema 1.1:** Sob certas condi√ß√µes de regularidade na distribui√ß√£o de recompensas, a taxa de converg√™ncia da estimativa do valor da a√ß√£o $Q_n$ para o valor verdadeiro $q_*(a)$ com tamanho de passo $\alpha_n = \frac{1}{n}$ √© $O(\frac{1}{\sqrt{n}})$.

*Prova:*
A prova deste teorema envolve a aplica√ß√£o do teorema central do limite (TCL). Como $Q_n$ √© a m√©dia amostral de $n$ recompensas independentes e identicamente distribu√≠das (i.i.d.), o TCL implica que a distribui√ß√£o de $Q_n$ se aproxima de uma distribui√ß√£o normal com m√©dia $q_*(a)$ e vari√¢ncia $\frac{\sigma^2}{n}$, onde $\sigma^2$ √© a vari√¢ncia da distribui√ß√£o de recompensas. Portanto, o erro $|Q_n - q_*(a)|$ √© da ordem de $\frac{1}{\sqrt{n}}$. Uma demonstra√ß√£o formal pode ser encontrada em textos avan√ßados sobre teoria da probabilidade e estat√≠stica. $\blacksquare$

Al√©m disso, podemos analisar o comportamento do erro na estimativa ao longo do tempo.

**Lema 1.1:** O erro quadr√°tico m√©dio (MSE) entre a estimativa $Q_n$ e o valor verdadeiro $q_*(a)$ diminui monotonicamente com o aumento de $n$ quando o tamanho do passo √© $\frac{1}{n}$ em ambientes estacion√°rios.

*Prova:*
O MSE √© definido como $E[(Q_n - q_*(a))^2]$. Como $Q_n$ converge para $q_*(a)$ pela lei dos grandes n√∫meros, e a vari√¢ncia da estimativa diminui com $\frac{1}{n}$, o MSE tamb√©m diminui monotonicamente. Formalmente,
$E[(Q_{n+1} - q_*(a))^2] = E[(\frac{1}{n}R_n + \frac{n-1}{n}Q_n - q_*(a))^2]$. Expandindo e utilizando o fato de que $E[R_n] = q_*(a)$ e que a vari√¢ncia diminui com $n$, pode-se mostrar que $E[(Q_{n+1} - q_*(a))^2] \le E[(Q_n - q_*(a))^2]$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Para visualizar a converg√™ncia e o decaimento do MSE, vamos simular a sele√ß√£o de uma a√ß√£o com valor verdadeiro $q_*(a) = 2$ por 1000 passos, onde as recompensas s√£o amostradas de uma distribui√ß√£o normal com m√©dia $q_*(a)$ e desvio padr√£o $\sigma = 1$.

A imagem a seguir ilustra bem o conceito de um problema de bandit com 10 bra√ßos e suas distribui√ß√µes.

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

> üí° **Exemplo Num√©rico (Continua√ß√£o):**  O exemplo num√©rico acima demonstra o funcionamento do algoritmo em um ambiente com essas caracter√≠sticas.

**Caixa de destaque:**

> A escolha do tamanho do passo $\frac{1}{n}$ √© ideal para ambientes estacion√°rios, garantindo a converg√™ncia para o valor verdadeiro da a√ß√£o, conforme comprovado pela lei dos grandes n√∫meros. No entanto, essa abordagem n√£o √© adequada para ambientes n√£o estacion√°rios, onde os valores das a√ß√µes mudam ao longo do tempo.

### Conclus√£o

A implementa√ß√£o incremental com tamanho de passo $\frac{1}{n}$ fornece uma maneira computacionalmente eficiente e teoricamente s√≥lida de estimar os valores das a√ß√µes em ambientes estacion√°rios [^7]. A diminui√ß√£o gradual do tamanho do passo garante que o algoritmo convirja para o valor verdadeiro da a√ß√£o, "esquecendo" flutua√ß√µes aleat√≥rias [^3]. No entanto, essa abordagem n√£o √© apropriada para ambientes n√£o estacion√°rios [^3], onde a capacidade de adaptar-se a mudan√ßas nos valores das a√ß√µes √© crucial. Em tais cen√°rios, estrat√©gias alternativas para ajustar o tamanho do passo, como o uso de um tamanho do passo constante, s√£o necess√°rias.

### Refer√™ncias

[^1]: Cap√≠tulo 2, Multi-armed Bandits
[^3]: Se√ß√£o 2.3, The 10-armed Testbed
[^6]: Se√ß√£o 2.2, Action-value Methods
[^7]: Se√ß√£o 2.4, Incremental Implementation
<!-- END -->