## Decaimento Exponencial e MÃ©dia Ponderada em Problemas NÃ£o-EstacionÃ¡rios

### IntroduÃ§Ã£o
Em continuidade Ã  discussÃ£o sobre **implementaÃ§Ã£o incremental** e o tratamento de **problemas nÃ£o-estacionÃ¡rios** no contexto de *k-armed bandits*, este capÃ­tulo aprofunda a anÃ¡lise do impacto do decaimento exponencial no cÃ¡lculo de mÃ©dias ponderadas. Anteriormente, foi introduzida a mÃ©dia ponderada como uma forma de dar maior relevÃ¢ncia a recompensas recentes em ambientes onde a distribuiÃ§Ã£o de recompensas pode mudar ao longo do tempo [^32]. Aqui, exploraremos em detalhes como o peso atribuÃ­do a cada recompensa passada ($R_i$) depende do nÃºmero de passos anteriores ($n-i$) e decai exponencialmente com um fator de $(1 - \alpha)$.

### Conceitos Fundamentais

Em problemas **nÃ£o-estacionÃ¡rios**, as probabilidades de recompensa das aÃ§Ãµes podem mudar ao longo do tempo, tornando as mÃ©dias simples (sample averages) inadequadas [^32]. Para lidar com essa *nÃ£o-estacionariedade*, Ã© Ãºtil dar mais peso Ã s recompensas recentes do que Ã s recompensas passadas. Uma das maneiras mais populares de fazer isso Ã© usar um **parÃ¢metro de step-size constante**, denotado por $\alpha$, no intervalo (0, 1].

A regra de atualizaÃ§Ã£o incremental modificada para um problema nÃ£o-estacionÃ¡rio Ã© dada por [^32]:

$$
Q_{n+1} = Q_n + \alpha[R_n - Q_n] \quad (2.5)
$$

onde $Q_n$ Ã© a estimativa da mÃ©dia das recompensas passadas, $R_n$ Ã© a n-Ã©sima recompensa e $\alpha$ Ã© o step-size constante.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que a estimativa inicial da recompensa para uma aÃ§Ã£o, $Q_n$, seja 10. A recompensa mais recente, $R_n$, obtida ao realizar essa aÃ§Ã£o Ã© 15. Se o step-size $\alpha$ for 0.1, a nova estimativa $Q_{n+1}$ serÃ¡:
>
> $Q_{n+1} = 10 + 0.1[15 - 10] = 10 + 0.1[5] = 10 + 0.5 = 10.5$
>
> Este exemplo demonstra como a estimativa Ã© atualizada incrementalmente, ponderando a diferenÃ§a entre a recompensa observada e a estimativa anterior pelo fator $\alpha$.

Agora, vamos examinar a relaÃ§Ã£o entre $Q_{n+1}$ e as recompensas anteriores. Expandindo a equaÃ§Ã£o acima, obtemos [^32]:

$$
Q_{n+1} = Q_n + \alpha[R_n - Q_n] = \alpha R_n + (1 - \alpha) Q_n
$$

Continuando a expansÃ£o de forma recursiva:

$$
Q_{n+1} = \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}] = \alpha R_n + \alpha (1 - \alpha) R_{n-1} + (1 - \alpha)^2 Q_{n-1}
$$

Repetindo o processo atÃ© $Q_1$, obtemos [^32]:

$$
Q_{n+1} = \alpha R_n + \alpha (1 - \alpha) R_{n-1} + \alpha (1 - \alpha)^2 R_{n-2} + \dots + \alpha (1 - \alpha)^{n-1} R_1 + (1 - \alpha)^n Q_1
$$

Esta equaÃ§Ã£o mostra que $Q_{n+1}$ Ã© uma **mÃ©dia ponderada** das recompensas passadas ($R_i$) e da estimativa inicial $Q_1$ [^32]. O peso atribuÃ­do a cada recompensa $R_i$ Ã© $\alpha(1 - \alpha)^{n-i}$, e o peso atribuÃ­do a $Q_1$ Ã© $(1 - \alpha)^n$.

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere $\alpha = 0.2$, $n = 3$, $R_1 = 5$, $R_2 = 8$, $R_3 = 12$, e $Q_1 = 10$. Podemos calcular $Q_4$ como:
>
> $Q_4 = 0.2 \cdot 12 + 0.2 \cdot (1 - 0.2) \cdot 8 + 0.2 \cdot (1 - 0.2)^2 \cdot 5 + (1 - 0.2)^3 \cdot 10$
> $Q_4 = 0.2 \cdot 12 + 0.2 \cdot 0.8 \cdot 8 + 0.2 \cdot 0.64 \cdot 5 + 0.512 \cdot 10$
> $Q_4 = 2.4 + 1.28 + 0.64 + 5.12 = 9.44$
>
> Isso demonstra como as recompensas mais recentes tÃªm um peso maior na determinaÃ§Ã£o da estimativa atual.

**Lemma:** A soma dos pesos em $Q_{n+1}$ Ã© igual a 1.

*Proof:*
A soma dos pesos Ã© dada por [^33]:

$$(1 - \alpha)^n + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i}$$

Podemos reescrever a soma como:

$$(1 - \alpha)^n + \alpha \sum_{i=1}^{n} (1 - \alpha)^{n-i} = (1 - \alpha)^n + \alpha \sum_{j=0}^{n-1} (1 - \alpha)^j$$

Onde $j = n - i$. A soma Ã© uma sÃ©rie geomÃ©trica com $n$ termos, primeiro termo 1 e razÃ£o $(1 - \alpha)$. Portanto:

$$(1 - \alpha)^n + \alpha \frac{1 - (1 - \alpha)^n}{1 - (1 - \alpha)} = (1 - \alpha)^n + \alpha \frac{1 - (1 - \alpha)^n}{\alpha} = (1 - \alpha)^n + 1 - (1 - \alpha)^n = 1$$

$\blacksquare$

O peso $\alpha(1 - \alpha)^{n-i}$ atribuÃ­do Ã  recompensa $R_i$ depende de quantos passos atrÃ¡s, $n-i$, a recompensa foi observada [^33]. A quantidade $1 - \alpha$ Ã© menor que 1, entÃ£o o peso dado a $R_i$ diminui Ã  medida que o nÃºmero de recompensas intervenientes aumenta. Na verdade, o peso decai *exponencialmente* de acordo com o expoente em $1 - \alpha$ [^33].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Se $\alpha = 0.1$, entÃ£o o peso da recompensa mais recente ($R_n$) Ã© $0.1$. O peso da recompensa anterior ($R_{n-1}$) Ã© $0.1 \cdot (1 - 0.1) = 0.09$. O peso da recompensa duas etapas atrÃ¡s ($R_{n-2}$) Ã© $0.1 \cdot (1 - 0.1)^2 = 0.081$. Isso mostra o decaimento exponencial dos pesos.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> alpha = 0.1
> n = 10
> weights = [alpha * (1 - alpha)**i for i in range(n)]
>
> plt.figure(figsize=(10, 6))
> plt.plot(range(n), weights, marker='o')
> plt.title("Decaimento Exponencial dos Pesos")
> plt.xlabel("Passos AtrÃ¡s (n-i)")
> plt.ylabel("Peso")
> plt.grid(True)
> plt.show()
> ```
>
> This code generates a plot that visually represents the exponential decay of weights as we look further back in time.  The y-axis represents the weight assigned to a reward, and the x-axis represents how many steps back in time the reward was received. The plot demonstrates that rewards received more recently have higher weights, reflecting the non-stationary nature of the problem.

Se $1 - \alpha = 0$, entÃ£o todo o peso vai para a Ãºltima recompensa, $R_n$, porque $0^0 = 1$ por convenÃ§Ã£o [^33].  Este caso extremo significa que apenas a recompensa mais recente Ã© considerada, e todas as recompensas anteriores sÃ£o ignoradas.

**ProposiÃ§Ã£o 1:** O nÃºmero de recompensas passadas que contribuem significativamente para a estimativa $Q_{n+1}$ Ã© inversamente proporcional a $\alpha$.

*Proof:* Podemos definir o horizonte efetivo $H$ como o nÃºmero de passos para trÃ¡s no tempo, a partir do presente, cuja contribuiÃ§Ã£o total para $Q_{n+1}$ representa uma fraÃ§Ã£o significativa, digamos $(1 - \epsilon)$, da soma total dos pesos das recompensas passadas (excluindo o peso de $Q_1$). Matematicamente, procuramos o menor inteiro $H$ tal que:

$$
\sum_{i=n-H+1}^{n} \alpha (1 - \alpha)^{n-i} \geq (1-\epsilon) \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i}
$$

Simplificando, temos:

$$
\sum_{k=0}^{H-1} \alpha (1 - \alpha)^{k} \geq (1 - \epsilon) [1 - (1 - \alpha)^n]
$$
onde $k=n-i$. Para $n$ suficientemente grande, $(1-\alpha)^n$ se aproxima de zero, resultando em:
$$
\sum_{k=0}^{H-1} \alpha (1 - \alpha)^{k} \geq (1 - \epsilon)
$$
A soma do lado esquerdo Ã© uma sÃ©rie geomÃ©trica finita, entÃ£o:
$$
\alpha \frac{1 - (1 - \alpha)^{H}}{1 - (1-\alpha)} = 1 - (1 - \alpha)^{H} \geq (1 - \epsilon)
$$
Isolando $(1 - \alpha)^H$, obtemos:
$$
(1 - \alpha)^{H} \leq \epsilon
$$
Aplicando o logaritmo natural em ambos os lados:
$$
H \ln(1 - \alpha) \leq \ln(\epsilon)
$$
Como $\ln(1 - \alpha)$ Ã© negativo, invertemos a desigualdade ao dividir:
$$
H \geq \frac{\ln(\epsilon)}{\ln(1 - \alpha)}
$$
Para pequenos valores de $\alpha$, podemos usar a aproximaÃ§Ã£o $\ln(1 - \alpha) \approx -\alpha$. Assim,
$$
H \approx \frac{\ln(\epsilon)}{-\alpha} = -\frac{\ln(\epsilon)}{\alpha}
$$
Portanto, $H$ Ã© aproximadamente inversamente proporcional a $\alpha$. Isso significa que, quanto menor o $\alpha$, maior o horizonte de recompensas passadas que contribuem significativamente para a estimativa $Q_{n+1}$. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos calcular o horizonte efetivo $H$ para diferentes valores de $\alpha$, assumindo que $\epsilon = 0.05$.
>
> *   Para $\alpha = 0.1$: $H \approx -\frac{\ln(0.05)}{0.1} \approx -\frac{-2.996}{0.1} \approx 30$
> *   Para $\alpha = 0.5$: $H \approx -\frac{\ln(0.05)}{0.5} \approx -\frac{-2.996}{0.5} \approx 6$
> *   Para $\alpha = 0.9$: $H \approx -\frac{\ln(0.05)}{0.9} \approx -\frac{-2.996}{0.9} \approx 3.33 \approx 4$
>
> Isso significa que com $\alpha = 0.1$, aproximadamente as Ãºltimas 30 recompensas contribuem significativamente para a estimativa, enquanto com $\alpha = 0.9$, apenas as Ãºltimas 4 recompensas tÃªm uma influÃªncia considerÃ¡vel.  Valores maiores de $\alpha$ tornam o agente mais reativo a mudanÃ§as recentes, mas tambÃ©m mais suscetÃ­vel a ruÃ­do.

Ã‰ importante notar que, como $\alpha$ Ã© constante, a condiÃ§Ã£o de convergÃªncia dada por $\sum_{n=1}^{\infty} \alpha_n(a) = \infty$ e $\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$ [^33] nÃ£o Ã© satisfeita, indicando que as estimativas nunca convergem completamente, mas continuam a variar em resposta Ã s recompensas mais recentemente recebidas. Isto Ã© desejÃ¡vel em ambientes nÃ£o-estacionÃ¡rios [^33].

**Teorema 1:** A variÃ¢ncia da estimativa $Q_{n+1}$ Ã© proporcional a $\alpha$ em um ambiente estacionÃ¡rio.

*Proof:*
Considere um ambiente estacionÃ¡rio onde as recompensas $R_i$ sÃ£o independentes e identicamente distribuÃ­das com mÃ©dia $\mu$ e variÃ¢ncia $\sigma^2$.  A estimativa $Q_{n+1}$ Ã© dada por:

$$
Q_{n+1} = \alpha R_n + \alpha (1 - \alpha) R_{n-1} + \alpha (1 - \alpha)^2 R_{n-2} + \dots + \alpha (1 - \alpha)^{n-1} R_1 + (1 - \alpha)^n Q_1
$$

Para simplificar a anÃ¡lise, assumimos que $n$ Ã© suficientemente grande para que $(1-\alpha)^n Q_1$ seja desprezÃ­vel. EntÃ£o,

$$
Q_{n+1} \approx \alpha \sum_{i=1}^{n} (1 - \alpha)^{n-i} R_i = \alpha \sum_{j=0}^{n-1} (1 - \alpha)^{j} R_{n-j}
$$

Como as recompensas sÃ£o independentes, a variÃ¢ncia de $Q_{n+1}$ Ã© dada por:

$$
Var(Q_{n+1}) \approx Var\left(\alpha \sum_{j=0}^{n-1} (1 - \alpha)^{j} R_{n-j}\right) = \alpha^2 \sum_{j=0}^{n-1} (1 - \alpha)^{2j} Var(R_{n-j})
$$

Dado que $Var(R_{n-j}) = \sigma^2$ para todo $j$, temos:

$$
Var(Q_{n+1}) \approx \alpha^2 \sigma^2 \sum_{j=0}^{n-1} (1 - \alpha)^{2j}
$$

A soma Ã© uma sÃ©rie geomÃ©trica com razÃ£o $(1-\alpha)^2$.  Para $n$ grande, a soma converge para $\frac{1}{1 - (1 - \alpha)^2}$.  Para mostrar isso rigorosamente:

I. A soma da sÃ©rie geomÃ©trica infinita $\sum_{j=0}^{\infty} r^j$ Ã© $\frac{1}{1-r}$ quando $|r| < 1$. Neste caso, $r = (1-\alpha)^2$. Como $0 < \alpha \leq 1$, temos $0 \leq (1-\alpha) < 1$, e portanto $0 \leq (1-\alpha)^2 < 1$.

II.  A soma finita $\sum_{j=0}^{n-1} (1-\alpha)^{2j}$ se aproxima da soma infinita $\sum_{j=0}^{\infty} (1-\alpha)^{2j}$ quando $n$ tende ao infinito. Portanto, para $n$ suficientemente grande, podemos aproximar a soma finita pela soma infinita.

III. Assim, $\sum_{j=0}^{n-1} (1 - \alpha)^{2j} \approx \frac{1}{1 - (1 - \alpha)^2}$.

Substituindo na equaÃ§Ã£o da variÃ¢ncia, temos:

$$
Var(Q_{n+1}) \approx \alpha^2 \sigma^2 \frac{1}{1 - (1 - \alpha)^2} = \alpha^2 \sigma^2 \frac{1}{1 - (1 - 2\alpha + \alpha^2)} = \alpha^2 \sigma^2 \frac{1}{2\alpha - \alpha^2} = \frac{\alpha \sigma^2}{2 - \alpha}
$$

Portanto, $Var(Q_{n+1}) \approx \frac{\alpha \sigma^2}{2 - \alpha}$. Para $\alpha$ pequeno, $Var(Q_{n+1}) \approx \frac{\alpha \sigma^2}{2}$. Assim, a variÃ¢ncia de $Q_{n+1}$ Ã© aproximadamente proporcional a $\alpha$ em um ambiente estacionÃ¡rio. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que em um ambiente estacionÃ¡rio, a variÃ¢ncia das recompensas seja $\sigma^2 = 4$. Vamos calcular a variÃ¢ncia de $Q_{n+1}$ para diferentes valores de $\alpha$:
>
> *   Para $\alpha = 0.1$: $Var(Q_{n+1}) \approx \frac{0.1 \cdot 4}{2 - 0.1} = \frac{0.4}{1.9} \approx 0.21$
> *   Para $\alpha = 0.5$: $Var(Q_{n+1}) \approx \frac{0.5 \cdot 4}{2 - 0.5} = \frac{2}{1.5} \approx 1.33$
> *   Para $\alpha = 0.9$: $Var(Q_{n+1}) \approx \frac{0.9 \cdot 4}{2 - 0.9} = \frac{3.6}{1.1} \approx 3.27$
>
> Estes resultados demonstram que, em um ambiente estacionÃ¡rio, aumentar $\alpha$ aumenta a variÃ¢ncia da estimativa $Q_{n+1}$. Isso torna a estimativa mais sensÃ­vel ao ruÃ­do e menos estÃ¡vel.

### ConclusÃ£o

A utilizaÃ§Ã£o de um parÃ¢metro de step-size constante $\alpha$ na atualizaÃ§Ã£o incremental permite que o algoritmo se adapte a problemas nÃ£o-estacionÃ¡rios, dando maior peso Ã s recompensas recentes e menor peso Ã s recompensas mais antigas [^32]. O decaimento exponencial dos pesos garante que o algoritmo seja mais sensÃ­vel Ã s mudanÃ§as recentes no ambiente, permitindo uma melhor adaptaÃ§Ã£o e desempenho em problemas onde a distribuiÃ§Ã£o de recompensas varia ao longo do tempo. A escolha apropriada de $\alpha$ Ã© crucial: um valor muito alto torna o algoritmo instÃ¡vel e sensÃ­vel ao ruÃ­do, enquanto um valor muito baixo torna o algoritmo lento para se adaptar Ã s mudanÃ§as no ambiente.

### ReferÃªncias
[^32]: CapÃ­tulo 2 do texto fornecido.
[^33]: CapÃ­tulo 2 do texto fornecido.
<!-- END -->