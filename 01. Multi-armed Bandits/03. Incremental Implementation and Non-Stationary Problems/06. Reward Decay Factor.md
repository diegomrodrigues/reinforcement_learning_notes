## Decaimento Exponencial e M√©dia Ponderada em Problemas N√£o-Estacion√°rios

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre **implementa√ß√£o incremental** e o tratamento de **problemas n√£o-estacion√°rios** no contexto de *k-armed bandits*, este cap√≠tulo aprofunda a an√°lise do impacto do decaimento exponencial no c√°lculo de m√©dias ponderadas. Anteriormente, foi introduzida a m√©dia ponderada como uma forma de dar maior relev√¢ncia a recompensas recentes em ambientes onde a distribui√ß√£o de recompensas pode mudar ao longo do tempo [^32]. Aqui, exploraremos em detalhes como o peso atribu√≠do a cada recompensa passada ($R_i$) depende do n√∫mero de passos anteriores ($n-i$) e decai exponencialmente com um fator de $(1 - \alpha)$.

### Conceitos Fundamentais

Em problemas **n√£o-estacion√°rios**, as probabilidades de recompensa das a√ß√µes podem mudar ao longo do tempo, tornando as m√©dias simples (sample averages) inadequadas [^32]. Para lidar com essa *n√£o-estacionariedade*, √© √∫til dar mais peso √†s recompensas recentes do que √†s recompensas passadas. Uma das maneiras mais populares de fazer isso √© usar um **par√¢metro de step-size constante**, denotado por $\alpha$, no intervalo (0, 1].

A regra de atualiza√ß√£o incremental modificada para um problema n√£o-estacion√°rio √© dada por [^32]:

$$
Q_{n+1} = Q_n + \alpha[R_n - Q_n] \quad (2.5)
$$

onde $Q_n$ √© a estimativa da m√©dia das recompensas passadas, $R_n$ √© a n-√©sima recompensa e $\alpha$ √© o step-size constante.

> üí° **Exemplo Num√©rico:**
>
> Suponha que a estimativa inicial da recompensa para uma a√ß√£o, $Q_n$, seja 10. A recompensa mais recente, $R_n$, obtida ao realizar essa a√ß√£o √© 15. Se o step-size $\alpha$ for 0.1, a nova estimativa $Q_{n+1}$ ser√°:
>
> $Q_{n+1} = 10 + 0.1[15 - 10] = 10 + 0.1[5] = 10 + 0.5 = 10.5$
>
> Este exemplo demonstra como a estimativa √© atualizada incrementalmente, ponderando a diferen√ßa entre a recompensa observada e a estimativa anterior pelo fator $\alpha$.

Agora, vamos examinar a rela√ß√£o entre $Q_{n+1}$ e as recompensas anteriores. Expandindo a equa√ß√£o acima, obtemos [^32]:

$$
Q_{n+1} = Q_n + \alpha[R_n - Q_n] = \alpha R_n + (1 - \alpha) Q_n
$$

Continuando a expans√£o de forma recursiva:

$$
Q_{n+1} = \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}] = \alpha R_n + \alpha (1 - \alpha) R_{n-1} + (1 - \alpha)^2 Q_{n-1}
$$

Repetindo o processo at√© $Q_1$, obtemos [^32]:

$$
Q_{n+1} = \alpha R_n + \alpha (1 - \alpha) R_{n-1} + \alpha (1 - \alpha)^2 R_{n-2} + \dots + \alpha (1 - \alpha)^{n-1} R_1 + (1 - \alpha)^n Q_1
$$

Esta equa√ß√£o mostra que $Q_{n+1}$ √© uma **m√©dia ponderada** das recompensas passadas ($R_i$) e da estimativa inicial $Q_1$ [^32]. O peso atribu√≠do a cada recompensa $R_i$ √© $\alpha(1 - \alpha)^{n-i}$, e o peso atribu√≠do a $Q_1$ √© $(1 - \alpha)^n$.

> üí° **Exemplo Num√©rico:**
>
> Considere $\alpha = 0.2$, $n = 3$, $R_1 = 5$, $R_2 = 8$, $R_3 = 12$, e $Q_1 = 10$. Podemos calcular $Q_4$ como:
>
> $Q_4 = 0.2 \cdot 12 + 0.2 \cdot (1 - 0.2) \cdot 8 + 0.2 \cdot (1 - 0.2)^2 \cdot 5 + (1 - 0.2)^3 \cdot 10$
> $Q_4 = 0.2 \cdot 12 + 0.2 \cdot 0.8 \cdot 8 + 0.2 \cdot 0.64 \cdot 5 + 0.512 \cdot 10$
> $Q_4 = 2.4 + 1.28 + 0.64 + 5.12 = 9.44$
>
> Isso demonstra como as recompensas mais recentes t√™m um peso maior na determina√ß√£o da estimativa atual.

**Lemma:** A soma dos pesos em $Q_{n+1}$ √© igual a 1.

*Proof:*
A soma dos pesos √© dada por [^33]:

$$(1 - \alpha)^n + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i}$$

Podemos reescrever a soma como:

$$(1 - \alpha)^n + \alpha \sum_{i=1}^{n} (1 - \alpha)^{n-i} = (1 - \alpha)^n + \alpha \sum_{j=0}^{n-1} (1 - \alpha)^j$$

Onde $j = n - i$. A soma √© uma s√©rie geom√©trica com $n$ termos, primeiro termo 1 e raz√£o $(1 - \alpha)$. Portanto:

$$(1 - \alpha)^n + \alpha \frac{1 - (1 - \alpha)^n}{1 - (1 - \alpha)} = (1 - \alpha)^n + \alpha \frac{1 - (1 - \alpha)^n}{\alpha} = (1 - \alpha)^n + 1 - (1 - \alpha)^n = 1$$

$\blacksquare$

O peso $\alpha(1 - \alpha)^{n-i}$ atribu√≠do √† recompensa $R_i$ depende de quantos passos atr√°s, $n-i$, a recompensa foi observada [^33]. A quantidade $1 - \alpha$ √© menor que 1, ent√£o o peso dado a $R_i$ diminui √† medida que o n√∫mero de recompensas intervenientes aumenta. Na verdade, o peso decai *exponencialmente* de acordo com o expoente em $1 - \alpha$ [^33].

> üí° **Exemplo Num√©rico:**
>
> Se $\alpha = 0.1$, ent√£o o peso da recompensa mais recente ($R_n$) √© $0.1$. O peso da recompensa anterior ($R_{n-1}$) √© $0.1 \cdot (1 - 0.1) = 0.09$. O peso da recompensa duas etapas atr√°s ($R_{n-2}$) √© $0.1 \cdot (1 - 0.1)^2 = 0.081$. Isso mostra o decaimento exponencial dos pesos.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> alpha = 0.1
> n = 10
> weights = [alpha * (1 - alpha)**i for i in range(n)]
>
> plt.figure(figsize=(10, 6))
> plt.plot(range(n), weights, marker='o')
> plt.title("Decaimento Exponencial dos Pesos")
> plt.xlabel("Passos Atr√°s (n-i)")
> plt.ylabel("Peso")
> plt.grid(True)
> plt.show()
> ```
>
> This code generates a plot that visually represents the exponential decay of weights as we look further back in time.  The y-axis represents the weight assigned to a reward, and the x-axis represents how many steps back in time the reward was received. The plot demonstrates that rewards received more recently have higher weights, reflecting the non-stationary nature of the problem.

Se $1 - \alpha = 0$, ent√£o todo o peso vai para a √∫ltima recompensa, $R_n$, porque $0^0 = 1$ por conven√ß√£o [^33].  Este caso extremo significa que apenas a recompensa mais recente √© considerada, e todas as recompensas anteriores s√£o ignoradas.

**Proposi√ß√£o 1:** O n√∫mero de recompensas passadas que contribuem significativamente para a estimativa $Q_{n+1}$ √© inversamente proporcional a $\alpha$.

*Proof:* Podemos definir o horizonte efetivo $H$ como o n√∫mero de passos para tr√°s no tempo, a partir do presente, cuja contribui√ß√£o total para $Q_{n+1}$ representa uma fra√ß√£o significativa, digamos $(1 - \epsilon)$, da soma total dos pesos das recompensas passadas (excluindo o peso de $Q_1$). Matematicamente, procuramos o menor inteiro $H$ tal que:

$$
\sum_{i=n-H+1}^{n} \alpha (1 - \alpha)^{n-i} \geq (1-\epsilon) \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i}
$$

Simplificando, temos:

$$
\sum_{k=0}^{H-1} \alpha (1 - \alpha)^{k} \geq (1 - \epsilon) [1 - (1 - \alpha)^n]
$$
onde $k=n-i$. Para $n$ suficientemente grande, $(1-\alpha)^n$ se aproxima de zero, resultando em:
$$
\sum_{k=0}^{H-1} \alpha (1 - \alpha)^{k} \geq (1 - \epsilon)
$$
A soma do lado esquerdo √© uma s√©rie geom√©trica finita, ent√£o:
$$
\alpha \frac{1 - (1 - \alpha)^{H}}{1 - (1-\alpha)} = 1 - (1 - \alpha)^{H} \geq (1 - \epsilon)
$$
Isolando $(1 - \alpha)^H$, obtemos:
$$
(1 - \alpha)^{H} \leq \epsilon
$$
Aplicando o logaritmo natural em ambos os lados:
$$
H \ln(1 - \alpha) \leq \ln(\epsilon)
$$
Como $\ln(1 - \alpha)$ √© negativo, invertemos a desigualdade ao dividir:
$$
H \geq \frac{\ln(\epsilon)}{\ln(1 - \alpha)}
$$
Para pequenos valores de $\alpha$, podemos usar a aproxima√ß√£o $\ln(1 - \alpha) \approx -\alpha$. Assim,
$$
H \approx \frac{\ln(\epsilon)}{-\alpha} = -\frac{\ln(\epsilon)}{\alpha}
$$
Portanto, $H$ √© aproximadamente inversamente proporcional a $\alpha$. Isso significa que, quanto menor o $\alpha$, maior o horizonte de recompensas passadas que contribuem significativamente para a estimativa $Q_{n+1}$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos calcular o horizonte efetivo $H$ para diferentes valores de $\alpha$, assumindo que $\epsilon = 0.05$.
>
> *   Para $\alpha = 0.1$: $H \approx -\frac{\ln(0.05)}{0.1} \approx -\frac{-2.996}{0.1} \approx 30$
> *   Para $\alpha = 0.5$: $H \approx -\frac{\ln(0.05)}{0.5} \approx -\frac{-2.996}{0.5} \approx 6$
> *   Para $\alpha = 0.9$: $H \approx -\frac{\ln(0.05)}{0.9} \approx -\frac{-2.996}{0.9} \approx 3.33 \approx 4$
>
> Isso significa que com $\alpha = 0.1$, aproximadamente as √∫ltimas 30 recompensas contribuem significativamente para a estimativa, enquanto com $\alpha = 0.9$, apenas as √∫ltimas 4 recompensas t√™m uma influ√™ncia consider√°vel.  Valores maiores de $\alpha$ tornam o agente mais reativo a mudan√ßas recentes, mas tamb√©m mais suscet√≠vel a ru√≠do.

√â importante notar que, como $\alpha$ √© constante, a condi√ß√£o de converg√™ncia dada por $\sum_{n=1}^{\infty} \alpha_n(a) = \infty$ e $\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$ [^33] n√£o √© satisfeita, indicando que as estimativas nunca convergem completamente, mas continuam a variar em resposta √†s recompensas mais recentemente recebidas. Isto √© desej√°vel em ambientes n√£o-estacion√°rios [^33].

**Teorema 1:** A vari√¢ncia da estimativa $Q_{n+1}$ √© proporcional a $\alpha$ em um ambiente estacion√°rio.

*Proof:*
Considere um ambiente estacion√°rio onde as recompensas $R_i$ s√£o independentes e identicamente distribu√≠das com m√©dia $\mu$ e vari√¢ncia $\sigma^2$.  A estimativa $Q_{n+1}$ √© dada por:

$$
Q_{n+1} = \alpha R_n + \alpha (1 - \alpha) R_{n-1} + \alpha (1 - \alpha)^2 R_{n-2} + \dots + \alpha (1 - \alpha)^{n-1} R_1 + (1 - \alpha)^n Q_1
$$

Para simplificar a an√°lise, assumimos que $n$ √© suficientemente grande para que $(1-\alpha)^n Q_1$ seja desprez√≠vel. Ent√£o,

$$
Q_{n+1} \approx \alpha \sum_{i=1}^{n} (1 - \alpha)^{n-i} R_i = \alpha \sum_{j=0}^{n-1} (1 - \alpha)^{j} R_{n-j}
$$

Como as recompensas s√£o independentes, a vari√¢ncia de $Q_{n+1}$ √© dada por:

$$
Var(Q_{n+1}) \approx Var\left(\alpha \sum_{j=0}^{n-1} (1 - \alpha)^{j} R_{n-j}\right) = \alpha^2 \sum_{j=0}^{n-1} (1 - \alpha)^{2j} Var(R_{n-j})
$$

Dado que $Var(R_{n-j}) = \sigma^2$ para todo $j$, temos:

$$
Var(Q_{n+1}) \approx \alpha^2 \sigma^2 \sum_{j=0}^{n-1} (1 - \alpha)^{2j}
$$

A soma √© uma s√©rie geom√©trica com raz√£o $(1-\alpha)^2$.  Para $n$ grande, a soma converge para $\frac{1}{1 - (1 - \alpha)^2}$.  Para mostrar isso rigorosamente:

I. A soma da s√©rie geom√©trica infinita $\sum_{j=0}^{\infty} r^j$ √© $\frac{1}{1-r}$ quando $|r| < 1$. Neste caso, $r = (1-\alpha)^2$. Como $0 < \alpha \leq 1$, temos $0 \leq (1-\alpha) < 1$, e portanto $0 \leq (1-\alpha)^2 < 1$.

II.  A soma finita $\sum_{j=0}^{n-1} (1-\alpha)^{2j}$ se aproxima da soma infinita $\sum_{j=0}^{\infty} (1-\alpha)^{2j}$ quando $n$ tende ao infinito. Portanto, para $n$ suficientemente grande, podemos aproximar a soma finita pela soma infinita.

III. Assim, $\sum_{j=0}^{n-1} (1 - \alpha)^{2j} \approx \frac{1}{1 - (1 - \alpha)^2}$.

Substituindo na equa√ß√£o da vari√¢ncia, temos:

$$
Var(Q_{n+1}) \approx \alpha^2 \sigma^2 \frac{1}{1 - (1 - \alpha)^2} = \alpha^2 \sigma^2 \frac{1}{1 - (1 - 2\alpha + \alpha^2)} = \alpha^2 \sigma^2 \frac{1}{2\alpha - \alpha^2} = \frac{\alpha \sigma^2}{2 - \alpha}
$$

Portanto, $Var(Q_{n+1}) \approx \frac{\alpha \sigma^2}{2 - \alpha}$. Para $\alpha$ pequeno, $Var(Q_{n+1}) \approx \frac{\alpha \sigma^2}{2}$. Assim, a vari√¢ncia de $Q_{n+1}$ √© aproximadamente proporcional a $\alpha$ em um ambiente estacion√°rio. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que em um ambiente estacion√°rio, a vari√¢ncia das recompensas seja $\sigma^2 = 4$. Vamos calcular a vari√¢ncia de $Q_{n+1}$ para diferentes valores de $\alpha$:
>
> *   Para $\alpha = 0.1$: $Var(Q_{n+1}) \approx \frac{0.1 \cdot 4}{2 - 0.1} = \frac{0.4}{1.9} \approx 0.21$
> *   Para $\alpha = 0.5$: $Var(Q_{n+1}) \approx \frac{0.5 \cdot 4}{2 - 0.5} = \frac{2}{1.5} \approx 1.33$
> *   Para $\alpha = 0.9$: $Var(Q_{n+1}) \approx \frac{0.9 \cdot 4}{2 - 0.9} = \frac{3.6}{1.1} \approx 3.27$
>
> Estes resultados demonstram que, em um ambiente estacion√°rio, aumentar $\alpha$ aumenta a vari√¢ncia da estimativa $Q_{n+1}$. Isso torna a estimativa mais sens√≠vel ao ru√≠do e menos est√°vel.

### Conclus√£o

A utiliza√ß√£o de um par√¢metro de step-size constante $\alpha$ na atualiza√ß√£o incremental permite que o algoritmo se adapte a problemas n√£o-estacion√°rios, dando maior peso √†s recompensas recentes e menor peso √†s recompensas mais antigas [^32]. O decaimento exponencial dos pesos garante que o algoritmo seja mais sens√≠vel √†s mudan√ßas recentes no ambiente, permitindo uma melhor adapta√ß√£o e desempenho em problemas onde a distribui√ß√£o de recompensas varia ao longo do tempo. A escolha apropriada de $\alpha$ √© crucial: um valor muito alto torna o algoritmo inst√°vel e sens√≠vel ao ru√≠do, enquanto um valor muito baixo torna o algoritmo lento para se adaptar √†s mudan√ßas no ambiente.

### Refer√™ncias
[^32]: Cap√≠tulo 2 do texto fornecido.
[^33]: Cap√≠tulo 2 do texto fornecido.
<!-- END -->