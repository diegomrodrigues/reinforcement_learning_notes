## Rastreando um Problema N√£o Estacion√°rio: A Pr√°tica Contra a Teoria

### Introdu√ß√£o

Este cap√≠tulo aborda o desafio de aprendizado por refor√ßo em ambientes n√£o estacion√°rios, onde as probabilidades de recompensa podem variar ao longo do tempo. O foco recai sobre a necessidade de adaptar m√©todos de aprendizado para ponderar mais as recompensas recentes em detrimento das passadas. Discutimos como a utiliza√ß√£o de um par√¢metro de tamanho de passo constante pode proporcionar uma resposta mais √°gil √†s mudan√ßas no ambiente, em contraste com os m√©todos de m√©dias amostrais. Esta se√ß√£o explora as limita√ß√µes das condi√ß√µes de converg√™ncia te√≥ricas em rela√ß√£o √† aplica√ß√£o pr√°tica [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

### Conceitos Fundamentais

Para lidar com problemas n√£o estacion√°rios, exploramos o uso de um par√¢metro de tamanho de passo constante, $\alpha$, na atualiza√ß√£o das estimativas de valor da a√ß√£o. A regra de atualiza√ß√£o incremental original, dada por $Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]$, onde $Q_n$ √© a estimativa no passo $n$, e $R_n$ √© a recompensa recebida no passo $n$, √© modificada para:
$$ Q_{n+1} = Q_n + \alpha[R_n - Q_n], $$
onde $\alpha \in (0, 1]$ [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Essa modifica√ß√£o resulta em $Q_{n+1}$ sendo uma m√©dia ponderada das recompensas anteriores, dando maior peso √†s recompensas mais recentes.

> üí° **Exemplo Num√©rico:** Suponha que temos uma estimativa inicial de valor da a√ß√£o $Q_1 = 10$, e recebemos uma sequ√™ncia de recompensas: $R_1 = 12$, $R_2 = 14$, $R_3 = 16$, $R_4 = 18$. Vamos calcular $Q_{n+1}$ usando um tamanho de passo $\alpha = 0.1$ e comparar com a m√©dia amostral.
>
> **Usando $\alpha = 0.1$:**
>
> *   $Q_2 = Q_1 + \alpha(R_1 - Q_1) = 10 + 0.1(12 - 10) = 10 + 0.2 = 10.2$
> *   $Q_3 = Q_2 + \alpha(R_2 - Q_2) = 10.2 + 0.1(14 - 10.2) = 10.2 + 0.38 = 10.58$
> *   $Q_4 = Q_3 + \alpha(R_3 - Q_3) = 10.58 + 0.1(16 - 10.58) = 10.58 + 0.542 = 11.122$
> *   $Q_5 = Q_4 + \alpha(R_4 - Q_4) = 11.122 + 0.1(18 - 11.122) = 11.122 + 0.6878 = 11.8098$
>
> **Usando a m√©dia amostral:**
>
> *   $Q_2 = 10 + \frac{1}{1}(12-10) = 12$
> *  $Q_3 = \frac{10+12+14}{3}= \frac{36}{3}=12$
> *   $Q_4 = \frac{10+12+14+16}{4} = \frac{52}{4} = 13$
> *  $Q_5 = \frac{10+12+14+16+18}{5} = \frac{70}{5}=14$
>
> Observe que com $\alpha=0.1$, $Q_{n+1}$ se adapta mais lentamente √†s novas recompensas, enquanto a m√©dia amostral √© mais sens√≠vel e converge rapidamente para a m√©dia das recompensas.

A expans√£o da equa√ß√£o $Q_{n+1}$ mostra que ela pode ser expressa como uma m√©dia ponderada exponencialmente das recompensas passadas e da estimativa inicial $Q_1$:
$$ Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1-\alpha)^{n-i} R_i $$
Nessa formula√ß√£o, o peso atribu√≠do √† recompensa $R_i$ depende de quanto tempo se passou desde que ela foi observada, com o peso decaindo exponencialmente √† medida que o tempo retrocede [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). O fator $(1-\alpha)$ governa a rapidez com que as recompensas passadas s√£o desconsideradas. Se $\alpha$ for 1, ent√£o apenas a recompensa mais recente ($R_n$) √© considerada.

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, vamos calcular os pesos de cada recompensa para $n=4$ e $\alpha=0.1$.
>
>  *  Peso de $R_1$: $\alpha(1-\alpha)^{4-1}=0.1(0.9)^3=0.0729$
>   * Peso de $R_2$: $\alpha(1-\alpha)^{4-2}=0.1(0.9)^2=0.081$
>   * Peso de $R_3$: $\alpha(1-\alpha)^{4-3}=0.1(0.9)^1=0.09$
>   * Peso de $R_4$: $\alpha(1-\alpha)^{4-4}=0.1(0.9)^0=0.1$
>   * Peso de $Q_1$: $(1-\alpha)^4=(0.9)^4=0.6561$
>
> Agora, calculamos $Q_5$:
>
> $Q_5 = (0.9)^4Q_1 + 0.1(0.9)^3R_1+0.1(0.9)^2R_2+0.1(0.9)^1R_3+0.1(0.9)^0R_4$
>
> $Q_5 = 0.6561 * 10 + 0.0729 * 12 + 0.081 * 14 + 0.09 * 16 + 0.1*18 = 6.561 + 0.8748 + 1.134 + 1.44 + 1.8 = 11.8098$. Este resultado corresponde ao valor calculado anteriormente, demonstrando a equival√™ncia das abordagens.
>
> ```mermaid
> graph LR
>     subgraph "C√°lculo de Q_{n+1}"
>     A("Q_1") --> B("Peso: (1-Œ±)^n");
>         B --> E("Termo 1: (1-Œ±)^n * Q_1");
>     C("R_i") --> D("Peso: Œ±(1-Œ±)^(n-i)");
>         D --> F("Termo 2: Œ£ Œ±(1-Œ±)^(n-i) * R_i");
>       E --> G("Q_{n+1} = Termo 1 + Termo 2");
>       F --> G
>     end
> ```

No contexto de algoritmos de aproxima√ß√£o estoc√°stica, h√° condi√ß√µes de converg√™ncia estabelecidas para garantir que as estimativas da a√ß√£o convirjam para os verdadeiros valores, com probabilidade de 1. Essas condi√ß√µes s√£o dadas por [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2):
$$ \sum_{n=1}^{\infty} \alpha_n(a) = \infty $$
$$ \sum_{n=1}^{\infty} \alpha_n^2(a) < \infty $$

A primeira condi√ß√£o garante que os passos sejam grandes o suficiente para superar quaisquer condi√ß√µes iniciais ou flutua√ß√µes aleat√≥rias. A segunda condi√ß√£o assegura que os passos se tornem suficientemente pequenos para garantir a converg√™ncia. Para o m√©todo de m√©dias amostrais, onde $\alpha_n(a) = \frac{1}{n}$, ambas as condi√ß√µes s√£o satisfeitas. No entanto, para o m√©todo com um par√¢metro de tamanho de passo constante ($\alpha_n(a) = \alpha$), a segunda condi√ß√£o n√£o √© satisfeita, indicando que as estimativas n√£o convergem completamente [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

> üí° **Exemplo Num√©rico:** Vamos considerar $\alpha_n(a)=\frac{1}{n}$ (m√©dia amostral) e $\alpha_n(a) = 0.1$ (constante).
>
> *   **M√©dia Amostral:**
>     *   $\sum_{n=1}^{\infty} \frac{1}{n} = 1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \ldots = \infty$ (S√©rie harm√¥nica, condi√ß√£o 1 satisfeita)
>     *   $\sum_{n=1}^{\infty} \frac{1}{n^2} = 1 + \frac{1}{4} + \frac{1}{9} + \frac{1}{16} + \ldots < \infty$ (S√©rie p-harm√¥nica com p=2, converge, condi√ß√£o 2 satisfeita)
> *   **Passo Constante $\alpha = 0.1$:**
>     *   $\sum_{n=1}^{\infty} 0.1 = 0.1 + 0.1 + 0.1 + \ldots = \infty$ (Condi√ß√£o 1 satisfeita)
>     *   $\sum_{n=1}^{\infty} 0.1^2 = 0.01 + 0.01 + 0.01 + \ldots = \infty$ (Condi√ß√£o 2 n√£o satisfeita)
>
>  Este exemplo mostra que a m√©dia amostral satisfaz ambas as condi√ß√µes, enquanto o passo constante n√£o satisfaz a segunda condi√ß√£o, o que indica que n√£o haver√° converg√™ncia no sentido estrito da defini√ß√£o da aproxima√ß√£o estoc√°stica.
> ```mermaid
> graph LR
>   subgraph "Condi√ß√µes de Converg√™ncia"
>   A("Œ£ Œ±_n(a) = ‚àû") --> B("Garante passos suficientes");
>     style A fill:#ccf,stroke:#333,stroke-width:2px
>   C("Œ£ Œ±_n¬≤(a) < ‚àû") --> D("Garante passos decrescentes para converg√™ncia");
>     style C fill:#ccf,stroke:#333,stroke-width:2px
>   end
>
>   subgraph "M√©dia Amostral (Œ±_n = 1/n)"
>   E("Condi√ß√£o 1: Œ£ 1/n = ‚àû") --> F("Satisfeita");
>     style E fill:#aaf,stroke:#333,stroke-width:2px
>   G("Condi√ß√£o 2: Œ£ 1/n¬≤ < ‚àû") --> H("Satisfeita");
>       style G fill:#aaf,stroke:#333,stroke-width:2px
>   end
>
>   subgraph "Passo Constante (Œ±_n = Œ±)"
>    I("Condi√ß√£o 1: Œ£ Œ± = ‚àû") --> J("Satisfeita");
>        style I fill:#faa,stroke:#333,stroke-width:2px
>   K("Condi√ß√£o 2: Œ£ Œ±¬≤ = ‚àû") --> L("N√£o Satisfeita");
>       style K fill:#faa,stroke:#333,stroke-width:2px
>   end
>    F --> M("Converg√™ncia Total");
>    H --> M;
>   J --> N("Converg√™ncia Limitada");
>   L --> N
>
>
> ```

**Lemma 1:** *A m√©dia ponderada exponencialmente, usando um par√¢metro de passo constante $\alpha$, atribui maior import√¢ncia √†s recompensas mais recentes, permitindo que o algoritmo se adapte a ambientes n√£o estacion√°rios, onde as recompensas podem mudar ao longo do tempo.*

*Prova*: Como demonstrado na expans√£o de $Q_{n+1}$, cada recompensa $R_i$ √© ponderada por um fator $\alpha(1-\alpha)^{n-i}$, que decai exponencialmente com o aumento da dist√¢ncia temporal $n-i$. Isso significa que as recompensas mais recentes t√™m pesos maiores, enquanto as recompensas mais antigas t√™m pesos menores. Isso permite que o algoritmo se adapte rapidamente a novas mudan√ßas no ambiente, sendo especialmente √∫til em cen√°rios n√£o estacion√°rios. $\blacksquare$

**Lemma 1.1:** *A soma dos pesos atribu√≠dos a todas as recompensas passadas na m√©dia ponderada exponencial √© igual a $1 - (1-\alpha)^n$.*

*Prova:*  A soma dos pesos √© dada por $\sum_{i=1}^{n} \alpha(1-\alpha)^{n-i}$.  Essa √© uma soma geom√©trica finita. Podemos reescrever essa soma como $\alpha(1-\alpha)^{n-1} + \alpha(1-\alpha)^{n-2} + \ldots + \alpha(1-\alpha)^0$. Fatorando $(1-\alpha)^{n-1}$, temos $\alpha (1-\alpha)^{n-1} \sum_{i=0}^{n-1} (1-\alpha)^{-i}$.  A soma geom√©trica $\sum_{i=0}^{n-1} x^i = \frac{1-x^n}{1-x}$.  Aplicando isso a nossa express√£o, temos $\alpha(1-\alpha)^{n-1}\frac{1-(1-\alpha)^{-n}}{1-(1-\alpha)^{-1}} = \alpha (1-\alpha)^{n-1}\frac{1-(1-\alpha)^{-n}}{\alpha/(1-\alpha)} = (1-\alpha)^n( (1-\alpha)^{-n} - 1 ) = 1 - (1-\alpha)^n $.   $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando $\alpha=0.2$ e $n=5$, a soma dos pesos das recompensas passadas √© $1-(1-0.2)^5 = 1-0.8^5 = 1-0.32768 = 0.67232$. Isso significa que as recompensas passadas somadas contribuem com 67.23% do valor de $Q_6$, enquanto a estimativa inicial $Q_1$ contribuir√° com $100\%-67.23\% = 32.77\%$.
>
> ```mermaid
> graph LR
>   subgraph "Pesos das Recompensas Passadas"
>   A("Soma dos Pesos") --> B("Œ£ Œ±(1-Œ±)^(n-i)");
>     B --> C("Reescrita: Œ±(1-Œ±)^(n-1) Œ£ (1-Œ±)^(-i)");
>    C --> D("Soma Geom√©trica: Œ±(1-Œ±)^(n-1) * [1-(1-Œ±)^(-n)]/[1-(1-Œ±)^(-1)]")
>   D --> E("Simplifica√ß√£o: 1 - (1-Œ±)^n");
>   end
> ```

**Lemma 1.2:** *√Ä medida que $n$ tende ao infinito, a soma dos pesos das recompensas passadas tende a 1, enquanto o peso da estimativa inicial $Q_1$ tende a 0.*

*Prova:* De acordo com a expans√£o de $Q_{n+1}$, temos que o peso de $Q_1$ √© $(1-\alpha)^n$. Como $0 < \alpha \leq 1$, temos que $0 \leq 1-\alpha < 1$. Portanto, quando $n \to \infty$, $(1-\alpha)^n \to 0$. Pelo Lema 1.1, a soma dos pesos das recompensas √© dada por $1-(1-\alpha)^n$. Logo, quando $n \to \infty$, $1-(1-\alpha)^n \to 1-0 = 1$.  $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\alpha = 0.1$, ent√£o $(1-\alpha)^n = (0.9)^n$. Quando $n$ aumenta, $(0.9)^n$ se aproxima de 0, portanto, a soma dos pesos das recompensas $1-(0.9)^n$ se aproxima de 1.
> *   Para $n=10$,  $(0.9)^{10} \approx 0.3487$ e $1-(0.9)^{10} \approx 0.6513$
> *   Para $n=100$, $(0.9)^{100} \approx 0.0000266$ e $1-(0.9)^{100} \approx 0.9999734$
> Isso mostra que, com o tempo, o peso das recompensas passadas domina, enquanto o peso da estimativa inicial $Q_1$ se torna desprez√≠vel.

Apesar dessas condi√ß√µes serem √∫teis em an√°lises te√≥ricas, **elas n√£o s√£o frequentemente usadas em aplica√ß√µes pr√°ticas e pesquisas emp√≠ricas**. O motivo √© que sequ√™ncias de par√¢metros de tamanho de passo que satisfazem as condi√ß√µes de converg√™ncia frequentemente levam a uma converg√™ncia muito lenta, e podem precisar de um ajuste consider√°vel para se atingir uma taxa de converg√™ncia satisfat√≥ria [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). O uso de um par√¢metro de tamanho de passo constante, apesar de n√£o garantir a converg√™ncia completa das estimativas, √© mais adapt√°vel a ambientes n√£o estacion√°rios e mais pr√°tico na maioria das situa√ß√µes de aprendizado por refor√ßo [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

**Corol√°rio 1:** *A n√£o satisfa√ß√£o da segunda condi√ß√£o de converg√™ncia para o par√¢metro de tamanho de passo constante ($\alpha_n(a) = \alpha$) n√£o impede seu uso em cen√°rios pr√°ticos, j√° que ele prioriza as recompensas recentes, sendo assim mais adequado para cen√°rios n√£o estacion√°rios, apesar de n√£o garantir uma converg√™ncia total.*

**Proposi√ß√£o 2:** *Em ambientes n√£o estacion√°rios, usar um tamanho de passo constante $\alpha$ pode levar a uma vari√¢ncia maior nas estimativas de valor da a√ß√£o em compara√ß√£o com m√©todos que usam um tamanho de passo decrescente, como a m√©dia amostral, mas essa vari√¢ncia √© um pre√ßo a ser pago para a adaptabilidade.*

*Prova:*  Com um tamanho de passo constante, o algoritmo continuamente responde √†s novas recompensas, ajustando suas estimativas de valor da a√ß√£o. Isso significa que flutua√ß√µes ou mudan√ßas nas recompensas podem ter um impacto significativo e imediato nas estimativas, levando a maior vari√¢ncia. Em contraste, m√©todos com um tamanho de passo decrescente tornam-se menos sens√≠veis a novas informa√ß√µes ao longo do tempo, o que reduz a vari√¢ncia, mas tamb√©m a capacidade de adapta√ß√£o. A vari√¢ncia aumentada √© um efeito colateral da adaptabilidade do tamanho de passo constante, permitindo que o algoritmo rastreie mudan√ßas no ambiente. $\blacksquare$

> üí° **Exemplo Num√©rico:** Imagine um cen√°rio onde a recompensa m√©dia muda repentinamente.
>
> *   **Cen√°rio:** Recompensas iniciais variam em torno de 10, depois mudam para 20.
> *   **M√©dia Amostral:** Come√ßa em 10, e gradualmente se aproxima de 20. A mudan√ßa √© lenta. A vari√¢ncia √© baixa, mas a adapta√ß√£o √© lenta.
> *   **Passo Constante ($\alpha = 0.1$):** Come√ßa em 10, e rapidamente se aproxima de 20, mas com mais flutua√ß√£o. A adapta√ß√£o √© r√°pida, mas a vari√¢ncia √© maior.
>
> ```mermaid
> graph LR
>     A[Recompensa M√©dia] --> B("M√©dia Amostral:\nAdapta√ß√£o Lenta, Baixa Vari√¢ncia");
>     A --> C("Passo Constante:\nAdapta√ß√£o R√°pida, Alta Vari√¢ncia");
>     style A fill:#f9f,stroke:#333,stroke-width:2px
> ```
>
> Este exemplo ilustra como o passo constante proporciona uma resposta mais r√°pida a mudan√ßas, ao custo de maior variabilidade nas estimativas.

### Conclus√£o

A adapta√ß√£o a ambientes n√£o estacion√°rios requer uma mudan√ßa de paradigma em rela√ß√£o aos m√©todos de m√©dias amostrais, que d√£o igual peso a todas as recompensas. A introdu√ß√£o de um par√¢metro de tamanho de passo constante, $\alpha$, permite um aprendizado mais √°gil, priorizando as recompensas mais recentes. Embora as condi√ß√µes de converg√™ncia em teoria guiem a escolha do par√¢metro de tamanho de passo em ambientes est√°ticos, elas s√£o menos aplic√°veis a ambientes din√¢micos e, portanto, menos utilizadas na pr√°tica. O balan√ßo entre converg√™ncia te√≥rica e adapta√ß√£o pr√°tica √© um aspecto chave na concep√ß√£o de algoritmos de aprendizado por refor√ßo robustos e eficazes.

### Refer√™ncias

[^1]: "In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation." *(Trecho de Cap√≠tulo 2: Multi-armed Bandits)*
[^2]: "Sometimes it is convenient to vary the step-size parameter from step to step. Let an(a) denote the step-size parameter used to process the reward received after the nth selection of action a. As we have noted, the choice an(a) == results in the sample-average method, which is guaranteed to converge to the true action values by the law of large numbers. But of course convergence is not guaranteed for all choices of the sequence {an(a)}. A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:
     $\sum_{n=1}^{\infty} \alpha_n(a) = \infty$
     and
      $\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$." *(Trecho de Cap√≠tulo 2: Multi-armed Bandits)*
