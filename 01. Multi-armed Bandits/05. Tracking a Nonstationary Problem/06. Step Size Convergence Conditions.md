## Rastreamento de Problemas N√£o Estacion√°rios: Condi√ß√µes de Converg√™ncia para Step-Sizes

### Introdu√ß√£o
Em problemas de **reinforcement learning**, frequentemente nos deparamos com ambientes *n√£o estacion√°rios*, onde as probabilidades de recompensa n√£o permanecem constantes ao longo do tempo [^1]. Nesses cen√°rios, √© crucial que os algoritmos de aprendizado atribuam maior peso √†s recompensas mais recentes, em detrimento das mais antigas [^1]. Para lidar com essa n√£o estacionariedade, √© comum utilizar um **step-size** (tamanho do passo) constante em vez de m√©dias amostrais, o que introduz um vi√©s permanente nas estimativas de valores de a√ß√£o, embora esse vi√©s diminua com o tempo [^1]. No entanto, ao utilizar step-sizes vari√°veis, √© essencial garantir que o algoritmo convirja para valores de a√ß√£o precisos com o decorrer do tempo. Esta se√ß√£o explorar√° as condi√ß√µes matem√°ticas necess√°rias para assegurar essa converg√™ncia, com foco nas propriedades do step-size vari√°vel $\alpha_n(a)$.

### Conceitos Fundamentais
A base para rastrear problemas n√£o estacion√°rios reside na utiliza√ß√£o de um *step-size constante* (Œ±), onde a atualiza√ß√£o da estimativa de valor de a√ß√£o $Q_{n+1}$ para o passo *n*, dada uma recompensa $R_n$, √© dada por [^1]:

$$Q_{n+1} = Q_n + \alpha[R_n - Q_n]$$

Essa formula√ß√£o pondera exponencialmente as recompensas passadas, atribuindo maior import√¢ncia √†s mais recentes [^1]. No entanto, quando se utiliza um step-size constante, as estimativas nunca convergem completamente e continuam a variar em resposta √†s recompensas mais recentes [^1]. Para garantir que as estimativas convirjam, mesmo com step-sizes vari√°veis, √© necess√°rio que a sequ√™ncia de step-sizes $\{\alpha_n(a)\}$ atenda a certas condi√ß√µes [^1]. Uma maneira de modelar o step-size √© usando $\alpha_n(a)$, que representa o step-size usado no processamento da n-√©sima recompensa para a a√ß√£o a [^1].

**Lemma 1**: *Condi√ß√µes de Converg√™ncia para Step-Sizes Vari√°veis*

A condi√ß√£o necess√°ria e suficiente para que as estimativas convirjam com probabilidade 1, quando se utilizam step-sizes vari√°veis $\{\alpha_n(a)\}$, √© que a sequ√™ncia de step-sizes satisfa√ßa as seguintes condi√ß√µes [^1]:

$$ \sum_{n=1}^{\infty} \alpha_n(a) = \infty \quad \text{e} \quad \sum_{n=1}^{\infty} \alpha_n^2(a) < \infty \qquad (2.7)$$

```mermaid
graph LR
    subgraph "Condi√ß√µes de Converg√™ncia"
    A["Soma dos step-sizes (Œ±_n(a))"] -- "deve ser" --> B{"Infinita (‚àû)"}
    C["Soma dos quadrados dos step-sizes (Œ±_n¬≤(a))"] -- "deve ser" --> D{"Finita (< ‚àû)"}
    end
    B -- "Garante aprendizado cont√≠nuo" --> E["Superar condi√ß√µes iniciais e flutua√ß√µes"]
    D -- "Garante converg√™ncia" --> F["Evitar oscila√ß√µes e estabilizar estimativas"]
```

**Prova**:

*   A primeira condi√ß√£o, $\sum_{n=1}^{\infty} \alpha_n(a) = \infty$, garante que os passos sejam grandes o suficiente para superar as condi√ß√µes iniciais e flutua√ß√µes aleat√≥rias [^1]. Se a soma dos step-sizes fosse finita, o algoritmo poderia parar de aprender antes de atingir uma solu√ß√£o √≥tima ou precisa, ficando preso em um valor sub√≥timo. Em outras palavras, essa condi√ß√£o garante que o algoritmo continue aprendendo e se adaptando ao longo do tempo, explorando as poss√≠veis a√ß√µes e ajustando suas estimativas de valor com base nas recompensas recebidas.

*   A segunda condi√ß√£o, $\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$, garante que os passos se tornem pequenos o suficiente para assegurar a converg√™ncia [^1]. A soma dos quadrados dos step-sizes deve ser finita. Caso contr√°rio, as varia√ß√µes introduzidas pelos passos nunca se tornariam suficientemente pequenas, e o algoritmo n√£o convergiria para valores de a√ß√£o est√°veis. Esta condi√ß√£o impede que as estimativas de valor oscilem indefinidamente, garantindo que, com o tempo, as estimativas se estabilizem em torno do valor real.

Essas duas condi√ß√µes s√£o essenciais para assegurar que, ao lidar com dados que podem variar ao longo do tempo, o algoritmo seja capaz de aprender e convergir, atingindo estimativas de valor de a√ß√£o precisas.

> üí° **Exemplo Num√©rico:** Para ilustrar a primeira condi√ß√£o, considere $\alpha_n = \frac{1}{n}$. A soma $\sum_{n=1}^{\infty} \frac{1}{n}$ √© a s√©rie harm√¥nica, que diverge para infinito. Isso significa que o algoritmo continua a fazer ajustes significativos ao longo do tempo.  Para a segunda condi√ß√£o, $\sum_{n=1}^{\infty} \left(\frac{1}{n}\right)^2 = \sum_{n=1}^{\infty} \frac{1}{n^2}$ que converge para $\frac{\pi^2}{6} \approx 1.645$. Isso significa que o impacto das atualiza√ß√µes diminui √† medida que $n$ aumenta, garantindo que a estimativa se estabilize.
>
>  ```python
> import numpy as np
>
> def harmonic_series(n_terms):
>     return np.sum(1/np.arange(1, n_terms + 1))
>
> def squared_harmonic_series(n_terms):
>     return np.sum(1/(np.arange(1, n_terms + 1)**2))
>
> print(f"Soma da s√©rie harm√¥nica at√© 1000: {harmonic_series(1000):.2f}")
> print(f"Soma da s√©rie harm√¥nica ao quadrado at√© 1000: {squared_harmonic_series(1000):.2f}")
>
> ```
> Este c√≥digo demonstra que, enquanto a soma da s√©rie harm√¥nica cresce indefinidamente, a soma da s√©rie harm√¥nica ao quadrado converge para um valor finito, satisfazendo as condi√ß√µes do Lema 1.

**Lema 1.1**: *Implica√ß√µes da Converg√™ncia em Vari√°veis Aleat√≥rias*

A converg√™ncia com probabilidade 1, mencionada no Lema 1, tamb√©m conhecida como converg√™ncia quase certa, implica que a sequ√™ncia de estimativas $Q_n(a)$ converge para o valor verdadeiro da a√ß√£o $q_*(a)$ para quase todas as poss√≠veis trajet√≥rias de recompensas e a√ß√µes. √â importante notar que essa converg√™ncia n√£o garante uma taxa de converg√™ncia espec√≠fica, apenas a certeza da converg√™ncia em longo prazo.

*   **Prova Estrat√©gica:**  Este resultado se apoia em teoremas da teoria de aproxima√ß√µes estoc√°sticas. Essencialmente, as condi√ß√µes sobre $\alpha_n(a)$ garantem que o algoritmo fa√ßa passos suficientes para escapar de m√≠nimos locais (primeira condi√ß√£o) e que, ao mesmo tempo, os passos se tornem pequenos o suficiente para evitar oscila√ß√µes em torno da solu√ß√£o (segunda condi√ß√£o).

**Corol√°rio 1**: *Step-size constante e suas implica√ß√µes*

A escolha de um *step-size constante* $\alpha_n(a) = \alpha$ satisfaz a primeira condi√ß√£o da equa√ß√£o (2.7) (pois $\sum_{n=1}^\infty \alpha = \infty$), mas n√£o a segunda (pois $\sum_{n=1}^\infty \alpha^2 = \infty$), indicando que as estimativas n√£o convergem completamente, mas continuam a se adaptar √†s recompensas mais recentes [^1]. Essa propriedade √© desej√°vel em ambientes n√£o estacion√°rios, mas n√£o √© adequada para ambientes estacion√°rios, nos quais a converg√™ncia para o valor verdadeiro √© necess√°ria.

```mermaid
graph LR
    A["Step-size constante (Œ±)"] --> B{"Soma dos step-sizes = ‚àû"};
    A --> C{"Soma dos quadrados dos step-sizes = ‚àû"};
    B -- "Atende √† primeira condi√ß√£o" --> D{"Continua aprendendo"}
    C -- "N√£o atende √† segunda condi√ß√£o" --> E{"N√£o converge para um valor √∫nico"}
    D --> F["Apropriado para Ambientes N√£o Estacion√°rios"];
    E --> G["Inadequado para Ambientes Estacion√°rios"];
```

> üí° **Exemplo Num√©rico:** Se $\alpha = 0.1$, ent√£o $\sum_{n=1}^{\infty} 0.1 = \infty$.  A segunda condi√ß√£o $\sum_{n=1}^{\infty} 0.1^2 = \sum_{n=1}^{\infty} 0.01$ tamb√©m diverge. Isso ilustra por que um step-size constante n√£o garante converg√™ncia em termos te√≥ricos para um valor √∫nico, mas permite adapta√ß√£o cont√≠nua √†s recompensas em ambientes n√£o estacion√°rios. Na pr√°tica, a estimativa $Q_n$ ir√° oscilar em torno do valor real, mas estar√° sempre pronta para se ajustar a novas mudan√ßas no ambiente.

**Corol√°rio 1.1**: *Step-size constante com decaimento*

Podemos estender a an√°lise do step-size constante considerando uma varia√ß√£o onde ele decai com o tempo, por exemplo, $\alpha_n(a) = \frac{\alpha}{n^p}$, onde $\alpha$ √© uma constante positiva e $p$ √© um par√¢metro. Para $p > 0$, a primeira condi√ß√£o de converg√™ncia ($\sum_{n=1}^{\infty} \alpha_n(a) = \infty$)  √© satisfeita se $p \leq 1$. A segunda condi√ß√£o ($\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$)  √© satisfeita se $2p > 1$, ou seja, $p > 1/2$. Portanto, para que ambas as condi√ß√µes sejam satisfeitas, devemos ter $\frac{1}{2} < p \leq 1$. Isso demonstra que √© poss√≠vel obter converg√™ncia, mesmo com um step-size que decai, desde que o decaimento seja lento o suficiente para satisfazer a primeira condi√ß√£o e r√°pido o suficiente para satisfazer a segunda.

```mermaid
graph LR
    subgraph "Step-size com Decaimento"
    A["Œ±_n(a) = Œ± / n^p"] --> B{"Primeira condi√ß√£o: p ‚â§ 1"}
    A --> C{"Segunda condi√ß√£o: p > 1/2"}
    end
    B & C --> D{"Condi√ß√µes satisfeitas: 1/2 < p ‚â§ 1"};
    D --> E{"Garante converg√™ncia com decaimento"};
```

> üí° **Exemplo Num√©rico:**  Considerando $\alpha = 0.5$ e $p=0.75$. Ent√£o $\alpha_n = \frac{0.5}{n^{0.75}}$. Para a primeira condi√ß√£o, a s√©rie $\sum_{n=1}^\infty \frac{0.5}{n^{0.75}}$ diverge (pois $0.75 \leq 1$). Para a segunda, $\sum_{n=1}^\infty \left(\frac{0.5}{n^{0.75}}\right)^2 = \sum_{n=1}^\infty \frac{0.25}{n^{1.5}}$ converge (pois $1.5 > 1$). Este caso satisfaz as condi√ß√µes de converg√™ncia do Lema 1.
>
>  ```python
> import numpy as np
>
> def decaying_step_size_sum(alpha, p, n_terms):
>   step_sizes = alpha / (np.arange(1, n_terms + 1)**p)
>   return np.sum(step_sizes), np.sum(step_sizes**2)
>
> alpha = 0.5
> p = 0.75
> sum_alpha, sum_alpha_squared = decaying_step_size_sum(alpha, p, 1000)
> print(f"Soma de alpha_n at√© 1000: {sum_alpha:.2f}")
> print(f"Soma de alpha_n ao quadrado at√© 1000: {sum_alpha_squared:.2f}")
>
> alpha = 0.5
> p = 0.5
> sum_alpha, sum_alpha_squared = decaying_step_size_sum(alpha, p, 1000)
> print(f"Soma de alpha_n at√© 1000 (p=0.5): {sum_alpha:.2f}")
> print(f"Soma de alpha_n ao quadrado at√© 1000 (p=0.5): {sum_alpha_squared:.2f}")
>
> ```
>
> Este exemplo mostra que, ao escolher um valor apropriado de *p*, garantimos que a soma dos step-sizes seja infinita (permitindo que o algoritmo explore e se ajuste) e a soma dos quadrados dos step-sizes seja finita (garantindo a converg√™ncia para um valor est√°vel). Se $p=0.5$, a soma dos quadrados diverge, ilustrando um caso onde as condi√ß√µes de converg√™ncia n√£o s√£o satisfeitas.

**Corol√°rio 2**: *Step-size para Sample-Average Method*

No m√©todo *sample-average*, o step-size √© definido como $\alpha_n(a) = \frac{1}{n}$, onde *n* √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada [^1]. Essa escolha satisfaz ambas as condi√ß√µes em (2.7): $\sum_{n=1}^{\infty} \frac{1}{n} = \infty$ (s√©rie harm√¥nica) e $\sum_{n=1}^{\infty} \frac{1}{n^2} < \infty$ (s√©rie p-harm√¥nica com p=2). Portanto, o m√©todo sample-average garante a converg√™ncia para os valores verdadeiros das a√ß√µes em cen√°rios estacion√°rios.

> üí° **Exemplo Num√©rico:**  Se a a√ß√£o 'a' foi selecionada 4 vezes, os step-sizes seriam: 1, 1/2, 1/3, 1/4.  A soma dessas s√©ries √© infinita (primeira condi√ß√£o), e a soma dos seus quadrados √© finita (segunda condi√ß√£o). Isso assegura que as estimativas dos valores de a√ß√£o convirjam ao longo do tempo.

**Proposi√ß√£o 1**: *Fam√≠lia de Step-Sizes que Satisfazem as Condi√ß√µes de Converg√™ncia*

Uma fam√≠lia de step-sizes que satisfaz as condi√ß√µes (2.7) √© dada por $\alpha_n(a) = \frac{c}{n^p}$, onde *c* √© uma constante positiva, e $1/2 < p \leq 1$.  Essa fam√≠lia de step-sizes inclui o m√©todo sample-average (com $c=1$ e $p=1$) e oferece uma flexibilidade para ajustar a taxa de decaimento dos step-sizes.

```mermaid
graph LR
    A["Fam√≠lia de Step-Sizes: Œ±_n(a) = c / n^p"] --> B{"c > 0, 1/2 < p ‚â§ 1"}
    B --> C{"Inclui Sample-Average (c=1, p=1)"}
    C --> D{"Flexibilidade na taxa de decaimento"};
    D --> E{"Satisfaz condi√ß√µes de converg√™ncia"};
```

*   **Prova Estrat√©gica:**  Podemos provar a proposi√ß√£o usando o teste da integral para avaliar a converg√™ncia das s√©ries. Para que $\sum_{n=1}^{\infty} \frac{c}{n^p}$ seja infinita, devemos ter $p \leq 1$. J√° para que $\sum_{n=1}^{\infty} \frac{c^2}{n^{2p}}$ seja finita, devemos ter $2p > 1$, o que implica $p > 1/2$. Portanto, $1/2 < p \leq 1$ garante o cumprimento de ambas as condi√ß√µes.

> üí° **Exemplo Num√©rico:** Se $c=2$ e $p=0.8$, ent√£o $\alpha_n = \frac{2}{n^{0.8}}$. A s√©rie $\sum_{n=1}^\infty \frac{2}{n^{0.8}}$ diverge (pois $0.8 \leq 1$), enquanto $\sum_{n=1}^\infty \left(\frac{2}{n^{0.8}}\right)^2 = \sum_{n=1}^\infty \frac{4}{n^{1.6}}$ converge (pois $1.6 > 1$).  Este caso tamb√©m satisfaz as condi√ß√µes de converg√™ncia. A constante *c* permite ajustar a magnitude inicial dos step-sizes, enquanto *p* controla a taxa de decaimento ao longo do tempo.

### Conclus√£o
O rastreamento eficaz de problemas n√£o estacion√°rios em reinforcement learning exige um balanceamento cuidadoso entre a adapta√ß√£o a novas informa√ß√µes e a converg√™ncia para valores de a√ß√£o est√°veis. As condi√ß√µes para a converg√™ncia dos step-sizes vari√°veis, expressas por $\sum_{n=1}^{\infty} \alpha_n(a) = \infty$ e $\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$, fornecem uma base te√≥rica para esse balan√ßo. Embora um step-size constante ofere√ßa adapta√ß√£o r√°pida √†s mudan√ßas, ele n√£o garante converg√™ncia completa, tornando-o adequado para ambientes n√£o estacion√°rios. Por outro lado, o m√©todo sample-average, embora garanta a converg√™ncia em ambientes estacion√°rios, pode ser menos eficiente em ambientes n√£o estacion√°rios, devido √† sua menor capacidade de adapta√ß√£o √†s informa√ß√µes mais recentes. A escolha do step-size adequado, portanto, deve ser orientada pelas caracter√≠sticas espec√≠ficas do problema e seus objetivos de aprendizado.

### Refer√™ncias
[^1]: "The averaging methods discussed so far are appropriate for stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule (2.3) for updating an average $Q_n$ of the n ‚àí 1 past rewards is modified to be
$Q_{n+1} = Q_n + \alpha[R_n - Q_n]$,
(2.5)
where the step-size parameter $\alpha \in (0, 1]$ is constant. This results in $Q_{n+1}$ being a weighted average of past rewards and the initial estimate $Q_1$:
$Q_{n+1} = Q_n + \alpha[R_n - Q_n]$
$= \alpha R_n + (1 - \alpha)Q_n$
$= \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha)Q_{n-1}]$
$= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 Q_{n-1}$
$= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 \alpha R_{n-2} + \ldots + (1 - \alpha)^{n-1} \alpha R_1 + (1 - \alpha)^n Q_1$
$=(1-\alpha)^nQ_1+\sum_{i=1}^n\alpha(1-\alpha)^{n-i}R_i$
(2.6)
We call this a weighted average because the sum of the weights is $(1-\alpha)^n+\sum_{i=1}^n \alpha(1-\alpha)^{n-i} = 1$, as you can check for yourself. Note that the weight, $\alpha(1 - \alpha)^{n-i}$, given to the reward $R_i$ depends on how many rewards ago, $n - i$, it was observed. The quantity $1-\alpha$ is less than 1, and thus the weight given to $R_i$ decreases as the number of intervening rewards increases. In fact, the weight decays exponentially according to the exponent on $1-\alpha$. (If $1-\alpha=0$, then all the weight goes on the very last reward, $R_n$, because of the convention that $0^0=1$.) Accordingly, this is sometimes called an exponential recency-weighted average.
Sometimes it is convenient to vary the step-size parameter from step to step. Let $\alpha_n(a)$ denote the step-size parameter used to process the reward received after the $n$th selection of action $a$. As we have noted, the choice $\alpha_n(a) = \frac{1}{n}$ results in the sample-average method, which is guaranteed to converge to the true action values by the law of large numbers. But of course convergence is not guaranteed for all choices of the sequence {$\alpha_n(a)$}. A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:
$\sum_{n=1}^{\infty} \alpha_n(a) = \infty$ and $\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$ (2.7)
The first condition is required to guarantee that the steps are large enough to eventually overcome any initial conditions or random fluctuations. The second condition guarantees that eventually the steps become small enough to assure convergence." *(Trecho de <Chapter 2: Multi-armed Bandits>)*
