## Tracking a Nonstationary Problem: Decaimento Exponencial da Recompensa

### Introdu√ß√£o
Em problemas de aprendizado por refor√ßo, o ambiente muitas vezes n√£o √© estacion√°rio, ou seja, as probabilidades de recompensa podem mudar com o tempo [^1]. Nesses casos, √© crucial que o agente de aprendizado seja capaz de se adaptar a essas mudan√ßas, dando maior peso √†s recompensas mais recentes e menos peso √†s recompensas passadas. Uma das maneiras mais eficazes de se fazer isso √© utilizar um par√¢metro de tamanho de passo constante, resultando em uma m√©dia ponderada exponencialmente por rec√™ncia [^2]. Este cap√≠tulo explora como o peso de uma recompensa anterior diminui exponencialmente com o n√∫mero de recompensas posteriores utilizando um par√¢metro de passo constante.

### Conceitos Fundamentais
#### M√©dia Ponderada por Rec√™ncia
Em ambientes n√£o estacion√°rios, o uso de m√©dias de amostra simples para estimar os valores das a√ß√µes pode n√£o ser ideal, pois trata todas as recompensas igualmente, independentemente de sua rec√™ncia. Para abordar essa limita√ß√£o, podemos usar um par√¢metro de tamanho de passo constante $\alpha$, onde $\alpha \in (0,1]$ [^2]. Isso modifica a regra de atualiza√ß√£o incremental para a m√©dia de recompensas $Q_n$ da seguinte forma:

$$Q_{n+1} = Q_n + \alpha[R_n - Q_n]$$

onde $R_n$ √© a n-√©sima recompensa recebida e $Q_{n+1}$ √© a nova estimativa do valor da a√ß√£o ap√≥s o recebimento da recompensa $R_n$. Essa atualiza√ß√£o resulta em $Q_{n+1}$ sendo uma m√©dia ponderada das recompensas anteriores, com pesos que decaem exponencialmente com a idade da recompensa [^2].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um cen√°rio onde as recompensas obtidas por um agente s√£o: $R_1 = 2$, $R_2 = 3$, $R_3 = 5$, $R_4 = 1$, $R_5 = 4$. Inicializamos $Q_1 = 0$ e usamos $\alpha = 0.3$.
>
> $\text{Step 1: } Q_2 = Q_1 + \alpha(R_1 - Q_1) = 0 + 0.3(2 - 0) = 0.6$
> $\text{Step 2: } Q_3 = Q_2 + \alpha(R_2 - Q_2) = 0.6 + 0.3(3 - 0.6) = 1.32$
> $\text{Step 3: } Q_4 = Q_3 + \alpha(R_3 - Q_3) = 1.32 + 0.3(5 - 1.32) = 2.424$
> $\text{Step 4: } Q_5 = Q_4 + \alpha(R_4 - Q_4) = 2.424 + 0.3(1 - 2.424) = 1.9968$
> $\text{Step 5: } Q_6 = Q_5 + \alpha(R_5 - Q_5) = 1.9968 + 0.3(4 - 1.9968) = 2.60096$
>
> Observe como $Q$ se adapta √†s mudan√ßas em $R$. No in√≠cio, $Q$ aumenta quando $R$ √© relativamente alto, mas depois diminui quando $R$ cai e volta a subir.

**Lemma 1**: *O valor de $Q_{n+1}$ √© uma m√©dia ponderada das recompensas anteriores, onde o peso da recompensa $R_i$ decresce exponencialmente com o n√∫mero de recompensas intermedi√°rias.*

*Prova:*
Come√ßamos pela atualiza√ß√£o recursiva de $Q_{n+1}$:

$$Q_{n+1} = Q_n + \alpha[R_n - Q_n]$$
$$Q_{n+1} = \alpha R_n + (1-\alpha)Q_n$$

Podemos expandir $Q_n$ recursivamente:
$$Q_{n+1} = \alpha R_n + (1-\alpha)(\alpha R_{n-1} + (1-\alpha)Q_{n-1})$$
$$Q_{n+1} = \alpha R_n + \alpha(1-\alpha) R_{n-1} + (1-\alpha)^2 Q_{n-1}$$

Continuando essa expans√£o at√© $Q_1$:

$$Q_{n+1} = \alpha R_n + \alpha(1-\alpha) R_{n-1} + \alpha(1-\alpha)^2 R_{n-2} + \ldots + \alpha(1-\alpha)^{n-1} R_1 + (1-\alpha)^n Q_1$$

Essa express√£o pode ser escrita de forma mais compacta como:
$$Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^{n}\alpha (1-\alpha)^{n-i}R_i$$

O peso da recompensa $R_i$ √© dado por $\alpha(1-\alpha)^{n-i}$. Observe que o expoente $(n-i)$ representa quantas recompensas aconteceram ap√≥s $R_i$ ter sido recebida. Conforme $n-i$ aumenta, ou seja, a recompensa fica mais antiga, o fator $(1-\alpha)^{n-i}$ diminui exponencialmente, fazendo com que o peso da recompensa tamb√©m diminua exponencialmente. O peso da recompensa mais recente, $R_n$, √© $\alpha(1-\alpha)^0 = \alpha$. Quando $i=1$, o peso da recompensa mais antiga √© $\alpha(1-\alpha)^{n-1}$ e o peso de $Q_1$ √© $(1-\alpha)^n$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Usando os mesmos valores de recompensa do exemplo anterior ($R_1=2$, $R_2=3$, $R_3=5$, $R_4=1$, $R_5=4$), com $Q_1=0$ e $\alpha=0.3$, vamos calcular o peso de cada recompensa em $Q_6$:
>
> $\text{Peso de } R_5 = \alpha = 0.3$
> $\text{Peso de } R_4 = \alpha(1-\alpha) = 0.3 * 0.7 = 0.21$
> $\text{Peso de } R_3 = \alpha(1-\alpha)^2 = 0.3 * 0.7^2 = 0.147$
> $\text{Peso de } R_2 = \alpha(1-\alpha)^3 = 0.3 * 0.7^3 = 0.1029$
> $\text{Peso de } R_1 = \alpha(1-\alpha)^4 = 0.3 * 0.7^4 = 0.07203$
> $\text{Peso de } Q_1 = (1-\alpha)^5 = 0.7^5 = 0.16807$
>
> Podemos ver como os pesos diminuem exponencialmente com a antiguidade da recompensa.
>
> ```mermaid
> graph LR
>     subgraph "C√°lculo de Q6"
>     R1["R1=2"] -- "Peso: 0.07203" --> Q6["Q6"]
>     R2["R2=3"] -- "Peso: 0.1029" --> Q6
>     R3["R3=5"] -- "Peso: 0.147" --> Q6
>     R4["R4=1"] -- "Peso: 0.21" --> Q6
>     R5["R5=4"] -- "Peso: 0.3" --> Q6
>     Q1["Q1=0"] -- "Peso: 0.16807" --> Q6
>     end
> ```

**Lemma 1.1:** *The sum of the weights of all rewards and the initial estimate $Q_1$ is 1, demonstrating that $Q_{n+1}$ is a true weighted average.*

*Proof:*
We need to show that the sum of the weights in the expression for $Q_{n+1}$ equals 1:

$$(1-\alpha)^n + \sum_{i=1}^{n}\alpha (1-\alpha)^{n-i} = 1$$

We can rewrite the sum as:

$$\alpha(1-\alpha)^{n-1} + \alpha(1-\alpha)^{n-2} + \dots + \alpha(1-\alpha) + \alpha$$

This is a geometric series with first term $\alpha$ and common ratio $(1-\alpha)$. The sum of the first *n* terms is given by:

$$ \alpha \frac{1 - (1-\alpha)^n}{1 - (1-\alpha)} =  \alpha \frac{1 - (1-\alpha)^n}{\alpha} = 1 - (1-\alpha)^n $$

Therefore, the sum of the weights on all rewards is $1-(1-\alpha)^n$. Adding the weight of $Q_1$ we obtain $(1-\alpha)^n + 1 - (1-\alpha)^n = 1$, which concludes the proof. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Retomando o exemplo anterior com $\alpha = 0.3$ e $n=5$, a soma dos pesos √©:
>
>  $0.07203 + 0.1029 + 0.147 + 0.21 + 0.3 + 0.16807 = 1$
>
>  Isso confirma que $Q_6$ √© uma m√©dia ponderada verdadeira das recompensas anteriores e da estimativa inicial $Q_1$.

**Corol√°rio 1:** *Quando $\alpha=1$, todo o peso √© dado √† recompensa mais recente $R_n$, ignorando as recompensas anteriores.*

*Prova:*
Se $\alpha=1$, ent√£o $Q_{n+1} = 1 * R_n + (1-1)Q_n = R_n$. Todas as recompensas anteriores s√£o ignoradas, com todo o peso em $R_n$ $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Se $\alpha = 1$ e as recompensas forem $R_1 = 2, R_2 = 3, R_3 = 5, R_4 = 1, R_5 = 4$, ent√£o:
>
> $Q_2 = R_1 = 2$
> $Q_3 = R_2 = 3$
> $Q_4 = R_3 = 5$
> $Q_5 = R_4 = 1$
> $Q_6 = R_5 = 4$
>
> A estimativa $Q$ sempre se iguala √† √∫ltima recompensa, desconsiderando as anteriores.
>
> ```mermaid
> sequenceDiagram
>     participant Q_n
>     participant R_n
>     participant Q_n+1
>     Q_n ->> R_n: "Recebe recompensa R_n"
>     R_n -->> Q_n+1: "Q_{n+1} = R_n"
>     activate Q_n+1
>     deactivate Q_n+1
> ```

**Corol√°rio 1.1:** *When $\alpha$ is close to 0, the update rule approximates a simple average if $n$ is large enough*.

*Proof:*
When $\alpha$ is close to 0, $(1-\alpha)$ is close to 1. From Lemma 1, we have

$$Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^{n}\alpha (1-\alpha)^{n-i}R_i$$

If $n$ is sufficiently large and $\alpha$ is sufficiently close to 0, $(1-\alpha)^n$ becomes negligibly small. In this case,
$$Q_{n+1} \approx \sum_{i=1}^{n}\alpha (1-\alpha)^{n-i}R_i$$
Approximating $(1-\alpha)^k$ by $1 - k \alpha$ for sufficiently small $ \alpha$ we have:
$$ Q_{n+1} \approx  \sum_{i=1}^{n}\alpha (1-(n-i)\alpha)R_i =  \alpha\sum_{i=1}^{n}R_i - \alpha^2 \sum_{i=1}^{n} (n-i)R_i $$
Given a large $n$ and small $\alpha$ the second term is small compared with the first, thus we have
$$Q_{n+1} \approx \alpha \sum_{i=1}^{n} R_i$$
Using the fact that we want the sum of the weights to be equal to 1, i.e.  $\sum_{i=1}^{n}\alpha (1-\alpha)^{n-i} \approx 1$, which is approximately equal to $n\alpha$ when $\alpha \to 0$. From which $\alpha \approx \frac{1}{n}$.
Thus,
$$Q_{n+1} \approx \frac{1}{n} \sum_{i=1}^{n} R_i$$
Which is the usual expression for a simple average. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos usar $\alpha = 0.01$ e $n=100$ com recompensas $R_i$ entre 1 e 5. Para $Q_{101}$ temos:
>
>  $Q_{101} \approx 0.01 * (R_1 + R_2 + ... + R_{100})$.
>
>  A m√©dia simples das recompensas √© $\frac{1}{100}(R_1 + R_2 + ... + R_{100})$. Se considerarmos que a soma das recompensas √© 300, teremos $Q_{101} \approx 0.01 * 300 = 3$.
>
> A m√©dia simples seria: $\frac{300}{100} = 3$.
>
> Quando $\alpha$ √© muito pequeno e $n$ √© grande, a m√©dia ponderada se aproxima da m√©dia simples.

**Corol√°rio 2:** *O fator de decaimento exponencial $(1-\alpha)$ determina a taxa na qual o peso de recompensas antigas diminui. Valores de $\alpha$ mais pr√≥ximos de 1 d√£o maior import√¢ncia para recompensas mais recentes, e valores menores d√£o mais import√¢ncia a recompensas mais antigas.*

*Prova:*
O peso da recompensa $R_i$ √© $\alpha(1-\alpha)^{n-i}$. O fator de decaimento exponencial √© $(1-\alpha)$. Se $\alpha$ est√° pr√≥ximo de 1, ent√£o $(1-\alpha)$ est√° pr√≥ximo de 0 e o peso das recompensas anteriores diminui muito rapidamente com o aumento de $(n-i)$. Se $\alpha$ est√° pr√≥ximo de 0, ent√£o $(1-\alpha)$ est√° pr√≥ximo de 1, e o peso das recompensas anteriores diminui lentamente com o aumento de $(n-i)$. Isso mostra que o fator de decaimento exponencial controla a import√¢ncia relativa entre recompensas recentes e antigas. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos comparar o decaimento para $\alpha = 0.1$ e $\alpha = 0.9$
>
> | n-i |  Peso com $\alpha=0.1$  | Peso com $\alpha=0.9$ |
> |-----|------------------------|-----------------------|
> | 0   |  0.1                   | 0.9                   |
> | 1   |  0.09                  | 0.09                  |
> | 2   |  0.081                 | 0.009                 |
> | 3   |  0.0729                | 0.0009                |
> | 4   |  0.06561               | 0.00009               |
>
> Observe que com $\alpha=0.9$, o peso decai muito mais rapidamente do que com $\alpha=0.1$, dando mais import√¢ncia √†s recompensas recentes.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> alpha1 = 0.1
> alpha2 = 0.9
> n_values = np.arange(0, 10)
>
> weights1 = alpha1 * (1 - alpha1)**n_values
> weights2 = alpha2 * (1 - alpha2)**n_values
>
> plt.plot(n_values, weights1, label='alpha=0.1')
> plt.plot(n_values, weights2, label='alpha=0.9')
> plt.xlabel('n-i (Number of steps since reward)')
> plt.ylabel('Weight of Reward')
> plt.title('Exponential Decay of Reward Weights')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> ```mermaid
>   graph LR
>       subgraph "Decaimento Exponencial"
>       direction LR
>       n0[n-i = 0] --> w01["Peso Œ±=0.1: 0.1"]
>       n0 --> w09["Peso Œ±=0.9: 0.9"]
>       n1[n-i = 1] --> w11["Peso Œ±=0.1: 0.09"]
>       n1 --> w19["Peso Œ±=0.9: 0.09"]
>       n2[n-i = 2] --> w21["Peso Œ±=0.1: 0.081"]
>       n2 --> w29["Peso Œ±=0.9: 0.009"]
>       n3[n-i = 3] --> w31["Peso Œ±=0.1: 0.0729"]
>       n3 --> w39["Peso Œ±=0.9: 0.0009"]
>       n4[n-i = 4] --> w41["Peso Œ±=0.1: 0.06561"]
>       n4 --> w49["Peso Œ±=0.9: 0.00009"]
>       end
> ```

**Proposi√ß√£o 1:** *The effective number of past rewards considered by this exponentially weighted average is approximately $\frac{1}{\alpha}$.*

*Proof:*
Let's define the effective number of steps considered as the number of rewards with a significant influence on $Q_{n+1}$. This can be quantified by the point where the exponentially decaying weights drop to a small fraction of their initial value.
The weight for $R_n$ is $\alpha$, and the weight for $R_{n-k}$ is $\alpha(1-\alpha)^k$. We are looking for a $k$ such that the weight $\alpha(1-\alpha)^k$ becomes negligible.
Let‚Äôs consider the number of steps when the weights drops to $\frac{\alpha}{e}$.
$$\alpha(1-\alpha)^k = \frac{\alpha}{e} $$
$$(1-\alpha)^k = \frac{1}{e}$$
Taking the logarithm of both sides:
$$k \ln(1-\alpha) = \ln(\frac{1}{e}) = -1$$
$$k = \frac{-1}{\ln(1-\alpha)}$$
When $\alpha$ is small, we can use the approximation $\ln(1-\alpha) \approx -\alpha$ so
$$k \approx \frac{-1}{-\alpha} = \frac{1}{\alpha}$$
Therefore, the effective number of past rewards considered is approximately $\frac{1}{\alpha}$. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Se $\alpha = 0.1$, o n√∫mero efetivo de recompensas passadas √© $\frac{1}{0.1} = 10$. Isso significa que, aproximadamente, as √∫ltimas 10 recompensas t√™m um impacto significativo na estimativa atual de $Q$. Se $\alpha = 0.01$, ent√£o o n√∫mero efetivo de passos ser√° $\frac{1}{0.01}=100$, ou seja, a m√©dia ponderada considera um hist√≥rico maior.
>
> Para $\alpha=0.2$ the effective number of past rewards is $\frac{1}{0.2}=5$
>
> ```python
> import numpy as np
>
> alpha_values = [0.1, 0.01, 0.2]
> effective_steps = [1/alpha for alpha in alpha_values]
>
> print("Alpha values:", alpha_values)
> print("Effective steps:", effective_steps)
> ```

### Conclus√£o
Em ambientes n√£o estacion√°rios, a capacidade de rastrear mudan√ßas e adaptar-se a novas condi√ß√µes √© vital para o aprendizado eficiente. A utiliza√ß√£o de uma atualiza√ß√£o de m√©dia ponderada com um par√¢metro de tamanho de passo constante $\alpha$ fornece um m√©todo eficaz para dar mais peso √†s recompensas recentes e menos peso √†s recompensas antigas. O decaimento exponencial do peso das recompensas anteriores garante que o agente de aprendizado se adapte rapidamente a mudan√ßas no ambiente, otimizando o desempenho a longo prazo. O ajuste adequado de $\alpha$ permite controlar a taxa de adapta√ß√£o e equil√≠brio entre a resposta r√°pida a mudan√ßas e a estabilidade nas estimativas de valor.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule (2.3) for updating an average Qn of the n ‚Äì 1 past rewards is modified to be Qn+1 = Qn + a[Rn - Qn], where the step-size parameter a ‚àà (0, 1] is constant." *(Trecho de Chapter 2: Multi-armed Bandits)*

**Summary of Changes:**

1.  **Lemma 1.1:** Added numerical example to demonstrate the weights sum to 1.
2.  **Corollary 1:** Added numerical example to illustrate that with $\alpha = 1$ the current value equals the current reward.
3.  **Corollary 1.1:** Added numerical example to show the approximation with simple average with small alpha and big n.
4.   **Corollary 2:** Added numerical example and chart comparing the decay with different alpha values.
5.  **Proposition 1:** Added numerical example to show the relation between alpha and the number of steps.
6.  Added code snippets, tables and charts for visualization.

These additions provide practical illustrations of the theoretical concepts, reinforcing the understanding of how the exponentially weighted average works. They also add depth to the explanations and provide tools for further analysis.
