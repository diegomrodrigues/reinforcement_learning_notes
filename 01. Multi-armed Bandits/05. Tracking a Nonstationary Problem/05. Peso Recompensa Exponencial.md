## Tracking a Nonstationary Problem: Decaimento Exponencial da Recompensa

### IntroduÃ§Ã£o
Em problemas de aprendizado por reforÃ§o, o ambiente muitas vezes nÃ£o Ã© estacionÃ¡rio, ou seja, as probabilidades de recompensa podem mudar com o tempo [^1]. Nesses casos, Ã© crucial que o agente de aprendizado seja capaz de se adaptar a essas mudanÃ§as, dando maior peso Ã s recompensas mais recentes e menos peso Ã s recompensas passadas. Uma das maneiras mais eficazes de se fazer isso Ã© utilizar um parÃ¢metro de tamanho de passo constante, resultando em uma mÃ©dia ponderada exponencialmente por recÃªncia [^2]. Este capÃ­tulo explora como o peso de uma recompensa anterior diminui exponencialmente com o nÃºmero de recompensas posteriores utilizando um parÃ¢metro de passo constante.

### Conceitos Fundamentais
#### MÃ©dia Ponderada por RecÃªncia
Em ambientes nÃ£o estacionÃ¡rios, o uso de mÃ©dias de amostra simples para estimar os valores das aÃ§Ãµes pode nÃ£o ser ideal, pois trata todas as recompensas igualmente, independentemente de sua recÃªncia. Para abordar essa limitaÃ§Ã£o, podemos usar um parÃ¢metro de tamanho de passo constante $\alpha$, onde $\alpha \in (0,1]$ [^2]. Isso modifica a regra de atualizaÃ§Ã£o incremental para a mÃ©dia de recompensas $Q_n$ da seguinte forma:

$$Q_{n+1} = Q_n + \alpha[R_n - Q_n]$$

onde $R_n$ Ã© a n-Ã©sima recompensa recebida e $Q_{n+1}$ Ã© a nova estimativa do valor da aÃ§Ã£o apÃ³s o recebimento da recompensa $R_n$. Essa atualizaÃ§Ã£o resulta em $Q_{n+1}$ sendo uma mÃ©dia ponderada das recompensas anteriores, com pesos que decaem exponencialmente com a idade da recompensa [^2].

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos considerar um cenÃ¡rio onde as recompensas obtidas por um agente sÃ£o: $R_1 = 2$, $R_2 = 3$, $R_3 = 5$, $R_4 = 1$, $R_5 = 4$. Inicializamos $Q_1 = 0$ e usamos $\alpha = 0.3$.
>
> $\text{Step 1: } Q_2 = Q_1 + \alpha(R_1 - Q_1) = 0 + 0.3(2 - 0) = 0.6$
> $\text{Step 2: } Q_3 = Q_2 + \alpha(R_2 - Q_2) = 0.6 + 0.3(3 - 0.6) = 1.32$
> $\text{Step 3: } Q_4 = Q_3 + \alpha(R_3 - Q_3) = 1.32 + 0.3(5 - 1.32) = 2.424$
> $\text{Step 4: } Q_5 = Q_4 + \alpha(R_4 - Q_4) = 2.424 + 0.3(1 - 2.424) = 1.9968$
> $\text{Step 5: } Q_6 = Q_5 + \alpha(R_5 - Q_5) = 1.9968 + 0.3(4 - 1.9968) = 2.60096$
>
> Observe como $Q$ se adapta Ã s mudanÃ§as em $R$. No inÃ­cio, $Q$ aumenta quando $R$ Ã© relativamente alto, mas depois diminui quando $R$ cai e volta a subir.

**Lemma 1**: *O valor de $Q_{n+1}$ Ã© uma mÃ©dia ponderada das recompensas anteriores, onde o peso da recompensa $R_i$ decresce exponencialmente com o nÃºmero de recompensas intermediÃ¡rias.*

*Prova:*
ComeÃ§amos pela atualizaÃ§Ã£o recursiva de $Q_{n+1}$:

$$Q_{n+1} = Q_n + \alpha[R_n - Q_n]$$
$$Q_{n+1} = \alpha R_n + (1-\alpha)Q_n$$

Podemos expandir $Q_n$ recursivamente:
$$Q_{n+1} = \alpha R_n + (1-\alpha)(\alpha R_{n-1} + (1-\alpha)Q_{n-1})$$
$$Q_{n+1} = \alpha R_n + \alpha(1-\alpha) R_{n-1} + (1-\alpha)^2 Q_{n-1}$$

Continuando essa expansÃ£o atÃ© $Q_1$:

$$Q_{n+1} = \alpha R_n + \alpha(1-\alpha) R_{n-1} + \alpha(1-\alpha)^2 R_{n-2} + \ldots + \alpha(1-\alpha)^{n-1} R_1 + (1-\alpha)^n Q_1$$

Essa expressÃ£o pode ser escrita de forma mais compacta como:
$$Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^{n}\alpha (1-\alpha)^{n-i}R_i$$

O peso da recompensa $R_i$ Ã© dado por $\alpha(1-\alpha)^{n-i}$. Observe que o expoente $(n-i)$ representa quantas recompensas aconteceram apÃ³s $R_i$ ter sido recebida. Conforme $n-i$ aumenta, ou seja, a recompensa fica mais antiga, o fator $(1-\alpha)^{n-i}$ diminui exponencialmente, fazendo com que o peso da recompensa tambÃ©m diminua exponencialmente. O peso da recompensa mais recente, $R_n$, Ã© $\alpha(1-\alpha)^0 = \alpha$. Quando $i=1$, o peso da recompensa mais antiga Ã© $\alpha(1-\alpha)^{n-1}$ e o peso de $Q_1$ Ã© $(1-\alpha)^n$. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Usando os mesmos valores de recompensa do exemplo anterior ($R_1=2$, $R_2=3$, $R_3=5$, $R_4=1$, $R_5=4$), com $Q_1=0$ e $\alpha=0.3$, vamos calcular o peso de cada recompensa em $Q_6$:
>
> $\text{Peso de } R_5 = \alpha = 0.3$
> $\text{Peso de } R_4 = \alpha(1-\alpha) = 0.3 * 0.7 = 0.21$
> $\text{Peso de } R_3 = \alpha(1-\alpha)^2 = 0.3 * 0.7^2 = 0.147$
> $\text{Peso de } R_2 = \alpha(1-\alpha)^3 = 0.3 * 0.7^3 = 0.1029$
> $\text{Peso de } R_1 = \alpha(1-\alpha)^4 = 0.3 * 0.7^4 = 0.07203$
> $\text{Peso de } Q_1 = (1-\alpha)^5 = 0.7^5 = 0.16807$
>
> Podemos ver como os pesos diminuem exponencialmente com a antiguidade da recompensa.
>
> ```mermaid
> graph LR
>     subgraph "CÃ¡lculo de Q6"
>     R1["R1=2"] -- "Peso: 0.07203" --> Q6["Q6"]
>     R2["R2=3"] -- "Peso: 0.1029" --> Q6
>     R3["R3=5"] -- "Peso: 0.147" --> Q6
>     R4["R4=1"] -- "Peso: 0.21" --> Q6
>     R5["R5=4"] -- "Peso: 0.3" --> Q6
>     Q1["Q1=0"] -- "Peso: 0.16807" --> Q6
>     end
> ```

**Lemma 1.1:** *The sum of the weights of all rewards and the initial estimate $Q_1$ is 1, demonstrating that $Q_{n+1}$ is a true weighted average.*

*Proof:*
We need to show that the sum of the weights in the expression for $Q_{n+1}$ equals 1:

$$(1-\alpha)^n + \sum_{i=1}^{n}\alpha (1-\alpha)^{n-i} = 1$$

We can rewrite the sum as:

$$\alpha(1-\alpha)^{n-1} + \alpha(1-\alpha)^{n-2} + \dots + \alpha(1-\alpha) + \alpha$$

This is a geometric series with first term $\alpha$ and common ratio $(1-\alpha)$. The sum of the first *n* terms is given by:

$$ \alpha \frac{1 - (1-\alpha)^n}{1 - (1-\alpha)} =  \alpha \frac{1 - (1-\alpha)^n}{\alpha} = 1 - (1-\alpha)^n $$

Therefore, the sum of the weights on all rewards is $1-(1-\alpha)^n$. Adding the weight of $Q_1$ we obtain $(1-\alpha)^n + 1 - (1-\alpha)^n = 1$, which concludes the proof. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Retomando o exemplo anterior com $\alpha = 0.3$ e $n=5$, a soma dos pesos Ã©:
>
>  $0.07203 + 0.1029 + 0.147 + 0.21 + 0.3 + 0.16807 = 1$
>
>  Isso confirma que $Q_6$ Ã© uma mÃ©dia ponderada verdadeira das recompensas anteriores e da estimativa inicial $Q_1$.

**CorolÃ¡rio 1:** *Quando $\alpha=1$, todo o peso Ã© dado Ã  recompensa mais recente $R_n$, ignorando as recompensas anteriores.*

*Prova:*
Se $\alpha=1$, entÃ£o $Q_{n+1} = 1 * R_n + (1-1)Q_n = R_n$. Todas as recompensas anteriores sÃ£o ignoradas, com todo o peso em $R_n$ $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Se $\alpha = 1$ e as recompensas forem $R_1 = 2, R_2 = 3, R_3 = 5, R_4 = 1, R_5 = 4$, entÃ£o:
>
> $Q_2 = R_1 = 2$
> $Q_3 = R_2 = 3$
> $Q_4 = R_3 = 5$
> $Q_5 = R_4 = 1$
> $Q_6 = R_5 = 4$
>
> A estimativa $Q$ sempre se iguala Ã  Ãºltima recompensa, desconsiderando as anteriores.
>
> ```mermaid
> sequenceDiagram
>     participant Q_n
>     participant R_n
>     participant Q_n+1
>     Q_n ->> R_n: "Recebe recompensa R_n"
>     R_n -->> Q_n+1: "Q_{n+1} = R_n"
>     activate Q_n+1
>     deactivate Q_n+1
> ```

**CorolÃ¡rio 1.1:** *When $\alpha$ is close to 0, the update rule approximates a simple average if $n$ is large enough*.

*Proof:*
When $\alpha$ is close to 0, $(1-\alpha)$ is close to 1. From Lemma 1, we have

$$Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^{n}\alpha (1-\alpha)^{n-i}R_i$$

If $n$ is sufficiently large and $\alpha$ is sufficiently close to 0, $(1-\alpha)^n$ becomes negligibly small. In this case,
$$Q_{n+1} \approx \sum_{i=1}^{n}\alpha (1-\alpha)^{n-i}R_i$$
Approximating $(1-\alpha)^k$ by $1 - k \alpha$ for sufficiently small $ \alpha$ we have:
$$ Q_{n+1} \approx  \sum_{i=1}^{n}\alpha (1-(n-i)\alpha)R_i =  \alpha\sum_{i=1}^{n}R_i - \alpha^2 \sum_{i=1}^{n} (n-i)R_i $$
Given a large $n$ and small $\alpha$ the second term is small compared with the first, thus we have
$$Q_{n+1} \approx \alpha \sum_{i=1}^{n} R_i$$
Using the fact that we want the sum of the weights to be equal to 1, i.e.  $\sum_{i=1}^{n}\alpha (1-\alpha)^{n-i} \approx 1$, which is approximately equal to $n\alpha$ when $\alpha \to 0$. From which $\alpha \approx \frac{1}{n}$.
Thus,
$$Q_{n+1} \approx \frac{1}{n} \sum_{i=1}^{n} R_i$$
Which is the usual expression for a simple average. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos usar $\alpha = 0.01$ e $n=100$ com recompensas $R_i$ entre 1 e 5. Para $Q_{101}$ temos:
>
>  $Q_{101} \approx 0.01 * (R_1 + R_2 + ... + R_{100})$.
>
>  A mÃ©dia simples das recompensas Ã© $\frac{1}{100}(R_1 + R_2 + ... + R_{100})$. Se considerarmos que a soma das recompensas Ã© 300, teremos $Q_{101} \approx 0.01 * 300 = 3$.
>
> A mÃ©dia simples seria: $\frac{300}{100} = 3$.
>
> Quando $\alpha$ Ã© muito pequeno e $n$ Ã© grande, a mÃ©dia ponderada se aproxima da mÃ©dia simples.

**CorolÃ¡rio 2:** *O fator de decaimento exponencial $(1-\alpha)$ determina a taxa na qual o peso de recompensas antigas diminui. Valores de $\alpha$ mais prÃ³ximos de 1 dÃ£o maior importÃ¢ncia para recompensas mais recentes, e valores menores dÃ£o mais importÃ¢ncia a recompensas mais antigas.*

*Prova:*
O peso da recompensa $R_i$ Ã© $\alpha(1-\alpha)^{n-i}$. O fator de decaimento exponencial Ã© $(1-\alpha)$. Se $\alpha$ estÃ¡ prÃ³ximo de 1, entÃ£o $(1-\alpha)$ estÃ¡ prÃ³ximo de 0 e o peso das recompensas anteriores diminui muito rapidamente com o aumento de $(n-i)$. Se $\alpha$ estÃ¡ prÃ³ximo de 0, entÃ£o $(1-\alpha)$ estÃ¡ prÃ³ximo de 1, e o peso das recompensas anteriores diminui lentamente com o aumento de $(n-i)$. Isso mostra que o fator de decaimento exponencial controla a importÃ¢ncia relativa entre recompensas recentes e antigas. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Vamos comparar o decaimento para $\alpha = 0.1$ e $\alpha = 0.9$
>
> | n-i |  Peso com $\alpha=0.1$  | Peso com $\alpha=0.9$ |
> |-----|------------------------|-----------------------|
> | 0   |  0.1                   | 0.9                   |
> | 1   |  0.09                  | 0.09                  |
> | 2   |  0.081                 | 0.009                 |
> | 3   |  0.0729                | 0.0009                |
> | 4   |  0.06561               | 0.00009               |
>
> Observe que com $\alpha=0.9$, o peso decai muito mais rapidamente do que com $\alpha=0.1$, dando mais importÃ¢ncia Ã s recompensas recentes.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> alpha1 = 0.1
> alpha2 = 0.9
> n_values = np.arange(0, 10)
>
> weights1 = alpha1 * (1 - alpha1)**n_values
> weights2 = alpha2 * (1 - alpha2)**n_values
>
> plt.plot(n_values, weights1, label='alpha=0.1')
> plt.plot(n_values, weights2, label='alpha=0.9')
> plt.xlabel('n-i (Number of steps since reward)')
> plt.ylabel('Weight of Reward')
> plt.title('Exponential Decay of Reward Weights')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
> ```mermaid
>   graph LR
>       subgraph "Decaimento Exponencial"
>       direction LR
>       n0[n-i = 0] --> w01["Peso Î±=0.1: 0.1"]
>       n0 --> w09["Peso Î±=0.9: 0.9"]
>       n1[n-i = 1] --> w11["Peso Î±=0.1: 0.09"]
>       n1 --> w19["Peso Î±=0.9: 0.09"]
>       n2[n-i = 2] --> w21["Peso Î±=0.1: 0.081"]
>       n2 --> w29["Peso Î±=0.9: 0.009"]
>       n3[n-i = 3] --> w31["Peso Î±=0.1: 0.0729"]
>       n3 --> w39["Peso Î±=0.9: 0.0009"]
>       n4[n-i = 4] --> w41["Peso Î±=0.1: 0.06561"]
>       n4 --> w49["Peso Î±=0.9: 0.00009"]
>       end
> ```

**ProposiÃ§Ã£o 1:** *The effective number of past rewards considered by this exponentially weighted average is approximately $\frac{1}{\alpha}$.*

*Proof:*
Let's define the effective number of steps considered as the number of rewards with a significant influence on $Q_{n+1}$. This can be quantified by the point where the exponentially decaying weights drop to a small fraction of their initial value.
The weight for $R_n$ is $\alpha$, and the weight for $R_{n-k}$ is $\alpha(1-\alpha)^k$. We are looking for a $k$ such that the weight $\alpha(1-\alpha)^k$ becomes negligible.
Letâ€™s consider the number of steps when the weights drops to $\frac{\alpha}{e}$.
$$\alpha(1-\alpha)^k = \frac{\alpha}{e} $$
$$(1-\alpha)^k = \frac{1}{e}$$
Taking the logarithm of both sides:
$$k \ln(1-\alpha) = \ln(\frac{1}{e}) = -1$$
$$k = \frac{-1}{\ln(1-\alpha)}$$
When $\alpha$ is small, we can use the approximation $\ln(1-\alpha) \approx -\alpha$ so
$$k \approx \frac{-1}{-\alpha} = \frac{1}{\alpha}$$
Therefore, the effective number of past rewards considered is approximately $\frac{1}{\alpha}$. $\blacksquare$

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Se $\alpha = 0.1$, o nÃºmero efetivo de recompensas passadas Ã© $\frac{1}{0.1} = 10$. Isso significa que, aproximadamente, as Ãºltimas 10 recompensas tÃªm um impacto significativo na estimativa atual de $Q$. Se $\alpha = 0.01$, entÃ£o o nÃºmero efetivo de passos serÃ¡ $\frac{1}{0.01}=100$, ou seja, a mÃ©dia ponderada considera um histÃ³rico maior.
>
> Para $\alpha=0.2$ the effective number of past rewards is $\frac{1}{0.2}=5$
>
> ```python
> import numpy as np
>
> alpha_values = [0.1, 0.01, 0.2]
> effective_steps = [1/alpha for alpha in alpha_values]
>
> print("Alpha values:", alpha_values)
> print("Effective steps:", effective_steps)
> ```

### ConclusÃ£o
Em ambientes nÃ£o estacionÃ¡rios, a capacidade de rastrear mudanÃ§as e adaptar-se a novas condiÃ§Ãµes Ã© vital para o aprendizado eficiente. A utilizaÃ§Ã£o de uma atualizaÃ§Ã£o de mÃ©dia ponderada com um parÃ¢metro de tamanho de passo constante $\alpha$ fornece um mÃ©todo eficaz para dar mais peso Ã s recompensas recentes e menos peso Ã s recompensas antigas. O decaimento exponencial do peso das recompensas anteriores garante que o agente de aprendizado se adapte rapidamente a mudanÃ§as no ambiente, otimizando o desempenho a longo prazo. O ajuste adequado de $\alpha$ permite controlar a taxa de adaptaÃ§Ã£o e equilÃ­brio entre a resposta rÃ¡pida a mudanÃ§as e a estabilidade nas estimativas de valor.

### ReferÃªncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule (2.3) for updating an average Qn of the n â€“ 1 past rewards is modified to be Qn+1 = Qn + a[Rn - Qn], where the step-size parameter a âˆˆ (0, 1] is constant." *(Trecho de Chapter 2: Multi-armed Bandits)*

**Summary of Changes:**

1.  **Lemma 1.1:** Added numerical example to demonstrate the weights sum to 1.
2.  **Corollary 1:** Added numerical example to illustrate that with $\alpha = 1$ the current value equals the current reward.
3.  **Corollary 1.1:** Added numerical example to show the approximation with simple average with small alpha and big n.
4.   **Corollary 2:** Added numerical example and chart comparing the decay with different alpha values.
5.  **Proposition 1:** Added numerical example to show the relation between alpha and the number of steps.
6.  Added code snippets, tables and charts for visualization.

These additions provide practical illustrations of the theoretical concepts, reinforcing the understanding of how the exponentially weighted average works. They also add depth to the explanations and provide tools for further analysis.
