### Rastreamento de um Problema N√£o Estacion√°rio: Condi√ß√µes de Converg√™ncia

### Introdu√ß√£o
Este cap√≠tulo explora o conceito de **multi-armed bandits**, focando particularmente em problemas n√£o estacion√°rios, onde as recompensas de cada a√ß√£o podem mudar ao longo do tempo [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Em contraste com cen√°rios estacion√°rios, onde as estat√≠sticas das recompensas permanecem constantes, os problemas n√£o estacion√°rios exigem que os algoritmos de aprendizagem se adaptem √†s mudan√ßas din√¢micas nas distribui√ß√µes de recompensa. Isso leva √† necessidade de m√©todos que deem mais peso √†s recompensas recentes, ao inv√©s de longas recompensas do passado. Especificamente, o presente t√≥pico se aprofunda nas condi√ß√µes de converg√™ncia, demonstrando como elas s√£o atendidas para a m√©dia amostral, mas n√£o para o tamanho de passo constante, explorando as implica√ß√µes desta diferen√ßa.

### Conceitos Fundamentais
#### M√©dia Amostral e o M√©todo do Tamanho de Passo Constante
Em problemas estacion√°rios, a **m√©dia amostral** √© um m√©todo natural para estimar os valores das a√ß√µes, em que a estimativa √© calculada como a m√©dia de todas as recompensas recebidas at√© o momento. Formalmente, a estimativa de valor $Q_t(a)$ para uma a√ß√£o $a$ no instante $t$ √© definida como:

$$ Q_t(a) = \frac{\text{soma das recompensas quando } a \text{ foi selecionada antes de } t}{\text{n√∫mero de vezes que } a \text{ foi selecionada antes de } t} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}} $$
onde $R_i$ √© a recompensa recebida no instante $i$, e $\mathbb{1}_{A_i=a}$ √© uma fun√ß√£o indicadora que vale 1 se a a√ß√£o $a$ foi selecionada no instante $i$ e 0 caso contr√°rio [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). Se o denominador for zero, definimos $Q_t(a)$ como um valor padr√£o, por exemplo, 0. O m√©todo da **m√©dia amostral** garante que $Q_t(a)$ convirja para o valor verdadeiro $q_*(a)$ conforme o n√∫mero de amostras tende ao infinito, pela lei dos grandes n√∫meros [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3).

> üí° **Exemplo Num√©rico:** Considere um cen√°rio com uma √∫nica a√ß√£o ($a$) que foi selecionada 5 vezes. As recompensas obtidas foram: 1, 3, 2, 4, 5. Usando a m√©dia amostral, a estimativa do valor da a√ß√£o no instante $t=6$ seria:
>
> $Q_6(a) = \frac{1 + 3 + 2 + 4 + 5}{5} = \frac{15}{5} = 3$.
>
> Este valor representa a m√©dia das recompensas at√© o momento. Se a a√ß√£o fosse selecionada novamente e rendesse uma recompensa de 6, a nova estimativa seria:
>
> $Q_7(a) = \frac{1 + 3 + 2 + 4 + 5 + 6}{6} = \frac{21}{6} = 3.5$.
>
> Observamos que a estimativa se ajusta lentamente √† medida que mais recompensas s√£o obtidas.

Por outro lado, em problemas n√£o estacion√°rios, √© vantajoso dar mais peso √†s recompensas recentes. Uma forma comum de fazer isso √© usar um **tamanho de passo constante**, dado por:

```mermaid
graph LR
    A["Q_n (Estimativa Atual)"] -->| "Recompensa R_n" | B("R_n - Q_n (Erro de Previs√£o)")
    B -->| "Taxa de Aprendizagem Œ±" | C("Œ± * (R_n - Q_n)")
    C --> D["Q_{n+1} (Nova Estimativa)"];
    A --> D
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

$$ Q_{n+1} = Q_n + \alpha [R_n - Q_n] $$

onde $\alpha \in (0, 1]$ √© uma constante, $R_n$ √© a recompensa recebida no n-√©simo instante, e $Q_n$ √© a estimativa de valor atualizada [8](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-8). Esta abordagem resulta em uma m√©dia ponderada das recompensas passadas, onde recompensas mais recentes t√™m um peso exponencialmente maior do que recompensas mais antigas [8](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-8).

> üí° **Exemplo Num√©rico:** Usando o mesmo cen√°rio anterior, vamos supor um tamanho de passo constante $\alpha = 0.1$ e um valor inicial $Q_1(a) = 0$. As recompensas s√£o as mesmas: 1, 3, 2, 4, 5. As atualiza√ß√µes seriam:
>
> $Q_2(a) = 0 + 0.1(1 - 0) = 0.1$
> $Q_3(a) = 0.1 + 0.1(3 - 0.1) = 0.1 + 0.1(2.9) = 0.39$
> $Q_4(a) = 0.39 + 0.1(2 - 0.39) = 0.39 + 0.1(1.61) = 0.551$
> $Q_5(a) = 0.551 + 0.1(4 - 0.551) = 0.551 + 0.1(3.449) = 0.8959$
> $Q_6(a) = 0.8959 + 0.1(5 - 0.8959) = 0.8959 + 0.1(4.1041) = 1.30631$
>
> Notamos que, ao contr√°rio da m√©dia amostral, o valor se ajusta mais rapidamente √†s novas recompensas. Se, ap√≥s a sexta a√ß√£o, a recompensa ca√≠sse para 0, a atualiza√ß√£o seria:
>
> $Q_7(a) = 1.30631 + 0.1(0 - 1.30631) = 1.30631 - 0.130631 = 1.175679$.
>
> Podemos observar que a estimativa √© mais sens√≠vel √†s mudan√ßas e n√£o mant√©m as recompensas antigas com tanta influ√™ncia como a m√©dia amostral.

**Proposi√ß√£o 1:** A atualiza√ß√£o do tamanho de passo constante pode ser expressa como uma m√©dia ponderada exponencial das recompensas.

*Demonstra√ß√£o:*
Podemos expandir recursivamente a atualiza√ß√£o do tamanho de passo constante:
\begin{align*}
Q_{n+1} &= Q_n + \alpha[R_n - Q_n] \\
&= \alpha R_n + (1-\alpha)Q_n \\
&= \alpha R_n + (1-\alpha)(\alpha R_{n-1} + (1-\alpha)Q_{n-1}) \\
&= \alpha R_n + \alpha(1-\alpha)R_{n-1} + (1-\alpha)^2 Q_{n-1} \\
&= \alpha R_n + \alpha(1-\alpha)R_{n-1} + \alpha(1-\alpha)^2 R_{n-2} + \dots + (1-\alpha)^n Q_1
\end{align*}
Portanto, podemos observar que $Q_{n+1}$ √© uma m√©dia ponderada das recompensas anteriores onde o peso da recompensa $R_i$ √© dado por $\alpha(1-\alpha)^{n-i}$. As recompensas mais recentes t√™m um peso exponencialmente maior, uma vez que $(1-\alpha)$ √© menor que 1.

> üí° **Exemplo Num√©rico:** Vamos usar novamente $\alpha = 0.1$. Se examinarmos os pesos das recompensas, vemos que a √∫ltima recompensa $R_n$ tem peso $\alpha = 0.1$, a pen√∫ltima $R_{n-1}$ tem peso $\alpha(1-\alpha) = 0.1 \times 0.9 = 0.09$, a antepen√∫ltima $R_{n-2}$ tem peso $\alpha(1-\alpha)^2 = 0.1 \times 0.9^2 = 0.081$, e assim por diante. As recompensas mais antigas rapidamente perdem import√¢ncia na estimativa de $Q_{n+1}$. Isso √© crucial em ambientes n√£o estacion√°rios onde as recompensas passadas podem n√£o ser relevantes para as recompensas futuras.

#### Condi√ß√µes de Converg√™ncia
A converg√™ncia de um algoritmo de aprendizagem para os valores reais das a√ß√µes √© crucial. Uma an√°lise te√≥rica da converg√™ncia no contexto da aproxima√ß√£o estoc√°stica define duas condi√ß√µes necess√°rias para garantir que as estimativas convirjam com probabilidade 1 [9](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-9):

1.  **Condi√ß√£o de Soma:** A soma dos tamanhos de passo ao longo do tempo deve tender ao infinito. Isso assegura que os passos sejam grandes o suficiente para superar as condi√ß√µes iniciais ou flutua√ß√µes aleat√≥rias. Formalmente,

    $$ \sum_{n=1}^{\infty} \alpha_n(a) = \infty $$
2. **Condi√ß√£o da Soma dos Quadrados:** A soma dos quadrados dos tamanhos de passo ao longo do tempo deve ser finita. Isso garante que os passos se tornem pequenos o suficiente para garantir a converg√™ncia. Formalmente:

    $$ \sum_{n=1}^{\infty} \alpha_n^2(a) < \infty $$
    onde $\alpha_n(a)$ √© o par√¢metro de tamanho de passo usado para processar a recompensa recebida ap√≥s a n-√©sima sele√ß√£o da a√ß√£o $a$.

**Lema 1:** A condi√ß√£o de soma $\sum_{n=1}^{\infty} \alpha_n(a) = \infty$ √© necess√°ria para que um algoritmo de atualiza√ß√£o de valores possa, em teoria, explorar todo o espa√ßo de poss√≠veis valores de a√ß√£o.
   *Demonstra√ß√£o:*  Se a soma dos tamanhos de passo fosse finita, a magnitude total das mudan√ßas nos valores de a√ß√£o seria limitada, independentemente de quantas atualiza√ß√µes fossem feitas. Isso impediria o algoritmo de escapar de valores iniciais sub√≥timos e de se ajustar a mudan√ßas nos valores reais das a√ß√µes.

#### An√°lise das Condi√ß√µes de Converg√™ncia

No caso da **m√©dia amostral**, o par√¢metro do tamanho de passo $\alpha_n(a) = \frac{1}{n}$ [9](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-9]. Ambas as condi√ß√µes s√£o satisfeitas:
   -   A soma dos inversos dos inteiros (s√©rie harm√¥nica) diverge, satisfazendo a primeira condi√ß√£o [9](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-9].
   -   A soma dos quadrados dos inversos dos inteiros converge, satisfazendo a segunda condi√ß√£o [9](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-9].

> üí° **Exemplo Num√©rico:** Para ilustrar a condi√ß√£o de soma com a m√©dia amostral, consideremos os primeiros termos da s√©rie harm√¥nica: $1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \ldots$. Se calcularmos a soma dos primeiros 10 termos, obtemos aproximadamente 2.929. A soma dos primeiros 100 termos √© aproximadamente 5.187, e dos primeiros 1000 termos √© aproximadamente 7.485. Vemos que, embora a soma cres√ßa muito lentamente, ela continua a crescer e, em teoria, diverge para infinito.
>
>  Para ilustrar a condi√ß√£o da soma dos quadrados, consideremos a s√©rie $\sum_{n=1}^{\infty} \frac{1}{n^2} = 1 + \frac{1}{4} + \frac{1}{9} + \frac{1}{16} + \ldots$. A soma dos primeiros 10 termos √© aproximadamente 1.549, dos primeiros 100 √© aproximadamente 1.634, e a soma para um n√∫mero muito grande de termos se aproxima de $\frac{\pi^2}{6} \approx 1.645$. A s√©rie converge para um valor finito, o que atende √† segunda condi√ß√£o.

```mermaid
graph LR
    subgraph "M√©dia Amostral"
    A["Œ±_n = 1/n"]
    B["Œ£(Œ±_n) = ‚àû (Diverge)"]
    C["Œ£(Œ±_n^2) < ‚àû (Converge)"]
    end
    A --> B
    A --> C
```

No entanto, para o m√©todo do **tamanho de passo constante**, $\alpha_n(a) = \alpha$, onde $\alpha$ √© uma constante entre 0 e 1.
   -  A primeira condi√ß√£o √© satisfeita, pois a soma de uma constante $\alpha$ ao longo de infinitos instantes diverge.
   - A segunda condi√ß√£o **n√£o** √© satisfeita, porque a soma do quadrado de uma constante $\alpha^2$ ao longo de infinitos instantes tamb√©m diverge.

> üí° **Exemplo Num√©rico:** Se $\alpha = 0.1$, a condi√ß√£o de soma √© $\sum_{n=1}^{\infty} 0.1 = 0.1 + 0.1 + 0.1 + \ldots$, que diverge para infinito. J√° a condi√ß√£o da soma dos quadrados √© $\sum_{n=1}^{\infty} 0.1^2 = \sum_{n=1}^{\infty} 0.01 = 0.01 + 0.01 + 0.01 + \ldots$, que tamb√©m diverge para infinito. Este comportamento impede a converg√™ncia da estimativa do valor da a√ß√£o, o que √© intencional em cen√°rios n√£o estacion√°rios para permitir adapta√ß√£o cont√≠nua.

```mermaid
graph LR
    subgraph "Tamanho de Passo Constante"
    A["Œ±_n = Œ±"]
    B["Œ£(Œ±_n) = ‚àû (Diverge)"]
    C["Œ£(Œ±_n^2) = ‚àû (Diverge)"]
    end
    A --> B
    A --> C
```

**Observa√ß√£o 1:** √â importante ressaltar que a n√£o satisfa√ß√£o da segunda condi√ß√£o de converg√™ncia pela atualiza√ß√£o do tamanho de passo constante n√£o √© necessariamente um problema em contextos n√£o estacion√°rios. De fato, a diverg√™ncia da soma dos quadrados dos tamanhos de passo √© uma caracter√≠stica desej√°vel que permite ao algoritmo manter-se receptivo a mudan√ßas nas recompensas.

Este resultado implica que, embora a m√©dia amostral convirja para o valor verdadeiro das a√ß√µes, o m√©todo do tamanho de passo constante n√£o garante tal converg√™ncia. Ele continuar√° variando em resposta a recompensas recentes. O n√£o cumprimento da segunda condi√ß√£o por esse m√©todo sinaliza que as estimativas nunca convergem completamente, mas continuam a variar em resposta a recompensas recebidas mais recentemente [9](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-9]. Esse comportamento, embora n√£o conduza √† converg√™ncia, √© desej√°vel em ambientes n√£o estacion√°rios, j√° que permite que o agente aprenda a se adaptar √†s mudan√ßas.

**Teorema 1:**  A m√©dia amostral, sob condi√ß√µes estacion√°rias, converge para o valor verdadeiro das a√ß√µes com probabilidade 1. No entanto, em cen√°rios n√£o estacion√°rios, ela pode se mostrar lenta ou inadequada para adapta√ß√£o √†s mudan√ßas nas recompensas, dada a sua caracter√≠stica de atribuir peso uniforme a todas as recompensas observadas.
   *Demonstra√ß√£o:* A converg√™ncia da m√©dia amostral em condi√ß√µes estacion√°rias √© um resultado direto da Lei dos Grandes N√∫meros. No entanto, em problemas n√£o estacion√°rios, as recompensas observadas em um per√≠odo podem n√£o ser representativas das recompensas em um per√≠odo posterior, tornando a m√©dia amostral uma estimativa inadequada, pois atribui igual peso a todas as recompensas passadas, n√£o priorizando as mais recentes que podem refletir melhor a din√¢mica atual do ambiente.

### Conclus√£o

A an√°lise das condi√ß√µes de converg√™ncia revela que a **m√©dia amostral** e o **tamanho de passo constante** s√£o abordagens fundamentalmente diferentes para estimar os valores das a√ß√µes em problemas de multi-armed bandits. Embora a m√©dia amostral seja garantidamente convergente em cen√°rios estacion√°rios, ela pode ser inadequada para problemas n√£o estacion√°rios. Por outro lado, o tamanho de passo constante, embora n√£o garanta a converg√™ncia das estimativas para valores fixos, permite que o algoritmo se adapte a mudan√ßas nas distribui√ß√µes de recompensa, o que o torna mais adequado para cen√°rios n√£o estacion√°rios. A escolha entre esses m√©todos depende da natureza do problema em quest√£o, com o m√©todo de tamanho de passo constante se tornando mais relevante em situa√ß√µes onde as recompensas podem mudar ao longo do tempo.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^3]: "Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: Qt(a) = sum of rewards when a taken prior to t / number of times a taken prior to t." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^8]: "One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule (2.3) for updating an average Qn of the n ‚Äì 1 past rewards is modified to be Qn+1 = Qn + a[Rn - Qn], where the step-size parameter a ‚àà (0, 1] is constant." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^9]: "A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1:  Œ£(an) = ‚àû and  Œ£(an^2) < ‚àû." *(Trecho de Chapter 2: Multi-armed Bandits)*
