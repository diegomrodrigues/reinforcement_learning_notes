## Rastreando Problemas N√£o Estacion√°rios com Tamanho de Passo Constante
### Introdu√ß√£o
No contexto de **multi-armed bandits**, os problemas podem ser classificados como estacion√°rios ou n√£o estacion√°rios. Em **problemas estacion√°rios**, as probabilidades de recompensa associadas a cada a√ß√£o permanecem constantes ao longo do tempo. No entanto, em **problemas n√£o estacion√°rios**, essas probabilidades podem mudar, tornando as abordagens de aprendizado mais desafiadoras [^1]. Este cap√≠tulo explora como lidar com problemas n√£o estacion√°rios, concentrando-se especificamente no uso de um tamanho de passo constante na atualiza√ß√£o incremental dos valores das a√ß√µes [^1].

### Conceitos Fundamentais
Em problemas de *multi-armed bandit* n√£o estacion√°rios, onde as recompensas podem mudar ao longo do tempo, dar mais peso √†s recompensas recentes torna-se essencial [^1]. Isso √© alcan√ßado atrav√©s do uso de um **tamanho de passo constante** na atualiza√ß√£o incremental dos valores das a√ß√µes. Ao contr√°rio do m√©todo de *sample-average*, que atribui pesos iguais a todas as recompensas, um tamanho de passo constante permite que o algoritmo aprenda continuamente e se adapte a mudan√ßas nas probabilidades de recompensa.

**Atualiza√ß√£o Incremental com Tamanho de Passo Constante**
A atualiza√ß√£o incremental com um **tamanho de passo constante** $\alpha$ pode ser expressa como:

$$Q_{n+1} = Q_n + \alpha[R_n - Q_n]$$

onde $Q_{n+1}$ √© a nova estimativa do valor da a√ß√£o ap√≥s o recebimento da n-√©sima recompensa $R_n$ e $Q_n$ √© a estimativa anterior [^1]. O tamanho de passo constante $\alpha$ est√° no intervalo (0,1] [^1]. Esta f√≥rmula modifica a m√©dia de recompensa para que ela tenha uma prefer√™ncia maior por recompensas mais recentes.

> üí° **Exemplo Num√©rico:**
> Suponha que temos uma a√ß√£o cujo valor estimado inicial √© $Q_1 = 10$. Recebemos uma recompensa $R_1 = 12$ ap√≥s realizar essa a√ß√£o pela primeira vez. Usando um tamanho de passo $\alpha = 0.1$, atualizamos o valor estimado como:
>
> $Q_2 = Q_1 + \alpha[R_1 - Q_1] = 10 + 0.1[12 - 10] = 10 + 0.1 * 2 = 10.2$
>
> Se na pr√≥xima vez a recompensa fosse $R_2 = 8$, a atualiza√ß√£o seria:
>
> $Q_3 = Q_2 + \alpha[R_2 - Q_2] = 10.2 + 0.1[8 - 10.2] = 10.2 + 0.1 * (-2.2) = 10.2 - 0.22 = 9.98$
>
> Este exemplo demonstra como a atualiza√ß√£o com tamanho de passo constante ajusta o valor da a√ß√£o em dire√ß√£o √† recompensa mais recente, mas sem desconsiderar completamente as recompensas anteriores.
>
> ```mermaid
>   graph LR
>       A["Q1=10"] -->| "R1=12, Œ±=0.1" | B("Q2=10.2")
>       B -->| "R2=8, Œ±=0.1" | C("Q3=9.98")
> ```

**An√°lise da M√©dia Ponderada**
A f√≥rmula de atualiza√ß√£o com tamanho de passo constante pode ser expandida para mostrar que $Q_{n+1}$ se torna uma m√©dia ponderada de todas as recompensas anteriores e a estimativa inicial $Q_1$:

$$Q_{n+1} = \alpha R_n + (1-\alpha)Q_n$$
$$= \alpha R_n + (1-\alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}]$$
$$= \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2Q_{n-1}$$
$$\vdots$$
$$= (1-\alpha)^n Q_1 + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i$$

A an√°lise da equa√ß√£o revela que o peso dado a uma recompensa $R_i$ √© $\alpha(1-\alpha)^{n-i}$, onde $n-i$ √© o n√∫mero de recompensas passadas desde a observa√ß√£o de $R_i$. O fator $(1-\alpha)$ √© menor que 1 e, portanto, o peso das recompensas decai exponencialmente com o tempo, atribuindo maior import√¢ncia √†s recompensas mais recentes [^1].

> üí° **Exemplo Num√©rico:**
> Usando o mesmo exemplo anterior, e assumindo $Q_1 = 10$, $R_1=12$, $R_2=8$, e $\alpha=0.1$, vamos calcular $Q_3$ diretamente pela f√≥rmula da m√©dia ponderada:
>
> $Q_3 = (1-0.1)^2 \times 10 + 0.1 \times (1-0.1)^{3-1} \times 12 + 0.1 \times (1-0.1)^{3-2} \times 8$
> $Q_3 = (0.9)^2 \times 10 + 0.1 \times (0.9)^2 \times 12 + 0.1 \times (0.9)^1 \times 8$
> $Q_3 = 0.81 \times 10 + 0.1 \times 0.81 \times 12 + 0.1 \times 0.9 \times 8$
> $Q_3 = 8.1 + 0.972 + 0.72 = 9.792$
>
> Vamos ver o que aconteceu com a primeira atualiza√ß√£o:
> $Q_2 = (1-0.1)^{1} \times 10 + 0.1 \times (1-0.1)^{1-1} \times 12 = 0.9 * 10 + 0.1 * 1 * 12 = 9 + 1.2 = 10.2$, igual ao que hav√≠amos calculado anteriormente.
>
> Note que $Q_3$ usando a expans√£o da m√©dia ponderada, 9.792 √© diferente do valor 9.98 que t√≠nhamos obtido. Isso ocorre por um erro de arredondamento na sequ√™ncia anterior. Para $Q_3$ obtivemos:
>
> $Q_3 = 10.2 + 0.1[8 - 10.2] = 10.2 + 0.1 * (-2.2) = 10.2 - 0.22 = 9.98$
>
> O valor exato de $Q_3$ usando a expans√£o da m√©dia ponderada √© 9.792.
>
> Podemos observar que o peso da recompensa $R_2=8$ √© $0.1 * (1 - 0.1)^{3-2} = 0.1 * 0.9 = 0.09$, enquanto o peso da recompensa $R_1=12$ √© $0.1 * (1 - 0.1)^{3-1} = 0.1 * 0.81 = 0.081$. O peso da estimativa inicial $Q_1=10$ √© $(1-0.1)^2 = 0.81$. O peso das recompensas decai exponencialmente, dando mais import√¢ncia √†s recompensas recentes.

**M√©dia Ponderada Exponencialmente Recente**
Devido ao decaimento exponencial dos pesos, a atualiza√ß√£o com tamanho de passo constante √© frequentemente chamada de m√©dia ponderada exponencialmente recente [^1]. Essa abordagem √© adequada para lidar com problemas n√£o estacion√°rios, pois atribui maior import√¢ncia √†s recompensas mais recentes.

**Considera√ß√µes sobre a Escolha do Tamanho de Passo**

A escolha do valor de $\alpha$ afeta a velocidade com que o algoritmo se adapta √†s mudan√ßas. Um valor de $\alpha$ mais alto resulta em uma maior √™nfase nas recompensas recentes e uma adapta√ß√£o mais r√°pida √†s mudan√ßas, mas tamb√©m uma maior sensibilidade ao ru√≠do. Por outro lado, um valor de $\alpha$ mais baixo leva a uma adapta√ß√£o mais lenta e suaviza√ß√£o das recompensas, mas pode ser mais robusto a mudan√ßas s√∫bitas e ru√≠do.

> üí° **Exemplo Num√©rico:**
> Vamos comparar o efeito de diferentes valores de $\alpha$ em uma sequ√™ncia de recompensas. Suponha que as recompensas sejam $R = [1, 2, 3, 8, 9, 10]$ e que $Q_1 = 0$. Vamos comparar os valores de $Q_n$ para $\alpha=0.1$ e $\alpha=0.5$.
>
> Para $\alpha = 0.1$:
> *   $Q_2 = 0 + 0.1(1-0) = 0.1$
> *   $Q_3 = 0.1 + 0.1(2-0.1) = 0.29$
> *   $Q_4 = 0.29 + 0.1(3-0.29) = 0.561$
> *   $Q_5 = 0.561 + 0.1(8-0.561) = 1.2949$
> *   $Q_6 = 1.2949 + 0.1(9-1.2949) = 2.06541$
> *   $Q_7 = 2.06541 + 0.1(10-2.06541) = 2.859$
>
> Para $\alpha = 0.5$:
> *   $Q_2 = 0 + 0.5(1-0) = 0.5$
> *   $Q_3 = 0.5 + 0.5(2-0.5) = 1.25$
> *   $Q_4 = 1.25 + 0.5(3-1.25) = 2.125$
> *   $Q_5 = 2.125 + 0.5(8-2.125) = 5.0625$
> *   $Q_6 = 5.0625 + 0.5(9-5.0625) = 7.03125$
> *  $Q_7 = 7.03125 + 0.5(10-7.03125) = 8.515625$
>
> Observe que com $\alpha = 0.5$, os valores $Q_n$ mudam mais rapidamente e se aproximam das recompensas recentes, enquanto com $\alpha = 0.1$ a adapta√ß√£o √© mais lenta. Isso ilustra o trade-off entre adapta√ß√£o r√°pida e suaviza√ß√£o.
>
> | n | R  | Q (Œ±=0.1)  | Q (Œ±=0.5)  |
> |---|----|-----------|-----------|
> | 1 | -  |   0       |    0      |
> | 2 | 1  |   0.1     |   0.5     |
> | 3 | 2  |   0.29    |   1.25    |
> | 4 | 3  |   0.561   |   2.125   |
> | 5 | 8  |   1.2949  |   5.0625  |
> | 6 | 9  |   2.0654  |   7.03125 |
> | 7 | 10 |   2.859   |  8.515625 |
>
>
> ```mermaid
>   graph LR
>       A["Recompensas"] --> B("Q - Œ±=0.1")
>       A --> C("Q - Œ±=0.5")
>       B --> D["Adapta√ß√£o Lenta"]
>       C --> E["Adapta√ß√£o R√°pida"]
> ```

**Lemma 1**
A atualiza√ß√£o com tamanho de passo constante produz uma m√©dia ponderada exponencialmente recente das recompensas com o peso de cada recompensa $R_i$ dado por  $\alpha(1-\alpha)^{n-i}$ e o peso da estimativa inicial $Q_1$ dado por $(1-\alpha)^n$, em que $n$ √© o n√∫mero de recompensas obtidas at√© o momento.
*Prova:*
A prova √© feita por indu√ß√£o a partir da expans√£o da equa√ß√£o de atualiza√ß√£o incremental com tamanho de passo constante:
$Q_{n+1} =  \alpha R_n + (1-\alpha)Q_n$
Expandindo $Q_n$ de forma recursiva, obtemos:
$Q_{n+1} = \alpha R_n + (1-\alpha)(\alpha R_{n-1} + (1-\alpha)Q_{n-1})$
$Q_{n+1} = \alpha R_n + \alpha(1-\alpha) R_{n-1} + (1-\alpha)^2Q_{n-1}$
Continuando recursivamente, chegamos em:
$Q_{n+1} = \alpha R_n + \alpha(1-\alpha) R_{n-1} + \alpha(1-\alpha)^2 R_{n-2} + \ldots + \alpha(1-\alpha)^{n-1}R_1 + (1-\alpha)^nQ_1$
Que √© a forma expandida de:
$$Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}R_i$$
$\blacksquare$

**Corol√°rio 1**
O peso total de todas as recompensas mais o peso da estimativa inicial totalizam 1.
*Prova:*
Para provar que a soma dos pesos das recompensas e a estimativa inicial √© igual a 1, podemos somar o peso da estimativa inicial  $(1-\alpha)^n$  com a soma dos pesos das recompensas   $\sum_{i=1}^n \alpha(1-\alpha)^{n-i}$,  e realizar uma simplifica√ß√£o:

$$(1-\alpha)^n + \sum_{i=1}^n \alpha(1-\alpha)^{n-i}$$
Essa soma pode ser reescrita expandindo-se os termos:
$$(1-\alpha)^n + \alpha(1-\alpha)^{n-1} + \alpha(1-\alpha)^{n-2} + \ldots + \alpha(1-\alpha) + \alpha$$
Podemos reescrever a soma de tr√°s para frente, ou seja, do √≠ndice 1 at√© n:
$$(1-\alpha)^n + \alpha \sum_{i=1}^n (1-\alpha)^{n-i}$$
Multiplicando por $(1-\alpha)$ e dividindo por $(1-\alpha)$:
$$(1-\alpha)^n + \alpha \frac{1}{1-\alpha} \sum_{i=1}^n (1-\alpha)^{n-i+1} - (1-\alpha)^{n-i} $$
$$(1-\alpha)^n + \frac{\alpha}{1-\alpha} \sum_{i=1}^n (1-\alpha)^{n-i+1} - (1-\alpha)^{n-i} $$
$$(1-\alpha)^n + \frac{\alpha}{1-\alpha} [ (1-\alpha)^n - (1-\alpha)^0 ]$$
$$(1-\alpha)^n + \frac{\alpha}{1-\alpha} [ (1-\alpha)^n - 1]$$
$$(1-\alpha)^n + \frac{\alpha}{1-\alpha} (1-\alpha)^n  - \frac{\alpha}{1-\alpha} $$
$$(1-\alpha)^n (1 + \frac{\alpha}{1-\alpha})  - \frac{\alpha}{1-\alpha} $$
$$(1-\alpha)^n (\frac{1-\alpha + \alpha}{1-\alpha})  - \frac{\alpha}{1-\alpha} $$
$$(1-\alpha)^n \frac{1}{1-\alpha}  - \frac{\alpha}{1-\alpha}$$
$$ (1-\alpha)^{n-1} -  \frac{\alpha}{1-\alpha}$$
A parte somat√≥ria √© a soma de uma s√©rie geom√©trica com o primeiro termo igual a 1 e a raz√£o igual a $(1-\alpha)$, cujo resultado √© igual a:

$$ (1-\alpha)^n + \alpha \frac{1-(1-\alpha)^n}{1 - (1-\alpha)} = (1-\alpha)^n + \alpha \frac{1-(1-\alpha)^n}{\alpha} = (1-\alpha)^n + (1-(1-\alpha)^n) = 1$$
$\blacksquare$

**Lema 1.1**
O peso total dado a todas as recompensas at√© o instante $n$ pode ser expresso como $1-(1-\alpha)^n$.
*Prova:*
O peso total dado a todas as recompensas √© dado pela soma $\sum_{i=1}^n \alpha(1-\alpha)^{n-i}$. Do Corol√°rio 1, sabemos que o peso total de todas as recompensas mais o peso da estimativa inicial √© igual a 1. O peso da estimativa inicial √© dado por $(1-\alpha)^n$. Portanto, o peso total das recompensas √© dado por $1 - (1-\alpha)^n$.
$\blacksquare$

> üí° **Exemplo Num√©rico:**
> Suponha que temos $\alpha = 0.2$. Ap√≥s 5 recompensas, o peso total das recompensas √© $1 - (1 - 0.2)^5 = 1 - 0.8^5 = 1 - 0.32768 = 0.67232$. O peso da estimativa inicial $Q_1$ √© $(1 - 0.2)^5 = 0.32768$. Como esperado, a soma dos pesos das recompensas mais o peso da estimativa inicial √© $0.67232 + 0.32768 = 1$. Este exemplo mostra que, √† medida que o n√∫mero de recompensas aumenta, o peso total das recompensas se aproxima de 1 e o peso da estimativa inicial se aproxima de 0.
> ```mermaid
>   graph LR
>     subgraph "C√°lculo dos Pesos"
>       A["Œ± = 0.2"] --> B["Ap√≥s 5 recompensas"]
>       B --> C{"Peso Total das Recompensas = 1 - (1-0.2)^5 = 0.67232"}
>        B --> D{"Peso de Q1 = (1 - 0.2)^5 = 0.32768"}
>       C --> E{"Soma dos Pesos = 0.67232 + 0.32768 = 1"}
>       D --> E
>    end
> ```

**Proposi√ß√£o 1**
Quando $\alpha \rightarrow 0$, a atualiza√ß√£o com tamanho de passo constante se aproxima da atualiza√ß√£o *sample-average*, em que todas as recompensas s√£o igualmente ponderadas.
*Prova:*
Quando $\alpha$ tende a zero, o termo $(1-\alpha)^n$ se aproxima de 1, e os pesos $\alpha(1-\alpha)^{n-i}$ tendem a 0. No entanto, o peso total das recompensas, $1-(1-\alpha)^n$, tamb√©m tende a 0. Isto significa que o peso das recompensas se espalha por todas as recompensas at√© um passado muito distante. A aproxima√ß√£o ocorre porque, quando $\alpha$ √© muito pequeno, o valor de $Q_{n+1}$ depende quase que completamente de $Q_n$, o que, por sua vez, depende de $Q_{n-1}$ e assim por diante, fazendo com que $Q_{n+1}$ dependa de todos os valores passados. No limite, a depend√™ncia de recompensas anteriores passa a ter um peso equiprov√°vel. Em outras palavras, quando $\alpha$ se aproxima de zero, a atualiza√ß√£o com tamanho de passo constante ret√©m as informa√ß√µes de recompensas passadas por mais tempo, da mesma forma como o sample-average, que armazena todas as recompensas e as usa para gerar uma m√©dia. A diferen√ßa √© que o sample-average usa todas as recompensas com o mesmo peso, enquanto que, quando alpha se aproxima de 0, o peso das recompensas antigas, na m√©dia ponderada exponencial, se aproxima tamb√©m de um peso igual.
$\blacksquare$

**Teorema 1**
Em um problema n√£o estacion√°rio, onde as recompensas mudam em fun√ß√£o do tempo,  o uso de um tamanho de passo constante $\alpha$ pode resultar em uma melhor performance em rela√ß√£o ao sample-average, se o valor de $\alpha$ for adequadamente ajustado para as mudan√ßas no problema.
*Prova:*
Em um problema n√£o estacion√°rio, o sample-average, que atribui pesos iguais a todas as recompensas, n√£o consegue se adaptar rapidamente √†s mudan√ßas nas probabilidades de recompensa. Como a m√©dia ponderada exponencialmente recente com um tamanho de passo $\alpha$ adequado atribui maior import√¢ncia √†s recompensas recentes, ela consegue acompanhar as mudan√ßas no problema. No entanto, um valor muito grande de $\alpha$ pode levar a uma instabilidade na estimativa, enquanto um valor muito pequeno de $\alpha$ n√£o se adapta t√£o rapidamente. O valor adequado de $\alpha$ depende das caracter√≠sticas das mudan√ßas do problema, como a velocidade e a magnitude dessas mudan√ßas.
$\blacksquare$

### Conclus√£o
O uso de um **tamanho de passo constante** na atualiza√ß√£o incremental √© uma t√©cnica fundamental para lidar com problemas de *multi-armed bandit* n√£o estacion√°rios. Ao dar maior peso √†s recompensas recentes, o algoritmo consegue adaptar-se rapidamente √†s mudan√ßas nas probabilidades de recompensa. A escolha do valor adequado para $\alpha$ √© crucial para balancear a adapta√ß√£o r√°pida e a robustez ao ru√≠do. Ao entender a m√©dia ponderada exponencialmente recente e os seus resultados, podemos ajustar e otimizar o processo de aprendizado para ambientes n√£o estacion√°rios.

### Refer√™ncias
[^1]: "In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback differs from, and yet can be combined with, instructive feedback." *(Trecho de Multi-armed Bandits)*
[^2]: "One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule (2.3) for updating an average Qn of the n ‚Äì 1 past rewards is modified to be Qn+1 = Qn + a[Rn - Qn], where the step-size parameter a ‚àà (0, 1] is constant. This results in Qn+1 being a weighted average of past rewards and the initial estimate Q1:" *(Trecho de Multi-armed Bandits)*
