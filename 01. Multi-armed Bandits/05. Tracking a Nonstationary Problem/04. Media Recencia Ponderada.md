## Rastreamento de Problemas N√£o Estacion√°rios: M√©dia de Rec√™ncia Ponderada Exponencial

### Introdu√ß√£o
No contexto de **multi-armed bandits**, a capacidade de lidar com problemas n√£o estacion√°rios √© crucial para o sucesso do aprendizado por refor√ßo [1]. Problemas n√£o estacion√°rios s√£o aqueles onde as probabilidades de recompensa associadas a cada a√ß√£o mudam ao longo do tempo, o que torna inadequado o uso de m√©todos que d√£o igual import√¢ncia a todas as recompensas passadas [2]. Essa se√ß√£o explora uma abordagem para lidar com n√£o estacionariedade, que envolve dar mais peso √†s recompensas recentes do que √†s recompensas mais antigas. Uma maneira popular de alcan√ßar isso √© utilizando uma m√©dia de rec√™ncia ponderada exponencial [2].

### Conceitos Fundamentais

Para entender a m√©dia de rec√™ncia ponderada exponencial, √© fundamental revisitar a atualiza√ß√£o incremental de valores de a√ß√£o. A forma incremental original, que considera todas as recompensas at√© o momento, calcula a m√©dia de todas as recompensas recebidas para cada a√ß√£o [2]:
$$
    Q_n = \frac{R_1 + R_2 + \ldots + R_{n-1}}{n-1}.
$$
Onde $Q_n$ √© a estimativa do valor da a√ß√£o ap√≥s $n-1$ vezes que ela foi selecionada e $R_i$ √© a recompensa recebida na $i$-√©sima sele√ß√£o da a√ß√£o. No entanto, essa m√©dia simples n√£o √© ideal em cen√°rios n√£o estacion√°rios, onde as recompensas mais recentes s√£o mais relevantes [2].

> üí° **Exemplo Num√©rico:** Considere uma a√ß√£o que foi selecionada 4 vezes com as recompensas [2, 4, 6, 8]. Usando a m√©dia simples, a estimativa do valor da a√ß√£o ap√≥s 4 sele√ß√µes √© $Q_5 = \frac{2+4+6+8}{4} = 5$. No entanto, se as recompensas mais recentes s√£o mais importantes, esta m√©dia pode n√£o refletir o valor atual da a√ß√£o.

Uma forma de abordar este problema √© modificar a regra de atualiza√ß√£o incremental usando um par√¢metro de tamanho de passo constante, denotado por $\alpha$, onde $\alpha \in (0, 1]$ [2]:
```mermaid
graph LR
    A["Q_n"] -->|"+"| B["alpha[R_n - Q_n]"]
    B --> C["Q_{n+1}"]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
```
$$
    Q_{n+1} = Q_n + \alpha[R_n - Q_n].
$$
Esta f√≥rmula d√° maior peso √† recompensa mais recente, $R_n$, do que √†s recompensas passadas. Este processo resulta em $Q_{n+1}$ ser uma m√©dia ponderada das recompensas passadas e da estimativa inicial $Q_1$. Expandindo a f√≥rmula, podemos ver que ela pode ser expressa como [2]:

```mermaid
graph LR
    subgraph "Expans√£o da Atualiza√ß√£o"
    A("Q_{n+1}") --> B("alpha R_n + (1-alpha)Q_n")
    B --> C("alpha R_n + (1-alpha)[alpha R_{n-1} + (1-alpha)Q_{n-1}]")
    C --> D("alpha R_n + (1-alpha)alpha R_{n-1} + (1-alpha)^2 Q_{n-1}")
    D --> E("...")
    E --> F("alpha R_n + (1-alpha)alpha R_{n-1} + (1-alpha)^2 alpha R_{n-2} + ... + (1-alpha)^{n-1} alpha R_1 + (1-alpha)^n Q_1")
    F --> G("(1-alpha)^n Q_1 + sum_{i=1}^{n} alpha (1-alpha)^{n-i} R_i")
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
    style G fill:#9f9,stroke:#333,stroke-width:2px

```
$$
  \begin{aligned}
    Q_{n+1} &= Q_n + \alpha [R_n - Q_n] \\
    &= \alpha R_n + (1-\alpha)Q_n \\
    &= \alpha R_n + (1-\alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}] \\
    &= \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2 Q_{n-1} \\
    &= \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2 [\alpha R_{n-2} + (1-\alpha)Q_{n-2}] \\
    &= \ldots \\
    &= \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2 \alpha R_{n-2} + \ldots + (1-\alpha)^{n-1} \alpha R_1 + (1-\alpha)^n Q_1 \\
    &= (1-\alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1-\alpha)^{n-i} R_i.
\end{aligned}
$$
Esta equa√ß√£o mostra que a atualiza√ß√£o de $Q_{n+1}$ √© uma m√©dia ponderada das recompensas passadas, onde o peso da recompensa $R_i$ √© dado por $\alpha(1-\alpha)^{n-i}$. O termo $(1-\alpha)^n$ representa o peso da estimativa inicial $Q_1$ [2].

> üí° **Exemplo Num√©rico:** Usando as mesmas recompensas [2, 4, 6, 8] e $\alpha=0.2$, assumindo que $Q_1=0$, vamos calcular $Q_2$ at√© $Q_5$:
> - $Q_2 = 0 + 0.2 * (2-0) = 0.4$
> - $Q_3 = 0.4 + 0.2 * (4 - 0.4) = 0.4 + 0.72 = 1.12$
> - $Q_4 = 1.12 + 0.2 * (6 - 1.12) = 1.12 + 0.976 = 2.096$
> - $Q_5 = 2.096 + 0.2 * (8 - 2.096) = 2.096 + 1.1808 = 3.2768$
> Observe que $Q_5$ (3.2768) √© menor que a m√©dia simples (5) e d√° mais peso √†s recompensas recentes.

**M√©dia de Rec√™ncia Ponderada Exponencial**

A express√£o acima √© o que se denomina de m√©dia de rec√™ncia ponderada exponencial [2]. O termo "exponencial" surge porque o peso atribu√≠do a cada recompensa decresce exponencialmente conforme a recompensa se torna mais antiga, devido √† presen√ßa do termo $(1-\alpha)^{n-i}$ [2]. As recompensas mais recentes t√™m um peso maior, permitindo que o algoritmo se adapte mais rapidamente √†s mudan√ßas nas probabilidades de recompensa. O peso da recompensa $R_i$ √© $\alpha(1-\alpha)^{n-i}$, que decresce √† medida que o intervalo de tempo $n-i$ aumenta [2]. Este decaimento exponencial garante que o algoritmo foque mais nas mudan√ßas recentes de recompensa.

> üí° **Exemplo Num√©rico:** Consideremos $\alpha = 0.1$ e $n=5$.  O peso da recompensa $R_5$ √© $0.1*(1-0.1)^{5-5} = 0.1$. O peso de $R_4$ √© $0.1*(1-0.1)^{5-4} = 0.09$. O peso de $R_3$ √© $0.1*(1-0.1)^{5-3} = 0.081$. O peso de $R_2$ √© $0.1*(1-0.1)^{5-2} = 0.0729$ e $R_1$ √© $0.1*(1-0.1)^{5-1} = 0.06561$. J√° o peso de $Q_1$ √© $(1-0.1)^5 = 0.59049$. Observe como os pesos diminuem exponencialmente √† medida que as recompensas ficam mais antigas.
```mermaid
graph LR
    subgraph "Pesos das Recompensas"
        direction TB
        R5["R_5: 0.1"]
        R4["R_4: 0.09"]
        R3["R_3: 0.081"]
        R2["R_2: 0.0729"]
        R1["R_1: 0.06561"]
        Q1["Q_1: 0.59049"]
        R5 --> R4
        R4 --> R3
        R3 --> R2
        R2 --> R1
        R1 --> Q1
    end
    style R5 fill:#ccf,stroke:#333,stroke-width:2px
    style R4 fill:#ccf,stroke:#333,stroke-width:2px
    style R3 fill:#ccf,stroke:#333,stroke-width:2px
    style R2 fill:#ccf,stroke:#333,stroke-width:2px
    style R1 fill:#ccf,stroke:#333,stroke-width:2px
    style Q1 fill:#9f9,stroke:#333,stroke-width:2px
```

O par√¢metro $\alpha$ controla o quanto o algoritmo valoriza as recompensas recentes em compara√ß√£o com as recompensas passadas [2]. Se $\alpha$ for pr√≥ximo de 1, o algoritmo dar√° muito peso √†s recompensas recentes e rapidamente se adaptar√° √†s mudan√ßas no ambiente. Por outro lado, se $\alpha$ for pr√≥ximo de 0, o algoritmo dar√° mais peso √†s recompensas mais antigas e se adaptar√° mais lentamente. Se $\alpha = 1$, toda a pondera√ß√£o recai sobre a recompensa mais recente, $R_n$ [2].

**Proposi√ß√£o 1:** A m√©dia de rec√™ncia ponderada exponencial pode ser expressa de forma recursiva.

**Declara√ß√£o:** A m√©dia de rec√™ncia ponderada exponencial $Q_{n+1}$ pode ser expressa como uma combina√ß√£o linear da recompensa mais recente $R_n$ e da estimativa anterior $Q_n$, ou seja, $Q_{n+1} = (1-\alpha)Q_n + \alpha R_n$.

**Prova:**  Esta express√£o j√° foi estabelecida anteriormente na deriva√ß√£o de $Q_{n+1}$.  Ela surge diretamente da regra de atualiza√ß√£o incremental com um tamanho de passo constante $\alpha$.  A recursividade √© evidente pois $Q_{n+1}$ √© definido em termos de $Q_n$ e $R_n$.  Essa forma recursiva √© computacionalmente eficiente, uma vez que n√£o requer o armazenamento e o processamento de todas as recompensas passadas.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, onde $Q_4 = 2.096$ e $R_4 = 6$, com $\alpha = 0.2$ podemos calcular $Q_5$ recursivamente: $Q_5 = (1-0.2) * 2.096 + 0.2 * 8 = 0.8 * 2.096 + 1.6 = 1.6768 + 1.6 = 3.2768$. O mesmo valor obtido anteriormente.

**Lema 1:** A soma dos pesos da m√©dia de rec√™ncia ponderada exponencial √© igual a 1.

**Declara√ß√£o:** A soma dos pesos associados √†s recompensas $R_i$ e √† estimativa inicial $Q_1$ na m√©dia de rec√™ncia ponderada exponencial √© igual a 1.

**Prova:** A soma dos pesos √© dada por
$$ (1-\alpha)^n + \sum_{i=1}^n \alpha (1-\alpha)^{n-i}.$$
Podemos reescrever a soma como
$$ (1-\alpha)^n + \alpha \sum_{i=1}^n  (1-\alpha)^{n-i}.$$
Seja $j = n-i$. Ent√£o, quando $i=1$, $j=n-1$ e quando $i=n$, $j=0$. Assim,
$$ (1-\alpha)^n + \alpha \sum_{j=0}^{n-1} (1-\alpha)^{j}.$$
A soma $\sum_{j=0}^{n-1} (1-\alpha)^{j}$ √© uma soma geom√©trica, que √© igual a $\frac{1-(1-\alpha)^n}{1-(1-\alpha)} = \frac{1-(1-\alpha)^n}{\alpha}$. Portanto, a soma dos pesos √©
$$ (1-\alpha)^n + \alpha \frac{1-(1-\alpha)^n}{\alpha} = (1-\alpha)^n + 1 - (1-\alpha)^n = 1.$$
$\blacksquare$

> üí° **Exemplo Num√©rico:** No exemplo com $\alpha=0.1$ e $n=5$, a soma dos pesos √© $0.59049 + 0.06561 + 0.0729 + 0.081 + 0.09 + 0.1 = 1.0$

**Lema 1.1:**  O peso da estimativa inicial $Q_1$ decresce exponencialmente com o n√∫mero de itera√ß√µes.

**Declara√ß√£o:**  O peso atribu√≠do √† estimativa inicial $Q_1$ na m√©dia de rec√™ncia ponderada exponencial, dado por $(1-\alpha)^n$, decresce exponencialmente conforme $n$ aumenta, se $\alpha \in (0,1)$.

**Prova:** Como $\alpha \in (0,1)$, temos que $0 < 1-\alpha < 1$.  Portanto, a fun√ß√£o $f(n) = (1-\alpha)^n$ √© uma fun√ß√£o exponencial decrescente de $n$.  √Ä medida que $n$ cresce, $(1-\alpha)^n$ se aproxima de zero, demonstrando que o peso de $Q_1$ diminui exponencialmente com o tempo.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Com $\alpha = 0.2$, o peso de $Q_1$ em $n=1$ √© $(1-0.2)^1=0.8$. Em $n=5$, $(1-0.2)^5=0.32768$. Em $n=10$, $(1-0.2)^{10}=0.10737$. Observe como o peso de $Q_1$ decresce exponencialmente.

**Corol√°rio 1:** Se $\alpha = 0$, ent√£o o peso √© concentrado na estimativa inicial $Q_1$, e todas as recompensas s√£o ignoradas.

**Declara√ß√£o:** Se o par√¢metro de tamanho de passo $\alpha$ for igual a 0, o peso de cada recompensa $R_i$ √© igual a 0 e o peso da estimativa inicial $Q_1$ √© igual a 1.

**Prova:** Da equa√ß√£o da m√©dia de rec√™ncia ponderada exponencial, temos
$$ (1-\alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1-\alpha)^{n-i} R_i.$$
Se $\alpha = 0$, ent√£o $(1-\alpha)^n = (1-0)^n = 1$ e $\alpha (1-\alpha)^{n-i} = 0$ para todo $i$. Portanto, a equa√ß√£o se torna
$$ 1\cdot Q_1 + \sum_{i=1}^{n} 0 \cdot R_i = Q_1.$$
$\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\alpha = 0$ e $Q_1 = 5$, ent√£o, independentemente das recompensas $R_i$,  $Q_{n+1}$ sempre ser√° igual a $5$, significando que o agente n√£o aprende e ignora todas as recompensas.

**Corol√°rio 1.1:** Se $\alpha = 1$, ent√£o a estimativa $Q_{n+1}$ √© igual √† recompensa mais recente $R_n$ e todas as recompensas anteriores s√£o ignoradas.

**Declara√ß√£o:** Se o par√¢metro de tamanho de passo $\alpha$ for igual a 1, o peso da recompensa mais recente $R_n$ √© 1 e o peso de todas as outras recompensas $R_i$ com $i < n$ e da estimativa inicial $Q_1$ √© 0.

**Prova:** Usando a forma recursiva da m√©dia ponderada exponencial,  $Q_{n+1} = (1-\alpha)Q_n + \alpha R_n$. Se $\alpha = 1$, ent√£o $Q_{n+1} = (1-1)Q_n + 1 \cdot R_n = R_n$.  Analisando a forma expandida, com $\alpha = 1$, temos
$Q_{n+1} =  (1-1)^n Q_1 + \sum_{i=1}^{n} 1 (1-1)^{n-i} R_i$.  Para $i=n$, $(1-1)^{n-n} = 1$ e para qualquer $i<n$ o termo $(1-1)^{n-i} = 0$, resultando em $Q_{n+1} = R_n$.
$\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\alpha = 1$, e as recompensas s√£o $R_1=2$, $R_2=4$, $R_3=6$ e $R_4=8$, ent√£o $Q_2=2$, $Q_3=4$, $Q_4=6$ e $Q_5=8$. O valor de a√ß√£o sempre reflete a recompensa mais recente.

### Conclus√£o
A m√©dia de rec√™ncia ponderada exponencial √© um m√©todo importante para adaptar algoritmos de aprendizado por refor√ßo a ambientes n√£o estacion√°rios. Atribuindo pesos diferentes √†s recompensas passadas, com maior peso √†s recompensas mais recentes, essa abordagem permite que o agente aprenda e se adapte rapidamente a mudan√ßas no ambiente, garantindo um desempenho mais robusto em cen√°rios din√¢micos. O par√¢metro $\alpha$ √© fundamental, e sua escolha influencia diretamente a capacidade de adapta√ß√£o do algoritmo.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Multi-armed Bandits)*
[^2]: "One of the most popular ways of doing this is to use a constant step-size parameter. For example, the incremental update rule (2.3) for updating an average Qn of the n ‚Äì 1 past rewards is modified to be Qn+1 = Qn + a[Rn - Qn], where the step-size parameter a ‚àà (0, 1] is constant. This results in Qn+1 being a weighted average of past rewards and the initial estimate Q1" *(Multi-armed Bandits)*
