## Rastreamento de Problemas N√£o Estacion√°rios em Multi-Armed Bandits

### Introdu√ß√£o

Em problemas de *reinforcement learning*, o agente interage com um ambiente para aprender a tomar decis√µes otimizadas ao longo do tempo [^1]. Diferentemente de outros tipos de aprendizagem, o *reinforcement learning* utiliza feedback avaliativo, indicando o qu√£o boa foi a a√ß√£o tomada, em vez de feedback instrutivo, que indicaria a a√ß√£o correta a ser tomada. Este feedback avaliativo √© crucial para criar a necessidade de explora√ß√£o ativa, buscando o melhor comportamento poss√≠vel [^1]. Este cap√≠tulo foca no aspecto avaliativo do *reinforcement learning* em um cen√°rio simplificado, que √© o problema do *k-armed bandit* [^1]. Este problema serve como um ponto de partida para introduzir m√©todos b√°sicos de aprendizagem, que posteriormente s√£o estendidos para o problema completo de *reinforcement learning* [^1]. O cap√≠tulo tamb√©m aborda o cen√°rio onde o problema do *bandit* torna-se associativo, ou seja, quando a melhor a√ß√£o depende da situa√ß√£o [^1].

Um aspecto crucial no *reinforcement learning* √© a considera√ß√£o de ambientes n√£o estacion√°rios, onde as probabilidades de recompensa mudam ao longo do tempo [^2]. Em tais ambientes, √© fundamental que o agente seja capaz de adaptar suas estrat√©gias de aprendizagem para refletir as mudan√ßas recentes nas recompensas. M√©todos que d√£o maior peso √†s recompensas mais recentes, como os m√©todos de passo constante, s√£o importantes para garantir o sucesso em ambientes n√£o estacion√°rios. Este cap√≠tulo explora essa din√¢mica em profundidade, utilizando o framework do *k-armed bandit* [^2].

### Conceitos Fundamentais

O problema do **k-armed bandit** envolve a escolha repetida entre *k* op√ß√µes, ou a√ß√µes [^1]. Cada a√ß√£o resulta em uma recompensa num√©rica proveniente de uma distribui√ß√£o de probabilidade estacion√°ria, dependente da a√ß√£o escolhida [^1]. O objetivo √© maximizar a recompensa total esperada ao longo de um per√≠odo de tempo [^2]. Este problema √© denominado dessa forma por analogia a uma m√°quina ca√ßa-n√≠queis com *k* alavancas, em que cada a√ß√£o corresponde a puxar uma das alavancas [^2]. Em termos formais, a recompensa esperada de uma a√ß√£o *a* √© denotada como $q_*(a)$ [^2]. O valor estimado da a√ß√£o *a* no instante de tempo *t* √© denotado por $Q_t(a)$ [^2].

A a√ß√£o selecionada no instante de tempo *t* √© representada por $A_t$, e a recompensa correspondente por $R_t$ [^2]. Uma estrat√©gia b√°sica para escolher a√ß√µes √© a **a√ß√£o gulosa** (*greedy action*), que seleciona a a√ß√£o com o maior valor estimado [^2]. Alternativamente, o agente pode escolher uma a√ß√£o n√£o gulosa, que √© denominada como **explora√ß√£o** (*exploration*) [^2]. A **explota√ß√£o** (*exploitation*) busca maximizar a recompensa imediata, enquanto a explora√ß√£o visa melhorar as estimativas de valor para o longo prazo [^2]. O equil√≠brio entre esses dois aspectos √© um desafio fundamental no *reinforcement learning* [^2].

Um m√©todo comum para equilibrar explora√ß√£o e explota√ß√£o √© o **m√©todo Œµ-guloso** (*Œµ-greedy method*), em que a maioria das vezes uma a√ß√£o gulosa √© escolhida, mas com uma pequena probabilidade Œµ, uma a√ß√£o aleat√≥ria √© selecionada [^3]. Uma forma de estimar o valor de uma a√ß√£o √© atrav√©s da **m√©dia amostral** (*sample-average method*), que calcula a m√©dia das recompensas recebidas quando a a√ß√£o √© escolhida [^3]. Formalmente, isso √© representado por:

$$
    Q_t(a) = \frac{\text{soma das recompensas quando a foi escolhida antes de t}}{\text{n√∫mero de vezes que a foi escolhida antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}
$$

onde $\mathbb{1}_{\text{predicado}}$ √© uma vari√°vel aleat√≥ria que vale 1 se o predicado for verdadeiro e 0 caso contr√°rio [^3]. Este m√©todo tem a propriedade de, pela lei dos grandes n√∫meros, convergir para o valor real da a√ß√£o $q_*(a)$ quando o n√∫mero de amostras tende ao infinito [^3].

> üí° **Exemplo Num√©rico:** Considere um problema de 3-armed bandit (k=3), onde escolhemos a a√ß√£o 1 (a=1) tr√™s vezes e recebemos as recompensas R1 = 2, R2 = 4, e R3 = 3. Usando a m√©dia amostral, o valor estimado da a√ß√£o 1 ap√≥s essas tr√™s tentativas √©:
>
> $$Q_4(1) = \frac{2 + 4 + 3}{3} = \frac{9}{3} = 3$$
>
> Portanto, a estimativa para o valor da a√ß√£o 1, $Q_4(1)$, √© 3. Isso representa a m√©dia das recompensas recebidas at√© agora para a a√ß√£o 1.

#### M√©todos Incrementais e o Passo Constante
Para implementar o m√©todo de m√©dias amostrais de forma eficiente computacionalmente, √© poss√≠vel usar m√©todos **incrementais**, que atualizam a estimativa do valor de uma a√ß√£o de forma a n√£o precisar armazenar o hist√≥rico completo de recompensas [^4]. A forma geral de atualiza√ß√£o incremental √©:

```mermaid
graph LR
    A["EstimativaAntiga"] -->|"+ TamanhoDoPasso* (Alvo - EstimativaAntiga)"| B("NovaEstimativa");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
```

$$
\text{NovaEstimativa} \leftarrow \text{EstimativaAntiga} + \text{TamanhoDoPasso} [\text{Alvo} - \text{EstimativaAntiga}]
$$
[^4].

No caso de m√©dias amostrais, o tamanho do passo √© $\frac{1}{n}$, onde $n$ √© o n√∫mero de vezes que a a√ß√£o foi selecionada [^4]. A adapta√ß√£o incremental da m√©dia amostral, utilizando um passo vari√°vel, √© dada por:

$$
Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]
$$

onde $Q_n$ √© a estimativa do valor ap√≥s *n-1* recompensas e $R_n$ √© a *n*-√©sima recompensa [^4].

**Proposi√ß√£o 1:** A atualiza√ß√£o incremental com passo vari√°vel da m√©dia amostral converge para a m√©dia verdadeira da recompensa da a√ß√£o quando o n√∫mero de amostras tende ao infinito.

*Prova:* A atualiza√ß√£o incremental da m√©dia amostral pode ser expressa como:
$Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]$. Esta formula√ß√£o √© uma implementa√ß√£o eficiente da m√©dia amostral padr√£o. Como a m√©dia amostral converge para a m√©dia verdadeira pela Lei dos Grandes N√∫meros, a atualiza√ß√£o incremental tamb√©m converge para a m√©dia verdadeira da recompensa da a√ß√£o quando o n√∫mero de amostras tende ao infinito $\blacksquare$.

> üí° **Exemplo Num√©rico:**  Continuando o exemplo anterior, suponha que ap√≥s as tr√™s primeiras tentativas (R1=2, R2=4, R3=3), escolhemos novamente a a√ß√£o 1 e recebemos uma recompensa R4=5. Anteriormente, $Q_4(1) = 3$. Usando a atualiza√ß√£o incremental, onde n=3 (a a√ß√£o 1 foi escolhida 3 vezes anteriormente), temos:
>
>$$Q_5(1) = Q_4(1) + \frac{1}{4}[R_4 - Q_4(1)] = 3 + \frac{1}{4}[5 - 3] = 3 + \frac{2}{4} = 3.5$$
>
>Observe que $Q_5(1) = 3.5$ √© o mesmo resultado que obter√≠amos se calcul√°ssemos a m√©dia amostral diretamente: $(2+4+3+5)/4 = 14/4 = 3.5$. A atualiza√ß√£o incremental, no entanto, n√£o requer que armazenemos todas as recompensas anteriores.

Em cen√°rios n√£o estacion√°rios, onde as probabilidades de recompensa podem mudar com o tempo, torna-se necess√°rio dar maior peso √†s recompensas mais recentes, pois estas refletem melhor a din√¢mica atual do problema [^5]. Para isso, √© poss√≠vel modificar a atualiza√ß√£o incremental usando um **passo constante** $\alpha$, onde $\alpha \in (0,1]$ [^5]. A f√≥rmula de atualiza√ß√£o com passo constante √©:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n]
$$
[^5].

Essa atualiza√ß√£o resulta em uma m√©dia ponderada das recompensas passadas, onde as recompensas mais recentes t√™m mais peso. O peso da recompensa $R_i$ depende de qu√£o atr√°s no tempo ela foi observada, sendo dado por $\alpha(1-\alpha)^{n-i}$ [^5]. Quanto maior o valor de $\alpha$, mais peso √© dado √†s recompensas recentes [^5].

**Lemma 1:** A atualiza√ß√£o com passo constante em problemas n√£o estacion√°rios gera uma m√©dia ponderada, dando maior import√¢ncia √†s recompensas recentes.

*Prova:* Expandindo a equa√ß√£o de atualiza√ß√£o com passo constante de forma recursiva, temos:

```mermaid
graph LR
    subgraph "Passo Constante - Atualiza√ß√£o Recursiva"
    A("Q_{n+1}") -->|"= Q_n + alpha[R_n - Q_n]"| B("alpha R_n + (1 - alpha) Q_n");
    B -->|"... aplicando recursivamente ..."| C("alpha R_n + alpha(1-alpha) R_{n-1} + ... + alpha(1-alpha)^{n-1}R_1 + (1-alpha)^n Q_1");
    C -->| " = Soma Ponderada de Recompensas"|D("Q_{n+1}  = (1 - alpha)^n Q_1 + Sum_{i=1}^n alpha(1 - alpha)^{n-i} R_i");
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n] \\
Q_{n+1} = \alpha R_n + (1-\alpha)Q_n \\
Q_{n+1} = \alpha R_n + (1-\alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}] \\
Q_{n+1} = \alpha R_n + \alpha(1-\alpha) R_{n-1} + (1-\alpha)^2 Q_{n-1} \\
... \\
Q_{n+1} = \alpha R_n + \alpha(1-\alpha) R_{n-1} + \alpha(1-\alpha)^2 R_{n-2} + \ldots + \alpha(1-\alpha)^{n-1}R_1 + (1-\alpha)^n Q_1 \\
Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^{n} \alpha(1-\alpha)^{n-i}R_i
$$

A equa√ß√£o final mostra que $Q_{n+1}$ √© uma m√©dia ponderada das recompensas passadas e da estimativa inicial $Q_1$. Os pesos das recompensas passadas, $\alpha(1-\alpha)^{n-i}$, decrescem exponencialmente √† medida que a recompensa √© observada mais no passado. O peso da estimativa inicial $Q_1$ √© $(1-\alpha)^n$, que decresce √† medida que $n$ aumenta. Isso demonstra que o m√©todo com passo constante d√° mais import√¢ncia √†s recompensas recentes $\blacksquare$.

> üí° **Exemplo Num√©rico:**  Vamos usar o mesmo exemplo das a√ß√µes e recompensas anteriores, mas agora com um passo constante $\alpha = 0.1$. Inicialmente, vamos supor que $Q_1(1) = 0$. Ent√£o, as primeiras quatro atualiza√ß√µes de $Q(1)$ seriam:
>
>$\text{Step 1: } Q_2(1) = Q_1(1) + \alpha [R_1 - Q_1(1)] = 0 + 0.1[2-0] = 0.2$
>
>$\text{Step 2: } Q_3(1) = Q_2(1) + \alpha [R_2 - Q_2(1)] = 0.2 + 0.1[4-0.2] = 0.2 + 0.1[3.8] = 0.58$
>
>$\text{Step 3: } Q_4(1) = Q_3(1) + \alpha [R_3 - Q_3(1)] = 0.58 + 0.1[3-0.58] = 0.58 + 0.1[2.42] = 0.822$
>
>$\text{Step 4: } Q_5(1) = Q_4(1) + \alpha [R_4 - Q_4(1)] = 0.822 + 0.1[5-0.822] = 0.822 + 0.1[4.178] = 1.2398$
>
>Note como a estimativa $Q_t(1)$ est√° sendo influenciada pelas recompensas mais recentes. As recompensas iniciais (R1, R2) t√™m um peso menor em $Q_5(1)$ comparado com R3 e R4. Se compararmos com a m√©dia amostral anterior, 3.5, vemos como o passo constante resulta em uma estimativa que responde mais rapidamente a varia√ß√µes nas recompensas.
>
>Para visualizar os pesos de cada recompensa, vamos calcular para R1, R2, R3, e R4 na atualiza√ß√£o de Q5(1):
>
>$\text{Peso de R1: } \alpha(1-\alpha)^{5-1} = 0.1 * (0.9)^4 = 0.06561$
>
>$\text{Peso de R2: } \alpha(1-\alpha)^{5-2} = 0.1 * (0.9)^3 = 0.0729$
>
>$\text{Peso de R3: } \alpha(1-\alpha)^{5-3} = 0.1 * (0.9)^2 = 0.081$
>
>$\text{Peso de R4: } \alpha(1-\alpha)^{5-4} = 0.1 * (0.9)^1 = 0.09$
>
> A recompensa R4 tem o maior peso (0.09), a recompensa R3 tem o segundo maior peso, e assim por diante, confirmando que recompensas mais recentes recebem maior aten√ß√£o na estimativa do valor.  O peso da estimativa inicial  $Q_1$ seria $(1 - 0.1)^4 = 0.6561$, que tamb√©m contribui para $Q_5(1)$
>

**Lemma 1.1:** O peso total das recompensas passadas na atualiza√ß√£o com passo constante converge para 1 quando o n√∫mero de amostras tende ao infinito.

*Prova:*  O peso total das recompensas passadas √© dado por $\sum_{i=1}^{n} \alpha(1-\alpha)^{n-i}$.  Podemos reescrever essa soma como $\sum_{j=0}^{n-1} \alpha(1-\alpha)^j$, fazendo a mudan√ßa de vari√°vel $j = n-i$. Esta √© a soma parcial de uma s√©rie geom√©trica, que pode ser calculada como:  $\alpha \frac{1-(1-\alpha)^n}{1-(1-\alpha)} = 1 - (1-\alpha)^n$.  Quando $n$ tende ao infinito, $(1-\alpha)^n$ tende a 0, pois $\alpha \in (0,1]$. Portanto, o peso total das recompensas passadas converge para 1, ou seja, $ \lim_{n\to\infty} (1 - (1-\alpha)^n) = 1 \blacksquare$.

```mermaid
graph LR
subgraph "Lemma 1.1 - Converg√™ncia do Peso Total"
    A("Peso Total = Sum_{i=1}^n alpha(1-alpha)^(n-i)") --> |"Mudan√ßa de vari√°vel: j = n - i"| B("= Sum_{j=0}^{n-1} alpha(1-alpha)^j");
    B --> |"Soma Parcial S√©rie Geom√©trica"| C("= alpha * (1 - (1-alpha)^n) / (1-(1-alpha))");
    C --> |"Simplificando"| D("= 1 - (1-alpha)^n");
    D -->| "Limite quando n -> infinito"|E("= 1");
    end
style A fill:#f9f,stroke:#333,stroke-width:2px
style B fill:#ccf,stroke:#333,stroke-width:2px
style C fill:#f9f,stroke:#333,stroke-width:2px
style D fill:#ccf,stroke:#333,stroke-width:2px
style E fill:#f9f,stroke:#333,stroke-width:2px
```
√â importante ressaltar que a escolha do tamanho do passo ($\alpha$) afeta a estabilidade da aprendizagem. Um valor muito alto de $\alpha$ pode fazer com que a aprendizagem seja inst√°vel e sens√≠vel a recompensas outliers [^5]. Por outro lado, um valor muito baixo pode tornar o processo de aprendizagem muito lento para se adaptar √†s mudan√ßas no ambiente [^5]. Assim, √© essencial ajustar o valor de $\alpha$ para um bom desempenho em problemas n√£o estacion√°rios [^5].

**Observa√ß√£o 1:** No limite, quando $\alpha$ tende a 1, a atualiza√ß√£o de valor se torna $Q_{n+1} = R_n$, ou seja, a estimativa do valor da a√ß√£o no passo $n+1$ √© igual √† recompensa observada no passo $n$. Este caso extremo faz com que a aprendizagem seja extremamente vol√°til, dando peso total √† √∫ltima recompensa observada.

### Conclus√£o
Em problemas n√£o estacion√°rios, √© crucial dar mais peso √†s recompensas recentes para se adaptar √†s mudan√ßas no ambiente. O uso de um passo de tamanho constante ($\alpha$) em m√©todos incrementais permite que a estimativa de valor se ajuste mais rapidamente a recompensas recentes, melhorando o desempenho em compara√ß√£o com m√©todos de m√©dias amostrais que d√£o peso igual a todas as recompensas. O *lemma* apresentado formaliza essa intui√ß√£o. Al√©m disso, o peso total das recompensas passadas converge para 1 quando o n√∫mero de amostras tende ao infinito.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Chapter 2)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action." *(Trecho de Chapter 2)*
[^3]: "One natural way to estimate this is by averaging the rewards actually received" *(Trecho de Chapter 2)*
[^4]: "As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward." *(Trecho de Chapter 2)*
[^5]: "In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter." *(Trecho de Chapter 2)*
