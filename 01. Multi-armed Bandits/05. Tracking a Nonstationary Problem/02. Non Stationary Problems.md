## Rastreamento de Problemas NÃ£o EstacionÃ¡rios em Multi-Armed Bandits

### IntroduÃ§Ã£o

Em problemas de *reinforcement learning*, o agente interage com um ambiente para aprender a tomar decisÃµes otimizadas ao longo do tempo [^1]. Diferentemente de outros tipos de aprendizagem, o *reinforcement learning* utiliza feedback avaliativo, indicando o quÃ£o boa foi a aÃ§Ã£o tomada, em vez de feedback instrutivo, que indicaria a aÃ§Ã£o correta a ser tomada. Este feedback avaliativo Ã© crucial para criar a necessidade de exploraÃ§Ã£o ativa, buscando o melhor comportamento possÃ­vel [^1]. Este capÃ­tulo foca no aspecto avaliativo do *reinforcement learning* em um cenÃ¡rio simplificado, que Ã© o problema do *k-armed bandit* [^1]. Este problema serve como um ponto de partida para introduzir mÃ©todos bÃ¡sicos de aprendizagem, que posteriormente sÃ£o estendidos para o problema completo de *reinforcement learning* [^1]. O capÃ­tulo tambÃ©m aborda o cenÃ¡rio onde o problema do *bandit* torna-se associativo, ou seja, quando a melhor aÃ§Ã£o depende da situaÃ§Ã£o [^1].

Um aspecto crucial no *reinforcement learning* Ã© a consideraÃ§Ã£o de ambientes nÃ£o estacionÃ¡rios, onde as probabilidades de recompensa mudam ao longo do tempo [^2]. Em tais ambientes, Ã© fundamental que o agente seja capaz de adaptar suas estratÃ©gias de aprendizagem para refletir as mudanÃ§as recentes nas recompensas. MÃ©todos que dÃ£o maior peso Ã s recompensas mais recentes, como os mÃ©todos de passo constante, sÃ£o importantes para garantir o sucesso em ambientes nÃ£o estacionÃ¡rios. Este capÃ­tulo explora essa dinÃ¢mica em profundidade, utilizando o framework do *k-armed bandit* [^2].

### Conceitos Fundamentais

O problema do **k-armed bandit** envolve a escolha repetida entre *k* opÃ§Ãµes, ou aÃ§Ãµes [^1]. Cada aÃ§Ã£o resulta em uma recompensa numÃ©rica proveniente de uma distribuiÃ§Ã£o de probabilidade estacionÃ¡ria, dependente da aÃ§Ã£o escolhida [^1]. O objetivo Ã© maximizar a recompensa total esperada ao longo de um perÃ­odo de tempo [^2]. Este problema Ã© denominado dessa forma por analogia a uma mÃ¡quina caÃ§a-nÃ­queis com *k* alavancas, em que cada aÃ§Ã£o corresponde a puxar uma das alavancas [^2]. Em termos formais, a recompensa esperada de uma aÃ§Ã£o *a* Ã© denotada como $q_*(a)$ [^2]. O valor estimado da aÃ§Ã£o *a* no instante de tempo *t* Ã© denotado por $Q_t(a)$ [^2].

A aÃ§Ã£o selecionada no instante de tempo *t* Ã© representada por $A_t$, e a recompensa correspondente por $R_t$ [^2]. Uma estratÃ©gia bÃ¡sica para escolher aÃ§Ãµes Ã© a **aÃ§Ã£o gulosa** (*greedy action*), que seleciona a aÃ§Ã£o com o maior valor estimado [^2]. Alternativamente, o agente pode escolher uma aÃ§Ã£o nÃ£o gulosa, que Ã© denominada como **exploraÃ§Ã£o** (*exploration*) [^2]. A **explotaÃ§Ã£o** (*exploitation*) busca maximizar a recompensa imediata, enquanto a exploraÃ§Ã£o visa melhorar as estimativas de valor para o longo prazo [^2]. O equilÃ­brio entre esses dois aspectos Ã© um desafio fundamental no *reinforcement learning* [^2].

Um mÃ©todo comum para equilibrar exploraÃ§Ã£o e explotaÃ§Ã£o Ã© o **mÃ©todo Îµ-guloso** (*Îµ-greedy method*), em que a maioria das vezes uma aÃ§Ã£o gulosa Ã© escolhida, mas com uma pequena probabilidade Îµ, uma aÃ§Ã£o aleatÃ³ria Ã© selecionada [^3]. Uma forma de estimar o valor de uma aÃ§Ã£o Ã© atravÃ©s da **mÃ©dia amostral** (*sample-average method*), que calcula a mÃ©dia das recompensas recebidas quando a aÃ§Ã£o Ã© escolhida [^3]. Formalmente, isso Ã© representado por:

$$
    Q_t(a) = \frac{\text{soma das recompensas quando a foi escolhida antes de t}}{\text{nÃºmero de vezes que a foi escolhida antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}
$$

onde $\mathbb{1}_{\text{predicado}}$ Ã© uma variÃ¡vel aleatÃ³ria que vale 1 se o predicado for verdadeiro e 0 caso contrÃ¡rio [^3]. Este mÃ©todo tem a propriedade de, pela lei dos grandes nÃºmeros, convergir para o valor real da aÃ§Ã£o $q_*(a)$ quando o nÃºmero de amostras tende ao infinito [^3].

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um problema de 3-armed bandit (k=3), onde escolhemos a aÃ§Ã£o 1 (a=1) trÃªs vezes e recebemos as recompensas R1 = 2, R2 = 4, e R3 = 3. Usando a mÃ©dia amostral, o valor estimado da aÃ§Ã£o 1 apÃ³s essas trÃªs tentativas Ã©:
>
> $$Q_4(1) = \frac{2 + 4 + 3}{3} = \frac{9}{3} = 3$$
>
> Portanto, a estimativa para o valor da aÃ§Ã£o 1, $Q_4(1)$, Ã© 3. Isso representa a mÃ©dia das recompensas recebidas atÃ© agora para a aÃ§Ã£o 1.

#### MÃ©todos Incrementais e o Passo Constante
Para implementar o mÃ©todo de mÃ©dias amostrais de forma eficiente computacionalmente, Ã© possÃ­vel usar mÃ©todos **incrementais**, que atualizam a estimativa do valor de uma aÃ§Ã£o de forma a nÃ£o precisar armazenar o histÃ³rico completo de recompensas [^4]. A forma geral de atualizaÃ§Ã£o incremental Ã©:

```mermaid
graph LR
    A["EstimativaAntiga"] -->|"+ TamanhoDoPasso* (Alvo - EstimativaAntiga)"| B("NovaEstimativa");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
```

$$
\text{NovaEstimativa} \leftarrow \text{EstimativaAntiga} + \text{TamanhoDoPasso} [\text{Alvo} - \text{EstimativaAntiga}]
$$
[^4].

No caso de mÃ©dias amostrais, o tamanho do passo Ã© $\frac{1}{n}$, onde $n$ Ã© o nÃºmero de vezes que a aÃ§Ã£o foi selecionada [^4]. A adaptaÃ§Ã£o incremental da mÃ©dia amostral, utilizando um passo variÃ¡vel, Ã© dada por:

$$
Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]
$$

onde $Q_n$ Ã© a estimativa do valor apÃ³s *n-1* recompensas e $R_n$ Ã© a *n*-Ã©sima recompensa [^4].

**ProposiÃ§Ã£o 1:** A atualizaÃ§Ã£o incremental com passo variÃ¡vel da mÃ©dia amostral converge para a mÃ©dia verdadeira da recompensa da aÃ§Ã£o quando o nÃºmero de amostras tende ao infinito.

*Prova:* A atualizaÃ§Ã£o incremental da mÃ©dia amostral pode ser expressa como:
$Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]$. Esta formulaÃ§Ã£o Ã© uma implementaÃ§Ã£o eficiente da mÃ©dia amostral padrÃ£o. Como a mÃ©dia amostral converge para a mÃ©dia verdadeira pela Lei dos Grandes NÃºmeros, a atualizaÃ§Ã£o incremental tambÃ©m converge para a mÃ©dia verdadeira da recompensa da aÃ§Ã£o quando o nÃºmero de amostras tende ao infinito $\blacksquare$.

> ðŸ’¡ **Exemplo NumÃ©rico:**  Continuando o exemplo anterior, suponha que apÃ³s as trÃªs primeiras tentativas (R1=2, R2=4, R3=3), escolhemos novamente a aÃ§Ã£o 1 e recebemos uma recompensa R4=5. Anteriormente, $Q_4(1) = 3$. Usando a atualizaÃ§Ã£o incremental, onde n=3 (a aÃ§Ã£o 1 foi escolhida 3 vezes anteriormente), temos:
>
>$$Q_5(1) = Q_4(1) + \frac{1}{4}[R_4 - Q_4(1)] = 3 + \frac{1}{4}[5 - 3] = 3 + \frac{2}{4} = 3.5$$
>
>Observe que $Q_5(1) = 3.5$ Ã© o mesmo resultado que obterÃ­amos se calculÃ¡ssemos a mÃ©dia amostral diretamente: $(2+4+3+5)/4 = 14/4 = 3.5$. A atualizaÃ§Ã£o incremental, no entanto, nÃ£o requer que armazenemos todas as recompensas anteriores.

Em cenÃ¡rios nÃ£o estacionÃ¡rios, onde as probabilidades de recompensa podem mudar com o tempo, torna-se necessÃ¡rio dar maior peso Ã s recompensas mais recentes, pois estas refletem melhor a dinÃ¢mica atual do problema [^5]. Para isso, Ã© possÃ­vel modificar a atualizaÃ§Ã£o incremental usando um **passo constante** $\alpha$, onde $\alpha \in (0,1]$ [^5]. A fÃ³rmula de atualizaÃ§Ã£o com passo constante Ã©:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n]
$$
[^5].

Essa atualizaÃ§Ã£o resulta em uma mÃ©dia ponderada das recompensas passadas, onde as recompensas mais recentes tÃªm mais peso. O peso da recompensa $R_i$ depende de quÃ£o atrÃ¡s no tempo ela foi observada, sendo dado por $\alpha(1-\alpha)^{n-i}$ [^5]. Quanto maior o valor de $\alpha$, mais peso Ã© dado Ã s recompensas recentes [^5].

**Lemma 1:** A atualizaÃ§Ã£o com passo constante em problemas nÃ£o estacionÃ¡rios gera uma mÃ©dia ponderada, dando maior importÃ¢ncia Ã s recompensas recentes.

*Prova:* Expandindo a equaÃ§Ã£o de atualizaÃ§Ã£o com passo constante de forma recursiva, temos:

```mermaid
graph LR
    subgraph "Passo Constante - AtualizaÃ§Ã£o Recursiva"
    A("Q_{n+1}") -->|"= Q_n + alpha[R_n - Q_n]"| B("alpha R_n + (1 - alpha) Q_n");
    B -->|"... aplicando recursivamente ..."| C("alpha R_n + alpha(1-alpha) R_{n-1} + ... + alpha(1-alpha)^{n-1}R_1 + (1-alpha)^n Q_1");
    C -->| " = Soma Ponderada de Recompensas"|D("Q_{n+1}  = (1 - alpha)^n Q_1 + Sum_{i=1}^n alpha(1 - alpha)^{n-i} R_i");
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n] \\
Q_{n+1} = \alpha R_n + (1-\alpha)Q_n \\
Q_{n+1} = \alpha R_n + (1-\alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}] \\
Q_{n+1} = \alpha R_n + \alpha(1-\alpha) R_{n-1} + (1-\alpha)^2 Q_{n-1} \\
... \\
Q_{n+1} = \alpha R_n + \alpha(1-\alpha) R_{n-1} + \alpha(1-\alpha)^2 R_{n-2} + \ldots + \alpha(1-\alpha)^{n-1}R_1 + (1-\alpha)^n Q_1 \\
Q_{n+1} = (1-\alpha)^n Q_1 + \sum_{i=1}^{n} \alpha(1-\alpha)^{n-i}R_i
$$

A equaÃ§Ã£o final mostra que $Q_{n+1}$ Ã© uma mÃ©dia ponderada das recompensas passadas e da estimativa inicial $Q_1$. Os pesos das recompensas passadas, $\alpha(1-\alpha)^{n-i}$, decrescem exponencialmente Ã  medida que a recompensa Ã© observada mais no passado. O peso da estimativa inicial $Q_1$ Ã© $(1-\alpha)^n$, que decresce Ã  medida que $n$ aumenta. Isso demonstra que o mÃ©todo com passo constante dÃ¡ mais importÃ¢ncia Ã s recompensas recentes $\blacksquare$.

> ðŸ’¡ **Exemplo NumÃ©rico:**  Vamos usar o mesmo exemplo das aÃ§Ãµes e recompensas anteriores, mas agora com um passo constante $\alpha = 0.1$. Inicialmente, vamos supor que $Q_1(1) = 0$. EntÃ£o, as primeiras quatro atualizaÃ§Ãµes de $Q(1)$ seriam:
>
>$\text{Step 1: } Q_2(1) = Q_1(1) + \alpha [R_1 - Q_1(1)] = 0 + 0.1[2-0] = 0.2$
>
>$\text{Step 2: } Q_3(1) = Q_2(1) + \alpha [R_2 - Q_2(1)] = 0.2 + 0.1[4-0.2] = 0.2 + 0.1[3.8] = 0.58$
>
>$\text{Step 3: } Q_4(1) = Q_3(1) + \alpha [R_3 - Q_3(1)] = 0.58 + 0.1[3-0.58] = 0.58 + 0.1[2.42] = 0.822$
>
>$\text{Step 4: } Q_5(1) = Q_4(1) + \alpha [R_4 - Q_4(1)] = 0.822 + 0.1[5-0.822] = 0.822 + 0.1[4.178] = 1.2398$
>
>Note como a estimativa $Q_t(1)$ estÃ¡ sendo influenciada pelas recompensas mais recentes. As recompensas iniciais (R1, R2) tÃªm um peso menor em $Q_5(1)$ comparado com R3 e R4. Se compararmos com a mÃ©dia amostral anterior, 3.5, vemos como o passo constante resulta em uma estimativa que responde mais rapidamente a variaÃ§Ãµes nas recompensas.
>
>Para visualizar os pesos de cada recompensa, vamos calcular para R1, R2, R3, e R4 na atualizaÃ§Ã£o de Q5(1):
>
>$\text{Peso de R1: } \alpha(1-\alpha)^{5-1} = 0.1 * (0.9)^4 = 0.06561$
>
>$\text{Peso de R2: } \alpha(1-\alpha)^{5-2} = 0.1 * (0.9)^3 = 0.0729$
>
>$\text{Peso de R3: } \alpha(1-\alpha)^{5-3} = 0.1 * (0.9)^2 = 0.081$
>
>$\text{Peso de R4: } \alpha(1-\alpha)^{5-4} = 0.1 * (0.9)^1 = 0.09$
>
> A recompensa R4 tem o maior peso (0.09), a recompensa R3 tem o segundo maior peso, e assim por diante, confirmando que recompensas mais recentes recebem maior atenÃ§Ã£o na estimativa do valor.  O peso da estimativa inicial  $Q_1$ seria $(1 - 0.1)^4 = 0.6561$, que tambÃ©m contribui para $Q_5(1)$
>

**Lemma 1.1:** O peso total das recompensas passadas na atualizaÃ§Ã£o com passo constante converge para 1 quando o nÃºmero de amostras tende ao infinito.

*Prova:*  O peso total das recompensas passadas Ã© dado por $\sum_{i=1}^{n} \alpha(1-\alpha)^{n-i}$.  Podemos reescrever essa soma como $\sum_{j=0}^{n-1} \alpha(1-\alpha)^j$, fazendo a mudanÃ§a de variÃ¡vel $j = n-i$. Esta Ã© a soma parcial de uma sÃ©rie geomÃ©trica, que pode ser calculada como:  $\alpha \frac{1-(1-\alpha)^n}{1-(1-\alpha)} = 1 - (1-\alpha)^n$.  Quando $n$ tende ao infinito, $(1-\alpha)^n$ tende a 0, pois $\alpha \in (0,1]$. Portanto, o peso total das recompensas passadas converge para 1, ou seja, $ \lim_{n\to\infty} (1 - (1-\alpha)^n) = 1 \blacksquare$.

```mermaid
graph LR
subgraph "Lemma 1.1 - ConvergÃªncia do Peso Total"
    A("Peso Total = Sum_{i=1}^n alpha(1-alpha)^(n-i)") --> |"MudanÃ§a de variÃ¡vel: j = n - i"| B("= Sum_{j=0}^{n-1} alpha(1-alpha)^j");
    B --> |"Soma Parcial SÃ©rie GeomÃ©trica"| C("= alpha * (1 - (1-alpha)^n) / (1-(1-alpha))");
    C --> |"Simplificando"| D("= 1 - (1-alpha)^n");
    D -->| "Limite quando n -> infinito"|E("= 1");
    end
style A fill:#f9f,stroke:#333,stroke-width:2px
style B fill:#ccf,stroke:#333,stroke-width:2px
style C fill:#f9f,stroke:#333,stroke-width:2px
style D fill:#ccf,stroke:#333,stroke-width:2px
style E fill:#f9f,stroke:#333,stroke-width:2px
```
Ã‰ importante ressaltar que a escolha do tamanho do passo ($\alpha$) afeta a estabilidade da aprendizagem. Um valor muito alto de $\alpha$ pode fazer com que a aprendizagem seja instÃ¡vel e sensÃ­vel a recompensas outliers [^5]. Por outro lado, um valor muito baixo pode tornar o processo de aprendizagem muito lento para se adaptar Ã s mudanÃ§as no ambiente [^5]. Assim, Ã© essencial ajustar o valor de $\alpha$ para um bom desempenho em problemas nÃ£o estacionÃ¡rios [^5].

**ObservaÃ§Ã£o 1:** No limite, quando $\alpha$ tende a 1, a atualizaÃ§Ã£o de valor se torna $Q_{n+1} = R_n$, ou seja, a estimativa do valor da aÃ§Ã£o no passo $n+1$ Ã© igual Ã  recompensa observada no passo $n$. Este caso extremo faz com que a aprendizagem seja extremamente volÃ¡til, dando peso total Ã  Ãºltima recompensa observada.

### ConclusÃ£o
Em problemas nÃ£o estacionÃ¡rios, Ã© crucial dar mais peso Ã s recompensas recentes para se adaptar Ã s mudanÃ§as no ambiente. O uso de um passo de tamanho constante ($\alpha$) em mÃ©todos incrementais permite que a estimativa de valor se ajuste mais rapidamente a recompensas recentes, melhorando o desempenho em comparaÃ§Ã£o com mÃ©todos de mÃ©dias amostrais que dÃ£o peso igual a todas as recompensas. O *lemma* apresentado formaliza essa intuiÃ§Ã£o. AlÃ©m disso, o peso total das recompensas passadas converge para 1 quando o nÃºmero de amostras tende ao infinito.

### ReferÃªncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Chapter 2)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action." *(Trecho de Chapter 2)*
[^3]: "One natural way to estimate this is by averaging the rewards actually received" *(Trecho de Chapter 2)*
[^4]: "As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward." *(Trecho de Chapter 2)*
[^5]: "In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter." *(Trecho de Chapter 2)*
