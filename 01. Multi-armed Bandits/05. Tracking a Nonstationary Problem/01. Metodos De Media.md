## M√©todos de Rastreamento em Problemas N√£o Estacion√°rios

### Introdu√ß√£o

No contexto de **multi-armed bandits**, o aprendizado por refor√ßo enfrenta o desafio de adaptar-se a ambientes n√£o estacion√°rios, onde as probabilidades de recompensa associadas a cada a√ß√£o podem variar ao longo do tempo [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Em contraste com os **problemas estacion√°rios**, nos quais as recompensas seguem distribui√ß√µes fixas, os ambientes n√£o estacion√°rios exigem m√©todos de aprendizado que possam se adaptar rapidamente √†s mudan√ßas nas din√¢micas de recompensa. A capacidade de ponderar recompensas recentes com mais √™nfase do que as recompensas passadas torna-se essencial nesses cen√°rios [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Este cap√≠tulo explora como as m√©dias podem ser adaptadas para rastrear essas mudan√ßas, mantendo a efici√™ncia computacional.

### Conceitos Fundamentais

A ess√™ncia do aprendizado em ambientes n√£o estacion√°rios reside na capacidade de dar mais peso √†s recompensas mais recentes, refletindo a cren√ßa de que as condi√ß√µes atuais do ambiente s√£o mais relevantes do que as experi√™ncias passadas [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Uma abordagem comum para lidar com essa n√£o estacionariedade √© usar um **par√¢metro de tamanho de passo constante**. Isso modifica a regra de atualiza√ß√£o incremental para calcular a m√©dia $Q_n$ das $n-1$ recompensas anteriores, ajustando-a para:

$$Q_{n+1} = Q_n + \alpha [R_n - Q_n],$$

onde $\alpha \in (0, 1]$ √© uma constante. Esta formula√ß√£o resulta em $Q_{n+1}$ sendo uma m√©dia ponderada das recompensas passadas e da estimativa inicial $Q_1$. A equa√ß√£o √© uma forma de m√©dia com **pondera√ß√£o por rec√™ncia exponencial** [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um *multi-armed bandit* com uma a√ß√£o, e as recompensas que recebemos s√£o $R_1 = 1$, $R_2 = 2$, $R_3 = 3$, e assim por diante. Inicializamos $Q_1 = 0$, e vamos considerar $\alpha = 0.1$. Vamos calcular os valores de $Q_n$ iterativamente:
>
> - $Q_2 = Q_1 + \alpha [R_1 - Q_1] = 0 + 0.1 [1 - 0] = 0.1$
> - $Q_3 = Q_2 + \alpha [R_2 - Q_2] = 0.1 + 0.1 [2 - 0.1] = 0.1 + 0.1 * 1.9 = 0.29$
> - $Q_4 = Q_3 + \alpha [R_3 - Q_3] = 0.29 + 0.1 [3 - 0.29] = 0.29 + 0.1 * 2.71 = 0.561$
>
> Observamos que $Q_n$ se move em dire√ß√£o a $R_n$, mas com um atraso e ponderado pelo $\alpha$. Se $\alpha$ fosse maior, digamos 0.5, a adapta√ß√£o seria mais r√°pida:
>
> - $Q_2 = 0 + 0.5[1-0] = 0.5$
> - $Q_3 = 0.5 + 0.5[2 - 0.5] = 0.5 + 0.75 = 1.25$
> - $Q_4 = 1.25 + 0.5[3-1.25] = 1.25 + 0.875 = 2.125$
>
> Como podemos ver, com $\alpha=0.5$ os valores de $Q_n$ se adaptam mais rapidamente aos novos valores de recompensa.

Para entender a natureza da pondera√ß√£o, podemos expandir a f√≥rmula recursivamente:

$$
\begin{aligned}
Q_{n+1} &= Q_n + \alpha [R_n - Q_n] \\
&= \alpha R_n + (1 - \alpha)Q_n \\
&= \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}] \\
&= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 Q_{n-1} \\
&= \ldots \\
&= (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i.
\end{aligned}
$$

Aqui, o peso dado a cada recompensa $R_i$ √© $\alpha(1-\alpha)^{n-i}$. Este peso diminui exponencialmente √† medida que a recompensa √© mais antiga, com o peso da recompensa $R_n$ sendo $\alpha$. A soma dos pesos √© igual a 1, como pode ser verificado:

$$(1-\alpha)^n + \sum_{i=1}^n \alpha (1-\alpha)^{n-i} = (1-\alpha)^n + \alpha\sum_{j=0}^{n-1} (1-\alpha)^j = (1-\alpha)^n + \alpha\frac{1-(1-\alpha)^n}{1-(1-\alpha)} = (1-\alpha)^n + 1-(1-\alpha)^n=1$$

Essa forma de m√©dia √© chamada de m√©dia ponderada exponencialmente pela rec√™ncia. Note que quando $\alpha$ se aproxima de 1, o valor de $Q_{n+1}$ se aproxima de $R_n$.
Ao contr√°rio do m√©todo de m√©dia amostral, o uso de um tamanho de passo constante resulta em estimativas que n√£o convergem completamente [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Em vez disso, as estimativas continuam a variar em resposta a recompensas recentes, o que √© desej√°vel em ambientes n√£o estacion√°rios. O uso de uma m√©dia simples n√£o seria adequado nesses casos, porque se baseia em recompensas do passado distante, que n√£o s√£o mais relevantes [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

```mermaid
graph LR
    subgraph "M√©dia Amostral (Problemas Estacion√°rios)"
    A["Recompensas Passadas"] --> B("M√©dia Simples");
    end
    subgraph "M√©dia Ponderada Exponencial (Problemas N√£o Estacion√°rios)"
    C["Recompensas Recentes"] --> D("Peso Exponencial");
    D --> E("Estimativa Adapt√°vel");
    end
    B --> F("Estima√ß√£o Lenta");
    E --> G("Estima√ß√£o R√°pida");
    F --> H("N√£o Adequada para N√£o Estacionariedade");
    G --> I("Adequada para N√£o Estacionariedade");

```

**Proposi√ß√£o 1**
A escolha do valor de $\alpha$ influencia diretamente a velocidade com que o algoritmo se adapta a mudan√ßas no ambiente. Um valor de $\alpha$ pr√≥ximo de 1 leva a uma adapta√ß√£o mais r√°pida, mas tamb√©m a uma maior vari√¢ncia na estimativa de $Q_{n+1}$. Por outro lado, um valor de $\alpha$ pr√≥ximo de 0 torna a adapta√ß√£o mais lenta, com menor vari√¢ncia, mas tamb√©m pode resultar em uma resposta defasada √†s mudan√ßas no ambiente.

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar a Proposi√ß√£o 1, vamos considerar dois cen√°rios em um ambiente n√£o estacion√°rio. Suponha que as recompensas de um determinado bra√ßo em um *multi-armed bandit* mudam repentinamente de 1 para 5 no passo 10. Vamos comparar o comportamento de $Q_n$ com $\alpha = 0.1$ e $\alpha = 0.8$, com $Q_1=0$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> alpha1 = 0.1
> alpha2 = 0.8
> rewards = np.concatenate((np.ones(9), np.ones(21) * 5))
>
> Q1 = np.zeros_like(rewards, dtype=float)
> Q2 = np.zeros_like(rewards, dtype=float)
>
> Q1[0] = 0
> Q2[0] = 0
>
> for i in range(1, len(rewards)):
>     Q1[i] = Q1[i-1] + alpha1 * (rewards[i] - Q1[i-1])
>     Q2[i] = Q2[i-1] + alpha2 * (rewards[i] - Q2[i-1])
>
> plt.plot(Q1, label=f'alpha = {alpha1}')
> plt.plot(Q2, label=f'alpha = {alpha2}')
> plt.plot(rewards, label='Recompensas', linestyle='--')
> plt.xlabel('Passo de tempo')
> plt.ylabel('Valor Q')
> plt.title('Efeito de alpha na Adapta√ß√£o')
> plt.legend()
> plt.show()
> ```
>
> Este c√≥digo plota a evolu√ß√£o de $Q_n$ para os dois valores de $\alpha$. Observamos que o Q com $\alpha = 0.8$ converge muito mais r√°pido para a recompensa de 5 ap√≥s a mudan√ßa, mas tamb√©m tem mais flutua√ß√µes. J√° o Q com $\alpha = 0.1$ converge mais lentamente, mas de forma mais est√°vel.
>
> ```mermaid
>  graph LR
>      A[In√≠cio] --> B{Recompensas = 1?};
>      B -- Sim --> C[Q(n) converge lentamente com alpha=0.1];
>      B -- N√£o --> D{Recompensas = 5?};
>      D -- Sim --> E[Q(n) converge rapidamente com alpha=0.8];
>      D -- N√£o --> F[Continua];
>      C --> G[Est√°vel];
>      E --> H[Inst√°vel];
>      F-->B
> ```

**Prova:**
Da expans√£o recursiva da equa√ß√£o de atualiza√ß√£o, $Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i$, podemos observar que quando $\alpha$ se aproxima de 1, o termo $(1 - \alpha)^n Q_1$ se torna desprez√≠vel rapidamente, e o peso das recompensas passadas $\alpha(1-\alpha)^{n-i}$ decai rapidamente, dando maior import√¢ncia √†s recompensas mais recentes, $R_n$ em particular. Quando $\alpha$ se aproxima de 0, o peso das recompensas passadas decai mais lentamente, e o termo $(1 - \alpha)^n Q_1$ ainda ter√° import√¢ncia mesmo em passos mais avan√ßados. Isso implica que a estimativa $Q_{n+1}$ depender√° mais da estimativa inicial $Q_1$ e de recompensas antigas, levando a uma adapta√ß√£o mais lenta. $\blacksquare$

### Lemma 1

A m√©dia ponderada da equa√ß√£o 2.5 √© equivalente √† m√©dia ponderada por rec√™ncia exponencial, onde a soma dos pesos √© igual a 1.

**Prova:**
Como demonstrado acima, a expans√£o da f√≥rmula recursiva mostra claramente que a soma dos pesos para a m√©dia ponderada da equa√ß√£o 2.5 √© igual a 1. $\blacksquare$

**Lemma 1.1**
A m√©dia ponderada da equa√ß√£o 2.5 pode ser reescrita em uma forma que explicita a influ√™ncia da estimativa anterior $Q_n$ e da nova recompensa $R_n$.

**Prova:**
A equa√ß√£o $Q_{n+1} = Q_n + \alpha [R_n - Q_n]$ pode ser reorganizada como $Q_{n+1} = (1 - \alpha)Q_n + \alpha R_n$. Essa forma mostra explicitamente como a nova estimativa √© uma combina√ß√£o linear da estimativa anterior e da nova recompensa. $\blacksquare$

```mermaid
graph LR
    A["Q_(n+1)"] --> B("Combina√ß√£o Linear");
    B --> C("Peso: (1-alpha) em Q_n");
    B --> D("Peso: alpha em R_n");
    C --> E("Estimativa Anterior");
    D --> F("Recompensa Atual");
    style B fill:#f9f,stroke:#333,stroke-width:2px
```

### Corol√°rio 1

A m√©dia ponderada da equa√ß√£o 2.5, utilizando um $\alpha$ constante, sempre tender√° a dar mais import√¢ncia √†s recompensas recentes.

**Prova:**
Como $\alpha \in (0,1]$, o fator $(1-\alpha)^{n-i}$ sempre ser√° menor quando $n-i$ for maior. Portanto, recompensas mais recentes ($n-i$ pequeno) ter√£o maior peso do que recompensas mais antigas. $\blacksquare$

**Corol√°rio 1.1**
O "horizonte" da m√©dia ponderada exponencialmente pela rec√™ncia √© controlado pelo par√¢metro $\alpha$. Um valor menor de $\alpha$ corresponde a um horizonte mais longo (mais recompensas passadas influenciam a m√©dia) e um valor maior de $\alpha$ corresponde a um horizonte mais curto (principalmente as recompensas recentes influenciam a m√©dia).

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar o Corol√°rio 1.1, podemos calcular o peso dado a recompensas de diferentes passos no tempo para diferentes valores de $\alpha$. Suponha $n=10$.
>
> | Passo ($i$) | $n-i$ | Peso ($\alpha=0.1$) | Peso ($\alpha=0.5$) |
> |-------------|-------|--------------------|--------------------|
> | 1           | 9     | $0.1*(0.9)^9 = 0.0387$  | $0.5*(0.5)^9 = 0.00097$  |
> | 5           | 5     | $0.1*(0.9)^5 = 0.059$ | $0.5*(0.5)^5 = 0.0156$ |
> | 9          | 1     | $0.1*(0.9)^1 = 0.09$ | $0.5*(0.5)^1 = 0.25$  |
> | 10          | 0     | $0.1*(0.9)^0 = 0.1$  | $0.5*(0.5)^0 = 0.5$ |
>
> Observe que com $\alpha=0.1$, as recompensas antigas ainda t√™m um peso relevante. J√° com $\alpha=0.5$, as recompensas mais recentes s√£o muito mais importantes.

**Prova:**
O peso de cada recompensa $R_i$ √© dado por $\alpha(1-\alpha)^{n-i}$. Se $\alpha$ √© pequeno, ent√£o o fator $(1-\alpha)$ ser√° pr√≥ximo de 1, e a queda exponencial do peso com a antiguidade da recompensa ser√° mais lenta, resultando em um maior horizonte. Se $\alpha$ √© grande, ent√£o $(1-\alpha)$ ser√° pequeno, e o peso de recompensas mais antigas decair√° rapidamente, diminuindo o horizonte da m√©dia. $\blacksquare$

```mermaid
graph LR
    subgraph "Horizonte da M√©dia"
    A["alpha Pequeno"] --> B("Horizonte Longo");
    B --> C("Recompensas Passadas Influenciam Mais");
    D["alpha Grande"] --> E("Horizonte Curto");
    E --> F("Recompensas Recentes Influenciam Mais");
    end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
```

Para rastrear a n√£o estacionariedade de forma mais adapt√°vel, √© poss√≠vel variar o par√¢metro de tamanho de passo de um passo de tempo para outro. No entanto, o uso de tamanhos de passo que atendem √†s condi√ß√µes de converg√™ncia (2.7) frequentemente resulta em converg√™ncia lenta ou requer um ajuste significativo para uma taxa satisfat√≥ria. Assim, embora essas sequ√™ncias de tamanhos de passo sejam √∫teis para fins te√≥ricos, elas n√£o s√£o amplamente utilizadas em aplica√ß√µes e pesquisa emp√≠rica [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2].

### Conclus√£o

Em resumo, enquanto os m√©todos de m√©dia de amostra s√£o ideais para **problemas estacion√°rios**, a necessidade de adapta√ß√£o r√°pida e flex√≠vel em ambientes **n√£o estacion√°rios** torna necess√°rio o uso de m√©todos que priorizem recompensas recentes. O uso de um tamanho de passo constante, conforme demonstrado, fornece um mecanismo simples e eficaz para o rastreamento de mudan√ßas nas din√¢micas de recompensa, garantindo que o agente de aprendizado possa se ajustar ao seu entorno mut√°vel. Este √© um conceito fundamental no estudo de **reinforcement learning** em ambientes do mundo real. A escolha entre m√©todos de m√©dia amostral e m√©todos com tamanho de passo constante depende essencialmente da natureza do problema enfrentado, com a **n√£o estacionariedade** exigindo estrat√©gias de aprendizagem mais adapt√°veis.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter." *(Trecho de Chapter 2: Multi-armed Bandits)*
