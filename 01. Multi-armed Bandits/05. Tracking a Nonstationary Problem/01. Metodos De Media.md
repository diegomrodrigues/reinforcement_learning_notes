## MÃ©todos de Rastreamento em Problemas NÃ£o EstacionÃ¡rios

### IntroduÃ§Ã£o

No contexto de **multi-armed bandits**, o aprendizado por reforÃ§o enfrenta o desafio de adaptar-se a ambientes nÃ£o estacionÃ¡rios, onde as probabilidades de recompensa associadas a cada aÃ§Ã£o podem variar ao longo do tempo [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Em contraste com os **problemas estacionÃ¡rios**, nos quais as recompensas seguem distribuiÃ§Ãµes fixas, os ambientes nÃ£o estacionÃ¡rios exigem mÃ©todos de aprendizado que possam se adaptar rapidamente Ã s mudanÃ§as nas dinÃ¢micas de recompensa. A capacidade de ponderar recompensas recentes com mais Ãªnfase do que as recompensas passadas torna-se essencial nesses cenÃ¡rios [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Este capÃ­tulo explora como as mÃ©dias podem ser adaptadas para rastrear essas mudanÃ§as, mantendo a eficiÃªncia computacional.

### Conceitos Fundamentais

A essÃªncia do aprendizado em ambientes nÃ£o estacionÃ¡rios reside na capacidade de dar mais peso Ã s recompensas mais recentes, refletindo a crenÃ§a de que as condiÃ§Ãµes atuais do ambiente sÃ£o mais relevantes do que as experiÃªncias passadas [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Uma abordagem comum para lidar com essa nÃ£o estacionariedade Ã© usar um **parÃ¢metro de tamanho de passo constante**. Isso modifica a regra de atualizaÃ§Ã£o incremental para calcular a mÃ©dia $Q_n$ das $n-1$ recompensas anteriores, ajustando-a para:

$$Q_{n+1} = Q_n + \alpha [R_n - Q_n],$$

onde $\alpha \in (0, 1]$ Ã© uma constante. Esta formulaÃ§Ã£o resulta em $Q_{n+1}$ sendo uma mÃ©dia ponderada das recompensas passadas e da estimativa inicial $Q_1$. A equaÃ§Ã£o Ã© uma forma de mÃ©dia com **ponderaÃ§Ã£o por recÃªncia exponencial** [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha que temos um *multi-armed bandit* com uma aÃ§Ã£o, e as recompensas que recebemos sÃ£o $R_1 = 1$, $R_2 = 2$, $R_3 = 3$, e assim por diante. Inicializamos $Q_1 = 0$, e vamos considerar $\alpha = 0.1$. Vamos calcular os valores de $Q_n$ iterativamente:
>
> - $Q_2 = Q_1 + \alpha [R_1 - Q_1] = 0 + 0.1 [1 - 0] = 0.1$
> - $Q_3 = Q_2 + \alpha [R_2 - Q_2] = 0.1 + 0.1 [2 - 0.1] = 0.1 + 0.1 * 1.9 = 0.29$
> - $Q_4 = Q_3 + \alpha [R_3 - Q_3] = 0.29 + 0.1 [3 - 0.29] = 0.29 + 0.1 * 2.71 = 0.561$
>
> Observamos que $Q_n$ se move em direÃ§Ã£o a $R_n$, mas com um atraso e ponderado pelo $\alpha$. Se $\alpha$ fosse maior, digamos 0.5, a adaptaÃ§Ã£o seria mais rÃ¡pida:
>
> - $Q_2 = 0 + 0.5[1-0] = 0.5$
> - $Q_3 = 0.5 + 0.5[2 - 0.5] = 0.5 + 0.75 = 1.25$
> - $Q_4 = 1.25 + 0.5[3-1.25] = 1.25 + 0.875 = 2.125$
>
> Como podemos ver, com $\alpha=0.5$ os valores de $Q_n$ se adaptam mais rapidamente aos novos valores de recompensa.

Para entender a natureza da ponderaÃ§Ã£o, podemos expandir a fÃ³rmula recursivamente:

$$
\begin{aligned}
Q_{n+1} &= Q_n + \alpha [R_n - Q_n] \\
&= \alpha R_n + (1 - \alpha)Q_n \\
&= \alpha R_n + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha) Q_{n-1}] \\
&= \alpha R_n + (1 - \alpha) \alpha R_{n-1} + (1 - \alpha)^2 Q_{n-1} \\
&= \ldots \\
&= (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i.
\end{aligned}
$$

Aqui, o peso dado a cada recompensa $R_i$ Ã© $\alpha(1-\alpha)^{n-i}$. Este peso diminui exponencialmente Ã  medida que a recompensa Ã© mais antiga, com o peso da recompensa $R_n$ sendo $\alpha$. A soma dos pesos Ã© igual a 1, como pode ser verificado:

$$(1-\alpha)^n + \sum_{i=1}^n \alpha (1-\alpha)^{n-i} = (1-\alpha)^n + \alpha\sum_{j=0}^{n-1} (1-\alpha)^j = (1-\alpha)^n + \alpha\frac{1-(1-\alpha)^n}{1-(1-\alpha)} = (1-\alpha)^n + 1-(1-\alpha)^n=1$$

Essa forma de mÃ©dia Ã© chamada de mÃ©dia ponderada exponencialmente pela recÃªncia. Note que quando $\alpha$ se aproxima de 1, o valor de $Q_{n+1}$ se aproxima de $R_n$.
Ao contrÃ¡rio do mÃ©todo de mÃ©dia amostral, o uso de um tamanho de passo constante resulta em estimativas que nÃ£o convergem completamente [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Em vez disso, as estimativas continuam a variar em resposta a recompensas recentes, o que Ã© desejÃ¡vel em ambientes nÃ£o estacionÃ¡rios. O uso de uma mÃ©dia simples nÃ£o seria adequado nesses casos, porque se baseia em recompensas do passado distante, que nÃ£o sÃ£o mais relevantes [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

```mermaid
graph LR
    subgraph "MÃ©dia Amostral (Problemas EstacionÃ¡rios)"
    A["Recompensas Passadas"] --> B("MÃ©dia Simples");
    end
    subgraph "MÃ©dia Ponderada Exponencial (Problemas NÃ£o EstacionÃ¡rios)"
    C["Recompensas Recentes"] --> D("Peso Exponencial");
    D --> E("Estimativa AdaptÃ¡vel");
    end
    B --> F("EstimaÃ§Ã£o Lenta");
    E --> G("EstimaÃ§Ã£o RÃ¡pida");
    F --> H("NÃ£o Adequada para NÃ£o Estacionariedade");
    G --> I("Adequada para NÃ£o Estacionariedade");

```

**ProposiÃ§Ã£o 1**
A escolha do valor de $\alpha$ influencia diretamente a velocidade com que o algoritmo se adapta a mudanÃ§as no ambiente. Um valor de $\alpha$ prÃ³ximo de 1 leva a uma adaptaÃ§Ã£o mais rÃ¡pida, mas tambÃ©m a uma maior variÃ¢ncia na estimativa de $Q_{n+1}$. Por outro lado, um valor de $\alpha$ prÃ³ximo de 0 torna a adaptaÃ§Ã£o mais lenta, com menor variÃ¢ncia, mas tambÃ©m pode resultar em uma resposta defasada Ã s mudanÃ§as no ambiente.

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Para ilustrar a ProposiÃ§Ã£o 1, vamos considerar dois cenÃ¡rios em um ambiente nÃ£o estacionÃ¡rio. Suponha que as recompensas de um determinado braÃ§o em um *multi-armed bandit* mudam repentinamente de 1 para 5 no passo 10. Vamos comparar o comportamento de $Q_n$ com $\alpha = 0.1$ e $\alpha = 0.8$, com $Q_1=0$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> alpha1 = 0.1
> alpha2 = 0.8
> rewards = np.concatenate((np.ones(9), np.ones(21) * 5))
>
> Q1 = np.zeros_like(rewards, dtype=float)
> Q2 = np.zeros_like(rewards, dtype=float)
>
> Q1[0] = 0
> Q2[0] = 0
>
> for i in range(1, len(rewards)):
>     Q1[i] = Q1[i-1] + alpha1 * (rewards[i] - Q1[i-1])
>     Q2[i] = Q2[i-1] + alpha2 * (rewards[i] - Q2[i-1])
>
> plt.plot(Q1, label=f'alpha = {alpha1}')
> plt.plot(Q2, label=f'alpha = {alpha2}')
> plt.plot(rewards, label='Recompensas', linestyle='--')
> plt.xlabel('Passo de tempo')
> plt.ylabel('Valor Q')
> plt.title('Efeito de alpha na AdaptaÃ§Ã£o')
> plt.legend()
> plt.show()
> ```
>
> Este cÃ³digo plota a evoluÃ§Ã£o de $Q_n$ para os dois valores de $\alpha$. Observamos que o Q com $\alpha = 0.8$ converge muito mais rÃ¡pido para a recompensa de 5 apÃ³s a mudanÃ§a, mas tambÃ©m tem mais flutuaÃ§Ãµes. JÃ¡ o Q com $\alpha = 0.1$ converge mais lentamente, mas de forma mais estÃ¡vel.
>
> ```mermaid
>  graph LR
>      A[InÃ­cio] --> B{Recompensas = 1?};
>      B -- Sim --> C[Q(n) converge lentamente com alpha=0.1];
>      B -- NÃ£o --> D{Recompensas = 5?};
>      D -- Sim --> E[Q(n) converge rapidamente com alpha=0.8];
>      D -- NÃ£o --> F[Continua];
>      C --> G[EstÃ¡vel];
>      E --> H[InstÃ¡vel];
>      F-->B
> ```

**Prova:**
Da expansÃ£o recursiva da equaÃ§Ã£o de atualizaÃ§Ã£o, $Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i$, podemos observar que quando $\alpha$ se aproxima de 1, o termo $(1 - \alpha)^n Q_1$ se torna desprezÃ­vel rapidamente, e o peso das recompensas passadas $\alpha(1-\alpha)^{n-i}$ decai rapidamente, dando maior importÃ¢ncia Ã s recompensas mais recentes, $R_n$ em particular. Quando $\alpha$ se aproxima de 0, o peso das recompensas passadas decai mais lentamente, e o termo $(1 - \alpha)^n Q_1$ ainda terÃ¡ importÃ¢ncia mesmo em passos mais avanÃ§ados. Isso implica que a estimativa $Q_{n+1}$ dependerÃ¡ mais da estimativa inicial $Q_1$ e de recompensas antigas, levando a uma adaptaÃ§Ã£o mais lenta. $\blacksquare$

### Lemma 1

A mÃ©dia ponderada da equaÃ§Ã£o 2.5 Ã© equivalente Ã  mÃ©dia ponderada por recÃªncia exponencial, onde a soma dos pesos Ã© igual a 1.

**Prova:**
Como demonstrado acima, a expansÃ£o da fÃ³rmula recursiva mostra claramente que a soma dos pesos para a mÃ©dia ponderada da equaÃ§Ã£o 2.5 Ã© igual a 1. $\blacksquare$

**Lemma 1.1**
A mÃ©dia ponderada da equaÃ§Ã£o 2.5 pode ser reescrita em uma forma que explicita a influÃªncia da estimativa anterior $Q_n$ e da nova recompensa $R_n$.

**Prova:**
A equaÃ§Ã£o $Q_{n+1} = Q_n + \alpha [R_n - Q_n]$ pode ser reorganizada como $Q_{n+1} = (1 - \alpha)Q_n + \alpha R_n$. Essa forma mostra explicitamente como a nova estimativa Ã© uma combinaÃ§Ã£o linear da estimativa anterior e da nova recompensa. $\blacksquare$

```mermaid
graph LR
    A["Q_(n+1)"] --> B("CombinaÃ§Ã£o Linear");
    B --> C("Peso: (1-alpha) em Q_n");
    B --> D("Peso: alpha em R_n");
    C --> E("Estimativa Anterior");
    D --> F("Recompensa Atual");
    style B fill:#f9f,stroke:#333,stroke-width:2px
```

### CorolÃ¡rio 1

A mÃ©dia ponderada da equaÃ§Ã£o 2.5, utilizando um $\alpha$ constante, sempre tenderÃ¡ a dar mais importÃ¢ncia Ã s recompensas recentes.

**Prova:**
Como $\alpha \in (0,1]$, o fator $(1-\alpha)^{n-i}$ sempre serÃ¡ menor quando $n-i$ for maior. Portanto, recompensas mais recentes ($n-i$ pequeno) terÃ£o maior peso do que recompensas mais antigas. $\blacksquare$

**CorolÃ¡rio 1.1**
O "horizonte" da mÃ©dia ponderada exponencialmente pela recÃªncia Ã© controlado pelo parÃ¢metro $\alpha$. Um valor menor de $\alpha$ corresponde a um horizonte mais longo (mais recompensas passadas influenciam a mÃ©dia) e um valor maior de $\alpha$ corresponde a um horizonte mais curto (principalmente as recompensas recentes influenciam a mÃ©dia).

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Para ilustrar o CorolÃ¡rio 1.1, podemos calcular o peso dado a recompensas de diferentes passos no tempo para diferentes valores de $\alpha$. Suponha $n=10$.
>
> | Passo ($i$) | $n-i$ | Peso ($\alpha=0.1$) | Peso ($\alpha=0.5$) |
> |-------------|-------|--------------------|--------------------|
> | 1           | 9     | $0.1*(0.9)^9 = 0.0387$  | $0.5*(0.5)^9 = 0.00097$  |
> | 5           | 5     | $0.1*(0.9)^5 = 0.059$ | $0.5*(0.5)^5 = 0.0156$ |
> | 9          | 1     | $0.1*(0.9)^1 = 0.09$ | $0.5*(0.5)^1 = 0.25$  |
> | 10          | 0     | $0.1*(0.9)^0 = 0.1$  | $0.5*(0.5)^0 = 0.5$ |
>
> Observe que com $\alpha=0.1$, as recompensas antigas ainda tÃªm um peso relevante. JÃ¡ com $\alpha=0.5$, as recompensas mais recentes sÃ£o muito mais importantes.

**Prova:**
O peso de cada recompensa $R_i$ Ã© dado por $\alpha(1-\alpha)^{n-i}$. Se $\alpha$ Ã© pequeno, entÃ£o o fator $(1-\alpha)$ serÃ¡ prÃ³ximo de 1, e a queda exponencial do peso com a antiguidade da recompensa serÃ¡ mais lenta, resultando em um maior horizonte. Se $\alpha$ Ã© grande, entÃ£o $(1-\alpha)$ serÃ¡ pequeno, e o peso de recompensas mais antigas decairÃ¡ rapidamente, diminuindo o horizonte da mÃ©dia. $\blacksquare$

```mermaid
graph LR
    subgraph "Horizonte da MÃ©dia"
    A["alpha Pequeno"] --> B("Horizonte Longo");
    B --> C("Recompensas Passadas Influenciam Mais");
    D["alpha Grande"] --> E("Horizonte Curto");
    E --> F("Recompensas Recentes Influenciam Mais");
    end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
```

Para rastrear a nÃ£o estacionariedade de forma mais adaptÃ¡vel, Ã© possÃ­vel variar o parÃ¢metro de tamanho de passo de um passo de tempo para outro. No entanto, o uso de tamanhos de passo que atendem Ã s condiÃ§Ãµes de convergÃªncia (2.7) frequentemente resulta em convergÃªncia lenta ou requer um ajuste significativo para uma taxa satisfatÃ³ria. Assim, embora essas sequÃªncias de tamanhos de passo sejam Ãºteis para fins teÃ³ricos, elas nÃ£o sÃ£o amplamente utilizadas em aplicaÃ§Ãµes e pesquisa empÃ­rica [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2].

### ConclusÃ£o

Em resumo, enquanto os mÃ©todos de mÃ©dia de amostra sÃ£o ideais para **problemas estacionÃ¡rios**, a necessidade de adaptaÃ§Ã£o rÃ¡pida e flexÃ­vel em ambientes **nÃ£o estacionÃ¡rios** torna necessÃ¡rio o uso de mÃ©todos que priorizem recompensas recentes. O uso de um tamanho de passo constante, conforme demonstrado, fornece um mecanismo simples e eficaz para o rastreamento de mudanÃ§as nas dinÃ¢micas de recompensa, garantindo que o agente de aprendizado possa se ajustar ao seu entorno mutÃ¡vel. Este Ã© um conceito fundamental no estudo de **reinforcement learning** em ambientes do mundo real. A escolha entre mÃ©todos de mÃ©dia amostral e mÃ©todos com tamanho de passo constante depende essencialmente da natureza do problema enfrentado, com a **nÃ£o estacionariedade** exigindo estratÃ©gias de aprendizagem mais adaptÃ¡veis.

### ReferÃªncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter." *(Trecho de Chapter 2: Multi-armed Bandits)*
