```mermaid
graph LR
    A["A√ß√£o a"] -->|Recompensa R_i| B("Estimativa Q_t(a)");
    B --> |Atualiza√ß√£o| B;
    style B fill:#f9f,stroke:#333,stroke-width:2px
    subgraph "M√©dia de Recompensas"
        A
        B
    end
```

### Implementa√ß√£o Incremental com Tamanho de Passo Vari√°vel

### Introdu√ß√£o
Este cap√≠tulo explora o problema dos **multi-armed bandits**, um cen√°rio simplificado de *reinforcement learning* onde o foco est√° na avalia√ß√£o das a√ß√µes e na busca por um comportamento √≥timo. Um aspecto crucial desses problemas √© a necessidade de equilibrar a **explora√ß√£o** e a **explota√ß√£o** [^1]. Uma forma natural de estimar o valor de uma a√ß√£o √© calculando a m√©dia das recompensas recebidas ao selecion√°-la. No entanto, calcular m√©dias de forma direta pode ser computacionalmente ineficiente, especialmente ao lidar com grandes conjuntos de dados ou sequ√™ncias longas de recompensas [^3]. A **implementa√ß√£o incremental** surge como uma solu√ß√£o para esse desafio, oferecendo uma maneira de atualizar m√©dias de maneira eficiente com **custo computacional constante** por passo de tempo e utilizando **mem√≥ria constante** [^3].

### Conceitos Fundamentais

####  M√©dia de Recompensas
O valor verdadeiro de uma a√ß√£o *a*, denotado por $q_*(a)$, √© a m√©dia das recompensas recebidas quando essa a√ß√£o √© selecionada. A estimativa desse valor no passo de tempo *t*, denotada por $Q_t(a)$, pode ser calculada usando a m√©dia amostral das recompensas recebidas at√© aquele ponto [^3]:

$$
Q_t(a) = \frac{\text{soma de recompensas quando a foi escolhida antes de t}}{\text{n√∫mero de vezes que a foi escolhida antes de t}}
$$

Essa abordagem, conhecida como *sample-average method*, converge para $q_*(a)$ √† medida que o n√∫mero de escolhas de *a* tende ao infinito, de acordo com a lei dos grandes n√∫meros. No entanto, a implementa√ß√£o direta dessa f√≥rmula exige o armazenamento de todas as recompensas anteriores, o que √© computacionalmente caro [^3].

####  Implementa√ß√£o Incremental
A implementa√ß√£o incremental oferece uma forma eficiente de calcular m√©dias. Para uma a√ß√£o espec√≠fica, denotamos a recompensa recebida ap√≥s a *i*-√©sima sele√ß√£o como $R_i$ e a estimativa do seu valor ap√≥s *n-1* sele√ß√µes como $Q_n$. A atualiza√ß√£o da m√©dia ap√≥s receber a *n*-√©sima recompensa, $R_n$, pode ser expressa como [^3]:

$$
Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i = \frac{1}{n} (R_n + \sum_{i=1}^{n-1} R_i)
$$
$$
Q_{n+1} = \frac{1}{n} (R_n + (n-1)Q_n) = \frac{1}{n} (R_n + nQ_n - Q_n)
$$

$$
Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]
$$

Essa equa√ß√£o mostra que a nova estimativa $Q_{n+1}$ pode ser obtida a partir da estimativa anterior $Q_n$, da recompensa $R_n$ e do n√∫mero de vezes que a a√ß√£o foi selecionada *n*. Isso elimina a necessidade de armazenar todas as recompensas anteriores, tornando o processo mais eficiente computacionalmente [^3].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos uma a√ß√£o *a* e queremos calcular sua m√©dia de recompensas de forma incremental. Inicialmente, $Q_1(a) = 0$.
>
> - **Passo 1:** A a√ß√£o *a* √© selecionada e recebemos uma recompensa $R_1 = 2$.
>
>   $Q_2(a) = Q_1(a) + \frac{1}{1}[R_1 - Q_1(a)] = 0 + 1 * (2 - 0) = 2$
>
> - **Passo 2:** A a√ß√£o *a* √© selecionada novamente e recebemos uma recompensa $R_2 = 4$.
>
>   $Q_3(a) = Q_2(a) + \frac{1}{2}[R_2 - Q_2(a)] = 2 + 0.5 * (4 - 2) = 3$
>
> - **Passo 3:** A a√ß√£o *a* √© selecionada novamente e recebemos uma recompensa $R_3 = 3$.
>
>   $Q_4(a) = Q_3(a) + \frac{1}{3}[R_3 - Q_3(a)] = 3 + \frac{1}{3} * (3 - 3) = 3$
>
> Repare que a m√©dia, utilizando o m√©todo incremental, √© atualizada a cada nova recompensa sem a necessidade de armazenar recompensas anteriores.

####  Tamanho do Passo ($Œ±$)
Na atualiza√ß√£o incremental, o termo $\frac{1}{n}$ desempenha o papel do **tamanho do passo**. De modo geral, podemos expressar a atualiza√ß√£o como:

$$
\text{Nova Estimativa} \leftarrow \text{Velha Estimativa} + \text{Tamanho do Passo} [\text{Alvo} - \text{Velha Estimativa}]
$$

Onde o termo [Alvo - Velha Estimativa] representa o erro na estimativa. O alvo pode ser ruidoso, como no caso acima onde √© a recompensa mais recente. No caso da m√©dia das amostras ($\frac{1}{n}$), o tamanho do passo diminui √† medida que o n√∫mero de sele√ß√µes da a√ß√£o aumenta. No entanto, em problemas **n√£o estacion√°rios** (onde as probabilidades de recompensa podem mudar com o tempo), essa abordagem pode n√£o ser a ideal [^5]. Nestes casos, √© prefer√≠vel dar maior peso √†s recompensas mais recentes, o que pode ser obtido usando um tamanho de passo constante ($\alpha$) [^5]:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n]
$$

onde $\alpha \in (0, 1]$ √© uma constante [^5]. Essa abordagem resulta em uma m√©dia ponderada exponencialmente das recompensas anteriores. Ou seja, as recompensas mais recentes t√™m maior peso. Isso permite que o algoritmo se adapte mais rapidamente √†s mudan√ßas nas recompensas, embora a estimativa nunca convirja completamente [^5].

> üí° **Exemplo Num√©rico:**
>
> Suponha que temos um problema n√£o estacion√°rio e queremos utilizar um tamanho de passo constante $\alpha=0.1$. Inicialmente $Q_1(a) = 0$
>
> - **Passo 1:** A a√ß√£o *a* √© selecionada e recebemos uma recompensa $R_1 = 2$.
>
>  $Q_2(a) = Q_1(a) + \alpha[R_1 - Q_1(a)] = 0 + 0.1 * (2 - 0) = 0.2$
>
> - **Passo 2:** A a√ß√£o *a* √© selecionada novamente e recebemos uma recompensa $R_2 = 4$.
>
>  $Q_3(a) = Q_2(a) + \alpha[R_2 - Q_2(a)] = 0.2 + 0.1 * (4 - 0.2) = 0.58$
>
> - **Passo 3:** A a√ß√£o *a* √© selecionada novamente e recebemos uma recompensa $R_3 = 1$.
>
>  $Q_4(a) = Q_3(a) + \alpha[R_3 - Q_3(a)] = 0.58 + 0.1 * (1 - 0.58) = 0.622$
>
> Observe que, com tamanho de passo constante, a estimativa $Q_t(a)$ responde mais rapidamente a mudan√ßas nas recompensas (como a recompensa de 1 no passo 3). Comparando com o exemplo anterior, onde o tamanho do passo era $\frac{1}{n}$, este m√©todo se adapta mais r√°pido a novas recompensas, dando menor peso √†s recompensas passadas. Isso √© √∫til em ambientes n√£o estacion√°rios.
>
> Vamos analisar como o valor de $Q_t(a)$ se comporta com diferentes valores de $\alpha$.
>
> | Itera√ß√£o | Recompensa ($R_t$) | Q com $\alpha = 0.1$ | Q com $\alpha = 0.5$ | Q com $\alpha = 1$ |
> | -------- | ------------------- | --------------- | --------------- | --------------- |
> | 1        | 2                   | 0.2             | 1               | 2               |
> | 2        | 4                   | 0.58            | 2.5             | 4               |
> | 3        | 1                   | 0.622           | 1.75            | 1               |
> | 4        | 5                   | 1.0598          | 3.375           | 5               |
>
> Como podemos ver, um $\alpha$ maior faz com que as estimativas se adaptem mais rapidamente √†s recompensas mais recentes, enquanto um $\alpha$ menor suaviza as atualiza√ß√µes, dando mais import√¢ncia √†s recompensas passadas. No caso de $\alpha = 1$, a estimativa $Q_{n+1}$ √© exatamente a recompensa mais recente $R_n$.

#####  Nota√ß√£o Generalizada para o Tamanho do Passo

No contexto geral, o tamanho do passo pode variar com o tempo e com a a√ß√£o, sendo denotado por $\alpha_t(a)$ [^8]. Isso permite uma maior flexibilidade na forma como as estimativas s√£o atualizadas. Em algumas situa√ß√µes, pode ser desej√°vel um tamanho de passo que diminui ao longo do tempo, em outras, um tamanho de passo constante ou que dependa da a√ß√£o. O uso de $\alpha$ ou $\alpha_t(a)$ permite generalizar o tamanho do passo para diferentes m√©todos e cen√°rios de *reinforcement learning* [^8]. O m√©todo de m√©dia amostral usa $\alpha_n(a) = \frac{1}{n}$, enquanto um tamanho de passo constante, $\alpha$, usa $\alpha_n(a) = \alpha$. Um pseudoc√≥digo para um algoritmo *bandit* completo utilizando m√©dias amostrais calculadas incrementalmente e sele√ß√£o de a√ß√£o $\epsilon$-greedy √© apresentado abaixo [^8]:

```
Initialize, for a = 1 to k:
    Q(a) ‚Üê 0
    N(a) ‚Üê 0

Loop forever:
    A ‚Üê { argmax_a Q(a) with probability 1 - Œµ (breaking ties randomly)
        { a random action with probability Œµ
    R ‚Üê bandit(A)
    N(A) ‚Üê N(A) + 1
    Q(A) ‚Üê Q(A) + 1/N(A) * [R - Q(A)]
```
**Observa√ß√£o:** Note que o pseudoc√≥digo acima usa $\frac{1}{N(A)}$ como tamanho do passo, o qual corresponde ao m√©todo de m√©dia amostral. Como vimos, um tamanho de passo constante $\alpha$ pode ser mais adequado para ambientes n√£o estacion√°rios.

```mermaid
graph LR
    subgraph "Implementa√ß√£o Incremental"
        A("Q_n") -->|R_n, Œ±| B("Q_{n+1}");
         B --> B;
         style B fill:#ccf,stroke:#333,stroke-width:2px
    end
    
```
#### Lemma 1
A atualiza√ß√£o incremental da m√©dia com tamanho de passo constante $\alpha$, $Q_{n+1} = Q_n + \alpha[R_n - Q_n]$ resulta em uma m√©dia ponderada das recompensas anteriores, onde o peso de cada recompensa decai exponencialmente [^5].

**Prova**

Come√ßando pela equa√ß√£o de atualiza√ß√£o:

$$Q_{n+1} = Q_n + \alpha [R_n - Q_n]$$

$$Q_{n+1} = \alpha R_n + (1 - \alpha)Q_n$$

Expandindo recursivamente $Q_n$

$$Q_{n+1} = \alpha R_n + (1 - \alpha)[\alpha R_{n-1} + (1 - \alpha)Q_{n-1}]$$

$$Q_{n+1} = \alpha R_n + \alpha(1 - \alpha)R_{n-1} + (1 - \alpha)^2Q_{n-1}$$
Continuando recursivamente at√© $Q_1$, onde $Q_1$ √© um valor inicial:

$$Q_{n+1} = \alpha R_n + \alpha(1 - \alpha)R_{n-1} + \alpha(1 - \alpha)^2R_{n-2} + \ldots + (1 - \alpha)^{n-1}Q_1$$

$$Q_{n+1} = \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i + (1 - \alpha)^{n}Q_1$$

Esta equa√ß√£o mostra que o peso da recompensa $R_i$ √© $\alpha(1 - \alpha)^{n-i}$, que decai exponencialmente com o tempo. O peso da estimativa inicial $Q_1$ √© $(1-\alpha)^n$ que tamb√©m decai com o tempo. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Para ilustrar o decaimento exponencial, vamos utilizar $\alpha=0.1$ e calcular o peso das √∫ltimas tr√™s recompensas $R_n, R_{n-1}, R_{n-2}$ quando temos $n=10$.
>
> - Peso de $R_{10}$: $\alpha(1-\alpha)^{10-10} = 0.1 * (0.9)^0 = 0.1$
> - Peso de $R_9$: $\alpha(1-\alpha)^{10-9} = 0.1 * (0.9)^1 = 0.09$
> - Peso de $R_8$: $\alpha(1-\alpha)^{10-8} = 0.1 * (0.9)^2 = 0.081$
>
> Este exemplo demonstra que, com $\alpha=0.1$, a recompensa mais recente (R_10) tem um peso de 0.1, enquanto as recompensas anteriores t√™m pesos que decaem exponencialmente (0.09 e 0.081).

**Lema 1.1**
A atualiza√ß√£o incremental com tamanho de passo constante $\alpha$ pode ser escrita como uma combina√ß√£o convexa entre a recompensa atual e a estimativa anterior.

**Prova**

A equa√ß√£o de atualiza√ß√£o √©:

$$Q_{n+1} = Q_n + \alpha[R_n - Q_n]$$

Reorganizando os termos, temos:

$$Q_{n+1} = \alpha R_n + (1 - \alpha) Q_n$$

Como $\alpha \in (0, 1]$, e consequentemente $1 - \alpha \in [0, 1)$, essa express√£o representa uma combina√ß√£o convexa entre $R_n$ e $Q_n$. $\blacksquare$

**Lema 1.2**
Se o tamanho do passo $\alpha_n(a)$ for escolhido de tal forma que $\sum_{n=1}^{\infty} \alpha_n(a) = \infty$ e $\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$, ent√£o a estimativa $Q_n(a)$ ir√° convergir para o valor verdadeiro $q_*(a)$, assumindo que a m√©dia das recompensas seja estacion√°ria.

**Prova (Esbo√ßo)**

Este resultado √© uma condi√ß√£o de converg√™ncia cl√°ssica para algoritmos de aproxima√ß√£o estoc√°stica.  A primeira condi√ß√£o, $\sum_{n=1}^{\infty} \alpha_n(a) = \infty$, garante que o algoritmo n√£o pare de aprender (atualizar a estimativa). A segunda condi√ß√£o, $\sum_{n=1}^{\infty} \alpha_n^2(a) < \infty$, garante que o processo de atualiza√ß√£o n√£o seja muito ruidoso e que as estimativas acabem se estabilizando. A prova formal envolve conceitos de converg√™ncia de martingales e pode ser encontrada em textos avan√ßados de *reinforcement learning* e otimiza√ß√£o estoc√°stica.
```mermaid
graph LR
    subgraph "Condi√ß√µes de Converg√™ncia"
      A["Œ£ Œ±_n(a) = ‚àû"]
      B["Œ£ Œ±_n¬≤(a) < ‚àû"]
      A --> C("Q_n(a) ‚Üí q_*(a)")
      B --> C
     style A fill:#ccf,stroke:#333,stroke-width:2px
     style B fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#f9f,stroke:#333,stroke-width:2px
    end
```

**Proposi√ß√£o 2**
O m√©todo de m√©dia amostral, onde $\alpha_n(a) = \frac{1}{n}$, satisfaz as condi√ß√µes de converg√™ncia descritas no **Lema 1.2**.

**Prova**

Para $\alpha_n(a) = \frac{1}{n}$, temos:
1.  $\sum_{n=1}^{\infty} \alpha_n(a) = \sum_{n=1}^{\infty} \frac{1}{n}$, que √© a s√©rie harm√¥nica e diverge para o infinito.
2.  $\sum_{n=1}^{\infty} \alpha_n^2(a) = \sum_{n=1}^{\infty} \frac{1}{n^2}$, que √© uma s√©rie p com p=2, e converge para $\frac{\pi^2}{6}$.

Portanto, o m√©todo de m√©dia amostral satisfaz ambas as condi√ß√µes de **Lema 1.2** e, portanto, converge para o valor verdadeiro $q_*(a)$ em problemas estacion√°rios. $\blacksquare$

### Conclus√£o

O uso de uma implementa√ß√£o incremental com tamanho de passo, $\alpha$ ou $\alpha_t(a)$, permite um c√°lculo eficiente da estimativa de valor das a√ß√µes em problemas de *reinforcement learning*. A escolha entre um tamanho de passo constante ou vari√°vel depende da natureza do problema. Para problemas estacion√°rios, um tamanho de passo que diminui com o tempo pode ser ideal. Para problemas n√£o estacion√°rios, √© prefer√≠vel um tamanho de passo constante ou que d√™ maior peso para as recompensas mais recentes [^5]. A generaliza√ß√£o para $\alpha_t(a)$ adiciona uma camada de flexibilidade que pode ser utilizada para melhorar o desempenho do aprendizado em diversos cen√°rios [^8]. O tamanho do passo, seja ele constante ou vari√°vel, desempenha um papel central na forma como as informa√ß√µes s√£o ponderadas, sendo crucial para um aprendizado eficaz [^5].

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Chapter 2)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods." *(Trecho de Chapter 2)*
[^5]: "The averaging methods discussed so far are appropriate for stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time. As noted earlier, we often encounter reinforcement learning problems that are effectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards." *(Trecho de Chapter 2)*
[^8]: "In this book we denote the step-size parameter by Œ± or, more generally, by Œ±t(a)." *(Trecho de Chapter 2)*
```