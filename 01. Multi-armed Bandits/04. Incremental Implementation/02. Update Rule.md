## Incremental Implementation e a Regra de Atualiza√ß√£o Geral

### Introdu√ß√£o

Este cap√≠tulo aborda a implementa√ß√£o eficiente de m√©todos de valor de a√ß√£o, focando em como calcular m√©dias de recompensas observadas de maneira computacionalmente eficiente, utilizando mem√≥ria constante e computa√ß√£o por etapa de tempo [^1]. A otimiza√ß√£o do c√°lculo dessas m√©dias √© crucial para a aplica√ß√£o pr√°tica de m√©todos de aprendizado por refor√ßo em problemas mais complexos [^1]. M√©todos *action-value* estimam o valor de a√ß√µes baseados em recompensas passadas, e uma maneira natural de fazer isso √© calculando a m√©dia das recompensas recebidas [^3]. No entanto, manter um registro de todas as recompensas e recalcular a m√©dia a cada nova recompensa pode ser ineficiente computacionalmente, especialmente em cen√°rios com um grande n√∫mero de recompensas observadas [^7]. Assim, √© necess√°rio um m√©todo que seja capaz de atualizar essas m√©dias de forma incremental, consumindo uma quantidade constante de recursos computacionais.

### Conceitos Fundamentais

A abordagem para realizar a estimativa de forma incremental envolve a deriva√ß√£o de f√≥rmulas para atualiza√ß√£o das m√©dias com computa√ß√£o pequena e constante, processando cada nova recompensa [^7]. Para uma a√ß√£o espec√≠fica, seja $R_i$ a recompensa recebida ap√≥s a $i$-√©sima sele√ß√£o desta a√ß√£o, e $Q_n$ a estimativa do valor da a√ß√£o ap√≥s ela ter sido selecionada $n-1$ vezes. Podemos expressar $Q_n$ como:

$$ Q_n = \frac{R_1 + R_2 + \ldots + R_{n-1}}{n-1} $$

A implementa√ß√£o direta dessa f√≥rmula demandaria manter um registro de todas as recompensas passadas e recalcular a soma a cada nova recompensa, o que seria computacionalmente caro. No entanto, podemos obter uma atualiza√ß√£o incremental. Dado $Q_n$ e a $n$-√©sima recompensa $R_n$, a nova m√©dia de todas as $n$ recompensas, $Q_{n+1}$, pode ser calculada como [^7]:

```mermaid
flowchart LR
    A["Qn"] -->|"(n-1)*Qn"| B("Soma das recompensas at√© n-1");
    C["Rn"] --> D("Soma das recompensas at√© n");
    B -->|"+"| D;
    D -->|"/n"| E["Qn+1"];
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px

```

$$ Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i = \frac{1}{n} (R_n + \sum_{i=1}^{n-1} R_i) = \frac{1}{n} (R_n + (n-1)Q_n) $$

$$ Q_{n+1} = \frac{1}{n} (R_n + nQ_n - Q_n) = Q_n + \frac{1}{n} (R_n - Q_n) $$

Esta forma incremental de atualizar a m√©dia requer que a mem√≥ria seja alocada somente para $Q_n$ e $n$, e apenas a pequena computa√ß√£o acima para cada nova recompensa [^7].

> üí° **Exemplo Num√©rico:**
> Vamos supor que um agente execute uma a√ß√£o e receba as seguintes recompensas sequencialmente: $R_1 = 2$, $R_2 = 4$, $R_3 = 1$, $R_4 = 3$. Inicialmente, $Q_1$ √© considerado 0 (ou qualquer valor inicial), e $n$ √© o n√∫mero de vezes que a a√ß√£o foi executada at√© agora.
>
> -   **$n = 1$**: $Q_1 = 0$ (valor inicial).
> -   **$n = 2$**: $Q_2 = Q_1 + \frac{1}{1}(R_1 - Q_1) = 0 + \frac{1}{1}(2 - 0) = 2$.  A m√©dia simples seria $\frac{2}{1} = 2$.
> -   **$n = 3$**: $Q_3 = Q_2 + \frac{1}{2}(R_2 - Q_2) = 2 + \frac{1}{2}(4 - 2) = 2 + 1 = 3$. A m√©dia simples seria $\frac{2 + 4}{2} = 3$.
> -   **$n = 4$**: $Q_4 = Q_3 + \frac{1}{3}(R_3 - Q_3) = 3 + \frac{1}{3}(1 - 3) = 3 - \frac{2}{3} = \frac{7}{3} \approx 2.33$.  A m√©dia simples seria $\frac{2 + 4 + 1}{3} = \frac{7}{3} \approx 2.33$.
> -   **$n = 5$**: $Q_5 = Q_4 + \frac{1}{4}(R_4 - Q_4) = \frac{7}{3} + \frac{1}{4}(3 - \frac{7}{3}) = \frac{7}{3} + \frac{1}{4}(\frac{2}{3}) = \frac{7}{3} + \frac{1}{6} = \frac{15}{6} = 2.5$. A m√©dia simples seria $\frac{2 + 4 + 1 + 3}{4} = \frac{10}{4} = 2.5$.
>
> A cada passo, a estimativa $Q_n$ se aproxima da m√©dia real das recompensas. Esta atualiza√ß√£o incremental evita o armazenamento de todas as recompensas passadas e recalcular a m√©dia do zero toda vez que uma nova recompensa √© observada.

**Lemma 1:** A atualiza√ß√£o incremental da m√©dia, dada por:
$$ Q_{n+1} = Q_n + \frac{1}{n}(R_n - Q_n) $$
√© equivalente ao c√°lculo da m√©dia por:
$$ Q_{n+1} = \frac{1}{n}\sum_{i=1}^{n}R_i $$

**Prova:** Podemos provar isso por indu√ß√£o. Para $n = 1$, temos $Q_1 = 0$. Quando $n=2$, $Q_2 = Q_1 + \frac{1}{1}(R_1 - Q_1) = R_1$. Assumindo que a equa√ß√£o seja v√°lida para $n$, i.e., $Q_n = \frac{1}{n-1}\sum_{i=1}^{n-1}R_i$. Ent√£o, para $n+1$:

$$ Q_{n+1} = Q_n + \frac{1}{n}(R_n - Q_n) = \frac{n-1}{n-1}Q_n + \frac{1}{n}(R_n - Q_n) = \frac{nQ_n - Q_n + R_n - Q_n}{n} $$

Substituindo $Q_n$, temos:

$$ Q_{n+1} = \frac{n\frac{1}{n-1}\sum_{i=1}^{n-1}R_i - \frac{1}{n-1}\sum_{i=1}^{n-1}R_i+ R_n - \frac{1}{n-1}\sum_{i=1}^{n-1}R_i}{n} = \frac{\frac{n-1}{n-1}\sum_{i=1}^{n-1}R_i + R_n}{n} $$

$$ Q_{n+1} = \frac{\sum_{i=1}^{n-1}R_i+ R_n}{n} = \frac{1}{n}\sum_{i=1}^{n}R_i  $$

$\blacksquare$

**Lema 1.1:**  Se a sequ√™ncia de recompensas $R_i$ √© limitada por um valor $M$, isto √©, $|R_i| \leq M$ para todo $i$, ent√£o a sequ√™ncia de estimativas $Q_n$ tamb√©m √© limitada.

**Prova:** A partir da f√≥rmula incremental, podemos ver que $Q_{n+1}$ √© uma m√©dia ponderada de $Q_n$ e $R_n$. Se assumirmos que a primeira estimativa $Q_1$ √© inicializada como 0, ou algum valor limitado, e que todas as recompensas $R_i$ s√£o limitadas por $M$, ent√£o $Q_2 = R_1$. Subsequentemente, $Q_3$ ser√° uma m√©dia de $Q_2$ e $R_2$, que tamb√©m ser√° limitada por $M$. Podemos continuar argumentando indutivamente que a sequ√™ncia de estimativas $Q_n$ tamb√©m ser√° limitada. Formalmente, se $|Q_n| \leq M$, ent√£o
$$|Q_{n+1}| = \left|Q_n + \frac{1}{n}(R_n - Q_n)\right| \leq |Q_n| + \frac{1}{n}|R_n - Q_n| \leq M + \frac{1}{n}(|R_n| + |Q_n|) \leq M + \frac{1}{n}(M + M) = M + \frac{2M}{n} $$
Como $\frac{2M}{n}$ decresce com o aumento de $n$, para $n\geq2$,  $|Q_{n+1}| \leq 2M$ e a sequ√™ncia $Q_n$ √© limitada. $\blacksquare$

Essa atualiza√ß√£o incremental √© um caso espec√≠fico de uma regra de atualiza√ß√£o geral que se manifesta em diversos algoritmos de aprendizado por refor√ßo [^7]:

**Nova Estimativa ‚Üê Antiga Estimativa + Tamanho do Passo [Alvo - Antiga Estimativa]**

Nesse contexto, o termo "Tamanho do Passo" representa a fra√ß√£o do "erro na estimativa" que √© utilizada para mover a "antiga estimativa" em dire√ß√£o ao "alvo". A express√£o `[Alvo - Antiga Estimativa]` representa o erro na estimativa [^7]. O "alvo" indica a dire√ß√£o desej√°vel de movimento. No exemplo acima, o alvo √© a n-√©sima recompensa $R_n$, que, embora ruidosa, aponta para a m√©dia verdadeira. O "Tamanho do Passo" √© dado por $1/n$, que diminui √† medida que mais recompensas s√£o observadas. Essa regra de atualiza√ß√£o garante que as estimativas sejam ajustadas gradualmente, melhorando-se √† medida que mais dados s√£o coletados.

```mermaid
flowchart LR
    subgraph "Regra Geral de Atualiza√ß√£o"
    A["Antiga Estimativa"] --> B("Erro: Alvo - Antiga Estimativa");
    C["Tamanho do Passo"] -->|*| D("Tamanho do Passo * Erro");
    B --> D
    D -->|"+"| E["Nova Estimativa"];
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

**Corol√°rio 1:** A atualiza√ß√£o incremental usando a regra geral com um step-size $\frac{1}{n}$ e tendo $R_n$ como target converge para a m√©dia verdadeira.

**Prova:** Vimos na prova do Lemma 1 que a atualiza√ß√£o incremental usando um step-size de $\frac{1}{n}$ e $R_n$ como target converge para a m√©dia das recompensas observadas. De acordo com a lei dos grandes n√∫meros, a m√©dia amostral converge para a m√©dia verdadeira √† medida que o n√∫mero de amostras tende ao infinito. Portanto, a regra geral aplicada dessa forma converge para a m√©dia verdadeira. $\blacksquare$

**Teorema 1:** A regra geral de atualiza√ß√£o, quando utilizada com um tamanho de passo constante $\alpha$, pode ser usada para rastrear a m√©dia de uma distribui√ß√£o n√£o estacion√°ria.

**Prova:** Considere a regra geral de atualiza√ß√£o: $Q_{n+1} = Q_n + \alpha(R_n - Q_n)$, onde $\alpha$ √© um tamanho de passo constante. Para valores de $\alpha$ entre 0 e 1, a estimativa $Q_{n+1}$ ser√° uma m√©dia ponderada entre a estimativa anterior $Q_n$ e a nova recompensa $R_n$. Ao contr√°rio do caso com tamanho de passo $1/n$, onde o peso dado a recompensas antigas diminui com o tempo, o uso de um tamanho de passo constante garante que as estimativas antigas n√£o s√£o ignoradas rapidamente. Isso permite que o valor estimado $Q_n$ acompanhe as mudan√ßas nas recompensas m√©dias devido a uma distribui√ß√£o n√£o estacion√°ria. De fato, ao reescrever a f√≥rmula como $Q_{n+1} = (1-\alpha)Q_n + \alpha R_n$, podemos ver que a nova estimativa √© uma m√©dia ponderada entre a estimativa anterior e a recompensa atual, com os pesos dados por $1-\alpha$ e $\alpha$ respectivamente. Esse tipo de atualiza√ß√£o √© √∫til em ambientes n√£o estacion√°rios nos quais a distribui√ß√£o de recompensas pode mudar ao longo do tempo. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Vamos usar um tamanho de passo constante $\alpha = 0.1$. Suponha que as recompensas recebidas sejam geradas por um processo n√£o estacion√°rio. Inicialmente, a m√©dia de recompensa √© pr√≥xima de 2, mas de repente muda para 8. Consideraremos a sequ√™ncia de recompensas como $R_1 = 2, R_2 = 4, R_3 = 1, R_4 = 8, R_5 = 9, R_6 = 7$. Inicializamos $Q_1 = 0$.
>
> - **$n = 1$**: $Q_1 = 0$
> - **$n = 2$**: $Q_2 = Q_1 + \alpha(R_1 - Q_1) = 0 + 0.1(2 - 0) = 0.2$.
> - **$n = 3$**: $Q_3 = Q_2 + \alpha(R_2 - Q_2) = 0.2 + 0.1(4 - 0.2) = 0.2 + 0.1(3.8) = 0.58$.
> - **$n = 4$**: $Q_4 = Q_3 + \alpha(R_3 - Q_3) = 0.58 + 0.1(1 - 0.58) = 0.58 + 0.1(0.42) = 0.622$.
> - **$n = 5$**: $Q_5 = Q_4 + \alpha(R_4 - Q_4) = 0.622 + 0.1(8 - 0.622) = 0.622 + 0.1(7.378) = 1.3598$.
> - **$n = 6$**: $Q_6 = Q_5 + \alpha(R_5 - Q_5) = 1.3598 + 0.1(9 - 1.3598) = 1.3598 + 0.1(7.6402) = 2.12382$.
> - **$n = 7$**: $Q_7 = Q_6 + \alpha(R_6 - Q_6) = 2.12382 + 0.1(7 - 2.12382) = 2.12382 + 0.1(4.87618) = 2.611438$.
>
>
> Observe que, ap√≥s a mudan√ßa na distribui√ß√£o das recompensas (a partir de R4), o valor estimado $Q_n$ aumenta rapidamente para se ajustar √† nova m√©dia. Se tiv√©ssemos utilizado o tamanho de passo $\frac{1}{n}$, a mudan√ßa seria muito mais lenta, pois recompensas antigas teriam um peso maior. Isso demonstra como o tamanho de passo constante permite acompanhar distribui√ß√µes n√£o estacion√°rias. Se  $\alpha$ fosse menor, por exemplo, 0.01, a mudan√ßa seria mais gradual, e se fosse maior, como 0.5, a resposta √† mudan√ßa seria mais abrupta.

**Observa√ß√£o 1:** Ao utilizar a regra geral de atualiza√ß√£o com um tamanho de passo constante $\alpha$, a estimativa $Q_n$ n√£o converge para a m√©dia verdadeira das recompensas observadas ao longo de toda a hist√≥ria, mas sim acompanha as √∫ltimas recompensas com mais intensidade, tornando o m√©todo adapt√°vel a varia√ß√µes no ambiente.

**Proposi√ß√£o 1:** Para um tamanho de passo constante $\alpha$, a atualiza√ß√£o incremental pode ser expressa como uma m√©dia ponderada exponencial das recompensas passadas, onde recompensas mais recentes t√™m maior peso.

**Prova:** Podemos reescrever a atualiza√ß√£o incremental como:
$Q_{n+1} = Q_n + \alpha(R_n - Q_n) = (1-\alpha)Q_n + \alpha R_n$
Expandindo recursivamente, temos:
$Q_{n+1} = \alpha R_n + (1-\alpha) (\alpha R_{n-1} + (1-\alpha)Q_{n-1}) =  \alpha R_n + \alpha(1-\alpha) R_{n-1} + (1-\alpha)^2 Q_{n-1}$
Continuando a expans√£o, chegamos em:
$Q_{n+1} = \alpha R_n + \alpha(1-\alpha)R_{n-1} + \alpha(1-\alpha)^2 R_{n-2} + \ldots + (1-\alpha)^n Q_1$
Esta √© uma m√©dia ponderada das recompensas, onde o peso de cada recompensa $R_i$ √© $\alpha(1-\alpha)^{n-i}$ e o peso da estimativa inicial $Q_1$ √© $(1-\alpha)^n$. Como $0 < \alpha < 1$, o peso das recompensas mais recentes √© maior, caracterizando uma m√©dia ponderada exponencial. $\blacksquare$

```mermaid
flowchart LR
    subgraph "M√©dia Ponderada Exponencial"
    A["Q_(n+1)"] -- "alpha * R_n" --> B;
    C["Q_n"] -- "(1-alpha)" --> D;
        D -->|"+"| B
    B --> F["Q_(n+1)"];
    
    
    
    E[ ] -- "alpha * (1-alpha)R_(n-1)" --> F
        
     style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px    
    
    end
    linkStyle 0,1,2,3,4 stroke:#333,stroke-width:2px
    
```

> üí° **Exemplo Num√©rico:**
>
> Para visualizar a m√©dia ponderada exponencial, consideremos novamente $\alpha = 0.1$ e a sequ√™ncia de recompensas  $R_1=2, R_2=4, R_3=1, R_4=8$.  Podemos calcular o peso de cada recompensa na estimativa $Q_5$, por exemplo, quando usamos um tamanho de passo constante. Para simplificar a demonstra√ß√£o, vamos considerar que $Q_1 = 0$.
>
>  - $Q_2 = 0.1 \cdot R_1 + (1-0.1) \cdot 0 = 0.1 \cdot 2 = 0.2$
>  - $Q_3 = 0.1 \cdot R_2 + 0.9 \cdot Q_2 = 0.1 \cdot 4 + 0.9 \cdot 0.2 = 0.4 + 0.18 = 0.58$
>  - $Q_4 = 0.1 \cdot R_3 + 0.9 \cdot Q_3 = 0.1 \cdot 1 + 0.9 \cdot 0.58 = 0.1 + 0.522 = 0.622$
>  - $Q_5 = 0.1 \cdot R_4 + 0.9 \cdot Q_4 = 0.1 \cdot 8 + 0.9 \cdot 0.622 = 0.8 + 0.5598 = 1.3598$
>
> Agora, expandindo $Q_5$ em termos de recompensas:
>
> $Q_5 = \alpha R_4 + \alpha(1-\alpha)R_3 + \alpha(1-\alpha)^2 R_2 + \alpha(1-\alpha)^3 R_1 + (1-\alpha)^4 Q_1$
>
> $Q_5 = 0.1 R_4 + 0.1(0.9)R_3 + 0.1(0.9)^2 R_2 + 0.1(0.9)^3 R_1 + (0.9)^4 (0)$
>
> $Q_5 = 0.1(8) + 0.09(1) + 0.081(4) + 0.0729(2) = 0.8 + 0.09 + 0.324 + 0.1458= 1.3598$
>
> Os pesos para cada recompensa s√£o:
>
> -   $R_4$: $0.1$ (maior peso)
> -   $R_3$: $0.09$
> -   $R_2$: $0.081$
> -   $R_1$: $0.0729$
>
> A recompensa mais recente ($R_4$) tem o maior peso, e as recompensas anteriores t√™m pesos que decrescem exponencialmente. Isso demonstra a natureza da m√©dia ponderada exponencial.

### Conclus√£o

A implementa√ß√£o incremental da estimativa de valores de a√ß√£o, juntamente com a regra geral de atualiza√ß√£o, fornece uma base s√≥lida para m√©todos eficientes de aprendizado por refor√ßo. Ao evitar o armazenamento de todas as recompensas passadas e ao processar as recompensas uma a uma, esses m√©todos s√£o escal√°veis e computacionalmente vi√°veis para aplica√ß√µes do mundo real. A regra de atualiza√ß√£o geral, expressa como Nova\_Estimativa ‚Üê Antiga\_Estimativa + Tamanho\_Passo [Alvo - Antiga\_Estimativa], √© um padr√£o comum em muitos algoritmos de aprendizado, oferecendo uma forma intuitiva e eficiente de ajustar os par√¢metros do modelo com base em feedback do ambiente. A converg√™ncia demonstrada por essas abordagens estabelece a sua efic√°cia e confiabilidade.  A adaptabilidade da regra geral utilizando diferentes tamanhos de passo, como um tamanho de passo constante, tamb√©m permite que essa regra seja usada em uma variedade de contextos, incluindo ambientes n√£o estacion√°rios.

### Refer√™ncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(/content/reinforcement_learning_notes/01. Multi-armed Bandits)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods." *(/content/reinforcement_learning_notes/01. Multi-armed Bandits)*
[^7]: "As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward." *(/content/reinforcement_learning_notes/01. Multi-armed Bandits)*
[^8]: "This update rule (2.3) is of a form that occurs frequently throughout this book. The general form is NewEstimate ‚Üê OldEstimate + StepSize [Target OldEstimate]." *(/content/reinforcement_learning_notes/01. Multi-armed Bandits)*
