```mermaid
flowchart LR
    A["In√≠cio: Q1"] --> B("n=1, Recebe R1");
    B --> C{"Q2 = Q1 + (1/1) * (R1 - Q1)"};
    C --> D("n=2, Recebe R2");
    D --> E{"Q3 = Q2 + (1/2) * (R2 - Q2)"};
    E --> F("n=3, Recebe R3");
    F --> G{"Q4 = Q3 + (1/3) * (R3 - Q3)"};
     G --> H("n=4, Recebe R4");
    H --> I{"Q5 = Q4 + (1/4) * (R4 - Q4)"};
    I --> J["..."];
    J --> K["Fim: Qn (converge para a m√©dia)"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style K fill:#ccf,stroke:#333,stroke-width:2px
```

### Implementa√ß√£o Incremental para C√°lculo de M√©dias Amostrais em Multi-armed Bandits

### Introdu√ß√£o

Em problemas de **Multi-armed Bandits**, o objetivo √© maximizar a recompensa total esperada ao longo do tempo, escolhendo repetidamente entre diversas op√ß√µes (bra√ßos). Uma das abordagens fundamentais √© estimar o valor de cada a√ß√£o, que corresponde √† recompensa m√©dia esperada ao selecionar tal a√ß√£o [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). O c√°lculo direto da m√©dia de recompensas, embora conceitualmente simples, pode se tornar computacionalmente caro e demandar muita mem√≥ria √† medida que o n√∫mero de recompensas observadas aumenta. Este cap√≠tulo explora a implementa√ß√£o incremental como uma solu√ß√£o eficiente para esse problema, permitindo o c√°lculo das m√©dias amostrais com custo computacional constante e uso de mem√≥ria limitado [6](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-6).

### Conceitos Fundamentais

A **m√©dia amostral** de um conjunto de recompensas √© calculada somando todas as recompensas observadas e dividindo pelo n√∫mero total de observa√ß√µes. Formalmente, a m√©dia amostral $Q_t(a)$ de uma a√ß√£o $a$ no passo de tempo $t$ pode ser expressa como [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3):
$$
Q_t(a) = \frac{\text{soma das recompensas quando a foi tomada at√© t}}{\text{n√∫mero de vezes que a foi tomada at√© t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$
onde $R_i$ √© a recompensa obtida no passo de tempo $i$, $A_i$ √© a a√ß√£o selecionada no passo de tempo $i$, e $\mathbb{1}_{A_i=a}$ √© uma fun√ß√£o indicadora que retorna 1 se $A_i = a$ e 0 caso contr√°rio. Essa abordagem direta requer o armazenamento de todas as recompensas e um c√°lculo da soma a cada passo de tempo.

> üí° **Exemplo Num√©rico:** Suponha que uma a√ß√£o 'a' foi tomada 4 vezes, com as seguintes recompensas: 2, 4, 1, 5. Usando o m√©todo tradicional, calcular√≠amos a m√©dia como $Q_4(a) = \frac{2 + 4 + 1 + 5}{4} = \frac{12}{4} = 3$. Para cada nova recompensa, ter√≠amos que adicionar essa nova recompensa √† soma total e incrementar o contador.

A **implementa√ß√£o incremental** oferece uma alternativa computacionalmente mais eficiente [6](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-6). Em vez de recalcular a m√©dia a cada passo, a implementa√ß√£o incremental atualiza a m√©dia anterior com base na nova recompensa observada. Seja $Q_n$ a estimativa do valor de uma a√ß√£o ap√≥s ela ter sido selecionada $n-1$ vezes, e $R_n$ a recompensa obtida ap√≥s a n-√©sima sele√ß√£o da a√ß√£o. O valor de $Q_{n+1}$, a estimativa ap√≥s n sele√ß√µes, √© calculado incrementalmente atrav√©s da seguinte f√≥rmula:

$$
Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i = \frac{1}{n} \left(R_n + \sum_{i=1}^{n-1} R_i\right) = \frac{1}{n} \left(R_n + (n-1)Q_n\right) = Q_n + \frac{1}{n} [R_n - Q_n]
$$

Esta formula√ß√£o mostra que a nova estimativa √© igual a antiga estimativa mais uma fra√ß√£o da diferen√ßa entre a recompensa atual e a estimativa anterior. Essa abordagem reduz significativamente o custo computacional, pois requer apenas um pequeno c√°lculo com os valores previamente armazenados, sem a necessidade de manter o hist√≥rico completo de recompensas. Esta formula√ß√£o se encaixa no padr√£o geral [7](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-7):

$$
\text{Nova Estimativa} \leftarrow \text{Antiga Estimativa} + \text{Tamanho do Passo} \ [\text{Alvo} - \text{Antiga Estimativa}]
$$
onde o tamanho do passo (step size) neste caso √© $\frac{1}{n}$ e o alvo √© a n-√©sima recompensa $R_n$. O termo  [Alvo - Antiga Estimativa] representa o erro na estimativa e a atualiza√ß√£o da estimativa √© dada na dire√ß√£o do "alvo", que pode ser ruidoso.

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo anterior (recompensas 2, 4, 1, 5), vamos calcular as m√©dias incrementalmente. Inicialmente, $Q_1 = 0$ (ou podemos definir com algum valor inicial como discutido mais adiante):
>
> - $n=1, R_1 = 2$:  $Q_2 = Q_1 + \frac{1}{1}[R_1 - Q_1] = 0 + 1[2 - 0] = 2$
> - $n=2, R_2 = 4$:  $Q_3 = Q_2 + \frac{1}{2}[R_2 - Q_2] = 2 + \frac{1}{2}[4 - 2] = 2 + 1 = 3$
> - $n=3, R_3 = 1$:  $Q_4 = Q_3 + \frac{1}{3}[R_3 - Q_3] = 3 + \frac{1}{3}[1 - 3] = 3 - \frac{2}{3} = 2.33$
> - $n=4, R_4 = 5$:  $Q_5 = Q_4 + \frac{1}{4}[R_4 - Q_4] = 2.33 + \frac{1}{4}[5 - 2.33] = 2.33 + 0.67 = 3$
>
> Observe que, ap√≥s as 4 recompensas, a m√©dia incremental $Q_5$ converge para a m√©dia calculada diretamente de 3. Cada atualiza√ß√£o usa apenas a recompensa atual e a m√©dia anterior.

**Lemma 1:** *A implementa√ß√£o incremental para o c√°lculo de m√©dias amostrais, apresentada como:*
$Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]$,
*requer mem√≥ria e computa√ß√£o constante a cada passo.*
**Prova:** A f√≥rmula de atualiza√ß√£o $Q_{n+1}$ usa apenas a estimativa anterior $Q_n$, a recompensa $R_n$, e o n√∫mero de vezes $n$ que a a√ß√£o foi selecionada. Estas tr√™s vari√°veis podem ser armazenadas e computadas com custo constante por passo, sem a necessidade de armazenar o hist√≥rico completo de recompensas.  O c√°lculo da atualiza√ß√£o √© realizado com um n√∫mero fixo de opera√ß√µes a cada passo. $\blacksquare$

**Corol√°rio 1:** *O m√©todo incremental converge para a m√©dia amostral real quando o n√∫mero de sele√ß√µes da a√ß√£o tende ao infinito.*
**Prova:** Quando $n$ tende ao infinito, $\frac{1}{n}$ tende a 0, indicando que a contribui√ß√£o de cada nova recompensa no ajuste da m√©dia torna-se cada vez menor, de forma que $Q_n$ tende √† m√©dia real. Esse resultado √© uma consequ√™ncia da lei dos grandes n√∫meros, e pode ser verificado quando o denominador da formula original da m√©dia amostral tende ao infinito [3](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-3). $\blacksquare$

```mermaid
flowchart LR
    subgraph "Atualiza√ß√£o Incremental (Lemma 1.1)"
        A["Qn"] -- "(1 - 1/n)" --> C["Multiplica√ß√£o"]
        B["Rn"] -- "(1/n)" --> D["Multiplica√ß√£o"]
        C --> E["Soma"]
        D --> E
        E --> F["Qn+1"]
    end
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

**Lema 1.1:** *A atualiza√ß√£o incremental pode ser expressa de forma equivalente como uma m√©dia ponderada entre a estimativa anterior e a nova recompensa:*

$Q_{n+1} =  \left(1-\frac{1}{n}\right)Q_n + \frac{1}{n}R_n$

**Prova:**  A partir da f√≥rmula de atualiza√ß√£o incremental $Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]$, podemos reorganizar os termos:

$Q_{n+1} = Q_n + \frac{1}{n}R_n - \frac{1}{n}Q_n = \left(1-\frac{1}{n}\right)Q_n + \frac{1}{n}R_n$.  Essa express√£o representa $Q_{n+1}$ como uma m√©dia ponderada, onde o peso da estimativa anterior $Q_n$ √© $1-\frac{1}{n}$ e o peso da nova recompensa $R_n$ √© $\frac{1}{n}$.  $\blacksquare$

> üí° **Exemplo Num√©rico:** Retomando o exemplo das recompensas 2, 4, 1, 5, na terceira itera√ß√£o, temos $Q_3 = 3$. A pr√≥xima atualiza√ß√£o, com $R_3=1$, pode ser expressa como:
>
> $Q_4 = (1 - \frac{1}{3})Q_3 + \frac{1}{3}R_3 = \frac{2}{3}(3) + \frac{1}{3}(1) = 2 + \frac{1}{3} = 2.33$. Isso demonstra que $Q_4$ √© uma m√©dia ponderada de $Q_3$ e $R_3$, onde $Q_3$ tem mais peso ($\frac{2}{3}$) do que a nova recompensa $R_3$ ($\frac{1}{3}$).

**Observa√ß√£o 1:** *Esta forma ponderada da atualiza√ß√£o incremental revela a natureza da atualiza√ß√£o: a nova estimativa $Q_{n+1}$ √© uma combina√ß√£o linear da estimativa anterior $Q_n$ e da nova recompensa $R_n$, com pesos que dependem do n√∫mero de vezes que a a√ß√£o foi selecionada.*

√â importante notar que o tamanho do passo $\frac{1}{n}$ varia ao longo do tempo, pois depende do n√∫mero de vezes que a a√ß√£o foi tomada [7](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-7). No entanto, a implementa√ß√£o incremental permite que as m√©dias amostrais sejam calculadas de maneira eficiente em termos de mem√≥ria e custo computacional.

**Teorema 1:** *A implementa√ß√£o incremental pode ser generalizada utilizando um tamanho de passo vari√°vel $\alpha_n$, de forma que $Q_{n+1} = Q_n + \alpha_n[R_n - Q_n]$. Se a sequ√™ncia de tamanhos de passo $\alpha_n$ satisfaz as condi√ß√µes $\sum_{n=1}^\infty \alpha_n = \infty$ e $\sum_{n=1}^\infty \alpha_n^2 < \infty$, ent√£o $Q_n$ converge para a m√©dia real da recompensa da a√ß√£o correspondente.*

**Prova:** Este resultado segue do teorema da converg√™ncia para algoritmos de aproxima√ß√£o estoc√°stica [8]. A condi√ß√£o $\sum_{n=1}^\infty \alpha_n = \infty$ garante que as estimativas n√£o estagnam no in√≠cio do processo, permitindo explorar o espa√ßo de busca. A condi√ß√£o $\sum_{n=1}^\infty \alpha_n^2 < \infty$ garante que a vari√¢ncia do processo de atualiza√ß√£o diminua √† medida que o n√∫mero de amostras aumenta, levando √† converg√™ncia. A escolha de $\alpha_n = \frac{1}{n}$ satisfaz estas condi√ß√µes, como pode ser verificado diretamente, e portanto, o m√©todo incremental b√°sico converge para a m√©dia real da recompensa. $\blacksquare$

> üí° **Exemplo Num√©rico:**  Vamos supor que queremos usar um tamanho de passo diferente de $\frac{1}{n}$. Uma possibilidade √© $\alpha_n = \frac{1}{10n}$. Vamos simular algumas atualiza√ß√µes com as mesmas recompensas (2, 4, 1, 5) e um valor inicial $Q_1 = 0$.
>
> - $n=1, R_1 = 2$: $Q_2 = 0 + \frac{1}{10}(2 - 0) = 0.2$
> - $n=2, R_2 = 4$: $Q_3 = 0.2 + \frac{1}{20}(4 - 0.2) = 0.2 + 0.19 = 0.39$
> - $n=3, R_3 = 1$: $Q_4 = 0.39 + \frac{1}{30}(1 - 0.39) = 0.39 + 0.02033 = 0.41$
> - $n=4, R_4 = 5$: $Q_5 = 0.41 + \frac{1}{40}(5 - 0.41) = 0.41 + 0.11475 = 0.52$
>
> Note que a m√©dia est√° se aproximando, mas muito mais lentamente em rela√ß√£o a usar $\frac{1}{n}$. Um passo menor implica em atualiza√ß√µes mais lentas. As condi√ß√µes $\sum_{n=1}^\infty \alpha_n = \infty$ e $\sum_{n=1}^\infty \alpha_n^2 < \infty$ garantem que $\alpha_n$ n√£o seja muito pequeno para impedir a converg√™ncia, mas decaia r√°pido o suficiente para diminuir a vari√¢ncia e garantir que a estimativa pare de flutuar.  $\alpha_n = \frac{1}{10n}$ tamb√©m satisfaz as condi√ß√µes do Teorema 1, embora em uma taxa de converg√™ncia diferente.

**Corol√°rio 1.1:** *Um tamanho de passo constante, $\alpha_n = \alpha$ para todo $n$, n√£o satisfaz as condi√ß√µes do Teorema 1 e, portanto, n√£o garante a converg√™ncia para a m√©dia real. Contudo, tamanhos de passo constantes podem ser desej√°veis em ambientes n√£o-estacion√°rios, onde a m√©dia real das recompensas muda ao longo do tempo.*

> üí° **Exemplo Num√©rico:**  Se usarmos $\alpha = 0.1$ como um tamanho de passo constante e repetirmos o processo com as mesmas recompensas (2, 4, 1, 5) e $Q_1=0$:
>
> - $n=1, R_1 = 2$: $Q_2 = 0 + 0.1(2 - 0) = 0.2$
> - $n=2, R_2 = 4$: $Q_3 = 0.2 + 0.1(4 - 0.2) = 0.2 + 0.38 = 0.58$
> - $n=3, R_3 = 1$: $Q_4 = 0.58 + 0.1(1 - 0.58) = 0.58 + 0.042 = 0.622$
> - $n=4, R_4 = 5$: $Q_5 = 0.622 + 0.1(5 - 0.622) = 0.622 + 0.4378 = 1.0598$
>
>Observe que, com tamanho de passo constante, $Q_n$ n√£o converge para a m√©dia real (3), mas continua se movendo conforme as recompensas s√£o observadas. Essa abordagem seria mais adequada em um ambiente n√£o estacion√°rio, onde a m√©dia das recompensas pode mudar com o tempo. O tamanho de passo constante permite ao algoritmo se adaptar √†s mudan√ßas.

```mermaid
flowchart LR
    A["Q1 (Inicializa√ß√£o)"] --> B{"Atualiza√ß√£o Incremental"};
    B -- "R1, n=1, alpha1" --> C["Q2"];
     C-- "R2, n=2, alpha2" --> D["Q3"];
      D-- "R3, n=3, alpha3" --> E["Q4"];
       E-- "R4, n=4, alpha4" --> F["Q5"];
       F --> G["..."];
    G --> H["Qn (Estimativa Final)"];
    subgraph "Processo Iterativo"
    B
    C
    D
    E
    F
    end
    style H fill:#ccf,stroke:#333,stroke-width:2px
```

**Proposi√ß√£o 1:** *A atualiza√ß√£o incremental pode ser inicializada com qualquer valor inicial para $Q_1$, e o valor final da estimativa ser√° independente do valor de inicializa√ß√£o no limite.*

**Prova:** No processo de atualiza√ß√£o incremental, o valor de inicializa√ß√£o de $Q_1$ tem um efeito cada vez menor √† medida que o n√∫mero de atualiza√ß√µes aumenta, devido ao termo $\frac{1}{n}$ na atualiza√ß√£o. Dado que este termo tende a zero quando $n$ tende a infinito, o valor inicial torna-se insignificante no limite, garantindo a converg√™ncia para a m√©dia real independente do valor inicial de $Q_1$.  $\blacksquare$

> üí° **Exemplo Num√©rico:**  Vamos demonstrar a independ√™ncia do valor inicial. Usando $\alpha_n = 1/n$ e as recompensas (2,4,1,5). Vamos comparar $Q$ come√ßando com $Q_1 = 0$ e com $Q_1 = 10$:
>
> **Caso 1: Q1 = 0**
> - $n=1, R_1=2: Q_2 = 0 + 1(2-0) = 2$
> - $n=2, R_2=4: Q_3 = 2 + 0.5(4-2) = 3$
> - $n=3, R_3=1: Q_4 = 3 + 0.33(1-3) = 2.33$
> - $n=4, R_4=5: Q_5 = 2.33 + 0.25(5-2.33) = 3$
>
> **Caso 2: Q1 = 10**
> - $n=1, R_1=2: Q_2 = 10 + 1(2-10) = 2$
> - $n=2, R_2=4: Q_3 = 2 + 0.5(4-2) = 3$
> - $n=3, R_3=1: Q_4 = 3 + 0.33(1-3) = 2.33$
> - $n=4, R_4=5: Q_5 = 2.33 + 0.25(5-2.33) = 3$
>
> Em ambos os casos, as m√©dias convergem para o mesmo valor (3), embora tenham pontos de partida diferentes. Isso confirma que, com o tempo, o valor inicial perde sua influ√™ncia.

### Conclus√£o

A implementa√ß√£o incremental para o c√°lculo de m√©dias amostrais √© uma t√©cnica essencial em problemas de Multi-armed Bandits. Sua capacidade de computar m√©dias com custo computacional e de mem√≥ria constantes a cada passo a torna uma solu√ß√£o pr√°tica e eficaz para problemas onde a complexidade computacional e de mem√≥ria s√£o restritivas. Essa abordagem permite que o agente foque no processo de aprendizagem e otimiza√ß√£o das a√ß√µes, em vez de ser sobrecarregado com os c√°lculos das m√©dias. Al√©m disso, o m√©todo converge para o valor real da a√ß√£o conforme o n√∫mero de itera√ß√µes aumenta, garantindo a precis√£o da estimativa.

### Refer√™ncias
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action." *(Trecho de Multi-armed Bandits)*
[^3]: "One natural way to estimate this is by averaging the rewards actually received:
$Q_t(a) = \frac{\text{sum of rewards when a taken prior to t}}{\text{number of times a taken prior to t}} = \frac{\sum_{i=1}^{t-1} R_i\mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}$" *(Trecho de Multi-armed Bandits)*
[^6]: "The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. We now turn to the question of how these averages can be computed in a computationally efficient manner, in particular, with constant memory and constant per-time-step computation." *(Trecho de Multi-armed Bandits)*
[^7]: "This update rule (2.3) is of a form that occurs frequently throughout this book. The general form is
NewEstimate $\leftarrow$ OldEstimate + StepSize [Target - OldEstimate]." *(Trecho de Multi-armed Bandits)*
[^8]: "Stochastic Approximation and Recursive Algorithms and Applications" (livro)
