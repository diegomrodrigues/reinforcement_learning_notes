```mermaid
graph LR
    A["A√ß√£o 'a'"] -->|Sele√ß√£o no tempo 't'| B("Recompensa R_t");
    B --> C{"Distribui√ß√£o de Probabilidade\n(Estacion√°ria)"};
    C --> |Depende da A√ß√£o 'a'| B;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
```

### Introdu√ß√£o
O aprendizado por refor√ßo (reinforcement learning) se distingue de outros tipos de aprendizado por utilizar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de simplesmente instruir as a√ß√µes corretas [^1]. Essa abordagem cria a necessidade de explora√ß√£o ativa, para uma busca expl√≠cita por um bom comportamento. O *feedback* puramente avaliativo indica a qualidade da a√ß√£o tomada, mas n√£o se foi a melhor ou a pior a√ß√£o poss√≠vel. Em contraste, o *feedback* puramente instrutivo indica a a√ß√£o correta a ser tomada, independentemente da a√ß√£o realmente realizada. O aprendizado supervisionado se baseia neste tipo de *feedback* [^1]. O foco deste cap√≠tulo √© o aspecto avaliativo do aprendizado por refor√ßo em um cen√°rio simplificado, o qual n√£o envolve aprendizado para agir em mais de uma situa√ß√£o. Este cen√°rio **n√£o associativo** evita a complexidade do problema completo do aprendizado por refor√ßo, permitindo analisar como o *feedback* avaliativo difere e como pode ser combinado com o *feedback* instrutivo. O problema espec√≠fico explorado √© uma vers√£o simples do problema do bandido de k bra√ßos, usando-o para introduzir m√©todos b√°sicos de aprendizado que ser√£o posteriormente estendidos [^1].

### Conceitos Fundamentais
O problema do **bandido de *k* bra√ßos** √© um cen√°rio onde um agente √© repetidamente confrontado com a escolha entre *k* op√ß√µes distintas, ou a√ß√µes. Ap√≥s cada escolha, o agente recebe uma recompensa num√©rica, sorteada de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o selecionada. O objetivo √© maximizar a recompensa total esperada em um dado per√≠odo de tempo, como em 1000 sele√ß√µes de a√ß√µes ou *time steps* [^2]. Cada a√ß√£o possui um valor esperado ou recompensa m√©dia, que chamamos de **valor da a√ß√£o**. O valor de uma a√ß√£o arbitr√°ria *a*, denotado por $q_*(a)$, √© a recompensa esperada dado que *a* foi selecionada [^2]:
$$q_*(a) = E[R_t | A_t = a]$$
onde $A_t$ √© a a√ß√£o selecionada no *time step* $t$, e $R_t$ √© a recompensa correspondente [^2]. O problema se torna trivial se o valor de cada a√ß√£o fosse conhecido, bastaria sempre selecionar a a√ß√£o com o maior valor. No entanto, assume-se que os valores das a√ß√µes n√£o s√£o conhecidos com certeza, embora possam ser estimados. A estimativa do valor da a√ß√£o *a* no *time step* $t$ √© denotada por $Q_t(a)$ [^2]. O objetivo √© que $Q_t(a)$ se aproxime de $q_*(a)$ [^2].

> üí° **Exemplo Num√©rico:** Imagine um bandido de 3 bra√ßos (k=3). As recompensas esperadas para cada bra√ßo s√£o $q_*(1) = 1$, $q_*(2) = 2$, e $q_*(3) = 3$. O objetivo √© descobrir qual bra√ßo oferece a maior recompensa m√©dia. Inicialmente, o agente n√£o sabe quais s√£o esses valores e come√ßa com estimativas $Q_1(1) = Q_1(2) = Q_1(3) = 0$. Ao longo do tempo, o agente atualiza suas estimativas $Q_t(a)$ com base nas recompensas observadas.

```mermaid
graph LR
    subgraph "A√ß√µes e Valores"
    A1("A√ß√£o 1") -->|Valor Esperado q_*(1)=1| Q1["Valor Q_t(1)"]
    A2("A√ß√£o 2") -->|Valor Esperado q_*(2)=2| Q2["Valor Q_t(2)"]
    A3("A√ß√£o 3") -->|Valor Esperado q_*(3)=3| Q3["Valor Q_t(3)"]
    end
    Q1-- "Atualiza√ß√£o com\n Recompensas" -->Q1
    Q2-- "Atualiza√ß√£o com\n Recompensas" -->Q2
    Q3-- "Atualiza√ß√£o com\n Recompensas" -->Q3
    style Q1 fill:#ccf,stroke:#333,stroke-width:2px
    style Q2 fill:#ccf,stroke:#333,stroke-width:2px
    style Q3 fill:#ccf,stroke:#333,stroke-width:2px
    style A1 fill:#f9f,stroke:#333,stroke-width:2px
    style A2 fill:#f9f,stroke:#333,stroke-width:2px
    style A3 fill:#f9f,stroke:#333,stroke-width:2px
```

As **a√ß√µes gananciosas** s√£o aquelas cuja estimativa de valor √© a maior em um dado momento. Quando uma dessas a√ß√µes √© selecionada, diz-se que o agente est√° *explorando* o conhecimento atual dos valores das a√ß√µes [^2]. Por outro lado, se uma a√ß√£o **n√£o gananciosa** √© escolhida, o agente est√° *explorando*, buscando melhorar a estimativa do valor de a√ß√µes n√£o gananciosas [^2]. Enquanto a explota√ß√£o maximiza a recompensa esperada em um √∫nico passo, a explora√ß√£o pode levar a uma recompensa total maior a longo prazo. O equil√≠brio entre explora√ß√£o e explota√ß√£o √© um problema fundamental em aprendizado por refor√ßo [^2]. A escolha entre explorar e explorar √© complexa e depende das estimativas, incertezas e do n√∫mero de passos restantes [^2].

**Lemma 1:** *O valor estimado de uma a√ß√£o, $Q_t(a)$, converge para o valor real da a√ß√£o $q_*(a)$ conforme o n√∫mero de vezes que a a√ß√£o a √© selecionada se aproxima de infinito, sob a premissa de que as recompensas s√£o amostradas de uma distribui√ß√£o estacion√°ria.*

**Prova:** A estimativa do valor de uma a√ß√£o, $Q_t(a)$, √© calculada pela m√©dia das recompensas recebidas ap√≥s a sele√ß√£o da a√ß√£o $a$. De acordo com a lei dos grandes n√∫meros, quando o n√∫mero de amostras se aproxima de infinito, a m√©dia amostral converge para a esperan√ßa matem√°tica. Assim, quando o denominador da equa√ß√£o (2.1) [^3] se aproxima do infinito, $Q_t(a)$ converge para $q_*(a)$, o valor esperado da a√ß√£o [^3]. $\blacksquare$

```mermaid
graph LR
    A["A√ß√£o 'a'"] --> B("Recompensas R_i");
    B --> C["M√©dia Amostral\n(Q_t(a))"];
    C --> D["Valor Real q_*(a)"];
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#cfc,stroke:#333,stroke-width:2px
    linkStyle 0,1,2 stroke:#333,stroke-width:2px
    D -.->|n -> ‚àû| C
```

**Lemma 1.1:** *A taxa de converg√™ncia de $Q_t(a)$ para $q_*(a)$ depende da vari√¢ncia da distribui√ß√£o de recompensas para a a√ß√£o $a$. Uma vari√¢ncia maior implica uma converg√™ncia mais lenta.*

**Prova:** A lei dos grandes n√∫meros garante a converg√™ncia, mas n√£o especifica a velocidade. A taxa de converg√™ncia √© afetada pela vari√¢ncia das amostras. Intuitivamente, recompensas com alta vari√¢ncia levam a flutua√ß√µes maiores na m√©dia amostral, retardando a converg√™ncia. Formalmente, pode-se demonstrar que a vari√¢ncia da m√©dia amostral decresce com o n√∫mero de amostras, mas a taxa dessa diminui√ß√£o depende da vari√¢ncia da distribui√ß√£o original. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere duas a√ß√µes, A e B. A a√ß√£o A tem uma recompensa m√©dia $q_*(A) = 2$ com uma vari√¢ncia de 1, e a a√ß√£o B tem uma recompensa m√©dia $q_*(B) = 2$ com uma vari√¢ncia de 5. A Lei dos Grandes N√∫meros garante que, com um n√∫mero suficiente de amostras, $Q_t(A)$ e $Q_t(B)$ convergir√£o para 2. No entanto, $Q_t(A)$ convergir√° mais rapidamente do que $Q_t(B)$ devido √† menor vari√¢ncia da a√ß√£o A. Isso significa que o agente precisar√° experimentar a a√ß√£o B muito mais vezes para ter uma estimativa confi√°vel de seu valor.

```mermaid
graph LR
    subgraph "Converg√™ncia"
    A[A√ß√£o A\n(Var=1)]-->|q_*(A) = 2| QA["Q_t(A)\n(Converg√™ncia R√°pida)"]
    B[A√ß√£o B\n(Var=5)]-->|q_*(B) = 2| QB["Q_t(B)\n(Converg√™ncia Lenta)"]
    end
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#f9f,stroke:#333,stroke-width:2px
    style QA fill:#ccf,stroke:#333,stroke-width:2px
    style QB fill:#ccf,stroke:#333,stroke-width:2px
```

**Corol√°rio 1:** *Sob as condi√ß√µes do Lemma 1, um agente que sempre escolhe a a√ß√£o com maior valor estimado (explota√ß√£o pura) ir√°, com o tempo, escolher a a√ß√£o com maior valor real, assumindo que todas as a√ß√µes foram exploradas o suficiente.*

**Prova:** Pelo Lemma 1, $Q_t(a)$ converge para $q_*(a)$. Se o agente sempre escolhe a a√ß√£o que maximiza $Q_t(a)$, em um tempo $t$ suficientemente grande, ele ir√° escolher a a√ß√£o que maximiza $q_*(a)$, que √© a a√ß√£o √≥tima. No entanto, a explora√ß√£o pode ser necess√°ria para garantir que todas as a√ß√µes foram suficientemente exploradas e que a estimativa de valor $Q_t(a)$ √© precisa. $\blacksquare$

> üí° **Exemplo Num√©rico:**  Imagine que o agente inicializa com $Q_1(1)=1$, $Q_1(2)=0.5$ e $Q_1(3)=0$. Se o agente usar pura explora√ß√£o, no primeiro passo ele selecionaria a a√ß√£o 1. Se a recompensa observada fosse 1.2,  $Q_2(1)$ atualizaria para 1.1.  Se ele continuasse com pura explora√ß√£o, sempre escolheria a a√ß√£o 1, mesmo que a a√ß√£o 3 fosse a melhor (valor 3, conforme o exemplo anterior), pois ele nunca exploraria as outras a√ß√µes.

```mermaid
graph LR
  subgraph "Explota√ß√£o Pura"
    A1("A√ß√£o 1\nQ_1(1)=1") --> R1("R_1=1.2")
    A2("A√ß√£o 2\nQ_1(2)=0.5")
    A3("A√ß√£o 3\nQ_1(3)=0")
   end
   R1 --> Q1("Q_2(1)=1.1")
   Q1 -.-> |Continua\nExplota√ß√£o| A1
  style A1 fill:#f9f,stroke:#333,stroke-width:2px
   style A2 fill:#f9f,stroke:#333,stroke-width:2px
    style A3 fill:#f9f,stroke:#333,stroke-width:2px
    style Q1 fill:#ccf,stroke:#333,stroke-width:2px
    style R1 fill:#ccf,stroke:#333,stroke-width:2px
```

**Corol√°rio 1.1:** *Um agente puramente explorat√≥rio, que escolhe a√ß√µes aleatoriamente, tamb√©m eventualmente convergir√° para a a√ß√£o √≥tima, mas a uma taxa de converg√™ncia muito mais lenta do que um agente que combina explora√ß√£o e explota√ß√£o.*

**Prova:** Em um cen√°rio puramente explorat√≥rio, todas as a√ß√µes s√£o amostradas repetidamente. Pelo Lemma 1, as estimativas $Q_t(a)$ convergir√£o para $q_*(a)$ para todas as a√ß√µes.  Embora a converg√™ncia ocorra, a falta de um direcionamento pela explota√ß√£o significa que o agente n√£o aproveitar√° o conhecimento adquirido durante o processo de explora√ß√£o, tornando a converg√™ncia mais lenta em compara√ß√£o com um agente que equilibra explora√ß√£o e explota√ß√£o. $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando o bandido de 3 bra√ßos anterior, um agente puramente explorat√≥rio escolheria cada bra√ßo aleatoriamente. Isso levaria um tempo maior para o agente ter uma boa estimativa de qual bra√ßo √© o melhor. Por exemplo, com 100 passos, o agente poderia ter escolhido o bra√ßo 1 30 vezes, o bra√ßo 2 35 vezes e o bra√ßo 3 35 vezes. Se ele escolher os bra√ßos de forma otimizada, ap√≥s 100 passos o bra√ßo 3 poderia ter sido escolhido por mais de 80 vezes, j√° que √© o melhor.

```mermaid
graph LR
    subgraph "Explora√ß√£o Pura"
    A1("A√ß√£o 1")
    A2("A√ß√£o 2")
    A3("A√ß√£o 3")
    end
    A1-- "Escolha Aleat√≥ria" -->|Recompensas R_i| Q1["Q_t(1)"]
     A2-- "Escolha Aleat√≥ria" -->|Recompensas R_i| Q2["Q_t(2)"]
     A3-- "Escolha Aleat√≥ria" -->|Recompensas R_i| Q3["Q_t(3)"]

    Q1-- "Atualiza√ß√£o Lenta" --> Q1
    Q2-- "Atualiza√ß√£o Lenta" --> Q2
    Q3-- "Atualiza√ß√£o Lenta" --> Q3
    style A1 fill:#f9f,stroke:#333,stroke-width:2px
    style A2 fill:#f9f,stroke:#333,stroke-width:2px
    style A3 fill:#f9f,stroke:#333,stroke-width:2px
    style Q1 fill:#ccf,stroke:#333,stroke-width:2px
    style Q2 fill:#ccf,stroke:#333,stroke-width:2px
    style Q3 fill:#ccf,stroke:#333,stroke-width:2px
```

O m√©todo de estimativa **sample-average** usa a m√©dia das recompensas observadas para estimar o valor de uma a√ß√£o, conforme [^3]:
$$Q_t(a) = \frac{\text{soma das recompensas quando a foi selecionada antes de t}}{\text{n√∫mero de vezes que a foi selecionada antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}$$
onde $\mathbb{1}_{\text{predicate}}$ √© uma vari√°vel aleat√≥ria que vale 1 se o predicado for verdadeiro e 0 caso contr√°rio. Se o denominador for zero, ent√£o $Q_t(a)$ √© definido como um valor padr√£o, como 0 [^3].

> üí° **Exemplo Num√©rico:** Suponha que a a√ß√£o 2 tenha sido selecionada tr√™s vezes e as recompensas recebidas foram 1, 3 e 2. Usando o m√©todo *sample-average*, a estimativa do valor da a√ß√£o 2 seria: $Q_4(2) = (1 + 3 + 2) / 3 = 2$. Se a a√ß√£o 2 fosse selecionada novamente e a recompensa fosse 4, a nova estimativa seria $Q_5(2) = (1 + 3 + 2 + 4) / 4 = 2.5$.

```mermaid
graph LR
    A["A√ß√£o 'a'"] -->|Recompensas R_i\n(1, 3, 2, ...)| B["Soma das Recompensas"];
    B --> C["N√∫mero de Sele√ß√µes"];
     C --> D["Q_t(a) = Soma/N√∫mero"];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#cfc,stroke:#333,stroke-width:2px
    linkStyle 0,1,2 stroke:#333,stroke-width:2px
```

Para selecionar a√ß√µes, a regra mais simples √© escolher a a√ß√£o com maior valor estimado, a **a√ß√£o gananciosa**:
$$A_t = \underset{a}{\text{argmax}} \, Q_t(a)$$
Contudo, essa abordagem ignora a possibilidade de a√ß√µes aparentemente inferiores serem, na verdade, melhores. Uma alternativa √© comportar-se de forma gananciosa na maior parte do tempo, mas, com uma pequena probabilidade $\epsilon$, selecionar aleatoriamente uma a√ß√£o. Esses s√£o os m√©todos **$\epsilon$-greedy** [^3].

> üí° **Exemplo Num√©rico:**  Se tivermos 3 a√ß√µes com estimativas $Q_t(1) = 1.5$, $Q_t(2) = 2.1$ e $Q_t(3) = 1.8$, a a√ß√£o gananciosa seria a a√ß√£o 2. Em um m√©todo $\epsilon$-greedy com $\epsilon = 0.1$, o agente escolheria a a√ß√£o 2 com probabilidade 0.9 e escolheria uma das tr√™s a√ß√µes aleatoriamente com probabilidade 0.1.

```mermaid
graph LR
    subgraph "A√ß√µes e Valores Estimados"
    A1["A√ß√£o 1\nQ_t(1)=1.5"]
    A2["A√ß√£o 2\nQ_t(2)=2.1"]
    A3["A√ß√£o 3\nQ_t(3)=1.8"]
    end
    A1 -- "Probabilidade Aleat√≥ria" -->|Œµ|  A
    A2 -- "Probabilidade Gananciosa" -->|1-Œµ|  A
     A3 -- "Probabilidade Aleat√≥ria" --> |Œµ|  A
  A["Sele√ß√£o de A√ß√£o"]
   style A1 fill:#f9f,stroke:#333,stroke-width:2px
   style A2 fill:#f9f,stroke:#333,stroke-width:2px
   style A3 fill:#f9f,stroke:#333,stroke-width:2px
   style A fill:#ccf,stroke:#333,stroke-width:2px
```

**Proposi√ß√£o 1:** *O m√©todo $\epsilon$-greedy garante que todas as a√ß√µes sejam exploradas um n√∫mero infinito de vezes, sob a condi√ß√£o de que $\epsilon$ seja estritamente maior que zero.*

**Prova:** Com probabilidade $\epsilon$, uma a√ß√£o √© selecionada uniformemente de todo o conjunto de a√ß√µes. Portanto, para qualquer a√ß√£o $a$, a probabilidade de selecion√°-la em um √∫nico passo √© no m√≠nimo $\frac{\epsilon}{k}$, onde $k$ √© o n√∫mero total de a√ß√µes. Como $\epsilon > 0$ e o n√∫mero de passos tende ao infinito, todas as a√ß√µes ser√£o selecionadas infinitas vezes, o que garante uma explora√ß√£o cont√≠nua. $\blacksquare$

> üí° **Exemplo Num√©rico:**  Com um bandido de 5 bra√ßos e $\epsilon = 0.2$, a cada passo h√° 20% de chance de escolher um bra√ßo aleatoriamente, o que garante que todos os bra√ßos sejam explorados eventualmente, mesmo que alguns deles tenham estimativas de valor bem baixas. A probabilidade de selecionar cada bra√ßo aleatoriamente √© de 0.2/5 = 0.04.

```mermaid
graph LR
    subgraph "Œµ-greedy"
    A1("A√ß√£o 1")
    A2("A√ß√£o 2")
    A3("A√ß√£o 3")
    A4("A√ß√£o 4")
    A5("A√ß√£o 5")
    end
    subgraph "Sele√ß√£o Aleat√≥ria"
    A1 -->|P=Œµ/k| S
    A2 -->|P=Œµ/k| S
    A3 -->|P=Œµ/k| S
    A4 -->|P=Œµ/k| S
    A5 -->|P=Œµ/k| S
     end
    S["Sele√ß√£o"]
     style A1 fill:#f9f,stroke:#333,stroke-width:2px
     style A2 fill:#f9f,stroke:#333,stroke-width:2px
     style A3 fill:#f9f,stroke:#333,stroke-width:2px
    style A4 fill:#f9f,stroke:#333,stroke-width:2px
    style A5 fill:#f9f,stroke:#333,stroke-width:2px
     style S fill:#ccf,stroke:#333,stroke-width:2px
```

Os m√©todos $\epsilon$-greedy garantem que, com o aumento do n√∫mero de passos, todas as a√ß√µes sejam amostradas um n√∫mero infinito de vezes, assegurando que $Q_t(a)$ convirja para $q_*(a)$ [^4]. Apesar disso, essas garantias s√£o assint√≥ticas e pouco revelam sobre a efetividade pr√°tica desses m√©todos. A vantagem dos m√©todos $\epsilon$-greedy sobre os m√©todos puramente gananciosos depende da tarefa. Por exemplo, em tarefas com alta vari√¢ncia de recompensa, a explora√ß√£o √© mais necess√°ria para encontrar a a√ß√£o √≥tima, o que favorece os m√©todos $\epsilon$-greedy [^6].

**Teorema 1:** *Para um problema de bandido de k bra√ßos com recompensas estacion√°rias e um n√∫mero infinito de passos, um m√©todo $\epsilon$-greedy com $\epsilon > 0$ converge para a escolha da a√ß√£o √≥tima, no sentido de que a probabilidade de selecionar a a√ß√£o √≥tima se aproxima de 1 - $\epsilon$ com o tempo.*

**Prova:**  Pelo Lemma 1, sabemos que $Q_t(a)$ converge para $q_*(a)$ para todas as a√ß√µes. Pela Proposi√ß√£o 1, todas as a√ß√µes ser√£o exploradas infinitamente. Com o tempo, o agente ter√° uma estimativa precisa para todas as a√ß√µes. Em cada passo, o m√©todo $\epsilon$-greedy explora com probabilidade $\epsilon$, e explota com probabilidade $1-\epsilon$.  Conforme a estimativa da a√ß√£o √≥tima se torna mais precisa, a probabilidade de escolher uma a√ß√£o sub√≥tima com explota√ß√£o tende a zero. Assim, no limite, a probabilidade de selecionar a a√ß√£o √≥tima converge para $1 - \epsilon$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\epsilon = 0.1$ e o agente aprendeu corretamente os valores das a√ß√µes, ele escolher√° a a√ß√£o √≥tima em 90% dos casos e, aleatoriamente, uma a√ß√£o sub√≥tima nos 10% restantes.

```mermaid
graph LR
    subgraph "Œµ-greedy Converg√™ncia"
        A("A√ß√µes") --> |Explora√ß√£o (Œµ)| S["A√ß√£o Sub√≥tima\n(prob=Œµ)"]
        A --> |Explota√ß√£o (1-Œµ)| O["A√ß√£o √ìtima\n(prob ‚âà 1-Œµ)"]
    end
     style A fill:#f9f,stroke:#333,stroke-width:2px
     style S fill:#ccf,stroke:#333,stroke-width:2px
     style O fill:#cfc,stroke:#333,stroke-width:2px
```

**Teorema 1.1:** *O limite superior da diferen√ßa entre a recompensa esperada obtida por um agente $\epsilon$-greedy e um agente que sempre joga a a√ß√£o √≥tima √© proporcional a $\epsilon$.*

**Prova:** Seja $q^*$ o valor da a√ß√£o √≥tima. Em cada passo, um agente $\epsilon$-greedy explora com probabilidade $\epsilon$ (e obt√©m, em m√©dia, uma recompensa menor que $q^*$) e explora com probabilidade $1-\epsilon$ (e obt√©m a recompensa $q^*$ se a a√ß√£o estimada for a a√ß√£o √≥tima). No limite, a diferen√ßa na recompensa esperada por passo ser√° no m√°ximo $\epsilon$ vezes a maior diferen√ßa entre a recompensa da a√ß√£o √≥tima e a pior a√ß√£o, que √© limitada (j√° que as recompensas v√™m de distribui√ß√µes estacion√°rias). $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde a recompensa m√°xima √© 10 e a recompensa m√≠nima √© 0. Se um agente $\epsilon$-greedy com $\epsilon=0.2$ est√° atuando, o limite superior da diferen√ßa de recompensa por passo √© de 0.2 * (10-0) = 2. Isso significa que, em m√©dia, o agente $\epsilon$-greedy perder√° no m√°ximo 2 unidades de recompensa por passo em rela√ß√£o a um agente que sempre escolhe a a√ß√£o √≥tima.

```mermaid
graph LR
    A["Agente Œµ-greedy"] --> |Explora√ß√£o (Œµ)| B["Recompensa Sub√≥tima"]
    A --> |Explota√ß√£o (1-Œµ)| C["Recompensa √ìtima (q*)"]
    C -->|Recompensa √ìtima| D["Agente √ìtimo"]
    B-->|Recompensa Sub√≥tima| D
    D -->|M√°xima diferen√ßa = Œµ*(max - min)| E["Limite Superior da Perda"]
  style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
    style D fill:#cfc,stroke:#333,stroke-width:2px
     style E fill:#ccf,stroke:#333,stroke-width:2px
   linkStyle 0,1,2,3,4 stroke:#333,stroke-width:2px
```

### Conclus√£o
Em resumo, o problema do bandido de *k* bra√ßos introduz o conceito crucial da necessidade de equilibrar explora√ß√£o e explota√ß√£o. A explota√ß√£o, focada na maximiza√ß√£o da recompensa imediata, pode levar o agente a se prender a a√ß√µes sub√≥timas, enquanto a explora√ß√£o, visando melhorar o conhecimento dos valores das a√ß√µes, pode levar a uma melhor recompensa a longo prazo. M√©todos como os m√©todos gananciosos, que se concentram apenas na explota√ß√£o, podem ser menos eficazes no longo prazo do que os m√©todos $\epsilon$-greedy, que permitem explorar diferentes a√ß√µes. O m√©todo sample-average fornece uma forma de estimar os valores das a√ß√µes a partir de recompensas observadas, e, apesar de simples, essa abordagem √© essencial para construir algoritmos mais sofisticados que lidam com a complexidade do aprendizado por refor√ßo.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken. In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback differs from, and yet can be combined with, instructive feedback. The particular nonassociative, evaluative feedback problem that we explore is a simple version of the k-armed bandit problem. We use this problem to introduce a number of basic learning methods which we extend in later chapters to apply to the full rein-forcement learning problem. At the end of this chapter, we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when the best action depends on the situation." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps. This is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or ‚Äúone-armed bandit,‚Äù except that it has k levers instead of one. Each action selection is like a play of one of the slot machine's levers, and the rewards are the payoffs for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers. Another analogy is that of a doctor choosing between experimental treatments for a series of seriously ill patients. Each action is the selection of a treatment, and each reward is the survival or well-being of the patient. Today the term ‚Äúbandit problem" is sometimes used for a generalization of the problem described above, but in this book we use it to refer just to this simple case. In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q*(a), is the expected reward given that a is selected: q*(a) = E[Rt | At=a]. If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time stept as Qt(a). We would like Qt(a) to be close to q*(a). If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action's value. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. For example, suppose a greedy action's value is known with certainty, while several other actions are estimated to be nearly as good but with substantial uncertainty. The uncertainty is such that at least one of these other actions probably is actually better than the greedy action, but you don't know which one. If you have many time steps ahead on which to make action selections, then it may be better to explore the nongreedy actions and discover which of them are better than the greedy action. Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the ‚Äúconflict‚Äù between exploration and exploitation." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: Qt(a) = sum of rewards when a taken prior to t / number of times a taken prior to t = sum t-1 (i=1) Ri 1(Ai=a) / sum t-1 (i=1) 1(Ai=a) (2.1) where 1(predicate) denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define Qt(a) as some default value, such as 0. As the denominator goes to infinity, by the law of large numbers, Qt(a) converges to q*(a). We call this the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards. Of course this is just one way to estimate action values, and not necessarily the best one. Nevertheless, for now let us stay with this simple estimation method and turn to the question of how the estimates might be used to select actions. The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as At = argmax(a) Qt(a), (2.2) where argmax(a) denotes the action a for which the expression that follows is maximized (with ties broken arbitrarily). Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability …õ, instead select randomly" *(Trecho de Chapter 2: Multi-armed Bandits)*
[^4]: "from among all the actions with equal probability, independently of the action-value estimates. We call methods using this near-greedy action selection rule …õ-greedy methods. An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the Qt(a) converge to their respective q*(a). This of course implies that the probability of selecting the optimal action converges to greater than 1 ‚Äì …õ, that is, to near certainty. These are just asymptotic guarantees, however, and say little about the practical effectiveness of the methods." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^6]: "The advantage of …õ-greedy over greedy methods depends on the task. For example, suppose the reward variance had been larger, say 10 instead of 1. With noisier rewards it takes more exploration to find the optimal action, and …õ-greedy methods should fare even better relative to the greedy method." *(Trecho de Chapter 2: Multi-armed Bandits)*
```