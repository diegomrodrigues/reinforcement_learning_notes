## Multi-armed Bandits: Analogias do Problema

### Introdu√ß√£o

O aprendizado por refor√ßo (reinforcement learning) difere de outros tipos de aprendizado por utilizar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas em vez de instruir quais a√ß√µes corretas devem ser executadas [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Esse aspecto cria a necessidade de explora√ß√£o ativa para encontrar um comportamento otimizado. O feedback avaliativo puro indica a qualidade da a√ß√£o realizada, mas n√£o se foi a melhor ou pior a√ß√£o poss√≠vel. O feedback instrutivo, por outro lado, indica qual a√ß√£o correta tomar, independente da a√ß√£o realizada. Este tipo de feedback √© a base do aprendizado supervisionado, que inclui classifica√ß√£o de padr√µes, redes neurais artificiais e identifica√ß√£o de sistemas [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

O estudo do aprendizado por refor√ßo √© frequentemente simplificado para focar em um cen√°rio onde n√£o h√° necessidade de aprender a agir em m√∫ltiplas situa√ß√µes, chamado de *setting nonassociative* [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Um problema espec√≠fico que explora o feedback avaliativo n√£o associativo √© o *k-armed bandit problem*, um problema que √© usado para introduzir v√°rios m√©todos b√°sicos de aprendizado [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

**Proposi√ß√£o 1** O *k-armed bandit problem* pode ser visto como um caso especial de um problema de decis√£o sequencial, onde o estado do agente n√£o muda com o tempo. Isso simplifica a an√°lise, pois n√£o h√° necessidade de modelar transi√ß√µes de estado, focando diretamente no balanceamento entre explora√ß√£o e explota√ß√£o.

### Conceitos Fundamentais

O *k-armed bandit problem* pode ser formulado como um cen√°rio onde um agente √© repetidamente confrontado com a escolha entre $k$ op√ß√µes diferentes, ou a√ß√µes [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Ap√≥s cada escolha, o agente recebe uma recompensa num√©rica retirada de uma distribui√ß√£o de probabilidade estacion√°ria, que depende da a√ß√£o selecionada. O objetivo do agente √© maximizar a recompensa total esperada ao longo de um per√≠odo de tempo, como por exemplo, 1000 sele√ß√µes de a√ß√µes [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

Essa formula√ß√£o do problema √© chamada de *k-armed bandit problem* devido √† analogia com uma m√°quina ca√ßa-n√≠queis, exceto que, em vez de uma alavanca, possui $k$ alavancas [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Cada sele√ß√£o de a√ß√£o √© como acionar uma das alavancas da m√°quina ca√ßa-n√≠quel, e as recompensas s√£o os pagamentos recebidos ao acertar o jackpot. O objetivo √© maximizar os ganhos concentrando as a√ß√µes nas melhores alavancas [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Outra analogia √© a de um m√©dico escolhendo entre tratamentos experimentais para uma s√©rie de pacientes gravemente enfermos. Cada a√ß√£o representa a sele√ß√£o de um tratamento, e cada recompensa √© a sobreviv√™ncia ou bem-estar do paciente. Embora o termo "bandit problem" seja usado para generaliza√ß√µes do problema descrito, neste contexto, o foco √© nessa vers√£o simples [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

Em nosso *k-armed bandit problem*, cada a√ß√£o $a$ possui uma recompensa esperada ou m√©dia, denotada por $q_*(a)$ [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Seja $A_t$ a a√ß√£o selecionada no instante $t$, e $R_t$ a recompensa correspondente. A fun√ß√£o $q_*(a)$ √© definida como o valor esperado da recompensa dado que a a√ß√£o $a$ √© selecionada, ou seja:

$$ q_*(a) = \mathbb{E}[R_t | A_t=a]$$

Se os valores de cada a√ß√£o fossem conhecidos, a solu√ß√£o do problema seria trivial: sempre selecionar a a√ß√£o de maior valor. No entanto, √© assumido que os valores das a√ß√µes s√£o desconhecidos, embora se possa ter estimativas [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). A estimativa do valor da a√ß√£o $a$ no instante $t$ √© denotada por $Q_t(a)$. O objetivo √© que $Q_t(a)$ seja o mais pr√≥ximo poss√≠vel de $q_*(a)$ [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

**Lema 1** Uma forma comum de estimar $Q_t(a)$ √© atrav√©s da m√©dia amostral das recompensas recebidas ao selecionar a a√ß√£o $a$ at√© o instante $t$. Se $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© o instante $t$, ent√£o:

$$ Q_t(a) = \frac{\sum_{i=1}^{t} R_i \mathbb{I}(A_i = a)}{N_t(a)} $$
onde $\mathbb{I}(A_i = a)$ √© a fun√ß√£o indicadora que vale 1 se $A_i = a$ e 0 caso contr√°rio. Essa estimativa √© n√£o-viesada e converge para $q_*(a)$ quando $N_t(a) \rightarrow \infty$.
*Proof strategy:* The proof follows directly from the definition of sample average and the law of large numbers.

> üí° **Exemplo Num√©rico:** Vamos considerar um problema com $k=3$ a√ß√µes. Suponha que em um instante $t=5$, as a√ß√µes tenham sido selecionadas e as recompensas obtidas conforme a tabela:
>
> | Instante ($i$) | A√ß√£o ($A_i$) | Recompensa ($R_i$) |
> |---|---|---|
> | 1 | 1 | 0 |
> | 2 | 2 | 1 |
> | 3 | 1 | 2 |
> | 4 | 3 | 0 |
> | 5 | 2 | 0 |
>
> Para calcular $Q_5(1)$, $Q_5(2)$ e $Q_5(3)$, aplicamos a f√≥rmula. Para $a=1$:
>
> $N_5(1) = 2$ (a a√ß√£o 1 foi selecionada 2 vezes)
>
> $Q_5(1) = \frac{0 + 2}{2} = 1$
>
> Para $a=2$:
>
> $N_5(2) = 2$ (a a√ß√£o 2 foi selecionada 2 vezes)
>
> $Q_5(2) = \frac{1 + 0}{2} = 0.5$
>
> Para $a=3$:
>
> $N_5(3) = 1$ (a a√ß√£o 3 foi selecionada 1 vez)
>
> $Q_5(3) = \frac{0}{1} = 0$
>
> Portanto, as estimativas dos valores das a√ß√µes no instante $t=5$ s√£o $Q_5(1) = 1$, $Q_5(2) = 0.5$ e $Q_5(3) = 0$. Note que essas s√£o apenas estimativas baseadas em poucas amostras e podem n√£o refletir os valores verdadeiros $q_*(a)$.

```mermaid
graph LR
subgraph "Estimativa de Q_t(a)"
    A("A√ß√£o a") --> B("Coleta de Recompensas R_i");
    B --> C("Calcula N_t(a)");
    C --> D("Calcula somat√≥rio de R_i onde A_i = a");
    D --> E("Q_t(a) = Somat√≥rio / N_t(a)");
    E --> F("Estimativa Q_t(a)");
 end
style E fill:#ccf,stroke:#333,stroke-width:2px
```

O problema da explora√ß√£o versus explota√ß√£o surge naturalmente. A√ß√µes com as maiores estimativas de valor s√£o chamadas de *greedy actions* [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Ao selecionar uma *greedy action*, estamos explorando o conhecimento atual do valor das a√ß√µes. Em contrapartida, selecionar uma a√ß√£o n√£o-greedy implica em *explora√ß√£o*, pois permite melhorar as estimativas dos valores das a√ß√µes menos exploradas [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). A explora√ß√£o visa maximizar a recompensa esperada a curto prazo, enquanto a explora√ß√£o pode gerar recompensas maiores a longo prazo [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). O balanceamento entre explora√ß√£o e explota√ß√£o √© essencial no *k-armed bandit problem* [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

**Lema 1.1**  Uma atualiza√ß√£o incremental da estimativa $Q_t(a)$ pode ser expressa da seguinte forma:

$$Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)+1}(R_{t+1} - Q_t(a)) $$
Essa formula√ß√£o √© computacionalmente mais eficiente que a abordagem de m√©dia amostral para grandes quantidades de dados, dado que dispensa a necessidade de manter o hist√≥rico de todas as recompensas.
*Proof strategy*: This is derived directly from the sample average update by expressing the running average at step $t+1$ in terms of the average at step $t$ and the new observation.

```mermaid
graph LR
subgraph "Atualiza√ß√£o Incremental de Q_t(a)"
    A("Q_t(a)") --> B("R_{t+1}");
    B --> C("Calcula 1/(N_t(a) + 1)");
    C --> D("R_{t+1} - Q_t(a)");
    D --> E("Multiplica resultado por 1/(N_t(a) + 1)");
    E --> F("Soma resultado a Q_t(a)");
    F --> G("Q_{t+1}(a)");
end
style G fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, suponha que no instante $t=6$ a a√ß√£o $A_6=1$ foi selecionada e a recompensa $R_6 = 3$ foi obtida. Vamos usar a atualiza√ß√£o incremental para calcular $Q_6(1)$. Sab√≠amos que $Q_5(1)=1$ e $N_5(1)=2$. Ent√£o:
>
> $Q_6(1) = Q_5(1) + \frac{1}{N_5(1)+1}(R_6 - Q_5(1))$
>
> $Q_6(1) = 1 + \frac{1}{2+1}(3 - 1)$
>
> $Q_6(1) = 1 + \frac{1}{3}(2)$
>
> $Q_6(1) = 1 + \frac{2}{3} = \frac{5}{3} \approx 1.67$
>
> Observe como a estimativa de $Q(1)$ foi atualizada sem a necessidade de recalcular toda a m√©dia amostral. Isso √© especialmente √∫til quando lidamos com grandes quantidades de dados, pois economiza mem√≥ria e poder computacional. A nova estimativa, $Q_6(1) \approx 1.67$, reflete a recompensa mais recente obtida com a a√ß√£o 1.

### Conclus√£o
O *k-armed bandit problem*, com suas analogias √†s m√°quinas ca√ßa-n√≠queis e tratamentos m√©dicos experimentais, serve como um modelo fundamental para entender o trade-off entre explora√ß√£o e explota√ß√£o no aprendizado por refor√ßo. A formula√ß√£o do problema destaca a necessidade de um equil√≠brio entre a busca por a√ß√µes conhecidas com altas recompensas e a explora√ß√£o de a√ß√µes menos conhecidas que podem potencialmente levar a recompensas ainda maiores. O desenvolvimento de m√©todos eficazes para lidar com esse balanceamento √© essencial para o sucesso de algoritmos de aprendizado por refor√ßo mais complexos.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of
learning is that it uses training information that evaluates the actions taken rather
than instructs by giving correct actions." *(Trecho de Multi-armed Bandits)*
[^2]: "This is the original form of the k-armed bandit problem, so named by analogy to a slot
machine, or ‚Äúone-armed bandit,‚Äù except that it has k levers instead of one. Each action
selection is like a play of one of the slot machine‚Äôs levers, and the rewards are the payoffs
for hitting the jackpot. Through repeated action selections you are to maximize your
winnings by concentrating your actions on the best levers." *(Trecho de Multi-armed Bandits)*
