## Estimativas de Valor de A√ß√£o no Problema k-Armed Bandit

### Introdu√ß√£o

O problema **k-armed bandit** √© um cen√°rio de aprendizado onde um agente deve repetidamente escolher entre *k* a√ß√µes diferentes, recebendo uma recompensa num√©rica ap√≥s cada escolha. O objetivo √© maximizar a recompensa total esperada ao longo do tempo [^1]. Este problema, inspirado em m√°quinas ca√ßa-n√≠queis com *k* alavancas, representa um desafio fundamental no aprendizado por refor√ßo, onde a explora√ß√£o (experimentar novas a√ß√µes) e a explota√ß√£o (usar o conhecimento atual para escolher a√ß√µes que maximizam a recompensa) precisam ser equilibradas [^1]. Este cap√≠tulo foca no aspecto avaliativo do aprendizado por refor√ßo, estudando como o feedback avaliativo difere do feedback instrutivo, bem como as peculiaridades da abordagem n√£o-associativa de problemas de aprendizado por refor√ßo, evitando a complexidade do problema completo de refor√ßo [^1]. No contexto do k-armed bandit, cada a√ß√£o *a* tem um valor esperado ou m√©dio de recompensa, denotado por $q_*(a)$, que √© a recompensa esperada ao se selecionar a a√ß√£o *a* [^2]. Como este valor √© geralmente desconhecido, o agente precisa estim√°-lo usando os dados dispon√≠veis.

### Conceitos Fundamentais
O cerne da solu√ß√£o do problema *k*-armed bandit reside na capacidade de estimar os valores de cada a√ß√£o e, assim, tomar decis√µes informadas. O valor verdadeiro de uma a√ß√£o, denotado por $q_*(a)$, √© a recompensa m√©dia esperada ao selecionar a a√ß√£o *a*. Matematicamente, este valor √© definido como [^2]:

$$q_*(a) = \mathbb{E}[R_t | A_t = a]$$

onde $A_t$ √© a a√ß√£o selecionada no instante *t*, e $R_t$ √© a recompensa correspondente. No entanto, esses valores s√£o desconhecidos *a priori* e precisam ser estimados a partir das intera√ß√µes do agente com o ambiente. Em cada passo, o agente seleciona uma a√ß√£o $A_t$ e recebe uma recompensa $R_t$. O objetivo principal √© estimar o valor de cada a√ß√£o, ou seja, aproximar $q_*(a)$ usando as recompensas observadas. Assim, surge o conceito de **estimativas de valor de a√ß√£o**, denotado por $Q_t(a)$, que representa a estimativa do valor da a√ß√£o *a* no instante *t* [^2]. A ideia √© que, com o tempo e mais experi√™ncias, $Q_t(a)$ se aproxime de $q_*(a)$.

Para estimar os valores de a√ß√£o, um m√©todo natural √© usar a **m√©dia amostral**, calculando a m√©dia das recompensas obtidas para cada a√ß√£o [^3]. A f√≥rmula para calcular $Q_t(a)$ usando a m√©dia amostral √© dada por:

$$Q_t(a) = \frac{\text{Soma das recompensas quando a foi tomada antes de t}}{\text{N√∫mero de vezes que a foi tomada antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}} $$

onde $\mathbb{1}_{\text{predicate}}$ √© uma fun√ß√£o indicadora que retorna 1 se `predicate` for verdadeiro e 0 caso contr√°rio. Se o denominador for zero, $Q_t(a)$ √© definido como um valor padr√£o, como 0. √Ä medida que o n√∫mero de vezes que a a√ß√£o *a* √© tomada tende ao infinito, pela lei dos grandes n√∫meros, $Q_t(a)$ converge para $q_*(a)$ [^3]. Este m√©todo √© conhecido como o m√©todo de **m√©dia amostral** para estimar os valores de a√ß√£o.
```mermaid
flowchart TD
    A[In√≠cio] --> B("Inicializar: N(a) = 0, Soma(a) = 0 para cada a√ß√£o a");
    B --> C("Para cada passo t=1, 2, ...");
    C --> D("Selecionar a√ß√£o A_t");
    D --> E("Receber recompensa R_t");
    E --> F("Atualizar: Soma(A_t) = Soma(A_t) + R_t");
    F --> G("Atualizar: N(A_t) = N(A_t) + 1");
    G --> H("Calcular: Q_t(A_t) = Soma(A_t) / N(A_t)");
    H --> I("Continuar ou parar?");
    I -- "Continuar" --> C;
    I -- "Parar" --> J[Fim];
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style J fill:#f9f,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Imagine um cen√°rio com 3 a√ß√µes (k=3). As recompensas obtidas para cada a√ß√£o ao longo de 5 passos s√£o:
>
> *   A√ß√£o 1: Recompensas = \[2, 3, 2, 4, 3]
> *   A√ß√£o 2: Recompensas = \[1, 0, 2, 1, 1]
> *   A√ß√£o 3: Recompensas = \[0, 5, 0, 0, 1]
>
> Vamos calcular $Q_t(a)$ para cada a√ß√£o ap√≥s 3 passos (t=3):
>
> *   $Q_3(1) = \frac{2 + 3 + 2}{3} = \frac{7}{3} \approx 2.33$
> *   $Q_3(2) = \frac{1 + 0 + 2}{3} = \frac{3}{3} = 1$
> *   $Q_3(3) = \frac{0 + 5 + 0}{3} = \frac{5}{3} \approx 1.67$
>
> Ap√≥s 5 passos (t=5):
>
> *   $Q_5(1) = \frac{2 + 3 + 2 + 4 + 3}{5} = \frac{14}{5} = 2.8$
> *   $Q_5(2) = \frac{1 + 0 + 2 + 1 + 1}{5} = \frac{5}{5} = 1$
> *   $Q_5(3) = \frac{0 + 5 + 0 + 0 + 1}{5} = \frac{6}{5} = 1.2$
>
> Este exemplo ilustra como a m√©dia amostral calcula a estimativa do valor de cada a√ß√£o ao longo do tempo. Note que, com mais intera√ß√µes, os valores de $Q_t(a)$ tendem a se estabilizar.

**Lema 1** O m√©todo da m√©dia amostral garante que, se cada a√ß√£o √© selecionada infinitas vezes, ent√£o a estimativa do valor da a√ß√£o $Q_t(a)$ converge para o valor verdadeiro $q_*(a)$ quase certamente.

*Proof:* This follows directly from the strong law of large numbers. Since each $R_i$ is a random variable, with a well defined expected value for action $a$, the sample average will converge almost surely to the expected value.

A principal quest√£o a ser respondida agora √©: Como usar as estimativas de valor de a√ß√£o para selecionar as a√ß√µes?  A abordagem mais simples √© selecionar a a√ß√£o com o maior valor estimado, ou seja, escolher a a√ß√£o *greedy*. Formalmente, isso pode ser escrito como:

$$A_t = \underset{a}{\text{argmax}} \, Q_t(a)$$

Essa abordagem, entretanto, n√£o explora novas a√ß√µes, pois sempre escolhe a que parece ser a melhor at√© o momento. Essa abordagem pode levar o algoritmo a ficar preso em escolhas sub√≥timas e impede uma busca completa por melhores alternativas, pois n√£o explora a√ß√µes que, porventura, tenham uma estimativa de valor inferior, mas que podem ser melhores a longo prazo [^2].

Uma alternativa simples √† explora√ß√£o pura √© o m√©todo **$\epsilon$-greedy**, onde a maioria das vezes o agente seleciona a a√ß√£o *greedy*, mas com uma probabilidade $\epsilon$, o agente seleciona uma a√ß√£o aleat√≥ria. Formalmente:

$$ A_t =
\begin{cases}
    \underset{a}{\text{argmax}} \, Q_t(a) & \text{com probabilidade } 1-\epsilon \\
    \text{a√ß√£o aleat√≥ria } & \text{com probabilidade } \epsilon
\end{cases}
$$
```mermaid
flowchart TD
    A[In√≠cio] --> B("Gerar n√∫mero aleat√≥rio 'rand' entre 0 e 1");
    B --> C{{"rand < epsilon?"}};
    C -- "Sim" --> D("Selecionar a√ß√£o aleat√≥ria A_t");
    C -- "N√£o" --> E("Selecionar a√ß√£o A_t = argmax Q_t(a)");
    D --> F[Fim];
    E --> F;
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#f9f,stroke:#333,stroke-width:2px
```
Este m√©todo garante que todas as a√ß√µes sejam exploradas infinitas vezes, garantindo a converg√™ncia de $Q_t(a)$ para $q_*(a)$ no limite [^4]. O valor de $\epsilon$ controla o equil√≠brio entre explora√ß√£o e explota√ß√£o [^4]. Valores maiores de $\epsilon$ levam a maior explora√ß√£o, enquanto valores menores focam mais na explota√ß√£o [^4].

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que temos 3 a√ß√µes e que as estimativas de valor ap√≥s algumas itera√ß√µes s√£o:
>
> *   $Q_t(1) = 2.5$
> *   $Q_t(2) = 1.8$
> *   $Q_t(3) = 3.1$
>
> A a√ß√£o *greedy* seria a a√ß√£o 3, pois tem o maior valor estimado. Agora, vamos aplicar o m√©todo $\epsilon$-greedy com $\epsilon = 0.2$. Isso significa que em 80% das vezes (probabilidade 1-0.2), a a√ß√£o selecionada ser√° a a√ß√£o *greedy*, que √© a a√ß√£o 3.  Nos 20% restantes das vezes (probabilidade 0.2), uma a√ß√£o ser√° selecionada aleatoriamente entre as 3 a√ß√µes.
>
> Se gerarmos um n√∫mero aleat√≥rio entre 0 e 1:
>
> *   Se o n√∫mero for menor que 0.2, escolhemos uma a√ß√£o aleat√≥ria.
>     * Por exemplo, se o n√∫mero aleat√≥rio for 0.1, podemos escolher a a√ß√£o 1.
> *   Se o n√∫mero for maior ou igual a 0.2, escolhemos a a√ß√£o *greedy*, que √© a a√ß√£o 3.
>     * Por exemplo, se o n√∫mero aleat√≥rio for 0.7, escolhemos a a√ß√£o 3.
>
> Este processo mostra como o m√©todo $\epsilon$-greedy balanceia a explora√ß√£o (a√ß√µes aleat√≥rias) e a explota√ß√£o (a√ß√£o *greedy*).

**Proposi√ß√£o 2** No m√©todo $\epsilon$-greedy, para $\epsilon > 0$, cada a√ß√£o √© selecionada infinitas vezes em um n√∫mero infinito de passos, garantindo que $Q_t(a)$ converge para $q_*(a)$ para toda a√ß√£o *a*.

*Proof:* Since there is a probability $\epsilon$ of selecting an action at random, in an infinite number of trials, each action will be selected an infinite number of times. Thus, with the strong law of large numbers, for every action $a$, $Q_t(a)$ will converge to $q_*(a)$.

Para refinar a abordagem de m√©dia amostral, √© poss√≠vel utilizar **implementa√ß√£o incremental**, que permite calcular a m√©dia sem armazenar todas as recompensas anteriores, o que √© computacionalmente mais eficiente. A f√≥rmula de atualiza√ß√£o incremental √© dada por [^7]:

$$Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n]$$

Esta atualiza√ß√£o calcula a nova m√©dia $Q_{n+1}$ com base na m√©dia anterior $Q_n$, no n√∫mero de amostras $n$ e na nova recompensa $R_n$.  Essa atualiza√ß√£o √© crucial para a constru√ß√£o de algoritmos eficientes que operam em tempo constante e mem√≥ria constante por passo. A forma geral dessa atualiza√ß√£o √©:

$$ \text{NovaEstimativa} \leftarrow \text{AntigaEstimativa} + \text{Passo} [\text{Alvo} - \text{AntigaEstimativa}]$$

onde o termo [Alvo - AntigaEstimativa] representa o erro na estimativa, que √© reduzido ao se dar um passo na dire√ß√£o do Alvo [^7].
```mermaid
flowchart TD
    A["In√≠cio: Q_n, n, R_n"] --> B("Calcular erro: erro = R_n - Q_n");
    B --> C("Calcular passo: passo = 1/n");
    C --> D("Atualizar Q_{n+1}: Q_{n+1} = Q_n + passo * erro");
     D --> E["Fim: Q_{n+1}"];
     style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#f9f,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**
>
> Vamos assumir que $Q_5(a) = 2$ e a pr√≥xima recompensa obtida ap√≥s a 6¬™ vez que a a√ß√£o *a* foi selecionada √© $R_6 = 4$. Usando a atualiza√ß√£o incremental, podemos calcular $Q_6(a)$:
>
> $Q_{6}(a) = Q_5(a) + \frac{1}{6} [R_6 - Q_5(a)]$
>
> $Q_{6}(a) = 2 + \frac{1}{6} [4 - 2]$
>
> $Q_{6}(a) = 2 + \frac{1}{6} [2]$
>
> $Q_{6}(a) = 2 + \frac{1}{3}$
>
> $Q_{6}(a) \approx 2.33$
>
> Este exemplo demonstra como a estimativa do valor da a√ß√£o √© atualizada incrementalmente, sem a necessidade de armazenar todas as recompensas anteriores.

**Teorema 3** (Converg√™ncia da Atualiza√ß√£o Incremental) Se a recompensa $R_n$ √© limitada e o tamanho do passo √© dado por $\frac{1}{n}$, ent√£o o processo de atualiza√ß√£o incremental converge para o valor m√©dio da a√ß√£o.

*Proof:*  The update rule is a stochastic approximation algorithm. With a step size of $\frac{1}{n}$, this is a Robbins-Monro algorithm. Since the rewards are bounded, it can be proven that $Q_n$ converges to the expected value of the rewards, which is the true value of the action $q_*(a)$.

**Corol√°rio 3.1** (Taxas de Aprendizagem) A implementa√ß√£o incremental com uma taxa de aprendizagem $\alpha$ constante (em vez de $\frac{1}{n}$) resultar√° em uma converg√™ncia mais r√°pida, por√©m para um valor que n√£o √© necessariamente o valor verdadeiro da a√ß√£o.

*Proof:* If $\alpha$ is kept constant, the impact of new rewards is weighted consistently with the initial rewards, thus giving more "emphasis" to the last rewards, making the estimate faster to reach a fixed point but biasing the final estimate with respect to the mean.

> üí° **Exemplo Num√©rico:**
>
> Considere novamente o cen√°rio anterior onde $Q_5(a) = 2$ e $R_6 = 4$. Em vez de usar o tamanho de passo $\frac{1}{n}$, vamos usar uma taxa de aprendizagem constante $\alpha = 0.1$.
>
> $Q_{6}(a) = Q_5(a) + \alpha [R_6 - Q_5(a)]$
>
> $Q_{6}(a) = 2 + 0.1 [4 - 2]$
>
> $Q_{6}(a) = 2 + 0.1 [2]$
>
> $Q_{6}(a) = 2 + 0.2$
>
> $Q_{6}(a) = 2.2$
>
> Observe que $Q_6(a) = 2.2$ com $\alpha = 0.1$ e  $Q_6(a) \approx 2.33$ com $\frac{1}{n}$. A converg√™ncia para um valor pr√≥ximo, mas diferente do valor m√©dio da a√ß√£o, ilustra o corol√°rio 3.1. O uso de um $\alpha$ constante proporciona uma atualiza√ß√£o mais r√°pida, mas que pode n√£o convergir para o valor verdadeiro, enquanto o $\frac{1}{n}$ garante converg√™ncia, mas de forma mais lenta.

**Lema 3.2** Se o passo √© dado por $\alpha_n$, onde $\sum_{n=1}^{\infty} \alpha_n = \infty$ e $\sum_{n=1}^{\infty} \alpha_n^2 < \infty$, ent√£o $Q_n$ converge para $q_*(a)$ com probabilidade 1.

*Proof:* This result is a classical theorem from stochastic approximation theory and is satisfied, for example, by the learning rate $\alpha_n = \frac{1}{n}$. The first condition guarantees that the steps are large enough to reach the optimal point, whereas the second condition guarantees that the steps diminish enough to converge to the optimal point.

### Conclus√£o
Este cap√≠tulo apresenta a base para a solu√ß√£o do problema *k*-armed bandit, destacando a import√¢ncia de estimar os valores das a√ß√µes, um processo fundamental para um aprendizado por refor√ßo. As estimativas de valor de a√ß√£o, $Q_t(a)$, s√£o cruciais para aproximar os valores reais, $q_*(a)$. Exploramos m√©todos como a m√©dia amostral, juntamente com abordagens como a sele√ß√£o *greedy* e $\epsilon$-greedy para balancear a explora√ß√£o e a explota√ß√£o, e a implementa√ß√£o incremental para maior efici√™ncia. Compreender essas estimativas e suas aplica√ß√µes √© essencial para o desenvolvimento de solu√ß√µes mais complexas em ambientes de aprendizado por refor√ßo.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected: q*(a) = E[Rt | At=a]." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^3]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received:
Qt(a) = sum of rewards when a taken prior to t / number of times a taken prior to t = ... where 1 predicate denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define Qt(a) as some default value, such as 0. As the denominator goes to infinity, by the law of large numbers, Qt(a) converges to q*(a)." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^4]: "We call methods using this near-greedy action selection rule …õ-greedy methods. An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the Qt(a) converge to their respective q*(a)." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^5]: "The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as At = argmaxa Qt(a), where argmaxa denotes the action a for which the expression that follows is maximized (with ties broken arbitrarily)." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^6]: "A simple alternative is to behave greedily most of the time, but every once in a while, say with small probability …õ, instead select randomly from among all the actions with equal probability, independently of the action-value estimates." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^7]: "To simplify notation we concentrate on a single action. Let R‚ÇÅ now denote the reward received after the ith selection of this action, and let Qn denote the estimate of its action value after it has been selected n ‚àí 1 times, which we can now write simply as Qn = R1 + R2 +‚Ä¶+ Rn-1 / n-1. The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, if this is done, then the memory and computational requirements would grow over time as more rewards are seen. Each additional reward would require additional memory to store it and additional computation to compute the sum in the numerator. As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. Given Qn and the nth reward, Rn, the new average of all n rewards can be computed by...which holds even for n = 1, obtaining Q2 = R‚ÇÅ for arbitrary Q1. This implementation requires memory only for Qn and n, and only the small computation (2.3) for each new reward." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^8]: "This update rule (2.3) is of a form that occurs frequently throughout this book. The general form is NewEstimate ‚Üê OldEstimate + StepSize [Target ‚àí OldEstimate]." *(Trecho de Chapter 2: Multi-armed Bandits)*
