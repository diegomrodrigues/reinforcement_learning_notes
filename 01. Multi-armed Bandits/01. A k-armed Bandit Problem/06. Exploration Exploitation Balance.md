## A Necessidade de Balancear Explora√ß√£o e Explota√ß√£o no Problema k-armed Bandit

### Introdu√ß√£o
No campo do **reinforcement learning**, uma caracter√≠stica fundamental que o distingue de outros tipos de aprendizado √© o uso de informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas em vez de instruir por meio de a√ß√µes corretas [^1]. Essa distin√ß√£o cria a necessidade de **explora√ß√£o ativa**, uma busca expl√≠cita por um comportamento adequado. O feedback avaliativo puro indica a qualidade da a√ß√£o tomada, mas n√£o se ela era a melhor ou a pior poss√≠vel, enquanto o feedback instrutivo indica a a√ß√£o correta a ser tomada, independentemente da a√ß√£o realizada. O feedback instrutivo √© a base do aprendizado supervisionado [^1]. Este cap√≠tulo foca no aspecto avaliativo do reinforcement learning em um cen√°rio simplificado, o qual n√£o envolve aprender a agir em m√∫ltiplas situa√ß√µes. Esse cen√°rio **n√£o associativo** √© onde a maioria dos trabalhos anteriores envolvendo feedback avaliativo foram realizados e evita grande parte da complexidade do problema completo de reinforcement learning [^1]. Assim, torna-se poss√≠vel observar de forma mais clara as diferen√ßas e como o feedback avaliativo pode ser combinado com o feedback instrutivo. O problema particular que ser√° explorado √© uma vers√£o simples do **k-armed bandit problem**, que servir√° de base para introduzir v√°rios m√©todos de aprendizado b√°sicos que ser√£o estendidos em cap√≠tulos posteriores para aplicar ao problema completo de reinforcement learning [^1].

### Conceitos Fundamentais
O problema k-armed bandit apresenta um cen√°rio de aprendizado onde se deve escolher repetidamente entre k op√ß√µes diferentes, ou a√ß√µes. Ap√≥s cada escolha, recebe-se uma recompensa num√©rica proveniente de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o selecionada [^1]. O objetivo √© maximizar a recompensa total esperada ao longo de um per√≠odo, por exemplo, 1000 sele√ß√µes de a√ß√µes, ou *time steps* [^2]. Este problema √© an√°logo a uma *slot machine* com k alavancas, onde o objetivo √© maximizar os ganhos concentrando as a√ß√µes nas melhores alavancas [^2]. Uma analogia adicional √© a de um m√©dico escolhendo entre tratamentos experimentais para pacientes gravemente enfermos. Cada a√ß√£o √© a sele√ß√£o de um tratamento e cada recompensa √© a sobreviv√™ncia ou bem-estar do paciente [^2].

No **k-armed bandit problem**, cada a√ß√£o possui uma recompensa esperada, ou m√©dia, dado que a a√ß√£o foi selecionada. Esse valor √© denotado como $q_*(a)$ [^2]. A a√ß√£o selecionada no *time step* t √© denotada como $A_t$ e a recompensa correspondente como $R_t$. A a√ß√£o $a$, que √© arbitr√°ria, possui seu valor definido como $q_*(a) = E[R_t | A_t=a]$ [^2].

**Proposi√ß√£o 1**  A recompensa esperada $q_*(a)$ representa o valor verdadeiro da a√ß√£o $a$.  A expectativa  $E[R_t | A_t = a]$  √© calculada em rela√ß√£o √† distribui√ß√£o de probabilidade estacion√°ria da recompensa, condicionada √† a√ß√£o $a$ ter sido escolhida. Este valor √© constante ao longo do tempo no problema *k-armed bandit* cl√°ssico.

> üí° **Exemplo Num√©rico:** Imagine um problema de 3-armed bandit (k=3). A a√ß√£o 1 tem uma recompensa esperada $q_*(1) = 2$, a a√ß√£o 2 tem $q_*(2) = 4$, e a a√ß√£o 3 tem $q_*(3) = 1$.  Esses valores s√£o *constantes* ao longo do tempo. Se a a√ß√£o 2 for escolhida, em m√©dia, a recompensa ser√° 4, mas cada recompensa individual pode variar, por exemplo, 3 ou 5, devido √† distribui√ß√£o de probabilidade.

Se os valores de cada a√ß√£o fossem conhecidos, o problema se tornaria trivial, bastaria escolher sempre a a√ß√£o com o maior valor. No entanto, assume-se que os valores das a√ß√µes n√£o s√£o conhecidos com certeza, apesar de existirem estimativas. O valor estimado da a√ß√£o $a$ no *time step* t √© denotado por $Q_t(a)$. O objetivo √© que $Q_t(a)$ se aproxime de $q_*(a)$ [^2].

**Lema 1**  A diferen√ßa entre a estimativa $Q_t(a)$ e o valor verdadeiro $q_*(a)$, ou seja, $|Q_t(a) - q_*(a)|$, representa o erro da estimativa da a√ß√£o $a$ no tempo *t*. O objetivo de qualquer algoritmo de aprendizagem no *k-armed bandit problem* √© minimizar este erro ao longo do tempo para todas as a√ß√µes.
```mermaid
graph LR
    A["$q_*(a)$"] -->|Verdadeiro Valor| B("A√ß√£o a");
    C["$Q_t(a)$"] -->|Valor Estimado| B;
    B -->|Recompensa $R_t$| D["Ambiente"];
    D -->|Feedback| C;
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ddf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, no *time step* t=1, podemos ter estimativas iniciais $Q_1(1) = 1.5$, $Q_1(2) = 3$, e $Q_1(3) = 1.2$. Os erros s√£o $|1.5 - 2| = 0.5$, $|3 - 4| = 1$, e $|1.2 - 1| = 0.2$ respectivamente. O objetivo do algoritmo √© reduzir esses erros ao longo do tempo, de forma que $Q_t(a)$ se aproxime de $q_*(a)$.

Para manter estimativas dos valores das a√ß√µes, √© preciso que em cada *time step* exista pelo menos uma a√ß√£o com o maior valor estimado. Essas a√ß√µes s√£o chamadas de **greedy actions**. Ao selecionar uma dessas a√ß√µes, o agente est√° **exploitando** seu conhecimento atual dos valores das a√ß√µes [^2]. Por outro lado, ao selecionar uma das a√ß√µes n√£o-greedy, o agente est√° **explorando**, o que permite melhorar as estimativas do valor das a√ß√µes n√£o-greedy. A explora√ß√£o √© importante para maximizar a recompensa total no longo prazo, enquanto a explota√ß√£o visa maximizar a recompensa imediata [^2].

**Teorema 1** A estrat√©gia puramente *greedy*, ou seja, selecionar sempre a a√ß√£o com maior valor estimado, n√£o garante a converg√™ncia para a a√ß√£o √≥tima a longo prazo, especialmente quando as estimativas iniciais $Q_t(a)$ s√£o imprecisas.  
*Proof:* Uma vez que a estrat√©gia *greedy* explora apenas a a√ß√£o com maior valor estimado, a√ß√µes com valores verdadeiros $q_*(a)$ ligeiramente menores, mas potencialmente melhores, podem ser ignoradas, levando a um √≥timo local, n√£o global.

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, no *time step* t=1, a a√ß√£o greedy seria a a√ß√£o 3, pois tem o maior valor estimado (1.2). Se usarmos uma estrat√©gia *greedy* pura, sempre selecionar√≠amos a a√ß√£o 3. No entanto, a a√ß√£o 2 √© realmente a melhor ($q_*(2) = 4$).  Se a estimativa inicial de Q(2) for muito baixa e n√£o explorarmos outras a√ß√µes, nunca descobriremos que a a√ß√£o 2 √© a melhor.  Isso demonstra o risco de ficar preso a um √≥timo local.

**Lema 1.1**  O conceito de valor estimado $Q_t(a)$ pode ser extendido para incluir uma incerteza associada a esta estimativa. Em muitas abordagens, esta incerteza √© usada para guiar a explora√ß√£o, promovendo a sele√ß√£o de a√ß√µes com estimativas mais incertas.
```mermaid
graph LR
    A["$Q_t(a)$"] -->|Valor Estimado| B("A√ß√£o a");
    C["$U_t(a)$"] -->|Incerteza| B;
    B -->|Sele√ß√£o| D("Agente");
     style A fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ddf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Imagine que, em vez de apenas $Q_t(a)$, temos tamb√©m um valor de incerteza $U_t(a)$. Por exemplo, se $Q_1(1)=1.5$ e $U_1(1)=0.8$, e $Q_1(2)=3$ e $U_1(2)=0.2$.  A a√ß√£o 1 tem mais incerteza que a a√ß√£o 2, mesmo que seu valor estimado seja menor.  Um m√©todo de explora√ß√£o baseado em incerteza poderia levar a escolher a a√ß√£o 1 para melhor estimar seu verdadeiro valor $q_*(1)$.

**O Dilema Explora√ß√£o vs. Explota√ß√£o:**
O conceito de explora√ß√£o e explota√ß√£o leva a um dilema. Em algumas situa√ß√µes, √© melhor explorar as a√ß√µes n√£o-greedy e descobrir aquelas que podem ser superiores √† a√ß√£o greedy, mesmo que a recompensa imediata seja menor. Em outras situa√ß√µes, a explota√ß√£o √© a estrat√©gia mais adequada quando a a√ß√£o *greedy* tem um alto valor conhecido [^2]. O *trade-off* entre explora√ß√£o e explota√ß√£o √© uma quest√£o complexa que depende dos valores exatos das estimativas, das incertezas e do n√∫mero de *time steps* restantes. Existem diversos m√©todos para balancear explora√ß√£o e explota√ß√£o para formula√ß√µes matem√°ticas espec√≠ficas do *k-armed bandit problem* e problemas relacionados [^2].

**Corol√°rio 1** A escolha √≥tima entre explora√ß√£o e explota√ß√£o n√£o √© est√°tica, dependendo do progresso do aprendizado. Inicialmente, a explora√ß√£o √© crucial para identificar as melhores a√ß√µes, enquanto em etapas posteriores, uma maior explota√ß√£o das a√ß√µes com melhor estimativa pode levar a um maior ganho.
```mermaid
graph LR
    subgraph "Fase Inicial"
    A("Alta Explora√ß√£o")
    end
    subgraph "Fase Tardia"
    B("Alta Explota√ß√£o")
    end
    A --> B
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ddf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** No in√≠cio do aprendizado, podemos ter estimativas muito imprecisas das a√ß√µes, ent√£o uma alta taxa de explora√ß√£o (selecionar a√ß√µes n√£o-greedy com frequ√™ncia) seria ben√©fico. Por exemplo, podemos definir que 80% das vezes o agente explora e 20% explota.  √Ä medida que aprendemos mais, podemos reduzir a explora√ß√£o para, digamos, 10% e aumentar a explota√ß√£o para 90%, focando nas a√ß√µes que j√° sabemos que s√£o boas.

A necessidade de balancear explora√ß√£o e explota√ß√£o √© um desafio particular no *reinforcement learning*. A vers√£o simplificada do problema *k-armed bandit* permite apresentar este conceito de forma clara [^3].

**Observa√ß√£o 1**  O problema *k-armed bandit* pode ser interpretado como um caso particular de um problema de tomada de decis√£o sob incerteza, em que n√£o h√° depend√™ncia temporal entre as decis√µes e as recompensas.  Esta aus√™ncia de depend√™ncia temporal simplifica o problema e possibilita analisar o balan√ßo entre explora√ß√£o e explota√ß√£o de forma mais isolada.

### Conclus√£o
O problema k-armed bandit √© um modelo fundamental no reinforcement learning que encapsula a tens√£o entre **explora√ß√£o** e **explota√ß√£o**. A necessidade de balancear essas duas abordagens √© crucial para otimizar o aprendizado e a tomada de decis√µes em ambientes incertos. O problema k-armed bandit nos permite analisar e experimentar com diversas estrat√©gias de aprendizado que visam encontrar o equil√≠brio ideal, o que nos d√° uma base s√≥lida para abordar problemas de reinforcement learning mais complexos.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken. In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback differs from, and yet can be combined with, instructive feedback." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps. This is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or ‚Äúone-armed bandit,‚Äù except that it has k levers instead of one. Each action selection is like a play of one of the slot machine's levers, and the rewards are the payoffs for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers. Another analogy is that of a doctor choosing between experimental treatments for a series of seriously ill patients. Each action is the selection of a treatment, and each reward is the survival or well-being of the patient. Today the term ‚Äúbandit problem‚Äù is sometimes used for a generalization of the problem described above, but in this book we use it to refer just to this simple case. In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected: $q_*(a) = E[R_t | A_t=a]$. If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time stept as Qt(a). We would like $Q_t(a)$ to be close to $q_*(a)$. If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action's value. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. For example, suppose a greedy action's value is known with certainty, while several other actions are estimated to be nearly as good but with substantial uncertainty. The uncertainty is such that at least one of these other actions probably is actually better than the greedy action, but you don't know which one. If you have many time steps ahead on which to make action selections, then it may be better to explore the nongreedy actions and discover which of them are better than the greedy action. Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the ‚Äúconflict‚Äù between exploration and exploitation." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^3]: "In any specific case, whether it is better to explore or exploit depends in a complex way on the precise values of the estimates, uncertainties, and the number of remaining steps. There are many sophisticated methods for balancing exploration and exploitation for particular mathematical formulations of the k-armed bandit and related problems. However, most of these methods make strong assumptions about stationarity and prior knowledge that are either violated or impossible to verify in most applications and in the full reinforcement learning problem that we consider in subsequent chapters. The guarantees of optimality or bounded loss for these methods are of little comfort when the assumptions of their theory do not apply. In this book we do not worry about balancing exploration and exploitation in a sophisticated way; we worry only about balancing them at all. In this chapter we present several simple balancing methods for the k-armed bandit problem and show that they work much better than methods that always exploit. The need to balance exploration and exploitation is a distinctive challenge that arises in reinforcement learning; the simplicity of our version of the k-armed bandit problem enables us to show this in a particularly clear form." *(Trecho de Chapter 2: Multi-armed Bandits)*
