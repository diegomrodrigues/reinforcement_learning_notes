## Multi-armed Bandits: O Valor da A√ß√£o

### Introdu√ß√£o

O aprendizado por refor√ßo (reinforcement learning) se distingue de outras formas de aprendizado por utilizar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de instruir por meio de a√ß√µes corretas [^1]. Essa caracter√≠stica singular gera a necessidade de explora√ß√£o ativa, buscando explicitamente por comportamentos vantajosos. Enquanto o feedback puramente avaliativo indica a qualidade da a√ß√£o tomada, mas n√£o se √© a melhor ou a pior poss√≠vel, o feedback puramente instrutivo aponta a a√ß√£o correta independentemente da a√ß√£o tomada [^1]. Este cap√≠tulo explora o aspecto avaliativo do aprendizado por refor√ßo em um cen√°rio simplificado, o problema do **k-armed bandit**, que n√£o envolve aprender a agir em m√∫ltiplas situa√ß√µes [^1]. Este problema permite analisar como o feedback avaliativo difere, e como ele pode ser combinado, com o feedback instrutivo [^1]. O problema do k-armed bandit √© usado para introduzir m√©todos de aprendizado que ser√£o expandidos em cap√≠tulos posteriores para abranger o problema completo do aprendizado por refor√ßo [^1].

### Conceitos Fundamentais

O problema do **k-armed bandit** envolve a tomada repetida de decis√µes entre *k* op√ß√µes distintas, ou a√ß√µes [^1]. Ap√≥s cada escolha, uma recompensa num√©rica √© recebida, proveniente de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o selecionada [^1]. O objetivo √© maximizar a recompensa total esperada em um per√≠odo espec√≠fico, como 1000 sele√ß√µes de a√ß√£o ou passos de tempo [^1]. Analogamente a uma m√°quina ca√ßa-n√≠queis com *k* alavancas, cada sele√ß√£o de a√ß√£o corresponde ao acionamento de uma alavanca, e as recompensas s√£o os ganhos obtidos [^2].

> üí° **Exemplo Num√©rico:** Imagine um cen√°rio com 3 alavancas (*k*=3). A alavanca 1 retorna recompensas com m√©dia de 1, alavanca 2 com m√©dia de 2, e alavanca 3 com m√©dia de 3. Cada alavanca, ao ser acionada, retorna uma recompensa aleat√≥ria, conforme uma distribui√ß√£o normal com o valor m√©dio indicado e desvio padr√£o de 1. O objetivo √©, ap√≥s um certo n√∫mero de acionamentos, maximizar a soma das recompensas obtidas.

O **valor de uma a√ß√£o**, denotado por **q\*(a)**, √© a recompensa esperada ou m√©dia ao selecionar a a√ß√£o *a* [^2]. Formalmente, essa rela√ß√£o √© expressa como:

$$ q^*(a) = E[R_t | A_t = a] $$,

onde $A_t$ representa a a√ß√£o selecionada no passo de tempo *t*, e $R_t$ √© a recompensa correspondente [^2]. Se os valores de cada a√ß√£o fossem conhecidos, a solu√ß√£o seria trivial: selecionar sempre a a√ß√£o com o maior valor [^2]. Entretanto, assume-se que os valores das a√ß√µes s√£o desconhecidos, embora seja poss√≠vel realizar estimativas [^2]. A estimativa do valor da a√ß√£o *a* no passo de tempo *t* √© denotada por $Q_t(a)$, e o objetivo √© que $Q_t(a)$ se aproxime de $q^*(a)$ [^2].

```mermaid
graph LR
    A["A√ß√£o \"a\" selecionada em \"t\""] -->| "R_t" (Recompensa) | B["Distribui√ß√£o de Probabilidade \"P(R_t | A_t = a)\""];
    B --> C{"Valor Verdadeiro" "q*(a) = E[R_t | A_t = a]"};
    C --> D["Estimativa do valor" "Q_t(a)"];
    D --> E{"Aproxima√ß√£o" "Q_t(a) -> q*(a)"}
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#9f9,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, inicialmente, n√£o conhecemos os valores de $q^*(a)$ para cada alavanca. Portanto, as estimativas iniciais $Q_t(a)$ podem ser, por exemplo, $Q_1(1) = 0$, $Q_1(2) = 0$, e $Q_1(3) = 0$. Ap√≥s acionar a alavanca 1, obtemos uma recompensa, digamos, $R_1 = 0.8$. Atualizamos ent√£o $Q_2(1)$ para a m√©dia das recompensas obtidas para a a√ß√£o 1 at√© agora, que √© 0.8. Ap√≥s um n√∫mero grande de itera√ß√µes, $Q_t(1)$ ir√° se aproximar de 1, $Q_t(2)$ de 2 e $Q_t(3)$ de 3, os valores de $q^*(a)$ para cada alavanca.

**Observa√ß√£o 1:** √â importante notar que $q^*(a)$ √© um valor fixo para cada a√ß√£o *a* no contexto do problema *k*-armed bandit, pois assumimos uma distribui√ß√£o de probabilidade estacion√°ria. A variabilidade nas recompensas $R_t$ para uma dada a√ß√£o *a* deriva da natureza estoc√°stica dessa distribui√ß√£o, n√£o de uma mudan√ßa em $q^*(a)$. Em outras palavras, $q^*(a)$ representa a m√©dia a longo prazo das recompensas obtidas pela a√ß√£o *a*.

Para entender a rela√ß√£o entre **explora√ß√£o** e **explota√ß√£o**, √© fundamental identificar as **a√ß√µes gananciosas (greedy actions)**, que s√£o aquelas com o maior valor estimado [^2]. A escolha de uma a√ß√£o gananciosa √© a *explota√ß√£o* do conhecimento atual dos valores das a√ß√µes [^2]. Por outro lado, selecionar uma a√ß√£o n√£o gananciosa √© *explorar*, o que possibilita refinar a estimativa do valor das a√ß√µes n√£o gananciosas [^2]. Embora a explota√ß√£o maximize a recompensa esperada em curto prazo, a explora√ß√£o pode gerar maiores recompensas totais a longo prazo [^2].

```mermaid
graph LR
    A["Valores Estimados" "Q_t(a)"] --> B{{"A√ß√£o Gananciosa" "max Q_t(a)"}};
    B -- "Explota√ß√£o" --> C("Recompensa Imediata");
    A --> D{{"A√ß√£o N√£o-Gananciosa"}};
     D --"Explora√ß√£o"--> E("Refina Q_t(a)");
     E --> F("Potencial Recompensa Futura");
     C --> G("Curto Prazo");
     F --> H("Longo Prazo");

    style B fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
     style G fill:#eee,stroke:#333,stroke-width:1px
     style H fill:#eee,stroke:#333,stroke-width:1px
```

> üí° **Exemplo Num√©rico:**  Suponha que, ap√≥s algumas itera√ß√µes no exemplo anterior, temos $Q_t(1) = 1.1$, $Q_t(2) = 1.8$ e $Q_t(3) = 2.5$. A a√ß√£o gananciosa neste momento seria a alavanca 3, pois possui o maior valor estimado. Se escolhermos a alavanca 3, estamos *explotando*. Se escolhermos a alavanca 1 ou 2, estamos *explorando* para potencialmente descobrir se seus valores verdadeiros s√£o maiores do que os valores estimados atualmente.

**Proposi√ß√£o 1:** Uma forma de quantificar a incerteza nas estimativas $Q_t(a)$ √© atrav√©s da vari√¢ncia emp√≠rica das recompensas obtidas ao selecionar a a√ß√£o *a*. Uma alta vari√¢ncia indica maior incerteza e, portanto, um poss√≠vel benef√≠cio em explorar a√ß√µes menos conhecidas.

> üí° **Exemplo Num√©rico:** Se ap√≥s 3 acionamentos da alavanca 1 obtivemos recompensas de 0.5, 1.2 e 1.0, a vari√¢ncia amostral seria  $var(R_1) = \frac{(0.5-0.9)^2+(1.2-0.9)^2+(1-0.9)^2}{3-1} = 0.13$. Se outra alavanca, digamos a alavanca 2, foi acionada apenas uma vez e obteve-se a recompensa 2.1, a vari√¢ncia amostral (com apenas uma amostra) n√£o est√° bem definida, mas podemos consider√°-la como sendo alta, refletindo a alta incerteza sobre o verdadeiro valor de $q^*(2)$.

**Lema 1:** Seja $N_t(a)$ o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o instante *t*. Se $N_t(a)$ for suficientemente grande, a lei dos grandes n√∫meros garante que a m√©dia das recompensas observadas ao selecionar *a* se aproximar√° de $q^*(a)$, e, portanto, $Q_t(a)$ se tornar√° uma estimativa mais precisa.

```mermaid
graph LR
    A["N√∫mero de Sele√ß√µes N_t(a)"] -->| "Aumenta" | B["M√©dia das Recompensas Q_t(a)"];
    B -->| "Lei dos Grandes N√∫meros" | C["Valor Verdadeiro q*(a)"];
     C -- "Aproxima√ß√£o" --> D ["Estimativa PrecisaQ_t(a) -> q*(a)"];
     style C fill:#ccf,stroke:#333,stroke-width:2px
     style D fill:#9f9,stroke:#333,stroke-width:2px

```

> üí° **Exemplo Num√©rico:**  No exemplo anterior, √† medida que $N_t(1)$ aumenta, a m√©dia das recompensas observadas para a alavanca 1 ($Q_t(1)$) ir√° se aproximar do valor verdadeiro $q^*(1)=1$, conforme a lei dos grandes n√∫meros. O mesmo ocorrer√° com as demais alavancas.

A decis√£o de explorar ou explotar depende da precis√£o das estimativas, das incertezas e do n√∫mero de passos de tempo restantes [^2]. O conflito entre explora√ß√£o e explota√ß√£o surge porque n√£o √© poss√≠vel explorar e explotar simultaneamente em uma √∫nica sele√ß√£o de a√ß√£o [^2]. Muitos m√©todos sofisticados existem para equilibrar esses dois processos para formula√ß√µes matem√°ticas espec√≠ficas do problema k-armed bandit e problemas relacionados [^2].

**Teorema 1:** (Teorema da Converg√™ncia Gulosa) Se todos os valores de a√ß√µes s√£o estimados e atualizados adequadamente, e a escolha de a√ß√µes se torna cada vez mais gananciosa com o tempo (i.e., a explora√ß√£o diminui), sob certas condi√ß√µes, a estrat√©gia converge para a sele√ß√£o da a√ß√£o √≥tima, ou seja, a a√ß√£o *a* tal que $q^*(a)$ √© m√°ximo.
*Estrat√©gia da prova:* Este teorema depende da converg√™ncia das estimativas $Q_t(a)$ para os verdadeiros valores $q^*(a)$, e da eventual escolha da a√ß√£o gananciosa que corresponde ao valor m√°ximo, uma vez que essas estimativas se tornam confi√°veis. Note que a prova rigorosa envolve a an√°lise da taxa de decaimento da explora√ß√£o e das condi√ß√µes para converg√™ncia das estimativas.

```mermaid
graph LR
subgraph "Condi√ß√µes"
    A["Estimativa Q_t(a)"] -- "Converg√™ncia" --> B["Valor Verdadeiro q*(a)"];
    C["Explora√ß√£o" ] -- "Decrescente" --> D("Explota√ß√£o Crescente");

end
    B --> E{"A√ß√£o Gananciosa max q*(a)"};
     D --> E;
    E --> F("A√ß√£o √ìtima");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#fcc,stroke:#333,stroke-width:2px
    style F fill:#9f9,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Uma estrat√©gia gulosa simples, como escolher sempre a a√ß√£o com maior $Q_t(a)$, pode n√£o convergir para a a√ß√£o √≥tima. Se a estimativa inicial de uma a√ß√£o √≥tima √© baixa, essa a√ß√£o pode nunca ser explorada. Por outro lado, uma estrat√©gia $\epsilon$-greedy, que explora com probabilidade $\epsilon$ e explora com probabilidade $1-\epsilon$, garante que todas as a√ß√µes sejam eventualmente exploradas. √Ä medida que o n√∫mero de itera√ß√µes aumenta, √© poss√≠vel reduzir $\epsilon$, favorecendo cada vez mais a explora√ß√£o. Um exemplo de decaimento de $\epsilon$ √© $\epsilon_t = \frac{1}{t}$, onde *t* √© o passo de tempo, ou $ \epsilon_t = \epsilon_0 \cdot \gamma ^ t $, onde $\gamma$ √© uma taxa de decaimento e $\epsilon_0$ √© o valor inicial.

### Conclus√£o

O conceito de valor de uma a√ß√£o, **q\*(a)**, como a recompensa esperada ao selecionar a a√ß√£o *a*, √© fundamental no problema do *k*-armed bandit. A din√¢mica entre explora√ß√£o e explota√ß√£o √© crucial para otimizar o aprendizado e a recompensa total esperada, e o entendimento do valor da a√ß√£o e da diferen√ßa entre a√ß√µes gulosas e n√£o-gulosas √© essencial para o estudo de modelos mais complexos de reinforcement learning.

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of
learning is that it uses training information that evaluates the actions taken rather
than instructs by giving correct actions. This is what creates the need for active
exploration, for an explicit search for good behavior. Purely evaluative feedback indicates
how good the action taken was, but not whether it was the best or the worst action
possible. Purely instructive feedback, on the other hand, indicates the correct action to
take, independently of the action taken. This kind of feedback is the basis of
supervised learning, which includes large parts of pattern classification, artificial neural
networks, and system identification. In their pure forms, these two kinds of feedback
are quite distinct: evaluative feedback depends entirely on the action taken, whereas
instructive feedback is independent of the action taken.
In this chapter we study the evaluative aspect of reinforcement learning in a simplified
setting, one that does not involve learning to act in more than one situation. This
nonassociative setting is the one in which most prior work involving evaluative feedback
has been done, and it avoids much of the complexity of the full reinforcement learning
problem. Studying this case enables us to see most clearly how evaluative feedback differs
from, and yet can be combined with, instructive feedback.
The particular nonassociative, evaluative feedback problem that we explore is a simple
version of the k-armed bandit problem. We use this problem to introduce a number
of basic learning methods which we extend in later chapters to apply to the full rein-
forcement learning problem. At the end of this chapter, we take a step closer to the full
reinforcement learning problem by discussing what happens when the bandit problem
becomes associative, that is, when the best action depends on the situation." *(Trecho de Chapter 2 - Multi-armed Bandits)*
[^2]: "Consider the following learning problem. You are faced repeatedly with a choice among
k different options, or actions. After each choice you receive a numerical reward chosen
from a stationary probability distribution that depends on the action you selected. Your
objective is to maximize the expected total reward over some time period, for example,
over 1000 action selections, or time steps.
This is the original form of the k-armed bandit problem, so named by analogy to a slot
machine, or ‚Äúone-armed bandit,‚Äù except that it has k levers instead of one. Each action
selection is like a play of one of the slot machine's levers, and the rewards are the payoffs
for hitting the jackpot. Through repeated action selections you are to maximize your
winnings by concentrating your actions on the best levers. Another analogy is that of
a doctor choosing between experimental treatments for a series of seriously ill patients.
Each action is the selection of a treatment, and each reward is the survival or well-being
of the patient. Today the term ‚Äúbandit problem" is sometimes used for a generalization
of the problem described above, but in this book we use it to refer just to this simple
case.
In our k-armed bandit problem, each of the k actions has an expected or mean reward
given that that action is selected; let us call this the value of that action. We denote the
action selected on time step t as At, and the corresponding reward as Rt. The value then
of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected:
q*(a) = E[Rt | At=a].
If you knew the value of each action, then it would be trivial to solve the k-armed bandit
problem: you would always select the action with highest value. We assume that you do
not know the action values with certainty, although you may have estimates. We denote
the estimated value of action a at time stept as Qt(a). We would like Qt(a) to be close
to q*(a).
If you maintain estimates of the action values, then at any time step there is at least
one action whose estimated value is greatest. We call these the greedy actions. When you
select one of these actions, we say that you are exploiting your current knowledge of the
values of the actions. If instead you select one of the nongreedy actions, then we say you
are exploring, because this enables you to improve your estimate of the nongreedy action's
value. Exploitation is the right thing to do to maximize the expected reward on the one
step, but exploration may produce the greater total reward in the long run. For example,
suppose a greedy action's value is known with certainty, while several other actions are
estimated to be nearly as good but with substantial uncertainty. The uncertainty is
such that at least one of these other actions probably is actually better than the greedy
action, but you don't know which one. If you have many time steps ahead on which
to make action selections, then it may be better to explore the nongreedy actions and
discover which of them are better than the greedy action. Reward is lower in the short
run, during exploration, but higher in the long run because after you have discovered
the better actions, you can exploit them many times. Because it is not possible both to
explore and to exploit with any single action selection, one often refers to the ‚Äúconflict‚Äù
between exploration and exploitation.
In any specific case, whether it is better to explore or exploit depends in a complex
way on the precise values of the estimates, uncertainties, and the number of remaining
steps. There are many sophisticated methods for balancing exploration and exploitation
for particular mathematical formulations of the k-armed bandit and related problems." *(Trecho de Chapter 2 - Multi-armed Bandits)*
