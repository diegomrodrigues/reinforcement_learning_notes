## Multi-armed Bandits: O Problema k-armed Bandit

### Introdu√ß√£o
O aprendizado por refor√ßo (reinforcement learning) se distingue de outras formas de aprendizado pelo uso de informa√ß√µes de treinamento que *avaliam* as a√ß√µes tomadas, em vez de *instruir* com a√ß√µes corretas [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Essa caracter√≠stica particular cria a necessidade de explora√ß√£o ativa, uma busca expl√≠cita por um bom comportamento [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). O feedback avaliativo indica o qu√£o boa foi a a√ß√£o tomada, mas n√£o se foi a melhor ou a pior a√ß√£o poss√≠vel. Em contraste, o feedback instrutivo indica a a√ß√£o correta a ser tomada, independentemente da a√ß√£o que foi realmente realizada [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Este √∫ltimo √© a base do aprendizado supervisionado [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Este cap√≠tulo foca no aspecto avaliativo do aprendizado por refor√ßo em um ambiente simplificado, o qual n√£o envolve o aprendizado para agir em m√∫ltiplas situa√ß√µes, conhecido como cen√°rio *n√£o associativo* [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). O problema *k-armed bandit* √© um caso espec√≠fico desse cen√°rio n√£o associativo com feedback avaliativo, servindo como ponto de partida para introduzir m√©todos b√°sicos de aprendizado que ser√£o expandidos em cap√≠tulos posteriores [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1).

### Conceitos Fundamentais

O problema *k-armed bandit* consiste em tomar decis√µes repetidas entre *k* op√ß√µes distintas, denominadas a√ß√µes [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). Ap√≥s cada escolha, um agente recebe uma recompensa num√©rica, retirada de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o selecionada [1](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-1). O objetivo √© maximizar a recompensa total esperada ao longo de um per√≠odo de tempo, como, por exemplo, em 1000 sele√ß√µes de a√ß√µes ou passos de tempo [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). A analogia √© com uma m√°quina ca√ßa-n√≠queis, ou "one-armed bandit", mas com *k* alavancas em vez de uma. Cada a√ß√£o √© como jogar uma alavanca, e as recompensas s√£o os pagamentos por acertar o jackpot [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Outra analogia √© um m√©dico escolhendo entre tratamentos experimentais para pacientes gravemente doentes, onde cada a√ß√£o √© a sele√ß√£o de um tratamento e cada recompensa √© a sobreviv√™ncia ou bem-estar do paciente [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

Em um problema *k-armed bandit*, cada a√ß√£o *a* possui um valor esperado ou recompensa m√©dia, denotado por $q_*(a)$ [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Se denotamos a a√ß√£o selecionada no instante *t* por $A_t$ e a recompensa correspondente por $R_t$, ent√£o o valor $q_*(a)$ √© definido como:

$$q_*(a) = E[R_t | A_t = a]$$

Se o valor de cada a√ß√£o fosse conhecido, a solu√ß√£o seria trivial: sempre selecionar a a√ß√£o com o maior valor [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). No entanto, assume-se que esses valores s√£o desconhecidos, e o agente deve estim√°-los [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). A estimativa do valor da a√ß√£o *a* no instante *t* √© denotada por $Q_t(a)$ [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). O objetivo √© que $Q_t(a)$ se aproxime de $q_*(a)$.

**Proposi√ß√£o 1**
Uma forma comum de estimar o valor de uma a√ß√£o *a* √© calculando a m√©dia amostral das recompensas obtidas ao longo do tempo em que a a√ß√£o *a* foi selecionada. Denotando o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o instante *t* por $N_t(a)$, a estimativa $Q_t(a)$ pode ser expressa como:

```mermaid
graph LR
    A("A√ß√£o \"a\"") --> B("Recompensas R_i para A=a");
    B --> C("Soma das recompensas: Œ£ R_i * I(A_i = a)");
    C --> D("N√∫mero de sele√ß√µes N_t(a)");
    D --> E("Estimativa Q_t(a) = (Œ£ R_i * I(A_i = a)) / N_t(a)");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
```

$$Q_t(a) = \frac{\sum_{i=1}^{t} R_i \cdot \mathbb{I}(A_i = a)}{N_t(a)}$$

onde $\mathbb{I}(A_i = a)$ √© uma fun√ß√£o indicadora que retorna 1 se a a√ß√£o selecionada no instante *i* for igual a *a*, e 0 caso contr√°rio. Quando $N_t(a)$ = 0, convenciona-se que $Q_t(a)$ = 0 (ou algum valor inicial). Esta estimativa √© um estimador n√£o viesado da recompensa esperada $q_*(a)$ para cada a√ß√£o.

> üí° **Exemplo Num√©rico:**
>
> Vamos supor que temos um problema *k-armed bandit* com 3 a√ß√µes (k=3). Ap√≥s algumas itera√ß√µes, as seguintes recompensas foram obtidas para a a√ß√£o *a=2*:
>
> - Passo 10: $A_{10}$ = 2, $R_{10}$ = 1
> - Passo 25: $A_{25}$ = 2, $R_{25}$ = 2
> - Passo 38: $A_{38}$ = 2, $R_{38}$ = 0
> - Passo 50: $A_{50}$ = 2, $R_{50}$ = 3
>
> E nenhuma outra a√ß√£o foi tomada, ou seja, $N_{50}(2) = 4$. Para calcular $Q_{50}(2)$, usamos a f√≥rmula da Proposi√ß√£o 1:
>
> $$Q_{50}(2) = \frac{1 + 2 + 0 + 3}{4} = \frac{6}{4} = 1.5$$
>
> Portanto, a estimativa do valor da a√ß√£o 2 neste momento (passo 50) √© 1.5.
>
> Agora, vamos calcular $Q_t(a)$ para a a√ß√£o *a=1*  onde:
>
> - Passo 5: $A_{5}$ = 1, $R_{5}$ = 0
> - Passo 15: $A_{15}$ = 1, $R_{15}$ = -1
>
> Temos $N_{15}(1) = 2$ e:
>
> $$Q_{15}(1) = \frac{0 + (-1)}{2} = -0.5$$
>
> Assim, a estimativa da a√ß√£o 1 no instante 15 √© -0.5.

As a√ß√µes com as maiores estimativas de valor s√£o denominadas **a√ß√µes gananciosas (greedy)** [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Selecionar uma dessas a√ß√µes √© dito como **explora√ß√£o**, o uso do conhecimento atual do valor das a√ß√µes [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Alternativamente, selecionar uma a√ß√£o n√£o-gananciosa √© denominado **explora√ß√£o**, uma maneira de refinar as estimativas de valor das a√ß√µes n√£o-gananciosas [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Enquanto a explora√ß√£o maximiza a recompensa no passo atual, a explora√ß√£o pode gerar uma recompensa maior a longo prazo [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Por exemplo, uma a√ß√£o gananciosa pode ser conhecida com certeza, enquanto outras a√ß√µes s√£o estimadas como quase t√£o boas, mas com incertezas significativas, sendo que uma dessas a√ß√µes pode ser melhor que a a√ß√£o gananciosa, mas n√£o se sabe qual [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). Esse √© o **trade-off** entre explora√ß√£o e explora√ß√£o [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2). A decis√£o entre explorar e explorar depende dos valores estimados, incertezas e n√∫mero de passos restantes [2](https://chatgpt.com/c/67829922-359c-8011-96dd-970c04ed772c#user-content-fn-2).

**Lema 1**
A atualiza√ß√£o incremental da m√©dia amostral de uma a√ß√£o *a*, denotada como $Q_t(a)$, pode ser calculada de forma eficiente usando a m√©dia amostral anterior $Q_{t-1}(a)$ e a nova recompensa $R_t$. Se denotarmos o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o instante *t-1* como $N_{t-1}(a)$, ent√£o:

```mermaid
graph LR
    A("Q_(t-1)(a)") --> B("R_t");
    B --> C("R_t - Q_(t-1)(a)");
    C --> D("1/N_t(a)");
    D --> E("(1/N_t(a)) * (R_t - Q_(t-1)(a))");
    A --> E;
    E --> F("Q_t(a) = Q_(t-1)(a) + (1/N_t(a)) * (R_t - Q_(t-1)(a))");
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

$$Q_t(a) = Q_{t-1}(a) + \frac{1}{N_t(a)} [R_t - Q_{t-1}(a)]$$

*Demonstra√ß√£o*:
Partindo da defini√ß√£o da Proposi√ß√£o 1, temos:

$$Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t} R_i \cdot \mathbb{I}(A_i=a) =  \frac{1}{N_t(a)} \left( \sum_{i=1}^{t-1} R_i \cdot \mathbb{I}(A_i=a) + R_t \cdot \mathbb{I}(A_t=a) \right)$$

Se $A_t = a$, ent√£o $N_t(a) = N_{t-1}(a) + 1$ e temos:

$$Q_t(a) = \frac{N_{t-1}(a)}{N_t(a)} \left( \frac{1}{N_{t-1}(a)}  \sum_{i=1}^{t-1} R_i \cdot \mathbb{I}(A_i=a) \right) + \frac{R_t}{N_t(a)}$$
$$Q_t(a) = \frac{N_{t-1}(a)}{N_t(a)} Q_{t-1}(a) + \frac{R_t}{N_t(a)}$$
$$Q_t(a) = \frac{N_{t}(a) -1 }{N_t(a)} Q_{t-1}(a) + \frac{R_t}{N_t(a)}$$
$$Q_t(a) = Q_{t-1}(a) - \frac{1}{N_t(a)} Q_{t-1}(a) + \frac{R_t}{N_t(a)}$$
$$Q_t(a) = Q_{t-1}(a) + \frac{1}{N_t(a)} [R_t - Q_{t-1}(a)]$$

Se $A_t \neq a$, ent√£o $N_t(a) = N_{t-1}(a)$, e temos $Q_t(a) = Q_{t-1}(a)$.

Esta atualiza√ß√£o incremental √© √∫til para reduzir a complexidade computacional no c√°lculo da estimativa da a√ß√£o, pois n√£o exige o armazenamento de todas as recompensas.

> üí° **Exemplo Num√©rico:**
>
> Retomando o exemplo anterior, com as recompensas para a a√ß√£o *a=2*:
>
> - Passo 10: $A_{10}$ = 2, $R_{10}$ = 1
> - Passo 25: $A_{25}$ = 2, $R_{25}$ = 2
> - Passo 38: $A_{38}$ = 2, $R_{38}$ = 0
> - Passo 50: $A_{50}$ = 2, $R_{50}$ = 3
>
> Vimos que  $Q_{50}(2) = 1.5$. Vamos calcular $Q_t(2)$ incrementalmente.
>
> Inicialmente, $Q_{0}(2) = 0$ e $N_{0}(2) = 0$. Ap√≥s o passo 10:
>
> $N_{10}(2) = 1$
>
> $$Q_{10}(2) = 0 + \frac{1}{1}(1 - 0) = 1$$
>
> Ap√≥s o passo 25:
>
> $N_{25}(2) = 2$
>
> $$Q_{25}(2) = 1 + \frac{1}{2}(2 - 1) = 1 + \frac{1}{2} = 1.5$$
>
> Ap√≥s o passo 38:
>
> $N_{38}(2) = 3$
>
> $$Q_{38}(2) = 1.5 + \frac{1}{3}(0 - 1.5) = 1.5 - 0.5 = 1$$
>
> Ap√≥s o passo 50:
>
> $N_{50}(2) = 4$
>
> $$Q_{50}(2) = 1 + \frac{1}{4}(3 - 1) = 1 + 0.5 = 1.5$$
>
> Obtemos o mesmo valor final para $Q_{50}(2)$, mas sem precisar armazenar todas as recompensas. Observe como o valor de $Q_t(2)$ √© atualizado a cada passo em que a a√ß√£o 2 √© selecionada.

**Teorema 1** (Garantia de Converg√™ncia da M√©dia Amostral)
Sob a condi√ß√£o de que o n√∫mero de vezes que uma a√ß√£o *a* √© selecionada tende ao infinito ($N_t(a)$ -> ‚àû), e de que as recompensas s√£o limitadas, a m√©dia amostral $Q_t(a)$ converge para o valor esperado real $q_*(a)$ da a√ß√£o *a*. Formalmente,

$$\lim_{N_t(a) \to \infty} Q_t(a) = q_*(a)$$

*Demonstra√ß√£o*:
Este resultado √© uma consequ√™ncia direta da Lei dos Grandes N√∫meros. Como as recompensas $R_t$ s√£o vari√°veis aleat√≥rias independentes e identicamente distribu√≠das (i.i.d.) condicionadas √† escolha da a√ß√£o *a*, a m√©dia amostral das recompensas $Q_t(a)$ converge para a m√©dia esperada da recompensa $q_*(a)$ quando $N_t(a)$ tende ao infinito. A condi√ß√£o das recompensas serem limitadas assegura que a vari√¢ncia das recompensas √© finita, uma condi√ß√£o necess√°ria para a Lei dos Grandes N√∫meros ser aplic√°vel.

```mermaid
graph LR
    A("A√ß√£o \"a\"") -->| "N_t(a) -> ‚àû"| B("Recompensas R_i (i.i.d.)");
    B --> C("M√©dia Amostral Q_t(a)");
    C --> D("Valor Esperado q*(a)");
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
     linkStyle 0,1,2 stroke:#333,stroke-width:2px
    D -- "Converge" --> C
```

Este teorema garante que, se cada a√ß√£o for selecionada um n√∫mero suficiente de vezes, o agente aprender√° seus respectivos valores esperados, o que √© crucial para o desenvolvimento de estrat√©gias de explora√ß√£o/explota√ß√£o.

### Conclus√£o
O problema do *k-armed bandit* √© uma formula√ß√£o fundamental que captura o essencial do dilema entre explora√ß√£o e explota√ß√£o no aprendizado por refor√ßo. Ao apresentar um cen√°rio simplificado, ele nos permite isolar esse trade-off e estudar v√°rios m√©todos de solu√ß√£o antes de lidarmos com a complexidade do problema de aprendizado por refor√ßo em sua totalidade. Este cap√≠tulo serviu como introdu√ß√£o ao problema do *k-armed bandit*, estabelecendo a base para discuss√µes mais aprofundadas de m√©todos e t√©cnicas a serem explorados nos cap√≠tulos seguintes.

### Refer√™ncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken. In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback differs from, and yet can be combined with, instructive feedback. The particular nonassociative, evaluative feedback problem that we explore is a simple version of the k-armed bandit problem. We use this problem to introduce a number of basic learning methods which we extend in later chapters to apply to the full reinforcement learning problem. At the end of this chapter, we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when the best action depends on the situation." *(Trecho de Multi-armed Bandits)*
[^2]: "Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps. This is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or ‚Äúone-armed bandit,‚Äù except that it has k levers instead of one. Each action selection is like a play of one of the slot machine's levers, and the rewards are the payoffs for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers. Another analogy is that of a doctor choosing between experimental treatments for a series of seriously ill patients. Each action is the selection of a treatment, and each reward is the survival or well-being of the patient. Today the term ‚Äúbandit problem" is sometimes used for a generalization of the problem described above, but in this book we use it to refer just to this simple case. In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected:  q*(a) = E[Rt | At=a]. If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time stept as Qt(a). We would like Qt(a) to be close to q*(a). If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action's value. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. For example, suppose a greedy action's value is known with certainty, while several other actions are estimated to be nearly as good but with substantial uncertainty. The uncertainty is such that at least one of these other actions probably is actually better than the greedy action, but you don't know which one. If you have many time steps ahead on which to make action selections, then it may be better to explore the nongreedy actions and discover which of them are better than the greedy action. Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the ‚Äúconflict‚Äù between exploration and exploitation. In any specific case, whether it is better to explore or exploit depends in a complex way on the precise values of the estimates, uncertainties, and the number of remaining steps. There are many sophisticated methods for balancing exploration and exploitation for particular mathematical formulations of the k-armed bandit and related problems." *(Trecho de Multi-armed Bandits)*
