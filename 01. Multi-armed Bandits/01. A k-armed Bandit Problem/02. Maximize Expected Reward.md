### Maximizar a Recompensa Total Esperada em Problemas k-armed Bandit

### Introdu√ß√£o
O problema do **k-armed bandit** surge no contexto do aprendizado por refor√ßo como um cen√°rio simplificado para explorar o *trade-off* entre explora√ß√£o e explota√ß√£o [1]. Diferentemente do aprendizado supervisionado, que se baseia em *feedback* instrutivo, o aprendizado por refor√ßo utiliza *feedback* avaliativo, indicando a qualidade de uma a√ß√£o tomada, mas n√£o necessariamente qual seria a a√ß√£o √≥tima [1]. Nesse cen√°rio, um agente precisa tomar decis√µes sequenciais em um ambiente onde as a√ß√µes geram recompensas num√©ricas, mas com uma din√¢mica probabil√≠stica e desconhecida. O objetivo central √© maximizar a recompensa total esperada ao longo do tempo, um desafio que imp√µe a necessidade de equilibrar a busca por novas op√ß√µes e o aproveitamento das op√ß√µes j√° conhecidas [1]. O problema do k-armed bandit √© uma simplifica√ß√£o do problema de *reinforcement learning* completo, no qual n√£o h√° m√∫ltiplos estados e o objetivo √© escolher a a√ß√£o que leva a melhor recompensa, sendo assim o foco deste cap√≠tulo [1].

**Proposi√ß√£o 1** A natureza sequencial das decis√µes no problema k-armed bandit implica que a escolha de uma a√ß√£o no instante $t$ afeta n√£o somente a recompensa $R_t$, mas tamb√©m o conhecimento do agente sobre o ambiente, influenciando as escolhas em instantes futuros.
*Proof:* A escolha de a√ß√µes influencia diretamente as estimativas $Q_t(a)$. A explora√ß√£o melhora o conhecimento do agente, ao custo de menor recompensa imediata, enquanto a explora√ß√£o busca recompensas imediatas baseadas em conhecimento pr√©vio, ent√£o a decis√£o $A_t$ influencia o conhecimento e consequentemente as decis√µes $A_{t+1}$, $A_{t+2}$ e assim por diante.

```mermaid
graph LR
    A[Instante "t"] -->|A√ß√£o "A_t"| B("Recompensa \"R_t\"")
    A -->|Influencia| C("Estimativa \"Q_t(a)\"")
    C --> D("Escolha de \"A_{t+1}\"")
    D --> E("Recompensa \"R_{t+1}\"")
    E --> F("Estimativa \"Q_{t+1}(a)\"")
     F--> G("Escolha de \"A_{t+2}\"")
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
    style G fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Imagine que temos 3 a√ß√µes (k=3), onde a a√ß√£o 1 tem uma recompensa m√©dia de 1, a√ß√£o 2 tem uma recompensa m√©dia de 2, e a√ß√£o 3 tem uma recompensa m√©dia de 3, mas desconhecemos esses valores. Inicialmente, $Q_t(a)$ para todas as a√ß√µes √© zero. Se no instante $t=1$, escolhermos a a√ß√£o 1, e recebermos uma recompensa $R_1=0$, ent√£o $Q_2(1)$ se atualiza, digamos para $0.1$ (dependendo da regra de atualiza√ß√£o que usarmos). Se escolhermos novamente a a√ß√£o 1 em $t=2$ e obtivermos $R_2=1$, $Q_3(1)$ se atualizar√° novamente. As escolhas subsequentes e suas recompensas afetam diretamente as estimativas de valor e, portanto, as pr√≥ximas decis√µes. Se, em vez disso, escolhermos explorar a a√ß√£o 3 no in√≠cio, podemos obter uma recompensa maior, o que influenciar√° as estimativas e decis√µes futuras.

### Conceitos Fundamentais
O **k-armed bandit problem** envolve um cen√°rio onde um agente deve escolher repetidamente entre $k$ diferentes op√ß√µes ou a√ß√µes [1]. Cada a√ß√£o, quando selecionada, resulta em uma recompensa num√©rica que √© sorteada de uma distribui√ß√£o de probabilidade estacion√°ria dependente da a√ß√£o escolhida [1]. O objetivo √© maximizar a recompensa total esperada ao longo de um per√≠odo de tempo, seja um n√∫mero finito de sele√ß√µes ou tempo de intera√ß√£o [2]. A analogia com uma m√°quina ca√ßa-n√≠queis, onde cada a√ß√£o √© o acionamento de uma alavanca diferente, d√° nome ao problema: cada alavanca tem um valor esperado diferente de recompensa, e o objetivo √© maximizar os ganhos concentrando as a√ß√µes nas melhores alavancas [2]. Formalmente, se $A_t$ representa a a√ß√£o selecionada no instante $t$ e $R_t$ a recompensa correspondente, o valor verdadeiro da a√ß√£o $a$, denotado por $q_*(a)$, √© definido como o valor esperado da recompensa condicionada √† escolha de $a$:

$$q_*(a) = \mathbb{E}[R_t | A_t = a].$$

A solu√ß√£o trivial, que √© sempre escolher a a√ß√£o com maior valor, √© invi√°vel, pois desconhecemos esses valores no in√≠cio do processo [2]. Por isso, precisamos manter estimativas dos valores das a√ß√µes, denotadas por $Q_t(a)$, que devem se aproximar dos valores reais $q_*(a)$ [2]. A escolha de a√ß√µes pode ser categorizada em duas abordagens: **explora√ß√£o** e **explota√ß√£o**. A explota√ß√£o consiste em selecionar a a√ß√£o com maior valor estimado no momento, denominada a√ß√£o *greedy* [2]. A explora√ß√£o, por outro lado, envolve a escolha de a√ß√µes n√£o *greedy*, com o objetivo de refinar as estimativas dos valores das a√ß√µes menos exploradas [2].

```mermaid
graph LR
subgraph "Defini√ß√µes"
    A("A√ß√£o \"a\"") -->|Recompensa| B("Recompensa \"R_t\"")
    B --> C("Valor Verdadeiro \"q_*(a)\" = E[R_t | A_t = a]")
    C --> D("Valor Estimado \"Q_t(a)\"")
end
subgraph "Escolha de A√ß√£o"
    D --> E("Explora√ß√£o (A√ß√£o n√£o-greedy)")
    D --> F("Explota√ß√£o (A√ß√£o greedy)")
end
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
    style E fill:#ccf,stroke:#333,stroke-width:2px
    style F fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Suponha que temos 4 a√ß√µes (k=4) e ap√≥s algumas itera√ß√µes, as estimativas de valor atuais s√£o: $Q_t(1) = 1.2$, $Q_t(2) = 2.5$, $Q_t(3) = 1.8$, e $Q_t(4) = 0.9$. A a√ß√£o *greedy* (explota√ß√£o) seria a a√ß√£o 2, pois tem o maior valor estimado. A explora√ß√£o significaria escolher uma das outras a√ß√µes (1, 3 ou 4) para tentar obter mais informa√ß√µes sobre o seu valor real.

**Lema 1** Se a a√ß√£o $a$ √© explorada repetidamente, a estimativa de seu valor, $Q_t(a)$, converge para o valor real, $q_*(a)$, sob certas condi√ß√µes.
*Proof:* A estimativa $Q_t(a)$ geralmente √© atualizada com base em alguma m√©dia das recompensas observadas. Sob condi√ß√µes como estacionariedade das distribui√ß√µes de recompensa e um esquema de atualiza√ß√£o que d√™ um peso decrescente a recompensas antigas, essa m√©dia converge para o valor esperado.

```mermaid
graph LR
    A("A√ß√£o \"a\" explorada repetidamente") --> B("Atualiza√ß√£o de \"Q_t(a)\" via m√©dia das recompensas")
     B --> C("Condi√ß√µes: Estacionaridade, peso decrescente")
    C --> D("Converg√™ncia: \"Q_t(a)\" -> \"q_*(a)\"")
        style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
    style D fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Vamos supor que a a√ß√£o 1 tenha um valor real $q_*(1) = 2$, mas inicialmente, temos $Q_1(1) = 0$. Se usarmos uma m√©dia m√≥vel para atualizar $Q_t(1)$, e se explorarmos a a√ß√£o 1 nas itera√ß√µes 1, 2, 3, 4 e 5 com recompensas 1, 3, 2, 2, e 2 respectivamente.
>
> *   $Q_2(1) = (0 + 1)/2 = 0.5$
> *   $Q_3(1) = (0.5 + 3)/2 = 1.75$
> *   $Q_4(1) = (1.75 + 2)/2 = 1.875$
> *   $Q_5(1) = (1.875 + 2)/2 = 1.9375$
> *   $Q_6(1) = (1.9375 + 2)/2 = 1.96875$
>
> Como podemos observar, a estimativa $Q_t(1)$ est√° a convergir para o valor real 2, a medida que a a√ß√£o √© explorada mais vezes.

**Lema 1.1** Se, para todo $a$, a a√ß√£o $a$ √© selecionada um n√∫mero infinito de vezes, e a m√©dia usada para estimar $Q_t(a)$ considera todas as recompensas anteriores, ent√£o $Q_t(a)$ converge para $q_*(a)$ quando $t$ tende ao infinito.
*Proof:* Este resultado √© uma consequ√™ncia direta da lei forte dos grandes n√∫meros, sob a hip√≥tese de que as recompensas s√£o i.i.d. para cada a√ß√£o $a$.

> üí° **Exemplo Num√©rico:**  Se tivermos 2 a√ß√µes com valores verdadeiros  $q_*(1) = 1$ e $q_*(2) = 2$. Se as explorarmos infinitamente, e usarmos uma m√©dia simples para estimar $Q_t(a)$, ent√£o  $Q_t(1)$  converge para 1 e $Q_t(2)$ converge para 2 quando $t$ tende a infinito. A lei dos grandes n√∫meros garante que a m√©dia amostral das recompensas se aproximar√° do valor esperado (valor verdadeiro da a√ß√£o) √† medida que o n√∫mero de amostras tende ao infinito.

**Corol√°rio 1.1** A explora√ß√£o continua √© necess√°ria, ou com uma chance n√£o nula, para garantir que as estimativas de valor convirjam para os valores verdadeiros de cada a√ß√£o.
*Proof:* Se nenhuma a√ß√£o for explorada infinitamente, o *Lema 1.1* implica que sua estimativa de valor pode n√£o convergir para seu valor verdadeiro.

```mermaid
graph LR
    A("Explora√ß√£o cont√≠nua de todas as a√ß√µes") -->|Implica|B("Estimativas \"Q_t(a)\" convergem para os valores verdadeiros \"q_*(a)\"")
    B-->C("Garantia: \"Lema 1.1\"")
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:** Imagine que paramos de explorar a a√ß√£o 2 e apenas exploramos a a√ß√£o 1, depois de algumas intera√ß√µes. Como consequ√™ncia a estimativa de valor da a√ß√£o 2, n√£o convergir√° para o seu valor real. Se, por outro lado, existe uma probabilidade n√£o nula de escolher a a√ß√£o 2 em qualquer instante, mesmo quando o valor estimado da a√ß√£o 1 √© maior, garantimos que $Q_t(2)$ continue a se aproximar de $q_*(2)$.

O *trade-off* entre explora√ß√£o e explota√ß√£o √© fundamental: a explota√ß√£o maximiza a recompensa imediata, enquanto a explora√ß√£o pode levar a recompensas maiores a longo prazo [2]. A decis√£o de explorar ou explotar depende do grau de certeza das estimativas, da incerteza sobre o valor de outras a√ß√µes e do horizonte temporal de intera√ß√£o. O desafio √© equilibrar essas abordagens para obter o m√°ximo de recompensa acumulada [2].

> üí° **Exemplo Num√©rico:** Suponha que ap√≥s algumas itera√ß√µes temos as seguintes estimativas:  $Q_t(1)=0.8$ e $Q_t(2)=1.2$. Uma estrat√©gia puramente explorat√≥ria pode escolher uma a√ß√£o aleatoriamente, com igual probabilidade. Uma estrat√©gia puramente explorat√≥ria escolhe sempre a a√ß√£o 2, com o intuito de maximizar a recompensa imediata. Um estrat√©gia mista, como a $\epsilon$-greedy, com $\epsilon=0.1$, escolheria a√ß√£o 2 com 90% de probabilidade e a a√ß√£o 1 com 10%. Isso permite que o agente explore novas a√ß√µes, mesmo quando j√° possui uma a√ß√£o com um valor estimado relativamente alto, para n√£o ficar preso a um √≥timo local.
>
> No come√ßo do processo de aprendizagem, √© vantajoso ter um valor mais alto de $\epsilon$, j√° que o agente tem pouco conhecimento sobre o ambiente. Ao longo do tempo o valor de $\epsilon$ vai diminuindo, dando mais prioridade a explora√ß√£o.

**Teorema 1** O problema do k-armed bandit n√£o possui uma solu√ß√£o √∫nica √≥tima, mas sim uma fam√≠lia de estrat√©gias √≥timas dependente do equil√≠brio entre explora√ß√£o e explota√ß√£o, e que esta estrat√©gia √≥tima depende do horizonte temporal de inter√ß√£o e do conhecimento inicial do agente.
*Proof:* Dado que diferentes n√≠veis de explora√ß√£o s√£o necess√°rios dependendo do horizonte de intera√ß√£o do agente, e da incerteza sobre os valores $q_*(a)$, n√£o h√° uma estrat√©gia √∫nica que maximize o retorno total para todos os cen√°rios. As estrat√©gias √≥timas s√£o aquelas que conseguem equilibrar a explora√ß√£o no inicio do aprendizado, para um melhor conhecimento das ac√ß√µes, com uma explora√ß√£o mais forte a medida que o agente se aproxima do fim do horizonte temporal.

```mermaid
graph LR
    A("Problema k-armed bandit") --> B("N√£o h√° solu√ß√£o √≥tima √∫nica")
    B --> C("Fam√≠lia de estrat√©gias √≥timas")
    C --> D("Equil√≠brio entre Explora√ß√£o e Explota√ß√£o")
     D --> E("Estrat√©gia √≥tima depende:")
     E --> F("Horizonte temporal de intera√ß√£o")
      E --> G("Conhecimento inicial do agente")
    style A fill:#ccf,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
     style C fill:#ccf,stroke:#333,stroke-width:2px
      style D fill:#ccf,stroke:#333,stroke-width:2px
       style E fill:#ccf,stroke:#333,stroke-width:2px
       style F fill:#ccf,stroke:#333,stroke-width:2px
       style G fill:#ccf,stroke:#333,stroke-width:2px
```

> üí° **Exemplo Num√©rico:**  Um agente que interage com o ambiente por apenas 10 passos e que sabe pouco sobre o ambiente, deve focar em explora√ß√£o no inicio. J√° um agente que interage com o ambiente por 1000 passos e que sabe pouco sobre o ambiente, pode focar em explora√ß√£o no inicio mas, a medida que o tempo passa, deve focar em explota√ß√£o. Por outro lado, se o agente j√° tiver um conhecimento pr√©vio das recompensas m√©dias das a√ß√µes, pode focar mais em explora√ß√£o, desde o in√≠cio. Estas considera√ß√µes mostram que n√£o h√° uma estrat√©gia √≥tima para todos os cen√°rios, mas que a estrategia ideal depende do horizonte de intera√ß√£o e do conhecimento pr√©vio sobre as a√ß√µes.

### Conclus√£o
O problema do **k-armed bandit** fornece um modelo simples, mas essencial, para estudar a din√¢mica do aprendizado por refor√ßo [1]. Ao concentrar-se na maximiza√ß√£o da recompensa total esperada, o problema nos for√ßa a lidar com a complexa rela√ß√£o entre explora√ß√£o e explota√ß√£o [2]. As a√ß√µes selecionadas, seja por explota√ß√£o ou explora√ß√£o, impactam diretamente a qualidade da estimativa dos valores das a√ß√µes. Compreender a formula√ß√£o do k-armed bandit, incluindo as defini√ß√µes do valor verdadeiro de uma a√ß√£o, a necessidade de estimar valores das a√ß√µes e a import√¢ncia do *trade-off* entre explora√ß√£o e explota√ß√£o √© crucial para o desenvolvimento de algoritmos eficazes que ser√£o apresentados ao longo deste cap√≠tulo [1].

### Refer√™ncias
[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken." *(Trecho de Chapter 2: Multi-armed Bandits)*
[^2]: "In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected:
q‚àó(a) = E[Rt | At=a]. If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time step t as Qt(a). We would like Qt(a) to be close to q‚àó(a). If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action's value. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. For example, suppose a greedy action‚Äôs value is known with certainty, while several other actions are estimated to be nearly as good but with substantial uncertainty. The uncertainty is such that at least one of these other actions probably is actually better than the greedy action, but you don't know which one. If you have many time steps ahead on which to make action selections, then it may be better to explore the nongreedy actions and discover which of them are better than the greedy action. Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the ‚Äúconflict‚Äù between exploration and exploitation." *(Trecho de Chapter 2: Multi-armed Bandits)*
