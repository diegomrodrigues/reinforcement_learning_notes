## O Conflito entre Explora√ß√£o e Explota√ß√£o em Problemas de *k*-armed Bandits

### Introdu√ß√£o
Em problemas de *reinforcement learning*, o aprendizado se distingue por usar informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas em vez de instruir quais a√ß√µes corretas devem ser tomadas [1]. Essa caracter√≠stica fundamental cria a necessidade de uma explora√ß√£o ativa, onde o agente busca explicitamente um comportamento √≥timo [1]. O *feedback* avaliativo puro indica o qu√£o boa foi a a√ß√£o tomada, mas n√£o informa se ela foi a melhor ou a pior a√ß√£o poss√≠vel [1]. Em contraste, o *feedback* instrutivo indica a a√ß√£o correta a ser tomada, independentemente da a√ß√£o que foi realmente executada [1]. O *feedback* instrutivo √© a base do aprendizado supervisionado, que abrange √°reas como classifica√ß√£o de padr√µes, redes neurais artificiais e identifica√ß√£o de sistemas [1]. Em suas formas puras, esses dois tipos de *feedback* s√£o distintos: o *feedback* avaliativo depende inteiramente da a√ß√£o tomada, enquanto o *feedback* instrutivo √© independente da a√ß√£o tomada [1]. Este cap√≠tulo se concentra no aspecto avaliativo do aprendizado por refor√ßo em um ambiente simplificado, onde o agente n√£o precisa aprender a agir em m√∫ltiplas situa√ß√µes, um cen√°rio *n√£o associativo* [1]. Este contexto *n√£o associativo* √© o cen√°rio mais comum em estudos de *feedback* avaliativo, permitindo uma an√°lise mais clara das distin√ß√µes e combina√ß√µes entre *feedback* avaliativo e instrutivo [1]. O problema particular explorado aqui √© uma vers√£o simplificada do problema do *k*-armed bandit [1].

### Conceitos Fundamentais

No problema do **k-armed bandit**, o agente √© confrontado repetidamente com a escolha entre *k* op√ß√µes distintas, ou a√ß√µes [1]. Ap√≥s cada escolha, o agente recebe uma recompensa num√©rica, selecionada de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o escolhida [1]. O objetivo √© maximizar a recompensa total esperada durante um per√≠odo de tempo, por exemplo, ao longo de 1000 sele√ß√µes de a√ß√µes ou passos de tempo [1]. O problema do *k*-armed bandit recebe este nome por analogia a uma m√°quina ca√ßa-n√≠queis (ou "one-armed bandit"), exceto que possui *k* alavancas em vez de uma [2]. Cada sele√ß√£o de a√ß√£o √© como jogar uma das alavancas da m√°quina ca√ßa-n√≠queis, e as recompensas s√£o os ganhos por acertar o jackpot [2]. A repeti√ß√£o de sele√ß√µes de a√ß√µes visa concentrar as a√ß√µes nas melhores alavancas para maximizar os ganhos [2]. Outra analogia √© a de um m√©dico escolhendo entre tratamentos experimentais para pacientes gravemente enfermos [2]. Cada a√ß√£o √© a sele√ß√£o de um tratamento, e cada recompensa √© a sobreviv√™ncia ou bem-estar do paciente [2].

Cada uma das *k* a√ß√µes possui um valor esperado ou recompensa m√©dia dada a a√ß√£o selecionada [2]. Esse valor √© denotado por $q_*(a)$ e definido como a recompensa esperada quando a a√ß√£o *a* √© selecionada:

$$q_*(a) = E[R_t|A_t = a]$$ [2]

onde $A_t$ √© a a√ß√£o selecionada no passo de tempo *t* e $R_t$ √© a recompensa correspondente [2].

> üí° **Exemplo Num√©rico:** Imagine um problema de 3-armed bandit (k=3) onde cada a√ß√£o (A√ß√£o 1, A√ß√£o 2, A√ß√£o 3) tem uma recompensa m√©dia diferente. Suponha que:
>   - $q_*(A_1) = 1$ (A√ß√£o 1 recompensa em m√©dia 1)
>   - $q_*(A_2) = 2$ (A√ß√£o 2 recompensa em m√©dia 2)
>   - $q_*(A_3) = 0.5$ (A√ß√£o 3 recompensa em m√©dia 0.5)
>
> O objetivo do agente √© descobrir que a A√ß√£o 2 √© a melhor (maior valor esperado) e maximizar suas recompensas ao longo do tempo. No entanto, no in√≠cio, o agente n√£o sabe esses valores e deve estim√°-los.
>
> As recompensas n√£o s√£o determin√≠sticas; ou seja, para a A√ß√£o 1, em um instante *t*, a recompensa *R\_t* pode ser 0.8, em outro pode ser 1.2. O valor esperado √© a m√©dia desses valores ao longo de muitas execu√ß√µes ($q_*(A_1) = E[R_t|A_t = A_1] = 1$).
>
> ```mermaid
> graph LR
>     subgraph "k-armed Bandit"
>         A("A√ß√£o 1") -->|Recompensa m√©dia 1| B("Recompensa");
>         C("A√ß√£o 2") -->|Recompensa m√©dia 2| D("Recompensa");
>         E("A√ß√£o 3") -->|Recompensa m√©dia 0.5| F("Recompensa");
>     end
>
> ```

O problema do *k*-armed bandit seria trivial se o valor de cada a√ß√£o fosse conhecido, pois o agente escolheria sempre a a√ß√£o com o maior valor [2]. Entretanto, assume-se que os valores das a√ß√µes s√£o desconhecidos, mas podem ser estimados. A estimativa do valor da a√ß√£o *a* no passo de tempo *t* √© denotada por $Q_t(a)$ [2]. O objetivo √© que $Q_t(a)$ seja uma aproxima√ß√£o precisa de $q_*(a)$ [2].

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, suponha que em um certo instante *t=5*, o agente j√° tenha escolhido cada a√ß√£o algumas vezes e tenha as seguintes estimativas:
>   -  $Q_5(A_1) = 0.9$
>   -  $Q_5(A_2) = 1.8$
>   -  $Q_5(A_3) = 0.6$
>
>   Observe que as estimativas $Q_t(a)$ s√£o diferentes dos valores reais $q_*(a)$ porque o agente est√° no processo de aprender. O objetivo √© que ao longo do tempo, $Q_t(a)$ se aproxime de $q_*(a)$.
>
>   Se o agente escolher a a√ß√£o com o maior valor estimado neste momento (explota√ß√£o), ele escolher√° a A√ß√£o 2. No entanto, pode ser que a A√ß√£o 1 seja ainda melhor (valor real 1) e deva ser explorada mais.

Um componente chave para resolver o problema do *k*-armed bandit √© o equil√≠brio entre **explora√ß√£o** e **explota√ß√£o** [2]. A **explota√ß√£o** significa selecionar as a√ß√µes com maior valor estimado no momento, as chamadas a√ß√µes *greedy* [2]. A **explora√ß√£o**, por outro lado, envolve selecionar a√ß√µes n√£o *greedy* para melhorar a estimativa do valor das a√ß√µes [2]. A explora√ß√£o √© necess√°ria porque a explota√ß√£o de conhecimento atual nem sempre leva √†s melhores recompensas a longo prazo [2].

A explota√ß√£o visa maximizar a recompensa imediata, enquanto a explora√ß√£o pode levar a recompensas totais maiores a longo prazo [2]. Por exemplo, uma a√ß√£o *greedy* pode ter seu valor conhecido com certeza, enquanto outras a√ß√µes podem ter valores estimados pr√≥ximos, mas com consider√°vel incerteza [2]. A incerteza pode indicar que uma das a√ß√µes n√£o-*greedy* √© melhor que a a√ß√£o *greedy*, mas isso n√£o √© conhecido de imediato [2]. Assim, a explora√ß√£o de a√ß√µes n√£o-*greedy* pode levar √† descoberta de a√ß√µes mais vantajosas [2]. Explorar resulta em recompensas menores a curto prazo, mas pode aumentar a recompensa a longo prazo [2]. Existe um conflito inerente entre explora√ß√£o e explota√ß√£o, pois n√£o √© poss√≠vel explorar e explotar em uma mesma sele√ß√£o de a√ß√£o [2]. A decis√£o entre explorar ou explotar depende de maneira complexa dos valores estimados, das incertezas e do n√∫mero de passos restantes [2]. Existem v√°rios m√©todos sofisticados para equilibrar explora√ß√£o e explota√ß√£o em formula√ß√µes matem√°ticas particulares do problema do *k*-armed bandit e problemas relacionados [2]. No entanto, a maioria destes m√©todos faz suposi√ß√µes fortes sobre estacionariedade e conhecimento pr√©vio, que muitas vezes n√£o s√£o verificadas em aplica√ß√µes pr√°ticas [3]. O foco neste cap√≠tulo √© balancear a explora√ß√£o e a explota√ß√£o de forma mais simples [3].

**Lemma 1 (Converg√™ncia da Estimativa de Valor):**  Se todas as a√ß√µes forem selecionadas um n√∫mero infinito de vezes, as estimativas de valores de a√ß√µes $Q_t(a)$ convergem para seus verdadeiros valores $q_*(a)$ [3, 4].

*Prova:* A estimativa de valor da a√ß√£o √© dada por [3]:
$$Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}$$
onde $\mathbb{1}_{A_i=a}$ √© uma fun√ß√£o indicadora que vale 1 quando a a√ß√£o *a* √© selecionada no passo de tempo *i* e 0 caso contr√°rio. Se o denominador dessa express√£o tende a infinito, a lei dos grandes n√∫meros garante que $Q_t(a)$ converge para o valor esperado da recompensa, $q_*(a)$. Assim, se todas as a√ß√µes forem selecionadas um n√∫mero infinito de vezes, a estimativa do valor de cada a√ß√£o converge para o seu valor verdadeiro. $\blacksquare$

> üí° **Exemplo Num√©rico:** Para ilustrar a converg√™ncia, vamos continuar com a A√ß√£o 1 do exemplo anterior ($q_*(A_1)=1$). Suponha que as recompensas obtidas ao longo do tempo para a a√ß√£o 1 sejam:
>
>   - Passo 1: A√ß√£o 1, $R_1 = 0.8$, $Q_1(A_1) = 0.8$ (1 vez escolhida)
>   - Passo 5: A√ß√£o 1, $R_5 = 1.2$, $Q_5(A_1) = (0.8 + 1.2)/2 = 1.0$ (2 vezes escolhida)
>   - Passo 10: A√ß√£o 1, $R_{10} = 0.9$, $Q_{10}(A_1) = (0.8 + 1.2 + 0.9)/3 = 0.966$ (3 vezes escolhida)
>   - ...
>   - Passo 100: A√ß√£o 1, $R_{100} = 1.1$, $Q_{100}(A_1) = 0.998$ (20 vezes escolhida, valores aleat√≥rios)
>
>  Podemos observar que √† medida que a A√ß√£o 1 √© selecionada mais vezes, a estimativa $Q_t(A_1)$ se aproxima cada vez mais do valor real $q_*(A_1) = 1$.
>
> ```python
> import numpy as np
>
> # Exemplo simulado de recompensas e Q_t(A_1)
> rewards = [0.8, 1.2, 0.9, 1.05, 0.95, 1.1, 0.88, 1.02, 0.99, 1.01]
> Qt_values = np.cumsum(rewards) / np.arange(1, len(rewards) + 1)
> print(f"Qt_values: {Qt_values}")
> print(f"Q_t(A_1) final: {Qt_values[-1]}")
> print(f"Valor real q*(A_1): {1}")
> ```
> ```mermaid
>  graph LR
>      subgraph "Converg√™ncia de Q_t(a)"
>          A["Recompensas R_i"]
>          B["A√ß√µes A_i"]
>          C["Fun√ß√£o Indicadora 1_(A_i=a)"]
>          D["Soma das Recompensas"]
>          E["Contador de A√ß√µes"]
>          F["Q_t(a)"]
>
>         A --> D
>         B --> C
>         C --> D
>         C --> E
>         D --> F
>         E --> F
>         F --> G["q*(a) (Converg√™ncia)"]
>       style F fill:#ccf,stroke:#333,stroke-width:2px
>     end
>
> ```

**Lemma 1.1 (Estimativa da M√©dia Amostral):** A express√£o para a estimativa do valor da a√ß√£o $Q_t(a)$ pode ser vista como a m√©dia amostral das recompensas obtidas quando a a√ß√£o *a* √© selecionada at√© o instante *t-1*.

*Prova:* A express√£o $\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}$ representa a soma de todas as recompensas obtidas quando a a√ß√£o *a* foi selecionada, enquanto $\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}$ representa o n√∫mero total de vezes em que a a√ß√£o *a* foi selecionada at√© o passo de tempo *t-1*. Portanto, a raz√£o entre essas duas quantidades √© a m√©dia das recompensas obtidas com a a√ß√£o *a*, o que corresponde √† m√©dia amostral. $\blacksquare$

> üí° **Exemplo Num√©rico:** No exemplo anterior, se considerarmos apenas at√© o passo 5, as recompensas para a A√ß√£o 1 foram 0.8 e 1.2.
> A m√©dia amostral √© $(0.8 + 1.2) / 2 = 1.0$, que √© exatamente o valor de $Q_5(A_1)$. Isso demonstra que a estimativa de valor $Q_t(a)$ √© calculada como a m√©dia amostral das recompensas recebidas para a a√ß√£o *a*.
>
> ```mermaid
> graph LR
>    subgraph "M√©dia Amostral"
>     A("Recompensas R_i para A√ß√£o a") --> B("Soma das Recompensas para A√ß√£o a")
>     C("N√∫mero de vezes que A√ß√£o a foi selecionada")
>     B --> D("Q_t(a) = M√©dia Amostral")
>     C --> D
>     style D fill:#ccf,stroke:#333,stroke-width:2px
>    end
> ```

**Corol√°rio 1 (Explora√ß√£o para Converg√™ncia):** Para garantir que a condi√ß√£o do Lemma 1 seja satisfeita, ou seja, todas as a√ß√µes sejam selecionadas infinitas vezes, m√©todos de explora√ß√£o s√£o necess√°rios.

*Prova:* A converg√™ncia da estimativa de valor das a√ß√µes requer que cada a√ß√£o seja selecionada um n√∫mero infinito de vezes. Se um agente usar apenas a a√ß√£o que parece melhor no momento, a a√ß√£o *greedy*, outras a√ß√µes podem n√£o ser selecionadas, portanto n√£o haver√° converg√™ncia. Portanto, algum tipo de explora√ß√£o √© necess√°ria para garantir que todas as a√ß√µes sejam selecionadas eventualmente. $\blacksquare$

> üí° **Exemplo Num√©rico:** Imagine um agente que come√ßa com valores estimados $Q_1(A_1) = 1.0$, $Q_1(A_2) = 1.5$ e $Q_1(A_3) = 0.5$. Se o agente sempre escolher a a√ß√£o *greedy*, ele sempre selecionar√° a a√ß√£o 2.
>
>   Sem explora√ß√£o, o agente nunca escolher√° as a√ß√µes 1 e 3 e n√£o ter√° a chance de atualizar suas estimativas. Isso significa que o agente nunca aprender√° se existem a√ß√µes melhores. Por exemplo, se $q_*(A_1) = 2$, o agente nunca descobrir√° que a a√ß√£o 1 √© a melhor.
>
>   Explora√ß√£o, mesmo que resulte em recompensas menores no in√≠cio, garante que todas as a√ß√µes sejam consideradas e, eventualmente, a a√ß√£o com o maior valor esperado seja identificada.

**Proposi√ß√£o 1 (Trade-off Explora√ß√£o-Explota√ß√£o):** A escolha entre explora√ß√£o e explota√ß√£o apresenta um *trade-off*, uma vez que, a explota√ß√£o maximiza o retorno imediato com base no conhecimento atual, enquanto a explora√ß√£o visa a obten√ß√£o de conhecimento sobre as a√ß√µes que pode levar a maiores retornos no futuro.

*Prova:* A explota√ß√£o se baseia no conhecimento atual dos valores das a√ß√µes e busca maximizar a recompensa imediata, mas pode ignorar a√ß√µes que s√£o melhores a longo prazo. A explora√ß√£o, por outro lado, ao selecionar a√ß√µes que n√£o s√£o atualmente consideradas √≥timas, pode gerar recompensas mais baixas no curto prazo, mas pode levar a uma melhor compreens√£o dos valores das a√ß√µes, permitindo recompensas maiores no futuro. Assim, qualquer decis√£o sobre a√ß√£o envolve um balan√ßo entre essas duas estrat√©gias, o que configura um *trade-off*. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde o agente tem duas op√ß√µes: A√ß√£o A (explota√ß√£o) com um valor estimado de $Q_t(A)=5$ e A√ß√£o B (explora√ß√£o) com valor estimado de $Q_t(B) = 4$. No entanto, h√° uma incerteza maior sobre A√ß√£o B.
>
>   - **Explota√ß√£o:** Escolher a A√ß√£o A imediatamente daria uma recompensa estimada de 5.
>   - **Explora√ß√£o:** Escolher a A√ß√£o B pode resultar em uma recompensa de 3 (menor no curto prazo), mas a pr√≥xima recompensa pode ser 7 (maior do que a A√ß√£o A). Se A√ß√£o B for realmente muito boa, mas isso for desconhecido inicialmente, a explora√ß√£o permitir√° a descoberta dessa recompensa maior. O *trade-off* √© que a explota√ß√£o foca em ganhos imediatos, enquanto a explora√ß√£o pode levar a ganhos maiores a longo prazo.
>
> ```mermaid
> graph LR
>     subgraph "Trade-off Explora√ß√£o-Explota√ß√£o"
>         A("Agente")
>         B("Explota√ß√£o (a√ß√£o greedy)")
>         C("Explora√ß√£o (a√ß√£o n√£o-greedy)")
>
>         A --> B
>         A --> C
>         B --> D("Recompensa Imediata")
>         C --> E("Recompensa Futura (potencialmente maior)")
>         D --> F("Maximiza curto prazo")
>         E --> G("Maximiza longo prazo")
>         style D fill:#ccf,stroke:#333,stroke-width:2px
>         style E fill:#ccf,stroke:#333,stroke-width:2px
>     end
> ```

**Teorema 1 (Regret do k-armed Bandit):** Seja $q_*$ o valor da a√ß√£o √≥tima e $q_*(A_t)$ o valor da a√ß√£o escolhida no tempo *t*. O *regret* em um tempo *t* √© definido como $q_* - q_*(A_t)$. O *regret* total at√© o tempo *T* √© dado por $\sum_{t=1}^T [q_* - q_*(A_t)]$. Para um problema *k*-armed bandit com recompensas limitadas, o *regret* total √© geralmente sublinear em *T* para algoritmos de explora√ß√£o-explota√ß√£o bem projetados.

*Prova:* A prova formal do limite sublinear de *regret* requer a an√°lise de algoritmos espec√≠ficos de explora√ß√£o-explota√ß√£o, como $\epsilon$-greedy, *Upper Confidence Bound* (UCB) e *Thompson Sampling*. Esses algoritmos s√£o projetados para minimizar o *regret* acumulado ao longo do tempo, explorando as a√ß√µes de forma inteligente para convergir para as a√ß√µes √≥timas. O *regret* sublinear significa que a m√©dia do *regret* por passo de tempo tende a zero quando *T* tende a infinito, garantindo que, a longo prazo, o agente tende a escolher as melhores a√ß√µes.  $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere o exemplo de 3-armed bandit com $q_*(A_1) = 1$, $q_*(A_2) = 2$, e $q_*(A_3) = 0.5$. A a√ß√£o √≥tima √© a A√ß√£o 2 com valor 2. Se em um instante *t* o agente escolher a A√ß√£o 1, o *regret* nesse instante ser√° $2 - 1 = 1$. Se o agente escolher a a√ß√£o √≥tima (A√ß√£o 2), o *regret* ser√° $2-2 = 0$.
>
>   O *regret* acumulado at√© o tempo *T* √© a soma dos *regrets* em cada passo. Algoritmos eficientes de explora√ß√£o-explota√ß√£o tendem a ter um *regret* total que cresce menos do que linearmente com *T*, indicando que o agente aprende a escolher as melhores a√ß√µes ao longo do tempo e minimiza as perdas (regrets). Se o *regret* total for linear com o tempo, ent√£o o agente est√° continuamente escolhendo a√ß√µes sub√≥timas, perdendo recompensas.
>
>   Por exemplo, se *T = 1000*, e um algoritmo eficiente tiver um *regret* total de 500, o *regret* m√©dio por passo √© 0.5. Isso indica que, em m√©dia, o agente perde 0.5 de recompensa por passo em rela√ß√£o √† a√ß√£o √≥tima. Um algoritmo sub√≥timo pode ter um *regret* total de 5000, com *regret* m√©dio de 5 por passo, demonstrando que o agente n√£o est√° aprendendo a escolher as a√ß√µes √≥timas t√£o bem.
> ```mermaid
> graph LR
>    subgraph "Regret"
>         A("A√ß√£o √ìtima q_*")
>         B("A√ß√£o Escolhida no tempo t: q_*(A_t)")
>         C("Regret em t = q_* - q_*(A_t)")
>         D("Regret Total at√© T = Œ£[q_* - q_*(A_t)]")
>
>         A --> C
>         B --> C
>         C --> D
>
>         style C fill:#ccf,stroke:#333,stroke-width:2px
>         style D fill:#ccf,stroke:#333,stroke-width:2px
>    end
> ```

### Conclus√£o

Este cap√≠tulo introduziu o problema do *k*-armed bandit, destacando a import√¢ncia do balan√ßo entre explora√ß√£o e explota√ß√£o no aprendizado por refor√ßo. A explora√ß√£o permite que o agente descubra a√ß√µes potencialmente melhores, enquanto a explota√ß√£o maximiza a recompensa imediata com base no conhecimento atual. O desafio fundamental √© encontrar um equil√≠brio ideal entre essas duas estrat√©gias, que s√£o intrinsecamente conflitantes. As discuss√µes e an√°lises apresentadas estabelecem a base para a compreens√£o de m√©todos mais avan√ßados e para a abordagem do problema completo do aprendizado por refor√ßo.

### Refer√™ncias

[^1]: "The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classification, artificial neural networks, and system identification. In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken.
In this chapter we study the evaluative aspect of reinforcement learning in a simplified setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback differs from, and yet can be combined with, instructive feedback.
The particular nonassociative, evaluative feedback problem that we explore is a simple version of the k-armed bandit problem." *(Trecho de Multi-armed Bandits)*
[^2]: "Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.
This is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or ‚Äúone-armed bandit,‚Äù except that it has k levers instead of one. Each action selection is like a play of one of the slot machine's levers, and the rewards are the payoffs for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers. Another analogy is that of a doctor choosing between experimental treatments for a series of seriously ill patients. Each action is the selection of a treatment, and each reward is the survival or well-being of the patient. Today the term ‚Äúbandit problem" is sometimes used for a generalization of the problem described above, but in this book we use it to refer just to this simple case.
In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action. We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q‚àó(a), is the expected reward given that a is selected:
q*(a) = E[Rt | At=a].
If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time stept as Qt(a). We would like Qt(a) to be close to q*(a).
If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action's value. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. For example, suppose a greedy action's value is known with certainty, while several other actions are estimated to be nearly as good but with substantial uncertainty. The uncertainty is such that at least one of these other actions probably is actually better than the greedy action, but you don't know which one. If you have many time steps ahead on which to make action selections, then it may be better to explore the nongreedy actions and discover which of them are better than the greedy action. Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the ‚Äúconflict‚Äù between exploration and exploitation." *(Trecho de Multi-armed Bandits)*
[^3]: "However, most of these methods make strong assumptions about stationarity and prior knowledge that are either violated or impossible to verify in most applications and in the full reinforcement learning problem that we consider in subsequent chapters. The guarantees of optimality or bounded loss for these methods are of little comfort when the assumptions of their theory do not apply.
In this book we do not worry about balancing exploration and exploitation in a sophisticated way; we worry only about balancing them at all. In this chapter we present several simple balancing methods for the k-armed bandit problem and show that they work much better than methods that always exploit. The need to balance exploration and exploitation is a distinctive challenge that arises in reinforcement learning; the simplicity of our version of the k-armed bandit problem enables us to show this in a particularly clear form." *(Trecho de Multi-armed Bandits)*
[^4]: "We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received:
Qt(a) = sum of rewards when a taken prior to t / number of times a taken prior to t = sum_{i=1}^{t-1} R_i * 1_{A_i=a} / sum_{i=1}^{t-1} 1_{A_i=a}
where 1_{predicate} denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define Qt(a) as some default value, such as 0. As the denominator goes to infinity, by the law of large numbers, Qt(a) converges to q*(a)." *(Trecho de Multi-armed Bandits)*

The text now includes several numerical examples and visualizations to illustrate the concepts, enhancing the reader's understanding of the *k*-armed bandit problem and the exploration-exploitation dilemma.
