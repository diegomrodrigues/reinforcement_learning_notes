## Evaluative vs. Instructive Feedback in Reinforcement Learning

### Introdu√ß√£o
No campo do *reinforcement learning* (RL), a distin√ß√£o fundamental entre **feedback avaliativo** e **feedback instrutivo** √© crucial para entender a natureza do problema de aprendizado e as abordagens necess√°rias para resolv√™-lo [^1]. Este cap√≠tulo aprofunda essa diferencia√ß√£o, explorando suas implica√ß√µes e como cada tipo de feedback influencia as estrat√©gias de aprendizado. A compreens√£o dessas nuances √© essencial para o desenvolvimento de algoritmos de RL eficazes, especialmente no contexto de *k-armed bandit problems* e suas extens√µes [^1].

### Conceitos Fundamentais

O *reinforcement learning* se distingue de outras formas de aprendizado pelo tipo de informa√ß√£o de treinamento que utiliza [^1]. Em vez de receber instru√ß√µes diretas sobre qual a√ß√£o tomar, o agente de RL recebe **feedback avaliativo**, que indica a qualidade da a√ß√£o tomada, mas n√£o necessariamente se foi a melhor a√ß√£o poss√≠vel [^1].

> A caracter√≠stica mais importante que distingue o reinforcement learning de outros tipos de aprendizado √© que ele usa informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de instruir, fornecendo a√ß√µes corretas [^1].

Este aspecto introduz a necessidade de **explora√ß√£o ativa**, onde o agente deve explicitamente procurar por um bom comportamento, testando diferentes a√ß√µes e observando suas consequ√™ncias [^1].

> üí° **Exemplo Num√©rico:** Imagine um rob√¥ aprendendo a andar. O feedback avaliativo seria o rob√¥ perceber que caiu (recompensa negativa) ou que deu um passo bem-sucedido (recompensa positiva). N√£o h√° um "professor" dizendo qual junta mover e em qual √¢ngulo. O rob√¥ deve descobrir isso por conta pr√≥pria atrav√©s de tentativas e erros.

Em contraste, o **feedback instrutivo** indica diretamente a a√ß√£o correta a ser tomada, independentemente da a√ß√£o que foi realmente executada [^1]. Este tipo de feedback √© a base do *supervised learning*, que inclui √°reas como classifica√ß√£o de padr√µes, redes neurais artificiais e identifica√ß√£o de sistemas [^1].

> O feedback puramente instrutivo, por outro lado, indica a a√ß√£o correta a ser tomada, independentemente da a√ß√£o realmente tomada. Esse tipo de feedback √© a base do aprendizado supervisionado [^1].

> üí° **Exemplo Num√©rico:** No exemplo do rob√¥, o feedback instrutivo seria o "professor" mostrando exatamente qual sequ√™ncia de movimentos leva a um passo bem-sucedido.

Em suas formas puras, os dois tipos de feedback s√£o distintos:
*   O **feedback avaliativo** depende inteiramente da a√ß√£o tomada.
*   O **feedback instrutivo** √© independente da a√ß√£o tomada [^1].

No entanto, √© importante notar que em muitas aplica√ß√µes pr√°ticas, pode haver uma combina√ß√£o de ambos os tipos de feedback.  Por exemplo, um sistema de recomenda√ß√£o pode fornecer feedback avaliativo (o usu√°rio gostou ou n√£o do produto recomendado) e, em alguns casos, feedback instrutivo (o usu√°rio explicitamente informa que prefere um tipo diferente de produto).

**Feedback Avaliativo:**

*   Indica o qu√£o boa foi a a√ß√£o tomada.
*   Requer explora√ß√£o para descobrir a√ß√µes melhores.
*   Exemplo: Receber uma recompensa num√©rica ap√≥s selecionar um bra√ßo em um *k-armed bandit* [^1].

**Feedback Instrutivo:**

*   Indica a a√ß√£o correta a ser tomada.
*   N√£o requer explora√ß√£o, pois o agente √© diretamente instru√≠do.
*   Exemplo: Ser corrigido por um professor ap√≥s uma resposta incorreta [^1].

#### A Necessidade de Explora√ß√£o

A principal consequ√™ncia do uso de **feedback avaliativo** √© a necessidade de balancear **explora√ß√£o** e **explota√ß√£o** [^2]. O agente deve *explorar* o ambiente para descobrir novas a√ß√µes que podem ser melhores, mas tamb√©m deve *explorar* seu conhecimento atual para maximizar a recompensa imediata [^2]. Este *trade-off* √© fundamental para o sucesso do *reinforcement learning*.

> üí° **Exemplo Num√©rico:** Considere um agente jogando um jogo de v√≠deo-game.
> *   **Explora√ß√£o:** Tentar uma nova combina√ß√£o de bot√µes (que pode levar a uma pontua√ß√£o maior ou √† morte do personagem).
> *   **Explota√ß√£o:** Usar a combina√ß√£o de bot√µes que o agente j√° sabe que funciona bem (para maximizar a pontua√ß√£o atual).

#### Formaliza√ß√£o Matem√°tica

Seja $A_t$ a a√ß√£o selecionada no tempo $t$, e $R_t$ a recompensa correspondente [^2]. O valor de uma a√ß√£o $a$, denotado por $q_*(a)$, √© o retorno esperado dado que a a√ß√£o $a$ √© selecionada [^2]:

$$
q_*(a) = \mathbb{E}[R_t | A_t = a]
$$

No contexto de feedback avaliativo, o agente n√£o conhece $q_*(a)$ e deve estim√°-lo atrav√©s da intera√ß√£o com o ambiente.  Podemos tamb√©m definir o valor estimado da a√ß√£o $a$ no tempo $t$ como $Q_t(a)$. O objetivo do agente √© fazer com que $Q_t(a)$ convirja para $q_*(a)$ o mais r√°pido poss√≠vel.

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de 3-armed bandit, onde as recompensas esperadas para cada bra√ßo s√£o:
> *   Bra√ßo 1: $q_*(1) = 2$
> *   Bra√ßo 2: $q_*(2) = 5$
> *   Bra√ßo 3: $q_*(3) = 1$
>
> Inicialmente, o agente n√£o sabe esses valores. Ele deve estim√°-los com base nas recompensas que recebe ao puxar cada bra√ßo. Se o agente puxar o bra√ßo 1 tr√™s vezes e receber recompensas de 1, 3 e 2, sua estimativa inicial para $Q_3(1)$ seria a m√©dia:
>
> $Q_3(1) = (1 + 3 + 2) / 3 = 2$.
>
> O objetivo √© que, ap√≥s muitas intera√ß√µes, $Q_t(1)$, $Q_t(2)$ e $Q_t(3)$ convirjam para 2, 5 e 1, respectivamente.



![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

#### Lemma 1: Trade-off Explora√ß√£o-Explota√ß√£o
Se um agente sempre explora, ele pode descobrir as melhores a√ß√µes, mas pode n√£o obter muitas recompensas durante o processo de explora√ß√£o. Se um agente sempre explota, ele pode se fixar em a√ß√µes sub√≥timas e perder oportunidades de descobrir a√ß√µes melhores [^2].

$\blacksquare$

#### Corol√°rio 1: Estrat√©gias de Balanceamento
Estrat√©gias eficazes de *reinforcement learning* devem equilibrar a explora√ß√£o e a explota√ß√£o para alcan√ßar um desempenho √≥timo a longo prazo [^2].

**Teorema 1:** Converg√™ncia do Valor Estimado sob Explora√ß√£o Suficiente

Se todas as a√ß√µes s√£o exploradas infinitas vezes, ent√£o $Q_t(a)$ converge para $q_*(a)$ para todo $a$ com probabilidade 1, assumindo que a recompensa esperada √© estacion√°ria.

*Proof strategy:* Este resultado √© uma consequ√™ncia direta da lei forte dos grandes n√∫meros. Se cada a√ß√£o √© amostrada infinitas vezes, a m√©dia amostral das recompensas associadas a cada a√ß√£o converge para a recompensa esperada dessa a√ß√£o.

**Prova do Teorema 1:**

Provaremos que se todas as a√ß√µes s√£o exploradas infinitas vezes, ent√£o $Q_t(a)$ converge para $q_*(a)$ para todo $a$ com probabilidade 1, assumindo que a recompensa esperada √© estacion√°ria.

I.  **Defini√ß√£o:** Seja $N_t(a)$ o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© o tempo $t$. Por hip√≥tese, $N_t(a) \rightarrow \infty$ quando $t \rightarrow \infty$ para todo $a$.

II. **Estimativa do valor da a√ß√£o:** O valor estimado $Q_t(a)$ √© a m√©dia amostral das recompensas obtidas ao selecionar a a√ß√£o $a$ at√© o tempo $t$:

$$Q_t(a) = \frac{\sum_{i=1}^{t} R_i \cdot \mathbb{I}(A_i = a)}{N_t(a)}$$
onde $\mathbb{I}(A_i = a)$ √© a fun√ß√£o indicadora que vale 1 se a a√ß√£o $a$ foi selecionada no tempo $i$ e 0 caso contr√°rio.

III. **Lei Forte dos Grandes N√∫meros:** A Lei Forte dos Grandes N√∫meros afirma que, para uma sequ√™ncia de vari√°veis aleat√≥rias independentes e identicamente distribu√≠das $X_1, X_2, \ldots$ com m√©dia $\mu$, a m√©dia amostral converge para a m√©dia verdadeira com probabilidade 1:

$$\frac{1}{n} \sum_{i=1}^{n} X_i \rightarrow \mu \quad \text{com probabilidade 1, quando } n \rightarrow \infty$$

IV. **Aplica√ß√£o da Lei Forte dos Grandes N√∫meros:** No nosso caso, as recompensas $R_i$ obtidas ao selecionar a a√ß√£o $a$ s√£o vari√°veis aleat√≥rias (assumindo que o ambiente √© estoc√°stico).  Como $N_t(a) \rightarrow \infty$ quando $t \rightarrow \infty$, podemos aplicar a Lei Forte dos Grandes N√∫meros √† estimativa $Q_t(a)$:

$$Q_t(a) = \frac{\sum_{i=1}^{t} R_i \cdot \mathbb{I}(A_i = a)}{N_t(a)} \rightarrow \mathbb{E}[R_t | A_t = a] = q_*(a) \quad \text{com probabilidade 1, quando } t \rightarrow \infty$$

V.  **Conclus√£o:** Portanto, o valor estimado $Q_t(a)$ converge para o valor verdadeiro $q_*(a)$ com probabilidade 1 quando todas as a√ß√µes s√£o exploradas infinitas vezes. ‚ñ†

Este teorema garante que, eventualmente, o agente aprender√° o valor verdadeiro de cada a√ß√£o, desde que explore o suficiente.  No entanto, a taxa de converg√™ncia e a recompensa acumulada durante o processo de aprendizado dependem da estrat√©gia de explora√ß√£o utilizada.

> üí° **Exemplo Num√©rico:** Suponha que em um problema de 2-armed bandit, o agente use uma estrat√©gia $\epsilon$-greedy com $\epsilon = 0.1$. Isso significa que 10% das vezes o agente escolhe uma a√ß√£o aleatoriamente (explora√ß√£o), e 90% das vezes escolhe a a√ß√£o com a melhor estimativa atual (explota√ß√£o). Se, ap√≥s 1000 passos, o agente tiver puxado cada bra√ßo cerca de 500 vezes e as estimativas $Q_t(1)$ e $Q_t(2)$ estiverem pr√≥ximas de $q_*(1)$ e $q_*(2)$, respectivamente, o agente estar√° convergindo para a solu√ß√£o √≥tima. No entanto, o $\epsilon$ garante que o agente continue explorando mesmo ap√≥s ter uma boa estimativa das recompensas, prevenindo que fique preso em um √≥timo local.



![Pseudoc√≥digo de um algoritmo de bandit simples com estrat√©gia Œµ-greedy para explora√ß√£o e explota√ß√£o.](./../images/image4.png)

### Conclus√£o
A distin√ß√£o entre **feedback avaliativo** e **feedback instrutivo** √© fundamental para o *reinforcement learning*. A presen√ßa de feedback avaliativo introduz a necessidade de explora√ß√£o ativa e estrat√©gias de balanceamento. A compreens√£o das nuances desses conceitos √© crucial para o desenvolvimento de algoritmos de RL eficazes em uma variedade de problemas. Nos cap√≠tulos subsequentes, m√©todos para lidar com este *trade-off* ser√£o explorados em detalhes.

### Refer√™ncias
[^1]: Chapter 2: Multi-armed Bandits
[^2]: Section 2.1: A k-armed Bandit Problem
<!-- END -->