## Associative Bandits: Bridging the Gap to Full Reinforcement Learning

### Introdu√ß√£o
O estudo dos *$k$*-armed bandits fornece uma base fundamental para entender os desafios de *exploration* e *exploitation* em *reinforcement learning* (RL) [^1]. Os m√©todos explorados em cen√°rios *n√£o-associativos*, onde o objetivo √© identificar uma √∫nica a√ß√£o √≥tima, preparam o terreno para problemas mais complexos [^1]. Este cap√≠tulo expande sobre essa base, abordando a transi√ß√£o crucial para problemas *associativos* (ou *contextuais*), onde a a√ß√£o √≥tima depende do estado ou contexto atual [^17]. Essa mudan√ßa √© um passo essencial em dire√ß√£o ao problema completo de RL, pois exige aprender uma *policy*, ou seja, um mapeamento de estados para a√ß√µes [^17].

### Conceitos Fundamentais

#### A Natureza dos Problemas Associativos
Em problemas *n√£o-associativos*, como o *$k$*-armed bandit tradicional, o agente est√° em uma √∫nica situa√ß√£o e busca a melhor a√ß√£o nessa situa√ß√£o [^1]. Em contraste, problemas *associativos* envolvem m√∫ltiplas situa√ß√µes, exigindo que o agente aprenda qual a√ß√£o √© mais apropriada para cada uma [^17]. Essa capacidade de associar a√ß√µes a contextos espec√≠ficos √© a ess√™ncia de uma *policy* em RL [^17].

Um exemplo ilustrativo √© apresentado em [^17]: imagine v√°rios *$k$*-armed bandit tasks diferentes, cada um selecionado aleatoriamente a cada passo. Se as probabilidades de sele√ß√£o de cada tarefa permanecerem constantes ao longo do tempo, o problema pode ser tratado como um √∫nico *$k$*-armed bandit estacion√°rio [^17]. No entanto, se o agente receber uma *clue* sobre a identidade da tarefa atual (por exemplo, a cor da slot machine), ele poder√° aprender uma *policy* que associa cada tarefa (cor) √† a√ß√£o apropriada [^17].

#### Formaliza√ß√£o do Problema Associativo
Matematicamente, em um problema *$k$*-armed bandit *associativo*, o agente observa um estado $s_t \in S$ no tempo *t*, onde $S$ √© o espa√ßo de estados. Com base nesse estado, o agente seleciona uma a√ß√£o $A_t \in A$, onde $A$ √© o conjunto de a√ß√µes. A transi√ß√£o para o pr√≥ximo estado e a recompensa $R_t$ dependem tanto do estado atual quanto da a√ß√£o selecionada. O objetivo √© aprender uma *policy* $\pi(a|s)$ que maximize a recompensa acumulada ao longo do tempo, ou seja, o agente aprende uma fun√ß√£o que mapeia cada estado para uma distribui√ß√£o de probabilidade sobre as a√ß√µes.

Para formalizar ainda mais, podemos definir a fun√ß√£o de valor $q(s, a)$ como a recompensa esperada ao tomar a a√ß√£o *a* no estado *s*:

$$q(s, a) = \mathbb{E}[R_t | s_t = s, A_t = a]$$

O objetivo do agente √© encontrar a *policy* √≥tima $\pi^*(a|s)$ que maximize a fun√ß√£o de valor em todos os estados:

$$\pi^*(a|s) = \underset{\pi}{\mathrm{argmax}} \mathbb{E}[R_t | s_t = s, A_t = a \sim \pi(a|s)]$$

> üí° **Exemplo Num√©rico:**
>
> Considere um problema com dois estados ($S = \{s_1, s_2\}$) e duas a√ß√µes ($A = \{a_1, a_2\}$). As recompensas esperadas para cada estado-a√ß√£o s√£o:
>
> *   $q(s_1, a_1) = 2$
> *   $q(s_1, a_2) = 1$
> *   $q(s_2, a_1) = 0$
> *   $q(s_2, a_2) = 3$
>
> A *policy* √≥tima $\pi^*(a|s)$ seria:
>
> *   $\pi^*(a_1|s_1) = 1$ (sempre escolher $a_1$ no estado $s_1$)
> *   $\pi^*(a_2|s_2) = 1$ (sempre escolher $a_2$ no estado $s_2$)
>
> Isso porque $a_1$ fornece a maior recompensa esperada em $s_1$, e $a_2$ fornece a maior recompensa esperada em $s_2$.
>
> ```mermaid
> graph LR
>     s1((s1)) --> a1[a1]
>     s1 --> a2[a2]
>     s2((s2)) --> a1
>     s2 --> a2
>     a1 -- q(s1, a1) = 2 --> r1((Recompensa))
>     a2 -- q(s1, a2) = 1 --> r2((Recompensa))
>     a1 -- q(s2, a1) = 0 --> r3((Recompensa))
>     a2 -- q(s2, a2) = 3 --> r4((Recompensa))
>     style s1 fill:#f9f,stroke:#333,stroke-width:2px
>     style s2 fill:#f9f,stroke:#333,stroke-width:2px
> ```

#### M√©todos para Problemas Associativos

A transi√ß√£o para problemas associativos exige a adapta√ß√£o dos m√©todos desenvolvidos para o *$k$*-armed bandit tradicional. Algumas abordagens comuns incluem:

1.  **Tabelas de Lookup:** A forma mais simples de abordar problemas associativos √© criar uma tabela separada de *action values* $Q(s, a)$ para cada estado *s* e a√ß√£o *a*. Isso permite que o agente aprenda uma estimativa da recompensa esperada para cada a√ß√£o em cada estado. A sele√ß√£o de a√ß√µes pode ent√£o ser feita usando m√©todos como *Œµ-greedy* ou *UCB* [^3, 11], aplicados a cada estado individualmente.

![Pseudoc√≥digo de um algoritmo de bandit simples com estrat√©gia Œµ-greedy para explora√ß√£o e explota√ß√£o.](./../images/image4.png)

**Lema 1:** *Em um ambiente estacion√°rio, com um n√∫mero finito de estados e a√ß√µes, o m√©todo de tabelas de lookup, combinado com a explora√ß√£o Œµ-greedy, converge para a policy √≥tima com probabilidade 1, desde que cada par estado-a√ß√£o seja visitado infinitas vezes.*

*Prova (Esbo√ßo):*  Como o ambiente √© estacion√°rio, as estimativas de $Q(s, a)$ convergem para $q(s, a)$ para cada estado-a√ß√£o visitado infinitas vezes. Com Œµ-greedy, existe uma probabilidade n√£o nula de explorar qualquer a√ß√£o em qualquer estado. Portanto, eventualmente, cada par estado-a√ß√£o ser√° explorado suficientemente para que $Q(s, a)$ converja para $q(s, a)$. Quando todas as estimativas $Q(s, a)$ estiverem pr√≥ximas o suficiente de seus respectivos $q(s, a)$, a pol√≠tica Œµ-greedy se aproximar√° da pol√≠tica √≥tima.

*Prova (Detalhada):*

I. **Defini√ß√µes:** Seja $Q_t(s, a)$ a estimativa do valor de tomar a a√ß√£o $a$ no estado $s$ no tempo $t$. Seja $q(s, a)$ o verdadeiro valor esperado de tomar a a√ß√£o $a$ no estado $s$. Seja $\pi_t(s)$ a pol√≠tica no tempo $t$, que especifica a probabilidade de selecionar cada a√ß√£o no estado $s$.

II. **Atualiza√ß√£o das Estimativas:** Com o m√©todo de tabelas de lookup, as estimativas de valor s√£o atualizadas usando a seguinte regra:
    $Q_{t+1}(s, a) = Q_t(s, a) + \alpha [R_t - Q_t(s, a)]$ se $s_t = s$ e $A_t = a$, onde $\alpha$ √© o *step-size parameter*.

III. **Converg√™ncia das Estimativas:** Em um ambiente estacion√°rio, com $\alpha$ satisfazendo as condi√ß√µes de Robbins-Monro ($\sum_{t=1}^{\infty} \alpha_t = \infty$ e $\sum_{t=1}^{\infty} \alpha_t^2 < \infty$), as estimativas $Q_t(s, a)$ convergem para os verdadeiros valores $q(s, a)$ com probabilidade 1.  Um exemplo comum √© usar $\alpha_t = \frac{1}{N(s, a)}$, onde $N(s, a)$ √© o n√∫mero de vezes que o par estado-a√ß√£o $(s, a)$ foi visitado.

IV. **Explora√ß√£o Œµ-greedy:** A pol√≠tica Œµ-greedy seleciona a a√ß√£o com a maior estimativa de valor com probabilidade $1 - \epsilon$, e seleciona uma a√ß√£o aleat√≥ria com probabilidade $\epsilon$. Isso garante que todas as a√ß√µes sejam exploradas infinitas vezes, desde que $\epsilon > 0$.

V. **Converg√™ncia para a Pol√≠tica √ìtima:** √Ä medida que as estimativas $Q_t(s, a)$ convergem para $q(s, a)$, a probabilidade de selecionar a a√ß√£o √≥tima (i.e., a a√ß√£o com o maior valor esperado) aumenta.  Como cada par estado-a√ß√£o √© visitado infinitas vezes (devido √† explora√ß√£o Œµ-greedy), as estimativas convergem para os valores verdadeiros. Eventualmente, a pol√≠tica Œµ-greedy se aproximar√° da pol√≠tica que sempre seleciona a a√ß√£o com o maior valor esperado (com probabilidade $1-\epsilon$), resultando em uma pol√≠tica aproximadamente √≥tima.

VI. **Pol√≠tica √ìtima com Probabilidade 1:** Para garantir converg√™ncia para a pol√≠tica √≥tima *exata* com probabilidade 1, podemos usar um esquema de *annealing* para $\epsilon$, onde $\epsilon$ diminui gradualmente para 0 ao longo do tempo, satisfazendo as condi√ß√µes $\sum_{t=1}^{\infty} \epsilon_t = \infty$ e $\sum_{t=1}^{\infty} \epsilon_t^2 < \infty$. Neste caso, a explora√ß√£o eventualmente cessa, e o agente sempre seleciona a a√ß√£o com a maior estimativa de valor, que converge para a a√ß√£o √≥tima. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior com dois estados e duas a√ß√µes, vamos simular o aprendizado com tabelas de lookup e $\epsilon$-greedy.
>
> Inicializamos $Q(s, a)$ aleatoriamente, por exemplo:
>
> *   $Q(s_1, a_1) = 0.5$
> *   $Q(s_1, a_2) = 0.2$
> *   $Q(s_2, a_1) = 0.8$
> *   $Q(s_2, a_2) = 0.1$
>
> Definimos $\epsilon = 0.1$ e $\alpha = 0.1$.
>
> Suponha que o agente observe o estado $s_1$ e, usando $\epsilon$-greedy, decide explorar e seleciona a a√ß√£o $a_2$. Recebe uma recompensa $R_t = 1$ (que √© amostrada da distribui√ß√£o de recompensas para $q(s_1, a_2) = 1$).
>
> A atualiza√ß√£o de $Q(s_1, a_2)$ seria:
>
> $Q(s_1, a_2) = 0.2 + 0.1 * (1 - 0.2) = 0.2 + 0.08 = 0.28$
>
> Ap√≥s v√°rias itera√ß√µes, $Q(s, a)$ se aproximar√° dos valores verdadeiros $q(s, a)$.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Inicializa√ß√£o
> Q = np.zeros((2, 2))  # 2 estados, 2 a√ß√µes
> alpha = 0.1
> epsilon = 0.1
> num_episodes = 1000
>
> # Valores verdadeiros (usados apenas para simula√ß√£o)
> q_true = np.array([[2, 1], [0, 3]])
>
> # Simula√ß√£o
> rewards_per_episode = []
>
> for episode in range(num_episodes):
>     # Escolhe um estado aleatoriamente
>     state = np.random.choice([0, 1])  # s1=0, s2=1
>
>     # Epsilon-greedy action selection
>     if np.random.rand() < epsilon:
>         action = np.random.choice([0, 1])
>     else:
>         action = np.argmax(Q[state])
>
>     # Simula a recompensa
>     reward = np.random.normal(q_true[state, action], 1) # Recompensa com ru√≠do
>
>     # Atualiza Q-table
>     Q[state, action] = Q[state, action] + alpha * (reward - Q[state, action])
>
>     rewards_per_episode.append(reward)
>
> # Plota a m√©dia das recompensas ao longo do tempo
> window = 50 # Smoothing window
> smoothed_rewards = np.convolve(rewards_per_episode, np.ones(window)/window, mode='valid')
>
> plt.figure(figsize=(10, 6))
> plt.plot(smoothed_rewards)
> plt.title('Recompensa M√©dia ao Longo do Tempo (Œµ-greedy)')
> plt.xlabel('Epis√≥dio')
> plt.ylabel('Recompensa M√©dia (Janela de {} Epis√≥dios)'.format(window))
> plt.grid(True)
> plt.show()
>
> print("Q-table aprendida:")
> print(Q)
> print("Valores verdadeiros:")
> print(q_true)
> ```
>
> Este exemplo demonstra como a Q-table se atualiza a cada epis√≥dio, convergindo gradualmente para os valores verdadeiros. A plotagem mostra o aumento da recompensa m√©dia ao longo do tempo, indicando o aprendizado da pol√≠tica.

2.  **Fun√ß√µes de Generaliza√ß√£o:** Em problemas com um grande n√∫mero de estados, manter uma tabela separada para cada um pode ser impratic√°vel. Nesses casos, √© necess√°rio usar fun√ß√µes de generaliza√ß√£o para estimar os *action values* com base em caracter√≠sticas dos estados. Isso pode ser feito usando m√©todos como redes neurais artificiais, fun√ß√µes lineares ou √°rvores de decis√£o.

Para expandir sobre fun√ß√µes de generaliza√ß√£o, considere que o estado $s$ pode ser representado por um vetor de caracter√≠sticas $\mathbf{x}(s) \in \mathbb{R}^d$. Podemos ent√£o aproximar a fun√ß√£o de valor $q(s, a)$ usando uma fun√ß√£o parametrizada $q(s, a, \mathbf{w})$, onde $\mathbf{w}$ √© um vetor de pesos. O objetivo √© aprender os pesos $\mathbf{w}$ que minimizem o erro entre a aproxima√ß√£o $q(s, a, \mathbf{w})$ e a verdadeira fun√ß√£o de valor $q(s, a)$.

**Teorema 1:** *Se a fun√ß√£o de valor $q(s, a)$ puder ser expressa exatamente pela fun√ß√£o de generaliza√ß√£o $q(s, a, \mathbf{w})$ para algum $\mathbf{w}$, e o algoritmo de aprendizado (e.g., gradiente descendente) convergir para um m√≠nimo global do erro quadr√°tico m√©dio, ent√£o a policy resultante ser√° √≥tima.*

*Prova (Esbo√ßo):* Se a fun√ß√£o de valor pode ser representada exatamente e o algoritmo encontra o m√≠nimo global, ent√£o a fun√ß√£o aprendida $q(s, a, \mathbf{w})$ ser√° igual a $q(s, a)$. Uma vez que temos a fun√ß√£o de valor correta, podemos derivar a pol√≠tica √≥tima escolhendo a a√ß√£o que maximiza $q(s, a)$ em cada estado.

*Prova (Detalhada):*

I. **Hip√≥tese:** Assumimos que existe um vetor de pesos $\mathbf{w}^*$ tal que $q(s, a) = q(s, a, \mathbf{w}^*)$ para todo $s \in S$ e $a \in A$. Isso significa que a fun√ß√£o de generaliza√ß√£o tem capacidade suficiente para representar a verdadeira fun√ß√£o de valor.

II. **Otimiza√ß√£o:** O algoritmo de aprendizado (e.g., gradiente descendente) busca encontrar o vetor de pesos $\mathbf{w}$ que minimize o erro quadr√°tico m√©dio:
    $J(\mathbf{w}) = \mathbb{E}[(q(s, a) - q(s, a, \mathbf{w}))^2]$

III. **M√≠nimo Global:** Assumimos que o algoritmo de aprendizado converge para um m√≠nimo global $\mathbf{w}^*$ de $J(\mathbf{w})$. Isso significa que, no limite, $\mathbf{w} = \mathbf{w}^*$.

IV. **Fun√ß√£o de Valor √ìtima:** Como $\mathbf{w} = \mathbf{w}^*$, temos que $q(s, a, \mathbf{w}) = q(s, a, \mathbf{w}^*) = q(s, a)$ para todo $s \in S$ e $a \in A$. Portanto, a fun√ß√£o de valor aprendida √© igual √† fun√ß√£o de valor verdadeira.

V. **Pol√≠tica √ìtima:** A pol√≠tica √≥tima $\pi^*(s)$ √© definida como a a√ß√£o que maximiza a fun√ß√£o de valor em cada estado:
    $\pi^*(s) = \underset{a \in A}{\mathrm{argmax}} \, q(s, a)$

VI. **Pol√≠tica Aprendida:** Como $q(s, a, \mathbf{w}) = q(s, a)$, a pol√≠tica aprendida usando a fun√ß√£o de valor aproximada √©:
     $\pi(s) = \underset{a \in A}{\mathrm{argmax}} \, q(s, a, \mathbf{w}) = \underset{a \in A}{\mathrm{argmax}} \, q(s, a) = \pi^*(s)$

VII. **Conclus√£o:** Portanto, a pol√≠tica aprendida $\pi(s)$ √© igual √† pol√≠tica √≥tima $\pi^*(s)$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um problema com dois estados, onde cada estado √© representado por um vetor de caracter√≠sticas de duas dimens√µes:
>
> *   $s_1 = [1, 0]$
> *   $s_2 = [0, 1]$
>
> Temos duas a√ß√µes: $a_1$ e $a_2$. Usaremos uma fun√ß√£o linear para aproximar a fun√ß√£o de valor:
>
> $q(s, a, \mathbf{w}) = \mathbf{x}(s)^T \mathbf{w}_a$
>
> onde $\mathbf{w}_a$ √© o vetor de pesos para a a√ß√£o $a$.  Suponha que inicializamos os pesos como:
>
> *   $\mathbf{w}_{a_1} = [0.1, 0.2]$
> *   $\mathbf{w}_{a_2} = [0.3, 0.4]$
>
> Ent√£o, as estimativas iniciais dos valores das a√ß√µes s√£o:
>
> *   $q(s_1, a_1, \mathbf{w}) = [1, 0] \cdot [0.1, 0.2] = 0.1$
> *   $q(s_1, a_2, \mathbf{w}) = [1, 0] \cdot [0.3, 0.4] = 0.3$
> *   $q(s_2, a_1, \mathbf{w}) = [0, 1] \cdot [0.1, 0.2] = 0.2$
> *   $q(s_2, a_2, \mathbf{w}) = [0, 1] \cdot [0.3, 0.4] = 0.4$
>
> Agora, suponha que estamos no estado $s_1$ e escolhemos a a√ß√£o $a_2$ (por exemplo, usando $\epsilon$-greedy). Recebemos uma recompensa $R_t = 2$.  Podemos usar o gradiente descendente para atualizar os pesos $\mathbf{w}_{a_2}$:
>
> $\mathbf{w}_{a_2} = \mathbf{w}_{a_2} + \alpha (R_t - q(s_1, a_2, \mathbf{w})) \mathbf{x}(s_1)$
>
> Com $\alpha = 0.1$:
>
> $\mathbf{w}_{a_2} = [0.3, 0.4] + 0.1 (2 - 0.3) [1, 0] = [0.3, 0.4] + 0.17 [1, 0] = [0.47, 0.4]$
>
> As outras a√ß√µes n√£o s√£o atualizadas neste passo.  Este processo √© repetido muitas vezes, eventualmente convergindo para os pesos √≥timos.
>
> Este exemplo demonstra como a fun√ß√£o linear se ajusta para aproximar os valores verdadeiros das a√ß√µes, resultando em um aprendizado da pol√≠tica.

3.  **Gradient Bandit com Caracter√≠sticas:** Os algoritmos *gradient bandit* [^13] podem ser estendidos para problemas associativos incorporando caracter√≠sticas do estado na fun√ß√£o de prefer√™ncia. A prefer√™ncia para cada a√ß√£o √© ent√£o uma fun√ß√£o das caracter√≠sticas do estado, permitindo que o algoritmo generalize entre estados semelhantes.

![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 bra√ßos.](./../images/image1.png)

#### O Desafio da N√£o-Estacionaridade

Em muitos problemas associativos, o ambiente pode ser *n√£o-estacion√°rio*, ou seja, as recompensas associadas a cada a√ß√£o podem mudar ao longo do tempo [^8]. Isso introduz um desafio adicional, pois o agente precisa ser capaz de acompanhar essas mudan√ßas e adaptar sua *policy* de acordo. M√©todos como o uso de um *constant step-size parameter* $\alpha$ [^8] podem ajudar a garantir que o agente continue aprendendo e se adaptando ao ambiente em evolu√ß√£o.

Al√©m do *constant step-size parameter*, outras t√©cnicas podem ser utilizadas para lidar com a n√£o-estacionaridade. Uma delas √© o uso de *discounting factors* ($\gamma$) para dar mais peso √†s recompensas recentes e menos peso √†s recompensas antigas. Outra t√©cnica √© o uso de algoritmos de *change detection* para detectar mudan√ßas no ambiente e reiniciar o processo de aprendizado quando uma mudan√ßa √© detectada.

**Proposi√ß√£o 1:** *Em um ambiente n√£o-estacion√°rio, o uso de um constant step-size parameter Œ± garante que as estimativas de valor reajam mais rapidamente a mudan√ßas no ambiente, mas tamb√©m introduz um vi√©s maior nas estimativas.*

*Prova (Esbo√ßo):* Um constant step-size parameter atribui um peso fixo a cada nova amostra observada, o que significa que as informa√ß√µes antigas s√£o gradualmente esquecidas. Isso permite que o agente se adapte rapidamente a mudan√ßas no ambiente. No entanto, o uso de um peso fixo tamb√©m significa que as estimativas de valor s√£o mais suscet√≠veis a flutua√ß√µes aleat√≥rias e ru√≠do no ambiente, resultando em um vi√©s maior.

*Prova (Detalhada):*

I. **Atualiza√ß√£o com Constant Step-Size:** A atualiza√ß√£o da estimativa de valor para uma a√ß√£o $a$ no tempo $t+1$ √© dada por:
   $Q_{t+1}(a) = Q_t(a) + \alpha [R_{t+1} - Q_t(a)]$

II. **Rea√ß√£o R√°pida:** Um valor constante de $\alpha$ significa que cada nova recompensa $R_{t+1}$ tem um impacto fixo na estimativa $Q_{t+1}(a)$. Isso permite que a estimativa se ajuste rapidamente a mudan√ßas nos valores reais das a√ß√µes. Se o ambiente mudar repentinamente, a estimativa $Q_{t+1}(a)$ se mover√° rapidamente em dire√ß√£o ao novo valor.

III. **Vi√©s:** No entanto, como $\alpha$ √© constante, a estimativa $Q_{t+1}(a)$ √© sempre influenciada pela recompensa mais recente $R_{t+1}$. Isso significa que a estimativa √© suscet√≠vel a ru√≠do e flutua√ß√µes aleat√≥rias.  Se a recompensa $R_{t+1}$ for atipicamente alta ou baixa devido ao acaso, a estimativa $Q_{t+1}(a)$ ser√° influenciada de forma desproporcional.  Este efeito √© ampliado em ambientes n√£o-estacion√°rios, onde os verdadeiros valores das a√ß√µes podem estar mudando continuamente, dificultando a obten√ß√£o de uma estimativa precisa.

IV. **Formaliza√ß√£o do Vi√©s:** Para ver isso formalmente, podemos reescrever a atualiza√ß√£o recursiva como uma m√©dia ponderada das recompensas passadas:
    $Q_{t+1}(a) = (1-\alpha)^{t+1} Q_0(a) + \sum_{i=1}^{t+1} \alpha (1-\alpha)^{t+1-i} R_i$
    onde $Q_0(a)$ √© a estimativa inicial.  Perceba que as recompensas mais recentes ($R_{t+1}$, $R_t$, \ldots) t√™m pesos maiores do que as recompensas mais antigas. Isso significa que o valor estimado √© mais sens√≠vel a recompensas recentes e, portanto, mais suscet√≠vel a vi√©s devido a ru√≠do recente.

V. **Compromisso:** Existe um compromisso entre a velocidade de adapta√ß√£o (controlada por $\alpha$) e o vi√©s das estimativas. Um valor maior de $\alpha$ permite uma adapta√ß√£o mais r√°pida, mas tamb√©m aumenta o vi√©s. Um valor menor de $\alpha$ reduz o vi√©s, mas torna a adapta√ß√£o mais lenta. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um ambiente n√£o-estacion√°rio onde a recompensa esperada para a a√ß√£o $a_1$ no estado $s_1$ muda ao longo do tempo. Inicialmente, $q(s_1, a_1) = 1$. Ap√≥s 500 passos, muda repentinamente para $q(s_1, a_1) = 5$.
>
> Podemos comparar o desempenho de dois agentes: um com $\alpha = 0.1$ (alta taxa de aprendizado) e outro com $\alpha = 0.01$ (baixa taxa de aprendizado).
>
> O agente com $\alpha = 0.1$ se adaptar√° mais rapidamente √† mudan√ßa, mas suas estimativas ser√£o mais ruidosas. O agente com $\alpha = 0.01$ se adaptar√° mais lentamente, mas suas estimativas ser√£o mais est√°veis.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Inicializa√ß√£o
> alpha1 = 0.1 # Alta taxa de aprendizado
> alpha2 = 0.01 # Baixa taxa de aprendizado
> num_episodes = 1000
>
> # Inicializa√ß√£o das estimativas de valor
> Q1 = 0
> Q2 = 0
>
> # Recompensa verdadeira (n√£o-estacion√°ria)
> def true_reward(episode):
>     if episode < 500:
>         return 1
>     else:
>         return 5
>
> # Simula√ß√£o
> rewards1 = []
> rewards2 = []
>
> for episode in range(num_episodes):
>     # Atualiza as estimativas de valor para cada agente
>     reward = true_reward(episode)
>     Q1 = Q1 + alpha1 * (reward - Q1)
>     Q2 = Q2 + alpha2 * (reward - Q2)
>
>     rewards1.append(Q1) # Armazena as estimativas
>     rewards2.append(Q2)
>
> # Plota as estimativas de valor ao longo do tempo
> plt.figure(figsize=(10, 6))
> plt.plot(rewards1, label='alpha = 0.1')
> plt.plot(rewards2, label='alpha = 0.01')
> plt.axvline(x=500, color='r', linestyle='--', label='Mudan√ßa no Ambiente')
> plt.title('Estimativas de Valor em Ambiente N√£o-Estacion√°rio')
> plt.xlabel('Epis√≥dio')
> plt.ylabel('Estimativa de Valor')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```
>
> A plotagem mostrar√° que o agente com $\alpha = 0.1$ se aproxima mais rapidamente do novo valor de recompensa ap√≥s a mudan√ßa no ambiente, mas tamb√©m apresenta maiores flutua√ß√µes nas estimativas antes e depois da mudan√ßa.

### Conclus√£o

A transi√ß√£o dos problemas *n√£o-associativos* para os *associativos* √© um passo cr√≠tico para abordar o problema completo de *reinforcement learning* [^17]. Ao aprender a associar a√ß√µes a estados espec√≠ficos, os agentes podem desenvolver *policies* que maximizam a recompensa em ambientes complexos e din√¢micos [^17]. Os m√©todos desenvolvidos para o *$k$*-armed bandit fornecem uma base s√≥lida para lidar com os desafios de *exploration* e *exploitation* nesses ambientes mais ricos, e a compreens√£o da transi√ß√£o para problemas associativos √© essencial para o avan√ßo na √°rea de *reinforcement learning*.

### Refer√™ncias
[^1]: Cap√≠tulo 2, Introdu√ß√£o
[^3]: Se√ß√£o 2.2, Action-value Methods
[^8]: Se√ß√£o 2.5, Tracking a Nonstationary Problem
[^11]: Se√ß√£o 2.7, Upper-Confidence-Bound Action Selection
[^13]: Se√ß√£o 2.8, Gradient Bandit Algorithms
[^17]: Se√ß√£o 2.9, Associative Search (Contextual Bandits)
<!-- END -->