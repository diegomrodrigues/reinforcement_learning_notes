## Multi-Armed Bandits: A Stepping Stone to Reinforcement Learning

### Introdu√ß√£o
O aprendizado por refor√ßo (RL) se distingue de outros paradigmas de aprendizado pelo uso de informa√ß√µes de treinamento que *avaliam* as a√ß√µes tomadas, em vez de *instruir* fornecendo a√ß√µes corretas [^1]. Essa caracter√≠stica central impulsiona a necessidade de explora√ß√£o ativa, buscando explicitamente por comportamentos √≥timos. O problema do **k-armed bandit** serve como um ambiente simplificado para estudar esse aspecto avaliativo do RL [^1]. Este cap√≠tulo se concentra nessa formula√ß√£o mais simples, oferecendo uma base s√≥lida para abordar os desafios mais complexos inerentes ao RL completo. Ao evitar o aprendizado de a√ß√µes em m√∫ltiplos contextos, o k-armed bandit permite uma an√°lise clara dos m√©todos fundamentais que podem ser estendidos para resolver problemas mais complexos [^1].

### Conceitos Fundamentais
No cerne do problema do k-armed bandit reside a repetida escolha entre *k* op√ß√µes distintas, ou **a√ß√µes** [^2]. Ap√≥s cada escolha, um agente recebe uma *recompensa num√©rica* retirada de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o selecionada [^2]. O objetivo primordial √© maximizar a recompensa total esperada ao longo de um determinado per√≠odo de tempo, como 1000 sele√ß√µes de a√ß√£o [^2].

**Defini√ß√£o Formal**
Seja $A_t$ a a√ß√£o selecionada no passo de tempo *t*, e $R_t$ a recompensa correspondente. O valor de uma a√ß√£o arbitr√°ria *a*, denotado por $q_*(a)$, √© a recompensa esperada dado que *a* √© selecionada [^2]:
$$q_*(a) = \mathbb{E}[R_t | A_t = a]$$
O problema do k-armed bandit reside na incerteza sobre os valores de cada a√ß√£o [^2]. Embora o agente possa n√£o conhecer os valores de a√ß√£o com certeza, ele mant√©m *estimativas*, denotadas por $Q_t(a)$, que representam o valor estimado da a√ß√£o *a* no passo de tempo *t* [^2]. O objetivo √© fazer com que $Q_t(a)$ se aproxime o m√°ximo poss√≠vel de $q_*(a)$ [^2].

> üí° **Exemplo Num√©rico:** Imagine um k-armed bandit com k=5. Os valores verdadeiros (desconhecidos para o agente) das a√ß√µes s√£o: $q_*(1) = 0.2$, $q_*(2) = -0.5$, $q_*(3) = 1.0$, $q_*(4) = 0.0$, $q_*(5) = -0.1$. O objetivo do agente √© aprender essas recompensas esperadas selecionando as a√ß√µes repetidamente.

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

**Explora√ß√£o vs. Explota√ß√£o**
Em qualquer passo de tempo, o agente deve decidir entre *explorar* as a√ß√µes menos conhecidas para refinar suas estimativas, ou *explorar* a a√ß√£o com o valor estimado mais alto [^2]. As a√ß√µes com o maior valor estimado atual s√£o chamadas de **a√ß√µes gananciosas** [^2]. A explota√ß√£o maximiza a recompensa esperada imediata, enquanto a explora√ß√£o pode produzir uma recompensa total maior a longo prazo [^2].

*Suponha que uma a√ß√£o gananciosa tenha um valor conhecido com certeza, enquanto v√°rias outras a√ß√µes s√£o estimadas como quase t√£o boas, mas com incerteza substancial. Pelo menos uma dessas outras a√ß√µes provavelmente √© melhor do que a a√ß√£o gananciosa, mas o agente n√£o sabe qual delas. Se o agente tem muitos passos de tempo para selecionar a√ß√µes, pode ser melhor explorar as a√ß√µes n√£o gananciosas e descobrir quais delas s√£o melhores do que a a√ß√£o gananciosa*[^2].

O dilema de **explora√ß√£o versus explota√ß√£o** √© um desafio fundamental no aprendizado por refor√ßo. A escolha entre os dois depende de uma forma complexa nos valores precisos das estimativas, nas incertezas e no n√∫mero de etapas restantes [^2].

**M√©todos de sele√ß√£o de a√ß√£o**
Uma abordagem para balancear explora√ß√£o e explota√ß√£o √© o m√©todo $\epsilon$-greedy [^3]. Com probabilidade $\epsilon$, o agente seleciona uma a√ß√£o aleatoriamente, independentemente das estimativas de valor [^3]. Caso contr√°rio, o agente escolhe a a√ß√£o gananciosa [^3]. Embora essa abordagem seja simples, ela garante que, no limite, todas as a√ß√µes sejam amostradas um n√∫mero infinito de vezes, levando √† converg√™ncia de $Q_t(a)$ para $q_*(a)$ [^3].

> üí° **Exemplo Num√©rico:** Seja $\epsilon = 0.1$. Isso significa que em 10% das vezes, o agente escolher√° uma a√ß√£o aleatoriamente, e em 90% das vezes escolher√° a a√ß√£o com a maior estimativa de valor. Se as estimativas atuais de valor forem $Q_t(1) = 0.1$, $Q_t(2) = -0.2$, $Q_t(3) = 0.5$, $Q_t(4) = 0.0$, $Q_t(5) = 0.2$, ent√£o a a√ß√£o gananciosa √© a a√ß√£o 3. Em um determinado passo de tempo, um n√∫mero aleat√≥rio √© gerado. Se esse n√∫mero for menor que 0.1, uma a√ß√£o √© escolhida aleatoriamente. Caso contr√°rio, a a√ß√£o 3 √© escolhida.

![Pseudoc√≥digo de um algoritmo de bandit simples com estrat√©gia Œµ-greedy para explora√ß√£o e explota√ß√£o.](./../images/image4.png)

Uma alternativa ao m√©todo $\epsilon$-greedy √© o m√©todo **softmax**, que utiliza uma distribui√ß√£o de probabilidade sobre as a√ß√µes baseada em suas estimativas de valor.

**Defini√ß√£o** (Softmax Action Selection)
A probabilidade de selecionar a a√ß√£o *a* no tempo *t* sob a pol√≠tica softmax √© dada por:
$$P(A_t = a) = \frac{e^{Q_t(a) / \tau}}{\sum_{b=1}^{k} e^{Q_t(b) / \tau}}$$
onde $\tau$ √© um par√¢metro de temperatura que controla a aleatoriedade da sele√ß√£o de a√ß√£o. Quando $\tau$ √© alto, todas as a√ß√µes t√™m aproximadamente a mesma probabilidade de serem selecionadas, incentivando a explora√ß√£o. Quando $\tau$ √© baixo, a a√ß√£o com o valor estimado mais alto √© selecionada com alta probabilidade, incentivando a explota√ß√£o.

> üí° **Exemplo Num√©rico:** Considere o mesmo cen√°rio do exemplo anterior, com $Q_t(1) = 0.1$, $Q_t(2) = -0.2$, $Q_t(3) = 0.5$, $Q_t(4) = 0.0$, $Q_t(5) = 0.2$. Seja $\tau = 0.5$. As probabilidades de selecionar cada a√ß√£o s√£o calculadas da seguinte forma:
>
> $P(A_t = 1) = \frac{e^{0.1 / 0.5}}{e^{0.1 / 0.5} + e^{-0.2 / 0.5} + e^{0.5 / 0.5} + e^{0.0 / 0.5} + e^{0.2 / 0.5}} \approx \frac{1.22}{1.22 + 0.67 + 2.72 + 1.00 + 1.49} \approx 0.145$
>
> $P(A_t = 2) = \frac{e^{-0.2 / 0.5}}{e^{0.1 / 0.5} + e^{-0.2 / 0.5} + e^{0.5 / 0.5} + e^{0.0 / 0.5} + e^{0.2 / 0.5}} \approx \frac{0.67}{1.22 + 0.67 + 2.72 + 1.00 + 1.49} \approx 0.080$
>
> $P(A_t = 3) = \frac{e^{0.5 / 0.5}}{e^{0.1 / 0.5} + e^{-0.2 / 0.5} + e^{0.5 / 0.5} + e^{0.0 / 0.5} + e^{0.2 / 0.5}} \approx \frac{2.72}{1.22 + 0.67 + 2.72 + 1.00 + 1.49} \approx 0.324$
>
> $P(A_t = 4) = \frac{e^{0.0 / 0.5}}{e^{0.1 / 0.5} + e^{-0.2 / 0.5} + e^{0.5 / 0.5} + e^{0.0 / 0.5} + e^{0.2 / 0.5}} \approx \frac{1.00}{1.22 + 0.67 + 2.72 + 1.00 + 1.49} \approx 0.119$
>
> $P(A_t = 5) = \frac{e^{0.2 / 0.5}}{e^{0.1 / 0.5} + e^{-0.2 / 0.5} + e^{0.5 / 0.5} + e^{0.0 / 0.5} + e^{0.2 / 0.5}} \approx \frac{1.49}{1.22 + 0.67 + 2.72 + 1.00 + 1.49} \approx 0.177$
>
> A a√ß√£o √© ent√£o selecionada com base nessas probabilidades. Observe que a a√ß√£o 3, que tem a maior estimativa de valor, tamb√©m tem a maior probabilidade de ser selecionada.

**Observa√ß√£o:** O m√©todo softmax generaliza a pol√≠tica $\epsilon$-greedy. √Ä medida que $\tau \to 0$, a pol√≠tica softmax se aproxima de uma pol√≠tica puramente gananciosa.  Al√©m disso, a escolha adequada do par√¢metro $\tau$ pode, em alguns casos, levar a um desempenho superior em compara√ß√£o com a estrat√©gia $\epsilon$-greedy.

**M√©todos de valor de a√ß√£o**
Os m√©todos de valor de a√ß√£o estimam os valores das a√ß√µes e usam essas estimativas para tomar decis√µes de sele√ß√£o de a√ß√£o [^3]. Um m√©todo natural para estimar o valor de uma a√ß√£o √© fazer a m√©dia das recompensas realmente recebidas [^3]:
$$Q_t(a) = \frac{\text{soma das recompensas quando a foi tomada antes de t}}{\text{n√∫mero de vezes que a foi tomada antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}$$
onde $\mathbb{1}_{\text{predicate}}$ denota a vari√°vel aleat√≥ria que √© 1 se o predicado for verdadeiro e 0 se n√£o for [^3]. Se o denominador for zero, ent√£o definimos $Q_t(a)$ como algum valor padr√£o, como 0 [^3]. √Ä medida que o denominador tende ao infinito, pela lei dos grandes n√∫meros, $Q_t(a)$ converge para $q_*(a)$ [^3].

> üí° **Exemplo Num√©rico:** Suponha que a a√ß√£o 1 tenha sido selecionada 3 vezes e as recompensas recebidas tenham sido 0.5, 0.2 e 0.8. Ent√£o, a estimativa de valor para a a√ß√£o 1 √©: $Q_t(1) = \frac{0.5 + 0.2 + 0.8}{3} = \frac{1.5}{3} = 0.5$.

Para implementar este c√°lculo de forma eficiente, podemos usar uma atualiza√ß√£o incremental [^3]. Seja $N_t(a)$ o n√∫mero de vezes que a a√ß√£o *a* foi selecionada antes do tempo *t*. Ent√£o podemos reescrever a equa√ß√£o acima como:
$$Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a}$$
Quando a a√ß√£o *a* √© selecionada no tempo *t*, $Q_{t+1}(a)$ pode ser atualizado incrementalmente como:
$$Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a) + 1} [R_t - Q_t(a)]$$
Essa atualiza√ß√£o incremental evita a necessidade de armazenar todas as recompensas passadas para cada a√ß√£o, tornando o c√°lculo mais eficiente em termos de mem√≥ria [^3].

**Lema 1** (Atualiza√ß√£o Incremental)
A atualiza√ß√£o incremental acima √© equivalente ao c√°lculo da m√©dia amostral.
*Prova:*
Seja $Q_t(a)$ a m√©dia amostral das recompensas para a a√ß√£o *a* ap√≥s *t-1* sele√ß√µes. Ent√£o,
$$Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a}}{N_t(a)}$$
Quando a a√ß√£o *a* √© selecionada no tempo *t*, temos:
$$Q_{t+1}(a) = \frac{\sum_{i=1}^{t} R_i \mathbb{1}_{A_i = a}}{N_t(a)+1} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a} + R_t}{N_t(a)+1}$$
$$= \frac{N_t(a) Q_t(a) + R_t}{N_t(a)+1} = \frac{N_t(a) Q_t(a) + R_t + Q_t(a) - Q_t(a)}{N_t(a)+1}$$
$$= Q_t(a) + \frac{R_t - Q_t(a)}{N_t(a)+1}$$
Portanto, a atualiza√ß√£o incremental √© equivalente ao c√°lculo da m√©dia amostral. $\blacksquare$

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, suponha que a a√ß√£o 1 seja selecionada novamente e a recompensa recebida seja 0.6. Usando a atualiza√ß√£o incremental, temos: $Q_{t+1}(1) = 0.5 + \frac{0.6 - 0.5}{3 + 1} = 0.5 + \frac{0.1}{4} = 0.5 + 0.025 = 0.525$.

Outra t√©cnica para estimar os valores das a√ß√µes √© usar uma **taxa de aprendizado** $\alpha$ constante [^3]. Neste caso, a atualiza√ß√£o para $Q_{t+1}(a)$ √© dada por:
$$Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]$$
onde $\alpha \in (0, 1]$ √© a taxa de aprendizado. Esta abordagem atribui mais peso √†s recompensas recentes, permitindo que o agente se adapte a ambientes n√£o-estacion√°rios [^3].

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo anterior, com $Q_t(1) = 0.5$ e a recompensa recebida $R_t = 0.6$, e definindo $\alpha = 0.1$, temos: $Q_{t+1}(1) = 0.5 + 0.1 [0.6 - 0.5] = 0.5 + 0.1 [0.1] = 0.5 + 0.01 = 0.51$. Observe que a atualiza√ß√£o √© menor do que com a m√©dia amostral, dando mais peso √†s estimativas anteriores.

**Lema 2** (M√©dia Ponderada Exponencialmente)
A atualiza√ß√£o com uma taxa de aprendizado constante $\alpha$ calcula uma m√©dia ponderada exponencialmente das recompensas passadas.
*Prova:*
Come√ßamos expandindo recursivamente a equa√ß√£o de atualiza√ß√£o:
I.  $Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]$

II. $Q_{t+1}(a) = \alpha R_t + (1 - \alpha) Q_t(a)$

III. Expandindo $Q_t(a)$: $Q_{t+1}(a) = \alpha R_{t} + (1 - \alpha)[\alpha R_{t-1} + (1 - \alpha)Q_{t-1}(a)]$

IV. Continuando a expans√£o recursiva at√© $Q_1(a)$:
$Q_{t+1}(a) = \alpha R_t + \alpha(1 - \alpha)R_{t-1} + \alpha(1 - \alpha)^2 R_{t-2} + \ldots + \alpha(1 - \alpha)^{t-1}R_1 + (1 - \alpha)^t Q_1(a)$

V. Reescrevendo a equa√ß√£o:
$Q_{t+1}(a) = (1 - \alpha)^t Q_1(a) + \sum_{i=1}^{t} \alpha (1 - \alpha)^{t-i} R_i$

Esta equa√ß√£o mostra que $Q_{t+1}(a)$ √© uma m√©dia ponderada de todas as recompensas passadas $R_1, \ldots, R_t$ e da estimativa inicial $Q_1(a)$. O peso de cada recompensa diminui exponencialmente com o tempo, sendo $\alpha$ a taxa de decaimento. Portanto, esta √© uma m√©dia ponderada exponencialmente. $\blacksquare$

**Inicializa√ß√£o Otimista**

Outro m√©todo para incentivar a explora√ß√£o √© inicializar as estimativas de valor de a√ß√£o, $Q_1(a)$, com valores altos [^3]. Isso incentiva o agente a explorar todas as a√ß√µes inicialmente, pois mesmo as a√ß√µes com recompensas iniciais baixas ainda ter√£o estimativas de valor relativamente altas.

**Defini√ß√£o** (Inicializa√ß√£o Otimista)
Inicializar as estimativas de valor de a√ß√£o $Q_1(a)$ com valores significativamente maiores do que as recompensas esperadas.

> üí° **Exemplo Num√©rico:** Suponha que as recompensas esperadas para as a√ß√µes estejam na faixa de 0 a 1. Poder√≠amos inicializar $Q_1(a) = 5$ para todas as a√ß√µes. Isso faria com que o agente explorasse todas as a√ß√µes pelo menos uma vez, pois as primeiras recompensas provavelmente seriam menores que 5, e o agente ajustaria suas estimativas de valor de acordo.

![Comparative performance of optimistic vs. realistic action-value initialization in a 10-armed testbed, illustrating the impact on optimal action selection over time.](./../images/image3.png)

**Observa√ß√£o:** A inicializa√ß√£o otimista √© particularmente √∫til em ambientes estacion√°rios, pois garante uma explora√ß√£o inicial adequada. No entanto, em ambientes n√£o-estacion√°rios, pode impedir que o agente se adapte rapidamente √†s mudan√ßas nas distribui√ß√µes de recompensa.

### Conclus√£o
O problema do k-armed bandit fornece uma estrutura valiosa para a compreens√£o dos desafios fundamentais do aprendizado por refor√ßo, particularmente o equil√≠brio entre explora√ß√£o e explota√ß√£o. Os m√©todos explorados neste cap√≠tulo servem como blocos de constru√ß√£o para o desenvolvimento de algoritmos mais sofisticados que podem lidar com os problemas de RL completos. Ao abstrair as complexidades do aprendizado em m√∫ltiplos estados, o k-armed bandit nos permite isolar e estudar os principais mecanismos do aprendizado baseado em avalia√ß√£o, abrindo caminho para uma compreens√£o mais profunda do campo do aprendizado por refor√ßo.

![Average performance of Œµ-greedy action-value methods on a 10-armed testbed, demonstrating the exploration-exploitation trade-off.](./../images/image6.png)

![Average performance comparison of UCB and Œµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 bra√ßos.](./../images/image1.png)

![Parameter study comparing bandit algorithms, showing average reward over 1000 steps as a function of algorithm-specific parameters.](./../images/image2.png)

### Refer√™ncias
[^1]: Chapter 2: Multi-armed Bandits, page 25
[^2]: Chapter 2: Multi-armed Bandits, page 26
[^3]: Chapter 2: Multi-armed Bandits, page 27
<!-- END -->