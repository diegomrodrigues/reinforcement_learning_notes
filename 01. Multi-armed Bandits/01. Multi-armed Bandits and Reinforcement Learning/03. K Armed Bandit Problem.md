## Multi-Armed Bandits: A Stepping Stone to Reinforcement Learning

### IntroduÃ§Ã£o
O aprendizado por reforÃ§o (RL) se distingue de outros paradigmas de aprendizado pelo uso de informaÃ§Ãµes de treinamento que *avaliam* as aÃ§Ãµes tomadas, em vez de *instruir* fornecendo aÃ§Ãµes corretas [^1]. Essa caracterÃ­stica central impulsiona a necessidade de exploraÃ§Ã£o ativa, buscando explicitamente por comportamentos Ã³timos. O problema do **k-armed bandit** serve como um ambiente simplificado para estudar esse aspecto avaliativo do RL [^1]. Este capÃ­tulo se concentra nessa formulaÃ§Ã£o mais simples, oferecendo uma base sÃ³lida para abordar os desafios mais complexos inerentes ao RL completo. Ao evitar o aprendizado de aÃ§Ãµes em mÃºltiplos contextos, o k-armed bandit permite uma anÃ¡lise clara dos mÃ©todos fundamentais que podem ser estendidos para resolver problemas mais complexos [^1].

### Conceitos Fundamentais
No cerne do problema do k-armed bandit reside a repetida escolha entre *k* opÃ§Ãµes distintas, ou **aÃ§Ãµes** [^2]. ApÃ³s cada escolha, um agente recebe uma *recompensa numÃ©rica* retirada de uma distribuiÃ§Ã£o de probabilidade estacionÃ¡ria que depende da aÃ§Ã£o selecionada [^2]. O objetivo primordial Ã© maximizar a recompensa total esperada ao longo de um determinado perÃ­odo de tempo, como 1000 seleÃ§Ãµes de aÃ§Ã£o [^2].

**DefiniÃ§Ã£o Formal**
Seja $A_t$ a aÃ§Ã£o selecionada no passo de tempo *t*, e $R_t$ a recompensa correspondente. O valor de uma aÃ§Ã£o arbitrÃ¡ria *a*, denotado por $q_*(a)$, Ã© a recompensa esperada dado que *a* Ã© selecionada [^2]:
$$q_*(a) = \mathbb{E}[R_t | A_t = a]$$
O problema do k-armed bandit reside na incerteza sobre os valores de cada aÃ§Ã£o [^2]. Embora o agente possa nÃ£o conhecer os valores de aÃ§Ã£o com certeza, ele mantÃ©m *estimativas*, denotadas por $Q_t(a)$, que representam o valor estimado da aÃ§Ã£o *a* no passo de tempo *t* [^2]. O objetivo Ã© fazer com que $Q_t(a)$ se aproxime o mÃ¡ximo possÃ­vel de $q_*(a)$ [^2].

> ğŸ’¡ **Exemplo NumÃ©rico:** Imagine um k-armed bandit com k=5. Os valores verdadeiros (desconhecidos para o agente) das aÃ§Ãµes sÃ£o: $q_*(1) = 0.2$, $q_*(2) = -0.5$, $q_*(3) = 1.0$, $q_*(4) = 0.0$, $q_*(5) = -0.1$. O objetivo do agente Ã© aprender essas recompensas esperadas selecionando as aÃ§Ãµes repetidamente.

![DistribuiÃ§Ãµes de recompensa para um problema de bandit de 10 braÃ§os.](./../images/image5.png)

**ExploraÃ§Ã£o vs. ExplotaÃ§Ã£o**
Em qualquer passo de tempo, o agente deve decidir entre *explorar* as aÃ§Ãµes menos conhecidas para refinar suas estimativas, ou *explorar* a aÃ§Ã£o com o valor estimado mais alto [^2]. As aÃ§Ãµes com o maior valor estimado atual sÃ£o chamadas de **aÃ§Ãµes gananciosas** [^2]. A explotaÃ§Ã£o maximiza a recompensa esperada imediata, enquanto a exploraÃ§Ã£o pode produzir uma recompensa total maior a longo prazo [^2].

*Suponha que uma aÃ§Ã£o gananciosa tenha um valor conhecido com certeza, enquanto vÃ¡rias outras aÃ§Ãµes sÃ£o estimadas como quase tÃ£o boas, mas com incerteza substancial. Pelo menos uma dessas outras aÃ§Ãµes provavelmente Ã© melhor do que a aÃ§Ã£o gananciosa, mas o agente nÃ£o sabe qual delas. Se o agente tem muitos passos de tempo para selecionar aÃ§Ãµes, pode ser melhor explorar as aÃ§Ãµes nÃ£o gananciosas e descobrir quais delas sÃ£o melhores do que a aÃ§Ã£o gananciosa*[^2].

O dilema de **exploraÃ§Ã£o versus explotaÃ§Ã£o** Ã© um desafio fundamental no aprendizado por reforÃ§o. A escolha entre os dois depende de uma forma complexa nos valores precisos das estimativas, nas incertezas e no nÃºmero de etapas restantes [^2].

**MÃ©todos de seleÃ§Ã£o de aÃ§Ã£o**
Uma abordagem para balancear exploraÃ§Ã£o e explotaÃ§Ã£o Ã© o mÃ©todo $\epsilon$-greedy [^3]. Com probabilidade $\epsilon$, o agente seleciona uma aÃ§Ã£o aleatoriamente, independentemente das estimativas de valor [^3]. Caso contrÃ¡rio, o agente escolhe a aÃ§Ã£o gananciosa [^3]. Embora essa abordagem seja simples, ela garante que, no limite, todas as aÃ§Ãµes sejam amostradas um nÃºmero infinito de vezes, levando Ã  convergÃªncia de $Q_t(a)$ para $q_*(a)$ [^3].

> ğŸ’¡ **Exemplo NumÃ©rico:** Seja $\epsilon = 0.1$. Isso significa que em 10% das vezes, o agente escolherÃ¡ uma aÃ§Ã£o aleatoriamente, e em 90% das vezes escolherÃ¡ a aÃ§Ã£o com a maior estimativa de valor. Se as estimativas atuais de valor forem $Q_t(1) = 0.1$, $Q_t(2) = -0.2$, $Q_t(3) = 0.5$, $Q_t(4) = 0.0$, $Q_t(5) = 0.2$, entÃ£o a aÃ§Ã£o gananciosa Ã© a aÃ§Ã£o 3. Em um determinado passo de tempo, um nÃºmero aleatÃ³rio Ã© gerado. Se esse nÃºmero for menor que 0.1, uma aÃ§Ã£o Ã© escolhida aleatoriamente. Caso contrÃ¡rio, a aÃ§Ã£o 3 Ã© escolhida.

![PseudocÃ³digo de um algoritmo de bandit simples com estratÃ©gia Îµ-greedy para exploraÃ§Ã£o e explotaÃ§Ã£o.](./../images/image4.png)

Uma alternativa ao mÃ©todo $\epsilon$-greedy Ã© o mÃ©todo **softmax**, que utiliza uma distribuiÃ§Ã£o de probabilidade sobre as aÃ§Ãµes baseada em suas estimativas de valor.

**DefiniÃ§Ã£o** (Softmax Action Selection)
A probabilidade de selecionar a aÃ§Ã£o *a* no tempo *t* sob a polÃ­tica softmax Ã© dada por:
$$P(A_t = a) = \frac{e^{Q_t(a) / \tau}}{\sum_{b=1}^{k} e^{Q_t(b) / \tau}}$$
onde $\tau$ Ã© um parÃ¢metro de temperatura que controla a aleatoriedade da seleÃ§Ã£o de aÃ§Ã£o. Quando $\tau$ Ã© alto, todas as aÃ§Ãµes tÃªm aproximadamente a mesma probabilidade de serem selecionadas, incentivando a exploraÃ§Ã£o. Quando $\tau$ Ã© baixo, a aÃ§Ã£o com o valor estimado mais alto Ã© selecionada com alta probabilidade, incentivando a explotaÃ§Ã£o.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere o mesmo cenÃ¡rio do exemplo anterior, com $Q_t(1) = 0.1$, $Q_t(2) = -0.2$, $Q_t(3) = 0.5$, $Q_t(4) = 0.0$, $Q_t(5) = 0.2$. Seja $\tau = 0.5$. As probabilidades de selecionar cada aÃ§Ã£o sÃ£o calculadas da seguinte forma:
>
> $P(A_t = 1) = \frac{e^{0.1 / 0.5}}{e^{0.1 / 0.5} + e^{-0.2 / 0.5} + e^{0.5 / 0.5} + e^{0.0 / 0.5} + e^{0.2 / 0.5}} \approx \frac{1.22}{1.22 + 0.67 + 2.72 + 1.00 + 1.49} \approx 0.145$
>
> $P(A_t = 2) = \frac{e^{-0.2 / 0.5}}{e^{0.1 / 0.5} + e^{-0.2 / 0.5} + e^{0.5 / 0.5} + e^{0.0 / 0.5} + e^{0.2 / 0.5}} \approx \frac{0.67}{1.22 + 0.67 + 2.72 + 1.00 + 1.49} \approx 0.080$
>
> $P(A_t = 3) = \frac{e^{0.5 / 0.5}}{e^{0.1 / 0.5} + e^{-0.2 / 0.5} + e^{0.5 / 0.5} + e^{0.0 / 0.5} + e^{0.2 / 0.5}} \approx \frac{2.72}{1.22 + 0.67 + 2.72 + 1.00 + 1.49} \approx 0.324$
>
> $P(A_t = 4) = \frac{e^{0.0 / 0.5}}{e^{0.1 / 0.5} + e^{-0.2 / 0.5} + e^{0.5 / 0.5} + e^{0.0 / 0.5} + e^{0.2 / 0.5}} \approx \frac{1.00}{1.22 + 0.67 + 2.72 + 1.00 + 1.49} \approx 0.119$
>
> $P(A_t = 5) = \frac{e^{0.2 / 0.5}}{e^{0.1 / 0.5} + e^{-0.2 / 0.5} + e^{0.5 / 0.5} + e^{0.0 / 0.5} + e^{0.2 / 0.5}} \approx \frac{1.49}{1.22 + 0.67 + 2.72 + 1.00 + 1.49} \approx 0.177$
>
> A aÃ§Ã£o Ã© entÃ£o selecionada com base nessas probabilidades. Observe que a aÃ§Ã£o 3, que tem a maior estimativa de valor, tambÃ©m tem a maior probabilidade de ser selecionada.

**ObservaÃ§Ã£o:** O mÃ©todo softmax generaliza a polÃ­tica $\epsilon$-greedy. Ã€ medida que $\tau \to 0$, a polÃ­tica softmax se aproxima de uma polÃ­tica puramente gananciosa.  AlÃ©m disso, a escolha adequada do parÃ¢metro $\tau$ pode, em alguns casos, levar a um desempenho superior em comparaÃ§Ã£o com a estratÃ©gia $\epsilon$-greedy.

**MÃ©todos de valor de aÃ§Ã£o**
Os mÃ©todos de valor de aÃ§Ã£o estimam os valores das aÃ§Ãµes e usam essas estimativas para tomar decisÃµes de seleÃ§Ã£o de aÃ§Ã£o [^3]. Um mÃ©todo natural para estimar o valor de uma aÃ§Ã£o Ã© fazer a mÃ©dia das recompensas realmente recebidas [^3]:
$$Q_t(a) = \frac{\text{soma das recompensas quando a foi tomada antes de t}}{\text{nÃºmero de vezes que a foi tomada antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}$$
onde $\mathbb{1}_{\text{predicate}}$ denota a variÃ¡vel aleatÃ³ria que Ã© 1 se o predicado for verdadeiro e 0 se nÃ£o for [^3]. Se o denominador for zero, entÃ£o definimos $Q_t(a)$ como algum valor padrÃ£o, como 0 [^3]. Ã€ medida que o denominador tende ao infinito, pela lei dos grandes nÃºmeros, $Q_t(a)$ converge para $q_*(a)$ [^3].

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que a aÃ§Ã£o 1 tenha sido selecionada 3 vezes e as recompensas recebidas tenham sido 0.5, 0.2 e 0.8. EntÃ£o, a estimativa de valor para a aÃ§Ã£o 1 Ã©: $Q_t(1) = \frac{0.5 + 0.2 + 0.8}{3} = \frac{1.5}{3} = 0.5$.

Para implementar este cÃ¡lculo de forma eficiente, podemos usar uma atualizaÃ§Ã£o incremental [^3]. Seja $N_t(a)$ o nÃºmero de vezes que a aÃ§Ã£o *a* foi selecionada antes do tempo *t*. EntÃ£o podemos reescrever a equaÃ§Ã£o acima como:
$$Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a}$$
Quando a aÃ§Ã£o *a* Ã© selecionada no tempo *t*, $Q_{t+1}(a)$ pode ser atualizado incrementalmente como:
$$Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a) + 1} [R_t - Q_t(a)]$$
Essa atualizaÃ§Ã£o incremental evita a necessidade de armazenar todas as recompensas passadas para cada aÃ§Ã£o, tornando o cÃ¡lculo mais eficiente em termos de memÃ³ria [^3].

**Lema 1** (AtualizaÃ§Ã£o Incremental)
A atualizaÃ§Ã£o incremental acima Ã© equivalente ao cÃ¡lculo da mÃ©dia amostral.
*Prova:*
Seja $Q_t(a)$ a mÃ©dia amostral das recompensas para a aÃ§Ã£o *a* apÃ³s *t-1* seleÃ§Ãµes. EntÃ£o,
$$Q_t(a) = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a}}{N_t(a)}$$
Quando a aÃ§Ã£o *a* Ã© selecionada no tempo *t*, temos:
$$Q_{t+1}(a) = \frac{\sum_{i=1}^{t} R_i \mathbb{1}_{A_i = a}}{N_t(a)+1} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a} + R_t}{N_t(a)+1}$$
$$= \frac{N_t(a) Q_t(a) + R_t}{N_t(a)+1} = \frac{N_t(a) Q_t(a) + R_t + Q_t(a) - Q_t(a)}{N_t(a)+1}$$
$$= Q_t(a) + \frac{R_t - Q_t(a)}{N_t(a)+1}$$
Portanto, a atualizaÃ§Ã£o incremental Ã© equivalente ao cÃ¡lculo da mÃ©dia amostral. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:** Continuando o exemplo anterior, suponha que a aÃ§Ã£o 1 seja selecionada novamente e a recompensa recebida seja 0.6. Usando a atualizaÃ§Ã£o incremental, temos: $Q_{t+1}(1) = 0.5 + \frac{0.6 - 0.5}{3 + 1} = 0.5 + \frac{0.1}{4} = 0.5 + 0.025 = 0.525$.

Outra tÃ©cnica para estimar os valores das aÃ§Ãµes Ã© usar uma **taxa de aprendizado** $\alpha$ constante [^3]. Neste caso, a atualizaÃ§Ã£o para $Q_{t+1}(a)$ Ã© dada por:
$$Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]$$
onde $\alpha \in (0, 1]$ Ã© a taxa de aprendizado. Esta abordagem atribui mais peso Ã s recompensas recentes, permitindo que o agente se adapte a ambientes nÃ£o-estacionÃ¡rios [^3].

> ğŸ’¡ **Exemplo NumÃ©rico:** Usando o mesmo exemplo anterior, com $Q_t(1) = 0.5$ e a recompensa recebida $R_t = 0.6$, e definindo $\alpha = 0.1$, temos: $Q_{t+1}(1) = 0.5 + 0.1 [0.6 - 0.5] = 0.5 + 0.1 [0.1] = 0.5 + 0.01 = 0.51$. Observe que a atualizaÃ§Ã£o Ã© menor do que com a mÃ©dia amostral, dando mais peso Ã s estimativas anteriores.

**Lema 2** (MÃ©dia Ponderada Exponencialmente)
A atualizaÃ§Ã£o com uma taxa de aprendizado constante $\alpha$ calcula uma mÃ©dia ponderada exponencialmente das recompensas passadas.
*Prova:*
ComeÃ§amos expandindo recursivamente a equaÃ§Ã£o de atualizaÃ§Ã£o:
I.  $Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]$

II. $Q_{t+1}(a) = \alpha R_t + (1 - \alpha) Q_t(a)$

III. Expandindo $Q_t(a)$: $Q_{t+1}(a) = \alpha R_{t} + (1 - \alpha)[\alpha R_{t-1} + (1 - \alpha)Q_{t-1}(a)]$

IV. Continuando a expansÃ£o recursiva atÃ© $Q_1(a)$:
$Q_{t+1}(a) = \alpha R_t + \alpha(1 - \alpha)R_{t-1} + \alpha(1 - \alpha)^2 R_{t-2} + \ldots + \alpha(1 - \alpha)^{t-1}R_1 + (1 - \alpha)^t Q_1(a)$

V. Reescrevendo a equaÃ§Ã£o:
$Q_{t+1}(a) = (1 - \alpha)^t Q_1(a) + \sum_{i=1}^{t} \alpha (1 - \alpha)^{t-i} R_i$

Esta equaÃ§Ã£o mostra que $Q_{t+1}(a)$ Ã© uma mÃ©dia ponderada de todas as recompensas passadas $R_1, \ldots, R_t$ e da estimativa inicial $Q_1(a)$. O peso de cada recompensa diminui exponencialmente com o tempo, sendo $\alpha$ a taxa de decaimento. Portanto, esta Ã© uma mÃ©dia ponderada exponencialmente. $\blacksquare$

**InicializaÃ§Ã£o Otimista**

Outro mÃ©todo para incentivar a exploraÃ§Ã£o Ã© inicializar as estimativas de valor de aÃ§Ã£o, $Q_1(a)$, com valores altos [^3]. Isso incentiva o agente a explorar todas as aÃ§Ãµes inicialmente, pois mesmo as aÃ§Ãµes com recompensas iniciais baixas ainda terÃ£o estimativas de valor relativamente altas.

**DefiniÃ§Ã£o** (InicializaÃ§Ã£o Otimista)
Inicializar as estimativas de valor de aÃ§Ã£o $Q_1(a)$ com valores significativamente maiores do que as recompensas esperadas.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que as recompensas esperadas para as aÃ§Ãµes estejam na faixa de 0 a 1. PoderÃ­amos inicializar $Q_1(a) = 5$ para todas as aÃ§Ãµes. Isso faria com que o agente explorasse todas as aÃ§Ãµes pelo menos uma vez, pois as primeiras recompensas provavelmente seriam menores que 5, e o agente ajustaria suas estimativas de valor de acordo.

![Comparative performance of optimistic vs. realistic action-value initialization in a 10-armed testbed, illustrating the impact on optimal action selection over time.](./../images/image3.png)

**ObservaÃ§Ã£o:** A inicializaÃ§Ã£o otimista Ã© particularmente Ãºtil em ambientes estacionÃ¡rios, pois garante uma exploraÃ§Ã£o inicial adequada. No entanto, em ambientes nÃ£o-estacionÃ¡rios, pode impedir que o agente se adapte rapidamente Ã s mudanÃ§as nas distribuiÃ§Ãµes de recompensa.

### ConclusÃ£o
O problema do k-armed bandit fornece uma estrutura valiosa para a compreensÃ£o dos desafios fundamentais do aprendizado por reforÃ§o, particularmente o equilÃ­brio entre exploraÃ§Ã£o e explotaÃ§Ã£o. Os mÃ©todos explorados neste capÃ­tulo servem como blocos de construÃ§Ã£o para o desenvolvimento de algoritmos mais sofisticados que podem lidar com os problemas de RL completos. Ao abstrair as complexidades do aprendizado em mÃºltiplos estados, o k-armed bandit nos permite isolar e estudar os principais mecanismos do aprendizado baseado em avaliaÃ§Ã£o, abrindo caminho para uma compreensÃ£o mais profunda do campo do aprendizado por reforÃ§o.

![Average performance of Îµ-greedy action-value methods on a 10-armed testbed, demonstrating the exploration-exploitation trade-off.](./../images/image6.png)

![Average performance comparison of UCB and Îµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 braÃ§os.](./../images/image1.png)

![Parameter study comparing bandit algorithms, showing average reward over 1000 steps as a function of algorithm-specific parameters.](./../images/image2.png)

### ReferÃªncias
[^1]: Chapter 2: Multi-armed Bandits, page 25
[^2]: Chapter 2: Multi-armed Bandits, page 26
[^3]: Chapter 2: Multi-armed Bandits, page 27
<!-- END -->