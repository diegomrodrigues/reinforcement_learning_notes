## A Fun√ß√£o de Valor Verdadeiro q*(a) e a Pol√≠tica √ìtima

### Introdu√ß√£o
No contexto do problema do *k-armed bandit*, a defini√ß√£o da fun√ß√£o de valor verdadeiro, denotada por $q*(a)$, √© fundamental para a formula√ß√£o de estrat√©gias de aprendizado por refor√ßo [^1]. Este cap√≠tulo explora em profundidade a import√¢ncia de $q*(a)$ na determina√ß√£o da pol√≠tica √≥tima, crucial para maximizar o retorno esperado ao longo do tempo.

### Conceitos Fundamentais
A fun√ß√£o de valor verdadeiro $q*(a)$ representa o *valor esperado* da recompensa $R_t$ ao selecionar a a√ß√£o $a$ no tempo $t$. Formalmente, ela √© definida como:
$$q*(a) = E[R_t | A_t = a]$$
onde $A_t$ √© a a√ß√£o selecionada no tempo $t$ [^2]. Em outras palavras, $q*(a)$ nos diz qual recompensa podemos esperar, *em m√©dia*, ao escolher repetidamente a a√ß√£o $a$ [^2].

> üí° **Exemplo Num√©rico:** Imagine um *2-armed bandit*. A√ß√£o 1 retorna 1 com probabilidade 0.3 e 0 com probabilidade 0.7. A√ß√£o 2 retorna 1 com probabilidade 0.6 e 0 com probabilidade 0.4. Ent√£o, $q*(1) = 0.3 * 1 + 0.7 * 0 = 0.3$ e $q*(2) = 0.6 * 1 + 0.4 * 0 = 0.6$. Neste caso, a a√ß√£o 2 √© melhor a longo prazo.

**Estimando q*(a):**
Como, na maioria dos casos, o valor verdadeiro $q*(a)$ √© desconhecido, torna-se necess√°rio estim√°-lo. Uma abordagem natural √© calcular a *m√©dia amostral* das recompensas obtidas ao selecionar a a√ß√£o $a$ [^3]:
$$Q_t(a) = \frac{\text{soma das recompensas quando a foi tomado antes de t}}{\text{n√∫mero de vezes que a foi tomado antes de t}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}}$$
onde $\mathbb{1}_{A_i = a}$ √© uma fun√ß√£o indicadora que vale 1 se $A_i = a$ e 0 caso contr√°rio [^3]. Essa abordagem √© conhecida como *m√©todo da m√©dia amostral* [^3]. √Ä medida que o n√∫mero de vezes que a a√ß√£o $a$ √© selecionada se aproxima do infinito, a lei dos grandes n√∫meros garante que $Q_t(a)$ converge para $q*(a)$ [^3].

> üí° **Exemplo Num√©rico:** Considere que jogamos a a√ß√£o 1 tr√™s vezes e obtemos as recompensas [0, 1, 0]. Ent√£o, $Q_4(1) = (0 + 1 + 0) / 3 = 1/3 \approx 0.33$. Se jogarmos a a√ß√£o 1 novamente e obtivermos uma recompensa de 1, ent√£o $Q_5(1) = (0 + 1 + 0 + 1) / 4 = 2/4 = 0.5$.

**Lema 1:** *Converg√™ncia da M√©dia Amostral*
Se a recompensa $R_t$ for limitada, ou seja, $|R_t| \leq M$ para algum $M > 0$, ent√£o a m√©dia amostral $Q_t(a)$ converge para $q*(a)$ quase certamente quando o n√∫mero de vezes que a a√ß√£o $a$ foi tomada antes de $t$ tende ao infinito.

*Prova:* Este resultado segue diretamente da Lei Forte dos Grandes N√∫meros. Seja $N_t(a) = \sum_{i=1}^{t-1} \mathbb{1}_{A_i = a}$ o n√∫mero de vezes que a a√ß√£o $a$ foi tomada antes de $t$. Se $N_t(a) \to \infty$ quando $t \to \infty$, ent√£o:
$$Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i = a} \xrightarrow{q.c.} E[R_t | A_t = a] = q*(a)$$
<!-- END: Lema 1 Proof -->

**A Pol√≠tica √ìtima:**
Se o valor de $q*(a)$ fosse conhecido para todas as a√ß√µes $a$, a pol√≠tica √≥tima seria simplesmente selecionar a a√ß√£o com o maior valor esperado [^2]. Formalmente, a a√ß√£o √≥tima $a^*$ √© dada por:
$$a^* = \text{argmax}_a \, q*(a)$$
No entanto, como $q*(a)$ √© desconhecido e deve ser estimado, a *explora√ß√£o* torna-se essencial. Estrat√©gias como $\epsilon$-greedy [^3] e Upper-Confidence-Bound (UCB) [^11] s√£o projetadas para equilibrar a *explora√ß√£o* (tentar a√ß√µes sub√≥timas para melhorar a estimativa de $q*(a)$) e a *explota√ß√£o* (selecionar a a√ß√£o com a maior estimativa atual $Q_t(a)$).

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior ($q*(1) = 0.3$ e $q*(2) = 0.6$), se us√°ssemos uma pol√≠tica $\epsilon$-greedy com $\epsilon = 0.1$, ent√£o em 10% das vezes escolher√≠amos uma a√ß√£o aleat√≥ria (explora√ß√£o), e em 90% das vezes escolher√≠amos a a√ß√£o com a maior estimativa atual de $q*(a)$ (explota√ß√£o). Inicialmente, nossas estimativas $Q_t(a)$ podem estar erradas, ent√£o a explora√ß√£o ajuda a refinar essas estimativas.

**Proposi√ß√£o 2:** *Rela√ß√£o entre Pol√≠tica √ìtima e q*(a)*
Uma pol√≠tica $\pi$ √© √≥tima se e somente se, para todo estado $s$ e a√ß√£o $a$,
$$\pi(a|s) > 0 \implies q(s,a) = q_*(s,a) = \max_{a' \in A} q_*(s,a')$$
onde $\pi(a|s)$ representa a probabilidade de tomar a a√ß√£o $a$ no estado $s$, $q(s,a)$ √© a fun√ß√£o de valor da pol√≠tica $\pi$, e $q_*(s,a)$ √© a fun√ß√£o de valor √≥tima.

*Prova:* Se a pol√≠tica $\pi$ √© √≥tima, ent√£o $q(s,a) = q_*(s,a)$ para todo $s$ e $a$. Se $\pi(a|s) > 0$, ent√£o a a√ß√£o $a$ √© tomada com probabilidade n√£o nula sob a pol√≠tica √≥tima. Portanto, $q_*(s,a)$ deve ser igual ao valor √≥timo para aquele estado, que √© o m√°ximo de $q_*(s,a')$ sobre todas as a√ß√µes $a' \in A$.

Reciprocamente, se para todo $s$ e $a$, $\pi(a|s) > 0 \implies q(s,a) = \max_{a' \in A} q_*(s,a')$, ent√£o a pol√≠tica $\pi$ sempre escolhe a√ß√µes que levam ao valor √≥timo, e portanto, √© uma pol√≠tica √≥tima.

<!-- END: Proposi√ß√£o 2 Proof -->

**Exemplo:**
Considere um problema de 3-armed bandit onde os valores verdadeiros das a√ß√µes s√£o $q*(1) = 0.2$, $q*(2) = 0.5$ e $q*(3) = 0.1$. Se conhecermos esses valores, a pol√≠tica √≥tima √© sempre selecionar a a√ß√£o 2, que oferece o maior retorno esperado [^2]. No entanto, na pr√°tica, esses valores s√£o desconhecidos e precisam ser aprendidos ao longo do tempo por meio de experimenta√ß√£o.

> üí° **Exemplo Num√©rico:** Para este exemplo, a pol√≠tica √≥tima $\pi(a|s)$ √©: $\pi(2|s) = 1$ e $\pi(1|s) = \pi(3|s) = 0$ para qualquer estado $s$. Isso significa que sempre escolheremos a a√ß√£o 2, porque ela tem o maior valor esperado.





![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

**Nonestacionariedade:**

√â importante notar que a defini√ß√£o de $q*(a)$ assume que a distribui√ß√£o de probabilidade das recompensas √© *estacion√°ria* [^2]. Ou seja, o valor esperado da recompensa para cada a√ß√£o n√£o muda ao longo do tempo. No entanto, em problemas *n√£o estacion√°rios* [^3], os valores $q*(a)$ podem variar ao longo do tempo, tornando o aprendizado mais desafiador. Nesses casos, √© √∫til dar mais peso √†s recompensas recentes para rastrear as mudan√ßas nos valores das a√ß√µes, como atrav√©s do uso de um *tamanho de passo constante*  $\alpha$ [^5]:
$$Q_{n+1} = Q_n + \alpha [R_n - Q_n]$$
onde $Q_{n+1}$ √© a nova estimativa, $Q_n$ √© a estimativa anterior, $R_n$ √© a recompensa recebida e $\alpha \in (0, 1]$ [^5].

> üí° **Exemplo Num√©rico:** Suponha que $Q_n(a) = 0.4$ e recebemos uma recompensa $R_n = 1$ ao selecionar a a√ß√£o $a$. Se usarmos $\alpha = 0.1$, ent√£o $Q_{n+1}(a) = 0.4 + 0.1 * (1 - 0.4) = 0.4 + 0.06 = 0.46$. Isso significa que a nova estimativa $Q_{n+1}(a)$ est√° mais pr√≥xima da recompensa recente, mas ainda ret√©m informa√ß√µes das estimativas anteriores. Se o ambiente √© n√£o estacion√°rio, este m√©todo permite que a estimativa $Q_n(a)$ rastreie as mudan√ßas em $q*(a)$.
```python
import numpy as np
import matplotlib.pyplot as plt

# Par√¢metros
alpha = 0.1
q_star = 0.5  # Valor verdadeiro da a√ß√£o (pode mudar ao longo do tempo em um ambiente n√£o estacion√°rio)
num_trials = 100

# Inicializa√ß√£o
Q = 0.0  # Estimativa inicial
rewards = []
estimates = []

# Simula√ß√£o
for i in range(num_trials):
    # Recompensa (simula√ß√£o)
    reward = np.random.normal(q_star, 1)  # Recompensa aleat√≥ria com m√©dia q_star e desvio padr√£o 1
    rewards.append(reward)

    # Atualiza√ß√£o da estimativa
    Q = Q + alpha * (reward - Q)
    estimates.append(Q)

    # Mudan√ßa n√£o estacion√°ria no valor verdadeiro (opcional)
    #q_star += np.random.normal(0, 0.01)  # Pequena mudan√ßa aleat√≥ria em q_star

# Plotagem
plt.figure(figsize=(10, 6))
plt.plot(estimates, label='Estimativa de Q(a)')
plt.axhline(y=0.5, color='r', linestyle='--', label='Valor Verdadeiro q*(a)')
plt.xlabel('Tentativa')
plt.ylabel('Valor')
plt.title('Estimativa de Q(a) ao Longo do Tempo')
plt.legend()
plt.grid(True)
plt.show()
```

**Teorema 3:** *Converg√™ncia com Tamanho de Passo Constante em Ambientes Estacion√°rios*
Se a sequ√™ncia de recompensas $\{R_n\}_{n=1}^{\infty}$ √© i.i.d. com m√©dia $\mu$ e vari√¢ncia finita $\sigma^2$, e se o tamanho de passo $\alpha$ √© constante e satisfaz $0 < \alpha < 1$, ent√£o $Q_n$ converge em m√©dia para $\mu$.

*Prova:*
I.  Definimos o erro na itera√ß√£o $n$ como $e_n = Q_n - \mu$, onde $\mu$ √© o valor m√©dio da recompensa.

II. Reescrevemos a atualiza√ß√£o de $Q_{n+1}$ em termos do erro $e_n$:
$$Q_{n+1} = Q_n + \alpha(R_n - Q_n)$$
$$Q_{n+1} - \mu = Q_n - \mu + \alpha(R_n - Q_n)$$
$$e_{n+1} = e_n + \alpha(R_n - Q_n)$$
$$e_{n+1} = e_n + \alpha(R_n - (e_n + \mu))$$
$$e_{n+1} = (1-\alpha)e_n + \alpha(R_n - \mu)$$

III. Tomamos a expectativa condicional de ambos os lados, dado $Q_n$ (e, portanto, $e_n$):
$$E[e_{n+1} | Q_n] = E[(1-\alpha)e_n + \alpha(R_n - \mu) | Q_n]$$
Como $e_n$ √© dado,
$$E[e_{n+1} | Q_n] = (1-\alpha)e_n + \alpha E[R_n - \mu | Q_n]$$

IV. Como $R_n$ √© i.i.d. e independente de $Q_n$, $E[R_n | Q_n] = E[R_n] = \mu$. Portanto,
$$E[e_{n+1} | Q_n] = (1-\alpha)e_n + \alpha(\mu - \mu) = (1-\alpha)e_n$$

V. Tomando a expectativa incondicional de ambos os lados:
$$E[e_{n+1}] = (1-\alpha)E[e_n]$$

VI. Aplicando recursivamente esta rela√ß√£o:
$$E[e_{n+1}] = (1-\alpha)E[e_n] = (1-\alpha)^2E[e_{n-1}] = \ldots = (1-\alpha)^{n+1}E[e_0]$$

VII. Como $0 < \alpha < 1$, $|1-\alpha| < 1$. Assim, $\lim_{n \to \infty} (1-\alpha)^n = 0$. Portanto,
$$\lim_{n \to \infty} E[e_n] = 0$$
Isto significa que $\lim_{n \to \infty} E[Q_n - \mu] = 0$, ou $\lim_{n \to \infty} E[Q_n] = \mu$.

VIII. Conclu√≠mos que $Q_n$ converge em m√©dia para $\mu$. ‚ñ†

<!-- END: Teorema 3 Proof -->

**Corol√°rio 3.1:** Sob as mesmas condi√ß√µes do Teorema 3, a vari√¢ncia de $Q_n$ converge para $\frac{\alpha \sigma^2}{2 - \alpha}$.

*Prova:*
I.  Come√ßamos com a equa√ß√£o derivada na prova do Teorema 3:
    $$e_{n+1} = (1 - \alpha)e_n + \alpha(R_n - \mu)$$

II. Elevamos ambos os lados ao quadrado:
    $$e_{n+1}^2 = (1 - \alpha)^2e_n^2 + 2\alpha(1 - \alpha)e_n(R_n - \mu) + \alpha^2(R_n - \mu)^2$$

III. Tomamos a expectativa de ambos os lados:
    $$E[e_{n+1}^2] = E[(1 - \alpha)^2e_n^2 + 2\alpha(1 - \alpha)e_n(R_n - \mu) + \alpha^2(R_n - \mu)^2]$$
    $$E[e_{n+1}^2] = (1 - \alpha)^2E[e_n^2] + 2\alpha(1 - \alpha)E[e_n(R_n - \mu)] + \alpha^2E[(R_n - \mu)^2]$$

IV. Como $R_n$ √© independente de $Q_n$ (e, portanto, de $e_n$), $E[e_n(R_n - \mu)] = E[e_n]E[R_n - \mu] = 0$ (j√° que $E[e_n] \to 0$ e $E[R_n - \mu] = 0$).  Tamb√©m, $E[(R_n - \mu)^2] = \sigma^2$. Assim,
    $$E[e_{n+1}^2] = (1 - \alpha)^2E[e_n^2] + \alpha^2\sigma^2$$

V. No estado estacion√°rio, assumimos que a vari√¢ncia converge, ou seja, $E[e_{n+1}^2] = E[e_n^2] = V$. Substitu√≠mos isso na equa√ß√£o acima:
    $$V = (1 - \alpha)^2V + \alpha^2\sigma^2$$

VI. Resolvemos para $V$:
    $$V - (1 - \alpha)^2V = \alpha^2\sigma^2$$
    $$V[1 - (1 - 2\alpha + \alpha^2)] = \alpha^2\sigma^2$$
    $$V(2\alpha - \alpha^2) = \alpha^2\sigma^2$$
    $$V = \frac{\alpha^2\sigma^2}{2\alpha - \alpha^2}$$
    $$V = \frac{\alpha\sigma^2}{2 - \alpha}$$

VII. Portanto, a vari√¢ncia de $Q_n$ converge para $\frac{\alpha\sigma^2}{2 - \alpha}$. ‚ñ†

<!-- END: Corol√°rio 3.1 Proof -->

> üí° **Exemplo Num√©rico:** Se $\alpha = 0.1$ e $\sigma^2 = 1$, a vari√¢ncia de $Q_n$ converge para $\frac{0.1 * 1}{2 - 0.1} = \frac{0.1}{1.9} \approx 0.0526$. Se $\alpha = 0.5$, a vari√¢ncia converge para $\frac{0.5 * 1}{2 - 0.5} = \frac{0.5}{1.5} \approx 0.333$. Observe que um $\alpha$ maior resulta em maior vari√¢ncia, o que significa que a estimativa flutua mais, mas tamb√©m se adapta mais rapidamente a mudan√ßas em ambientes n√£o estacion√°rios.

### Conclus√£o
A fun√ß√£o de valor verdadeiro $q*(a)$ √© um conceito central no problema do *k-armed bandit* e no aprendizado por refor√ßo em geral. Embora geralmente desconhecido, estimar $q*(a)$ com precis√£o √© crucial para determinar a pol√≠tica √≥tima que maximiza o retorno esperado ao longo do tempo. Estrat√©gias eficazes equilibram a explora√ß√£o e a explota√ß√£o para aprender $q*(a)$ de forma eficiente, especialmente em ambientes *n√£o estacion√°rios* [^5], onde os valores das a√ß√µes podem variar ao longo do tempo.

### Refer√™ncias
[^1]: Chapter 2: Multi-armed Bandits
[^2]: Section 2.1: A k-armed Bandit Problem
[^3]: Section 2.2: Action-value Methods
[^4]: Figure 2.1: An example bandit problem from the 10-armed testbed. The true value q*(a) of each of the ten actions was selected according to a normal distribution with mean zero and unit variance, and then the actual rewards were selected according to a mean q*(a), unit-variance normal distribution, as suggested by these gray distributions.
[^5]: Section 2.5: Tracking a Nonstationary Problem
[^11]: Section 2.7: Upper-Confidence-Bound Action Selection
<!-- END -->