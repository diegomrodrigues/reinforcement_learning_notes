## A Natureza Explorat√≥ria do Problema k-armed Bandit

### Introdu√ß√£o
O problema do **k-armed bandit** [^1] serve como um ambiente simplificado, mas rico em nuances, para o estudo dos desafios fundamentais do *reinforcement learning* (RL). Diferentemente de outros paradigmas de aprendizado, o RL se distingue pelo uso de informa√ß√µes de treinamento que avaliam as a√ß√µes tomadas, em vez de instruir fornecendo as a√ß√µes corretas [^1]. Essa caracter√≠stica inerente ao RL gera a necessidade cr√≠tica de **explora√ß√£o ativa** [^1], impulsionando uma busca expl√≠cita por comportamentos que otimizem a recompensa. Este cap√≠tulo se aprofunda no problema do k-armed bandit, analisando como os agentes aprendem a equilibrar a explora√ß√£o (descobrir novas a√ß√µes) com a explora√ß√£o (maximizar a recompensa imediata) em um ambiente onde as recompensas s√£o estoc√°sticas e dependentes das a√ß√µes escolhidas.

### Conceitos Fundamentais

No cen√°rio do **k-armed bandit**, um agente √© confrontado repetidamente com a tarefa de escolher entre *k* diferentes op√ß√µes, denotadas como a√ß√µes [^1]. Ap√≥s cada escolha, o agente recebe uma *recompensa num√©rica* [^1] amostrada de uma *distribui√ß√£o de probabilidade estacion√°ria* [^1] espec√≠fica para a a√ß√£o selecionada. O objetivo central √© **maximizar a recompensa total esperada** [^1] ao longo de um per√≠odo de tempo definido, como 1000 sele√ß√µes de a√ß√£o, ou *time steps* [^1].

O problema √© an√°logo a um ca√ßa-n√≠queis com *k* alavancas [^2], onde cada a√ß√£o √© equivalente a puxar uma alavanca [^2], e a recompensa √© o *payoff* obtido [^2]. A complexidade reside no fato de que o agente n√£o conhece a distribui√ß√£o de probabilidade de cada alavanca [^2]. O agente deve, portanto, aprender a **concentrar suas a√ß√µes nas "melhores" alavancas** [^2], aquelas que oferecem as maiores recompensas esperadas.

Formalmente, denotamos a a√ß√£o selecionada no *time step* *t* como $A_t$ [^2] e a recompensa correspondente como $R_t$ [^2]. A *valor verdadeiro* ou *valor esperado* [^2] de uma a√ß√£o arbitr√°ria *a*, denotado por $q_*(a)$, √© a recompensa esperada dado que a a√ß√£o *a* √© selecionada [^2]:

$$
q_*(a) = \mathbb{E}[R_t | A_t = a] \text{.}
$$

Se o agente conhecesse o valor exato de cada a√ß√£o, a solu√ß√£o para o problema do k-armed bandit seria trivial: sempre selecionar a a√ß√£o com o maior valor [^2]. No entanto, na pr√°tica, o agente n√£o possui esse conhecimento pr√©vio e deve estimar os valores das a√ß√µes com base nas recompensas observadas [^2]. Denotamos o *valor estimado* da a√ß√£o *a* no *time step* *t* como $Q_t(a)$ [^2]. O objetivo √© fazer com que $Q_t(a)$ convirja para $q_*(a)$ [^2].

> üí° **Exemplo Num√©rico:**
> Suponha que temos um problema de 3-armed bandit (k=3). As recompensas esperadas verdadeiras para cada a√ß√£o s√£o:
> - A√ß√£o 1: $q_*(1) = 1$
> - A√ß√£o 2: $q_*(2) = 2$
> - A√ß√£o 3: $q_*(3) = 3$
>
> Neste caso, a A√ß√£o 3 √© a a√ß√£o √≥tima. Se o agente soubesse esses valores, sempre escolheria a A√ß√£o 3 para maximizar a recompensa esperada. No entanto, o agente precisa aprender esses valores atrav√©s da explora√ß√£o.
>
> Inicialmente, as estimativas podem ser $Q_0(1) = 0$, $Q_0(2) = 0$, e $Q_0(3) = 0$. O agente precisa experimentar cada a√ß√£o para obter recompensas e atualizar suas estimativas.

A necessidade de equilibrar **explora√ß√£o** e **explota√ß√£o** [^2] surge naturalmente. A **explora√ß√£o** envolve a sele√ß√£o de a√ß√µes *n√£o-gananciosas* [^2] (ou seja, a√ß√µes com valores estimados inferiores ao m√°ximo atual) para melhorar a estimativa do valor dessas a√ß√µes [^2]. A **explota√ß√£o** consiste na sele√ß√£o da a√ß√£o com o maior valor estimado atual, tamb√©m conhecida como a√ß√£o *gananciosa* [^2]. Embora a **explota√ß√£o** maximize a recompensa esperada no *time step* atual [^2], a **explora√ß√£o** pode levar a recompensas totais mais altas a longo prazo [^2], descobrindo a√ß√µes que s√£o realmente melhores, apesar de suas estimativas iniciais mais baixas.

Para quantificar a "bondade" de uma a√ß√£o, podemos definir o conceito de *gap* ou folga de otimalidade.

**Defini√ß√£o 1** (Gap de Otimalidade): O *gap de otimalidade* de uma a√ß√£o *a* no *time step* *t*, denotado por $\Delta_t(a)$, √© a diferen√ßa entre o valor estimado da a√ß√£o √≥tima e o valor estimado da a√ß√£o *a*:

$$
\Delta_t(a) = \max_{a'} Q_t(a') - Q_t(a) \text{.}
$$

Obviamente, o gap de otimalidade de uma a√ß√£o √≥tima √© sempre zero. A seguir, apresentamos um resultado simples que relaciona o gap de otimalidade com a recompensa esperada.

**Proposi√ß√£o 1** Se, para todo *a*, $Q_t(a)$ converge para $q_*(a)$ quando $t \to \infty$, ent√£o a probabilidade de selecionar uma a√ß√£o sub-√≥tima converge para zero.

*Prova*: Seja $a^*$ uma a√ß√£o √≥tima, isto √©, $q_*(a^*) = \max_a q_*(a)$. Ent√£o, para qualquer a√ß√£o sub-√≥tima $a$, temos $q_*(a^*) > q_*(a)$. Seja $\epsilon > 0$ tal que $q_*(a^*) - q_*(a) > \epsilon$. Como $Q_t(a)$ converge para $q_*(a)$ para todo *a*, existe um $T$ tal que para todo $t > T$, $|Q_t(a) - q_*(a)| < \epsilon/2$ para todo *a*. Ent√£o, para $t > T$,

$$
Q_t(a^*) > q_*(a^*) - \epsilon/2 > q_*(a) + \epsilon - \epsilon/2 > Q_t(a) + \epsilon/2 > Q_t(a) \text{.}
$$

Portanto, para $t > T$, a probabilidade de selecionar uma a√ß√£o sub-√≥tima *a* √© zero, e a probabilidade de selecionar uma a√ß√£o sub-√≥tima converge para zero quando $t \to \infty$.

√â importante notar que diferentes m√©todos de estima√ß√£o de $Q_t(a)$ podem levar a diferentes taxas de converg√™ncia para $q_*(a)$. A seguir, discutimos um m√©todo simples e amplamente utilizado para estimar os valores das a√ß√µes.

#### Estimativa da M√©dia Amostral

Uma abordagem direta para estimar o valor de uma a√ß√£o √© usar a **m√©dia amostral** das recompensas recebidas ao selecionar essa a√ß√£o [^2]. Formalmente, se denotarmos por $N_t(a)$ o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o *time step* *t*, ent√£o o valor estimado da a√ß√£o *a* no *time step* *t* √© dado por:

$$
Q_t(a) = \frac{\sum_{i=1}^{t} R_i \cdot \mathbb{I}(A_i = a)}{N_t(a)} \text{,}
$$

onde $\mathbb{I}(A_i = a)$ √© uma fun√ß√£o indicadora que vale 1 se $A_i = a$ e 0 caso contr√°rio. Quando $N_t(a) = 0$, definimos $Q_t(a)$ como um valor padr√£o, como 0.

> üí° **Exemplo Num√©rico:**
> Considere um cen√°rio em que a A√ß√£o 1 foi selecionada 3 vezes, resultando nas seguintes recompensas: 2, 3, 4. Ent√£o, $N_t(1) = 3$ e a estimativa da m√©dia amostral seria:
> $Q_t(1) = \frac{2 + 3 + 4}{3} = \frac{9}{3} = 3$.
> Este valor estimado, $Q_t(1) = 3$, representa a nossa melhor estimativa do valor real $q_*(1)$ da A√ß√£o 1, com base nas observa√ß√µes at√© o momento.

Para evitar o c√°lculo expl√≠cito da soma a cada *time step*, podemos usar uma **atualiza√ß√£o incremental** [^2]. Seja $Q_n$ a estimativa do valor de uma a√ß√£o ap√≥s *n-1* vezes que essa a√ß√£o foi selecionada, e seja $R_n$ a *n*-√©sima recompensa recebida ap√≥s selecionar essa a√ß√£o. Ent√£o, a nova estimativa $Q_{n+1}$ pode ser calculada como:

$$
Q_{n+1} = Q_n + \frac{1}{n} [R_n - Q_n] \text{.}
$$

Esta f√≥rmula representa uma forma geral de **atualiza√ß√£o incremental** [^2], onde a nova estimativa √© igual √† estimativa antiga mais um passo na dire√ß√£o do "erro", $R_n - Q_n$. O termo $\frac{1}{n}$ √© o tamanho do passo, que diminui com o tempo. Esta escolha de tamanho de passo garante a converg√™ncia de $Q_t(a)$ para $q_*(a)$ sob certas condi√ß√µes, como discutido a seguir.

> üí° **Exemplo Num√©rico:**
> Suponha que a estimativa atual para uma a√ß√£o √© $Q_n = 5$, e a recompensa obtida ap√≥s selecionar essa a√ß√£o √© $R_n = 8$. Se esta √© a 4¬™ vez que essa a√ß√£o √© selecionada (n=4), a atualiza√ß√£o incremental seria:
> $Q_{n+1} = 5 + \frac{1}{4} [8 - 5] = 5 + \frac{1}{4} [3] = 5 + 0.75 = 5.75$.
> A nova estimativa, $Q_{n+1} = 5.75$, est√° mais pr√≥xima da recompensa observada, e a mudan√ßa √© proporcional √† diferen√ßa entre a recompensa e a estimativa anterior.
>
> Aqui est√° o mesmo exemplo em c√≥digo Python:
> ```python
> Q_n = 5
> R_n = 8
> n = 4
> Q_n_plus_1 = Q_n + (1/n) * (R_n - Q_n)
> print(Q_n_plus_1)
> ```
>
> ```text
> 5.75
> ```

**Teorema 1** (Converg√™ncia da M√©dia Amostral): Se a distribui√ß√£o de recompensas para cada a√ß√£o for estacion√°ria e se todas as a√ß√µes forem selecionadas infinitas vezes, ent√£o $Q_t(a)$ converge para $q_*(a)$ com probabilidade 1 para todo *a*.

*Prova*: Para provar o Teorema 1, mostraremos que o estimador de m√©dia amostral √© consistente sob as condi√ß√µes fornecidas.

I.  **Defini√ß√£o do Estimador:** O estimador de m√©dia amostral para o valor da a√ß√£o *a* no tempo *t* √© dado por:
    $$Q_t(a) = \frac{\sum_{i=1}^{t} R_i \cdot \mathbb{I}(A_i = a)}{N_t(a)}$$
    onde $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o tempo *t*, $R_i$ √© a recompensa no tempo *i*, e $\mathbb{I}(A_i = a)$ √© a fun√ß√£o indicadora.

II. **Lei Forte dos Grandes N√∫meros:** Pela Lei Forte dos Grandes N√∫meros, se $X_1, X_2, \ldots$ s√£o vari√°veis aleat√≥rias independentes e identicamente distribu√≠das com m√©dia $\mu$, ent√£o:
    $$\frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{a.s.} \mu$$
    onde $\xrightarrow{a.s.}$ denota converg√™ncia quase certa (com probabilidade 1).

III. **Aplica√ß√£o da Lei:** No contexto do problema k-armed bandit, para cada a√ß√£o *a*, as recompensas $R_i$ obtidas ao selecionar *a* s√£o independentes e identicamente distribu√≠das (i.i.d.) com m√©dia $q_*(a)$, dado que a distribui√ß√£o de recompensas √© estacion√°ria.

IV. **Condi√ß√£o de Explora√ß√£o Infinita:** Dado que todas as a√ß√µes s√£o selecionadas infinitas vezes, $N_t(a) \to \infty$ quando $t \to \infty$ para cada a√ß√£o *a*.

V.  **Converg√™ncia:** Portanto, aplicando a Lei Forte dos Grandes N√∫meros:
    $$Q_t(a) = \frac{1}{N_t(a)}\sum_{i=1}^{t} R_i \cdot \mathbb{I}(A_i = a) \xrightarrow{a.s.} q_*(a)$$
    Isso significa que $Q_t(a)$ converge para $q_*(a)$ com probabilidade 1 para todo *a*.

Assim, demonstramos que sob as condi√ß√µes de distribui√ß√£o de recompensas estacion√°rias e explora√ß√£o infinita de todas as a√ß√µes, o estimador de m√©dia amostral converge para o valor verdadeiro da a√ß√£o com probabilidade 1. ‚ñ†

No entanto, a condi√ß√£o de que todas as a√ß√µes sejam selecionadas infinitas vezes nem sempre √© satisfeita em algoritmos de RL, especialmente aqueles que favorecem a explota√ß√£o.

Para lidar com ambientes n√£o-estacion√°rios, onde a distribui√ß√£o de recompensas pode mudar ao longo do tempo, √© comum usar um tamanho de passo constante $\alpha \in (0, 1]$ [^2]:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n] \text{.}
$$

Nesse caso, as recompensas mais recentes t√™m um peso maior na estimativa atual, permitindo que o agente se adapte a mudan√ßas no ambiente. No entanto, a converg√™ncia n√£o √© garantida nesse cen√°rio.

> üí° **Exemplo Num√©rico:**
> Suponha que a estimativa atual de uma a√ß√£o √© $Q_n = 5$, e a recompensa obtida √© $R_n = 8$. Usando um tamanho de passo constante de $\alpha = 0.1$, a atualiza√ß√£o seria:
> $Q_{n+1} = 5 + 0.1 [8 - 5] = 5 + 0.1 [3] = 5 + 0.3 = 5.3$.
>
> Comparando com o exemplo anterior onde o tamanho do passo era $\frac{1}{n}$, notamos que com $\alpha = 0.1$, a estimativa muda menos drasticamente. Isso permite que o agente se adapte a mudan√ßas no ambiente sem esquecer completamente o que aprendeu antes.
> ```python
> Q_n = 5
> R_n = 8
> alpha = 0.1
> Q_n_plus_1 = Q_n + alpha * (R_n - Q_n)
> print(Q_n_plus_1)
> ```
> ```text
> 5.3
> ```

**Observa√ß√£o 1** (Ambientes N√£o-Estacion√°rios): Em ambientes n√£o-estacion√°rios, um tamanho de passo constante $\alpha$ pode levar a uma estimativa de valor que oscila em torno do valor verdadeiro, mas permite que o agente rastreie as mudan√ßas no ambiente.

### Conclus√£o

O problema do **k-armed bandit** encapsula o *trade-off* fundamental entre **explora√ß√£o** e **explota√ß√£o** [^2] que est√° presente em muitos problemas de RL [^1]. A escolha entre explorar e explorar depende da complexidade do ambiente, das incertezas nas estimativas e do n√∫mero de *time steps* restantes [^2]. Os m√©todos para lidar com este *trade-off* s√£o variados, desde abordagens simples como m√©todos $\epsilon$-greedy [^3] at√© m√©todos mais sofisticados como Upper Confidence Bound (UCB) [^11]. O estudo do **k-armed bandit** fornece uma base essencial para a compreens√£o e desenvolvimento de algoritmos de RL mais complexos que ser√£o explorados em cap√≠tulos posteriores [^1].

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

### Refer√™ncias
[^1]: Cap√≠tulo 2, Multi-armed Bandits
[^2]: Se√ß√£o 2.1, A k-armed Bandit Problem
[^3]: Se√ß√£o 2.3, The 10-armed Testbed
[^11]: Se√ß√£o 2.7, Upper-Confidence-Bound Action Selection
<!-- END -->