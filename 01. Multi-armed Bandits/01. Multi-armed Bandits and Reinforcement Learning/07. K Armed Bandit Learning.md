## Estimativa de Valores de Ação e Estratégias de Seleção em Problemas K-Armed Bandit

### Introdução
Este capítulo explora o aprendizado em **problemas k-armed bandit**, focando na estimativa de valores de ação $Q_t(a)$ e sua utilização em **estratégias de seleção de ação** como os métodos *greedy* e *ε-greedy* [^1]. O objetivo principal é **minimizar a diferença entre os valores de ação estimados $Q_t(a)$ e os valores de ação verdadeiros $q_*(a)$** [^2].

### Conceitos Fundamentais

Em problemas *k-armed bandit*, cada ação *a* tem um valor esperado ou recompensa média, denotado por $q_*(a) = E[R_t | A_t=a]$ [^3]. No entanto, esses valores são desconhecidos inicialmente, e o agente deve aprendê-los através da **exploração** e **explotação**.

![Distribuições de recompensa para um problema de bandit de 10 braços.](./../images/image5.png)

**Estimativa de Valores de Ação:**

Um método natural para estimar os valores de ação é a **média amostral** dos recompensas recebidas:
$$
Q_t(a) = \frac{\text{soma das recompensas quando *a* foi tomada antes de *t*}} {\text{número de vezes que *a* foi tomada antes de *t*}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$
onde $\mathbb{1}_{\text{predicate}}$ é uma variável aleatória que é 1 se o *predicate* é verdadeiro e 0 caso contrário [^4]. Se o denominador for zero, $Q_t(a)$ é definido como algum valor padrão. Pela lei dos grandes números, $Q_t(a)$ converge para $q_*(a)$ conforme o denominador tende ao infinito [^4].

> 💡 **Exemplo Numérico:** Suponha que temos um problema de 3-armed bandit. Após 5 tentativas, a ação 1 foi selecionada 2 vezes e retornou recompensas de 1 e 2. Então, $Q_6(1) = \frac{1+2}{2} = 1.5$.

Para garantir que todas as ações sejam consideradas no início do aprendizado, uma inicialização otimista dos valores de ação pode ser empregada.

**Inicialização Otimista de Valores de Ação:**

Consiste em inicializar $Q_1(a)$ com valores altos. Esta técnica incentiva a exploração no início do processo de aprendizado. Uma vez que a ação *a* é selecionada, a recompensa observada geralmente é menor do que o valor inicial otimista, o que leva à seleção de outras ações até que suas estimativas também convirjam.

> 💡 **Exemplo Numérico:** Considere um 2-armed bandit. Inicializamos $Q_1(1) = 5$ e $Q_1(2) = 5$. Se selecionarmos a ação 1 e recebermos uma recompensa de 1, $Q_2(1)$ será atualizado usando a média amostral. Após a atualização, $Q_2(1)$ será menor que 5, incentivando a exploração da ação 2.

**Proposição 1** A inicialização otimista de valores de ação garante a exploração de todas as ações pelo menos uma vez, assumindo que as recompensas são limitadas superiormente.

*Prova:* Seja $B$ uma cota superior para as recompensas, ou seja, $R_t \leq B$ para todo $t$. Se inicializarmos $Q_1(a) > B$ para todo *a*, então, na primeira vez que uma ação *a* é selecionada, $Q_2(a)$ será menor que $Q_1(a)$ devido à recompensa ser menor que o valor inicial. Isso incentiva a seleção de outras ações até que todas tenham sido escolhidas pelo menos uma vez.

**Estratégias de Seleção de Ação:**

1.  **Greedy:** Essa estratégia sempre seleciona a ação com o maior valor estimado [^4]:
    $$
    A_t = \underset{a}{\text{argmax}} \ Q_t(a)
    $$
    O método *greedy* explora o conhecimento atual para maximizar a recompensa imediata, mas pode ficar preso em ações subótimas.

    > 💡 **Exemplo Numérico:** Se $Q_t(1) = 2$, $Q_t(2) = 3$, e $Q_t(3) = 1$, a estratégia *greedy* selecionaria a ação 2, pois tem o maior valor estimado.

2.  **ε-greedy:** Essa estratégia seleciona a ação *greedy* com probabilidade $1 - \varepsilon$ e uma ação aleatória com probabilidade $\varepsilon$ [^4]. Isso garante que todas as ações serão amostradas infinitas vezes à medida que o número de etapas aumenta, assegurando que $Q_t(a)$ convirja para $q_*(a)$ [^4].

    > 💡 **Exemplo Numérico:** Se $\varepsilon = 0.1$, então em 90% das vezes a ação com maior valor estimado é selecionada, e em 10% das vezes uma ação é selecionada aleatoriamente. Se $Q_t(1) = 2$, $Q_t(2) = 3$, e $Q_t(3) = 1$, a ação 2 seria selecionada com probabilidade 0.9. As ações 1 e 3 seriam selecionadas com probabilidade 0.05 cada.

![Pseudocódigo de um algoritmo de bandit simples com estratégia ε-greedy para exploração e explotação.](./../images/image4.png)

Além do método ε-greedy, existem outras estratégias de seleção de ação que equilibram exploração e explotação, como a *softmax action selection*.

**Softmax Action Selection (ou Boltzmann distribution):**

Esta estratégia usa uma distribuição de probabilidade sobre as ações, onde a probabilidade de selecionar uma ação é proporcional ao seu valor estimado. A probabilidade de selecionar a ação *a* é dada por:

$$
P(A_t = a) = \frac{e^{Q_t(a) / \tau}}{\sum_{b=1}^{k} e^{Q_t(b) / \tau}}
$$

onde $\tau$ é um parâmetro de temperatura que controla a aleatoriedade da seleção. Quando $\tau$ é alto, todas as ações têm aproximadamente a mesma probabilidade de serem selecionadas (exploração). Quando $\tau$ é baixo, a ação com o maior valor estimado é selecionada com alta probabilidade (explotação).

> 💡 **Exemplo Numérico:** Considere três ações com $Q_t(1) = 1$, $Q_t(2) = 2$, e $Q_t(3) = 3$. Se $\tau = 1$, as probabilidades de selecionar cada ação são:
>
> $P(A_t = 1) = \frac{e^{1/1}}{e^{1/1} + e^{2/1} + e^{3/1}} \approx \frac{2.72}{2.72 + 7.39 + 20.09} \approx 0.09$
>
> $P(A_t = 2) = \frac{e^{2/1}}{e^{1/1} + e^{2/1} + e^{3/1}} \approx \frac{7.39}{2.72 + 7.39 + 20.09} \approx 0.25$
>
> $P(A_t = 3) = \frac{e^{3/1}}{e^{1/1} + e^{2/1} + e^{3/1}} \approx \frac{20.09}{2.72 + 7.39 + 20.09} \approx 0.66$
>
> Se $\tau = 0.1$, a probabilidade de selecionar a ação 3 se aproxima de 1, e as outras probabilidades se aproximam de 0.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Valores de ação
> Q = np.array([1, 2, 3])
>
> # Temperaturas para testar
> tau_values = [0.1, 1, 10]
>
> # Calcular probabilidades para cada temperatura
> probabilities = []
> for tau in tau_values:
>     probabilities.append(np.exp(Q / tau) / np.sum(np.exp(Q / tau)))
>
> # Plotar os resultados
> plt.figure(figsize=(10, 6))
> for i, tau in enumerate(tau_values):
>     plt.plot(Q, probabilities[i], marker='o', label=f'τ = {tau}')
>
> plt.title('Softmax Action Selection Probabilities')
> plt.xlabel('Action Value (Q_t(a))')
> plt.ylabel('Probability P(A_t = a)')
> plt.xticks(Q)
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

**Observação 1:** A estratégia softmax action selection generaliza a estratégia greedy. Quando $\tau \rightarrow 0$, a probabilidade da ação com maior $Q_t(a)$ tende a 1, recuperando a estratégia greedy.

**Conflito Exploração-Explotação:**

A escolha entre exploração e explotação é central nos problemas *k-armed bandit* [^2]. A explotação maximiza a recompensa imediata, enquanto a exploração melhora a estimativa dos valores de ação e pode levar a maiores recompensas no longo prazo [^2]. É importante notar que não é possível explorar e explorar com a mesma ação, o que gera um conflito entre os dois [^2].

![Average performance of ε-greedy action-value methods on a 10-armed testbed, demonstrating the exploration-exploitation trade-off.](./../images/image6.png)

**Implementação Incremental:**

Para calcular as médias amostrais de forma eficiente, podemos usar uma **implementação incremental** [^5]. Dada $Q_n$ (a estimativa após n-1 seleções) e a n-ésima recompensa $R_n$, a nova média é:

$$
Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]
$$

Esta atualização tem a forma geral:

$$
\text{NovoEstimate} \leftarrow \text{OldEstimate} + \text{StepSize} [\text{Target} - \text{OldEstimate}]
$$

onde *StepSize* é $\frac{1}{n}$ neste caso.

> 💡 **Exemplo Numérico:** Suponha que $Q_5(a) = 2$ e a 5ª recompensa $R_5$ é 3. Então, $Q_6(a) = 2 + \frac{1}{5}[3 - 2] = 2 + 0.2 = 2.2$.

Para demonstrar a equivalência entre a fórmula incremental e a média amostral, apresentamos a seguinte prova:

*Prova:*
Queremos mostrar que a atualização incremental
$$Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]$$
é equivalente a calcular a média amostral após *n* recompensas.

I. Assumimos que $Q_n$ é a média amostral das primeiras $n-1$ recompensas:
   $$Q_n = \frac{1}{n-1} \sum_{i=1}^{n-1} R_i$$

II. Substituímos $Q_n$ na fórmula de atualização incremental:
    $$Q_{n+1} = \frac{1}{n-1} \sum_{i=1}^{n-1} R_i + \frac{1}{n} \left[ R_n - \frac{1}{n-1} \sum_{i=1}^{n-1} R_i \right]$$

III. Simplificamos a expressão:
     $$Q_{n+1} = \frac{n}{n(n-1)} \sum_{i=1}^{n-1} R_i + \frac{1}{n} R_n - \frac{1}{n(n-1)} \sum_{i=1}^{n-1} R_i$$
     $$Q_{n+1} = \frac{n-1}{n(n-1)} \sum_{i=1}^{n-1} R_i + \frac{1}{n} R_n$$
     $$Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n-1} R_i + \frac{1}{n} R_n$$

IV. Combinamos as somas:
    $$Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i$$

V. Portanto, $Q_{n+1}$ é a média amostral das primeiras *n* recompensas. $\blacksquare$

**Rastreamento de um Problema Não Estacionário:**

Em problemas não estacionários (onde as probabilidades de recompensa mudam ao longo do tempo), é mais sensato dar mais peso às recompensas recentes do que às recompensas antigas [^6]. Isso pode ser feito usando um parâmetro de tamanho de passo constante $\alpha \in (0, 1]$ [^6]:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n]
$$

Neste caso, $Q_{n+1}$ se torna uma média ponderada de recompensas passadas e a estimativa inicial $Q_1$:
$$
Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i
$$
Essa técnica atribui maior peso às recompensas recentes, o que é útil em ambientes não estacionários, mas resulta em um **viés permanente** nas estimativas, conforme expresso na equação [^6].

> 💡 **Exemplo Numérico:** Seja $\alpha = 0.1$, $Q_1 = 0$, e as primeiras recompensas $R_1 = 1$, $R_2 = 2$, $R_3 = 3$.
>
> $Q_2 = Q_1 + \alpha [R_1 - Q_1] = 0 + 0.1[1 - 0] = 0.1$
>
> $Q_3 = Q_2 + \alpha [R_2 - Q_2] = 0.1 + 0.1[2 - 0.1] = 0.1 + 0.19 = 0.29$
>
> $Q_4 = Q_3 + \alpha [R_3 - Q_3] = 0.29 + 0.1[3 - 0.29] = 0.29 + 0.271 = 0.561$
>
> Usando a fórmula direta:
>
> $Q_4 = (1 - \alpha)^3 Q_1 + \sum_{i=1}^{3} \alpha (1 - \alpha)^{3-i} R_i = (0.9)^3 \cdot 0 + \alpha (0.9)^2 R_1 + \alpha (0.9)^1 R_2 + \alpha (0.9)^0 R_3 = 0 + 0.1(0.81)(1) + 0.1(0.9)(2) + 0.1(1)(3) = 0.081 + 0.18 + 0.3 = 0.561$

Para demonstrar a validade desta expressão para $Q_{n+1}$, podemos proceder por indução:
*Prova:*

I. Caso base (n = 1):
   $$Q_2 = Q_1 + \alpha [R_1 - Q_1] = (1 - \alpha) Q_1 + \alpha R_1$$
   A fórmula dada se reduz a:
   $$(1 - \alpha)^1 Q_1 + \sum_{i=1}^{1} \alpha (1 - \alpha)^{1-i} R_i = (1 - \alpha) Q_1 + \alpha (1 - \alpha)^0 R_1 = (1 - \alpha) Q_1 + \alpha R_1$$
   Portanto, a fórmula é válida para n = 1.

II. Hipótese indutiva:
    Assumimos que a fórmula é válida para algum n = k:
    $$Q_{k+1} = (1 - \alpha)^k Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i$$

III. Passo indutivo:
     Precisamos mostrar que a fórmula é válida para n = k + 1. Usando a atualização incremental:
     $$Q_{k+2} = Q_{k+1} + \alpha [R_{k+1} - Q_{k+1}]$$
     Substituímos $Q_{k+1}$ pela hipótese indutiva:
     $$Q_{k+2} = (1 - \alpha)^k Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i + \alpha \left[ R_{k+1} - \left((1 - \alpha)^k Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i \right) \right]$$
     $$Q_{k+2} = (1 - \alpha)^k Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i + \alpha R_{k+1} - \alpha (1 - \alpha)^k Q_1 - \alpha \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i $$
     $$Q_{k+2} = (1 - \alpha)^k Q_1 - \alpha (1 - \alpha)^k Q_1+ \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i -  \alpha \sum_{i=1}^{k} (1 - \alpha)^{k-i} R_i+ \alpha R_{k+1} $$
     $$Q_{k+2} = (1 - \alpha)^{k+1} Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i+1-1} R_i  + \alpha R_{k+1} $$
     $$Q_{k+2} = (1 - \alpha)^{k+1} Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{(k+1)-i-1} R_i  + \alpha R_{k+1} $$
    $$Q_{k+2} = (1 - \alpha)^{k+1} Q_1 + \sum_{i=1}^{k+1} \alpha (1 - \alpha)^{(k+1)-i} R_i $$

IV. Conclusão:
    Portanto, a fórmula é válida para n = k + 1. Pelo princípio da indução matemática, a fórmula é válida para todos os valores de n >= 1. $\blacksquare$

Podemos quantificar esse viés.

**Teorema 1** Em um ambiente não-estacionário, usando um tamanho de passo constante $\alpha$, o viés na estimativa $Q_{n+1}$ é dado por $E[Q_{n+1}] - q_*(a)$, onde $q_*(a)$ é o valor verdadeiro da ação no instante $n+1$.

*Prova (Esboço):*  A prova envolve tomar o valor esperado da equação $Q_{n+1} = Q_n + \alpha [R_n - Q_n]$ e iterativamente expandir $Q_n$ em termos de recompensas anteriores e $Q_1$. Sob a condição de não-estacionariedade, $E[R_i]$ não é constante e, portanto, a expressão resultante não se simplifica para $q_*(a)$. A diferença entre $E[Q_{n+1}]$ e $q_*(a)$ representa o viés. Uma análise mais aprofundada requer modelar a evolução de $q_*(a)$ ao longo do tempo.

### Conclusão

Este capítulo apresentou os conceitos fundamentais para o aprendizado em problemas *k-armed bandit*, incluindo a **estimativa de valores de ação** e **estratégias de seleção de ação**. Métodos como o ε-greedy permitem um equilíbrio entre exploração e explotação, e a implementação incremental torna o cálculo das médias mais eficiente [^4]. Adicionalmente, em problemas não estacionários, é importante dar maior peso para as recompensas mais recentes [^6]. A inicialização otimista de valores de ação e a estratégia softmax são alternativas importantes para lidar com o dilema exploração-explotação.

![Comparative performance of optimistic vs. realistic action-value initialization in a 10-armed testbed, illustrating the impact on optimal action selection over time.](./../images/image3.png)

![Average performance comparison of UCB and ε-greedy action selection on a 10-armed testbed.](./../images/image7.png)

![Parameter study comparing bandit algorithms, showing average reward over 1000 steps as a function of algorithm-specific parameters.](./../images/image2.png)

![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 braços.](./../images/image1.png)

### Referências
[^1]: Chapter 2: Multi-armed Bandits
[^2]: 2.1 A k-armed Bandit Problem
[^3]: 2.1 A k-armed Bandit Problem
[^4]: 2.2 Action-value Methods
[^5]: 2.4 Incremental Implementation
[^6]: 2.5 Tracking a Nonstationary Problem
<!-- END -->