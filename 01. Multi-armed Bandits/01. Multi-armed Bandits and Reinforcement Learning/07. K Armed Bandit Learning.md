## Estimativa de Valores de A√ß√£o e Estrat√©gias de Sele√ß√£o em Problemas K-Armed Bandit

### Introdu√ß√£o
Este cap√≠tulo explora o aprendizado em **problemas k-armed bandit**, focando na estimativa de valores de a√ß√£o $Q_t(a)$ e sua utiliza√ß√£o em **estrat√©gias de sele√ß√£o de a√ß√£o** como os m√©todos *greedy* e *Œµ-greedy* [^1]. O objetivo principal √© **minimizar a diferen√ßa entre os valores de a√ß√£o estimados $Q_t(a)$ e os valores de a√ß√£o verdadeiros $q_*(a)$** [^2].

### Conceitos Fundamentais

Em problemas *k-armed bandit*, cada a√ß√£o *a* tem um valor esperado ou recompensa m√©dia, denotado por $q_*(a) = E[R_t | A_t=a]$ [^3]. No entanto, esses valores s√£o desconhecidos inicialmente, e o agente deve aprend√™-los atrav√©s da **explora√ß√£o** e **explota√ß√£o**.

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

**Estimativa de Valores de A√ß√£o:**

Um m√©todo natural para estimar os valores de a√ß√£o √© a **m√©dia amostral** dos recompensas recebidas:
$$
Q_t(a) = \frac{\text{soma das recompensas quando *a* foi tomada antes de *t*}} {\text{n√∫mero de vezes que *a* foi tomada antes de *t*}} = \frac{\sum_{i=1}^{t-1} R_i \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}}
$$
onde $\mathbb{1}_{\text{predicate}}$ √© uma vari√°vel aleat√≥ria que √© 1 se o *predicate* √© verdadeiro e 0 caso contr√°rio [^4]. Se o denominador for zero, $Q_t(a)$ √© definido como algum valor padr√£o. Pela lei dos grandes n√∫meros, $Q_t(a)$ converge para $q_*(a)$ conforme o denominador tende ao infinito [^4].

> üí° **Exemplo Num√©rico:** Suponha que temos um problema de 3-armed bandit. Ap√≥s 5 tentativas, a a√ß√£o 1 foi selecionada 2 vezes e retornou recompensas de 1 e 2. Ent√£o, $Q_6(1) = \frac{1+2}{2} = 1.5$.

Para garantir que todas as a√ß√µes sejam consideradas no in√≠cio do aprendizado, uma inicializa√ß√£o otimista dos valores de a√ß√£o pode ser empregada.

**Inicializa√ß√£o Otimista de Valores de A√ß√£o:**

Consiste em inicializar $Q_1(a)$ com valores altos. Esta t√©cnica incentiva a explora√ß√£o no in√≠cio do processo de aprendizado. Uma vez que a a√ß√£o *a* √© selecionada, a recompensa observada geralmente √© menor do que o valor inicial otimista, o que leva √† sele√ß√£o de outras a√ß√µes at√© que suas estimativas tamb√©m convirjam.

> üí° **Exemplo Num√©rico:** Considere um 2-armed bandit. Inicializamos $Q_1(1) = 5$ e $Q_1(2) = 5$. Se selecionarmos a a√ß√£o 1 e recebermos uma recompensa de 1, $Q_2(1)$ ser√° atualizado usando a m√©dia amostral. Ap√≥s a atualiza√ß√£o, $Q_2(1)$ ser√° menor que 5, incentivando a explora√ß√£o da a√ß√£o 2.

**Proposi√ß√£o 1** A inicializa√ß√£o otimista de valores de a√ß√£o garante a explora√ß√£o de todas as a√ß√µes pelo menos uma vez, assumindo que as recompensas s√£o limitadas superiormente.

*Prova:* Seja $B$ uma cota superior para as recompensas, ou seja, $R_t \leq B$ para todo $t$. Se inicializarmos $Q_1(a) > B$ para todo *a*, ent√£o, na primeira vez que uma a√ß√£o *a* √© selecionada, $Q_2(a)$ ser√° menor que $Q_1(a)$ devido √† recompensa ser menor que o valor inicial. Isso incentiva a sele√ß√£o de outras a√ß√µes at√© que todas tenham sido escolhidas pelo menos uma vez.

**Estrat√©gias de Sele√ß√£o de A√ß√£o:**

1.  **Greedy:** Essa estrat√©gia sempre seleciona a a√ß√£o com o maior valor estimado [^4]:
    $$
    A_t = \underset{a}{\text{argmax}} \ Q_t(a)
    $$
    O m√©todo *greedy* explora o conhecimento atual para maximizar a recompensa imediata, mas pode ficar preso em a√ß√µes sub√≥timas.

    > üí° **Exemplo Num√©rico:** Se $Q_t(1) = 2$, $Q_t(2) = 3$, e $Q_t(3) = 1$, a estrat√©gia *greedy* selecionaria a a√ß√£o 2, pois tem o maior valor estimado.

2.  **Œµ-greedy:** Essa estrat√©gia seleciona a a√ß√£o *greedy* com probabilidade $1 - \varepsilon$ e uma a√ß√£o aleat√≥ria com probabilidade $\varepsilon$ [^4]. Isso garante que todas as a√ß√µes ser√£o amostradas infinitas vezes √† medida que o n√∫mero de etapas aumenta, assegurando que $Q_t(a)$ convirja para $q_*(a)$ [^4].

    > üí° **Exemplo Num√©rico:** Se $\varepsilon = 0.1$, ent√£o em 90% das vezes a a√ß√£o com maior valor estimado √© selecionada, e em 10% das vezes uma a√ß√£o √© selecionada aleatoriamente. Se $Q_t(1) = 2$, $Q_t(2) = 3$, e $Q_t(3) = 1$, a a√ß√£o 2 seria selecionada com probabilidade 0.9. As a√ß√µes 1 e 3 seriam selecionadas com probabilidade 0.05 cada.

![Pseudoc√≥digo de um algoritmo de bandit simples com estrat√©gia Œµ-greedy para explora√ß√£o e explota√ß√£o.](./../images/image4.png)

Al√©m do m√©todo Œµ-greedy, existem outras estrat√©gias de sele√ß√£o de a√ß√£o que equilibram explora√ß√£o e explota√ß√£o, como a *softmax action selection*.

**Softmax Action Selection (ou Boltzmann distribution):**

Esta estrat√©gia usa uma distribui√ß√£o de probabilidade sobre as a√ß√µes, onde a probabilidade de selecionar uma a√ß√£o √© proporcional ao seu valor estimado. A probabilidade de selecionar a a√ß√£o *a* √© dada por:

$$
P(A_t = a) = \frac{e^{Q_t(a) / \tau}}{\sum_{b=1}^{k} e^{Q_t(b) / \tau}}
$$

onde $\tau$ √© um par√¢metro de temperatura que controla a aleatoriedade da sele√ß√£o. Quando $\tau$ √© alto, todas as a√ß√µes t√™m aproximadamente a mesma probabilidade de serem selecionadas (explora√ß√£o). Quando $\tau$ √© baixo, a a√ß√£o com o maior valor estimado √© selecionada com alta probabilidade (explota√ß√£o).

> üí° **Exemplo Num√©rico:** Considere tr√™s a√ß√µes com $Q_t(1) = 1$, $Q_t(2) = 2$, e $Q_t(3) = 3$. Se $\tau = 1$, as probabilidades de selecionar cada a√ß√£o s√£o:
>
> $P(A_t = 1) = \frac{e^{1/1}}{e^{1/1} + e^{2/1} + e^{3/1}} \approx \frac{2.72}{2.72 + 7.39 + 20.09} \approx 0.09$
>
> $P(A_t = 2) = \frac{e^{2/1}}{e^{1/1} + e^{2/1} + e^{3/1}} \approx \frac{7.39}{2.72 + 7.39 + 20.09} \approx 0.25$
>
> $P(A_t = 3) = \frac{e^{3/1}}{e^{1/1} + e^{2/1} + e^{3/1}} \approx \frac{20.09}{2.72 + 7.39 + 20.09} \approx 0.66$
>
> Se $\tau = 0.1$, a probabilidade de selecionar a a√ß√£o 3 se aproxima de 1, e as outras probabilidades se aproximam de 0.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Valores de a√ß√£o
> Q = np.array([1, 2, 3])
>
> # Temperaturas para testar
> tau_values = [0.1, 1, 10]
>
> # Calcular probabilidades para cada temperatura
> probabilities = []
> for tau in tau_values:
>     probabilities.append(np.exp(Q / tau) / np.sum(np.exp(Q / tau)))
>
> # Plotar os resultados
> plt.figure(figsize=(10, 6))
> for i, tau in enumerate(tau_values):
>     plt.plot(Q, probabilities[i], marker='o', label=f'œÑ = {tau}')
>
> plt.title('Softmax Action Selection Probabilities')
> plt.xlabel('Action Value (Q_t(a))')
> plt.ylabel('Probability P(A_t = a)')
> plt.xticks(Q)
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

**Observa√ß√£o 1:** A estrat√©gia softmax action selection generaliza a estrat√©gia greedy. Quando $\tau \rightarrow 0$, a probabilidade da a√ß√£o com maior $Q_t(a)$ tende a 1, recuperando a estrat√©gia greedy.

**Conflito Explora√ß√£o-Explota√ß√£o:**

A escolha entre explora√ß√£o e explota√ß√£o √© central nos problemas *k-armed bandit* [^2]. A explota√ß√£o maximiza a recompensa imediata, enquanto a explora√ß√£o melhora a estimativa dos valores de a√ß√£o e pode levar a maiores recompensas no longo prazo [^2]. √â importante notar que n√£o √© poss√≠vel explorar e explorar com a mesma a√ß√£o, o que gera um conflito entre os dois [^2].

![Average performance of Œµ-greedy action-value methods on a 10-armed testbed, demonstrating the exploration-exploitation trade-off.](./../images/image6.png)

**Implementa√ß√£o Incremental:**

Para calcular as m√©dias amostrais de forma eficiente, podemos usar uma **implementa√ß√£o incremental** [^5]. Dada $Q_n$ (a estimativa ap√≥s n-1 sele√ß√µes) e a n-√©sima recompensa $R_n$, a nova m√©dia √©:

$$
Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]
$$

Esta atualiza√ß√£o tem a forma geral:

$$
\text{NovoEstimate} \leftarrow \text{OldEstimate} + \text{StepSize} [\text{Target} - \text{OldEstimate}]
$$

onde *StepSize* √© $\frac{1}{n}$ neste caso.

> üí° **Exemplo Num√©rico:** Suponha que $Q_5(a) = 2$ e a 5¬™ recompensa $R_5$ √© 3. Ent√£o, $Q_6(a) = 2 + \frac{1}{5}[3 - 2] = 2 + 0.2 = 2.2$.

Para demonstrar a equival√™ncia entre a f√≥rmula incremental e a m√©dia amostral, apresentamos a seguinte prova:

*Prova:*
Queremos mostrar que a atualiza√ß√£o incremental
$$Q_{n+1} = Q_n + \frac{1}{n}[R_n - Q_n]$$
√© equivalente a calcular a m√©dia amostral ap√≥s *n* recompensas.

I. Assumimos que $Q_n$ √© a m√©dia amostral das primeiras $n-1$ recompensas:
   $$Q_n = \frac{1}{n-1} \sum_{i=1}^{n-1} R_i$$

II. Substitu√≠mos $Q_n$ na f√≥rmula de atualiza√ß√£o incremental:
    $$Q_{n+1} = \frac{1}{n-1} \sum_{i=1}^{n-1} R_i + \frac{1}{n} \left[ R_n - \frac{1}{n-1} \sum_{i=1}^{n-1} R_i \right]$$

III. Simplificamos a express√£o:
     $$Q_{n+1} = \frac{n}{n(n-1)} \sum_{i=1}^{n-1} R_i + \frac{1}{n} R_n - \frac{1}{n(n-1)} \sum_{i=1}^{n-1} R_i$$
     $$Q_{n+1} = \frac{n-1}{n(n-1)} \sum_{i=1}^{n-1} R_i + \frac{1}{n} R_n$$
     $$Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n-1} R_i + \frac{1}{n} R_n$$

IV. Combinamos as somas:
    $$Q_{n+1} = \frac{1}{n} \sum_{i=1}^{n} R_i$$

V. Portanto, $Q_{n+1}$ √© a m√©dia amostral das primeiras *n* recompensas. $\blacksquare$

**Rastreamento de um Problema N√£o Estacion√°rio:**

Em problemas n√£o estacion√°rios (onde as probabilidades de recompensa mudam ao longo do tempo), √© mais sensato dar mais peso √†s recompensas recentes do que √†s recompensas antigas [^6]. Isso pode ser feito usando um par√¢metro de tamanho de passo constante $\alpha \in (0, 1]$ [^6]:

$$
Q_{n+1} = Q_n + \alpha [R_n - Q_n]
$$

Neste caso, $Q_{n+1}$ se torna uma m√©dia ponderada de recompensas passadas e a estimativa inicial $Q_1$:
$$
Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^{n} \alpha (1 - \alpha)^{n-i} R_i
$$
Essa t√©cnica atribui maior peso √†s recompensas recentes, o que √© √∫til em ambientes n√£o estacion√°rios, mas resulta em um **vi√©s permanente** nas estimativas, conforme expresso na equa√ß√£o [^6].

> üí° **Exemplo Num√©rico:** Seja $\alpha = 0.1$, $Q_1 = 0$, e as primeiras recompensas $R_1 = 1$, $R_2 = 2$, $R_3 = 3$.
>
> $Q_2 = Q_1 + \alpha [R_1 - Q_1] = 0 + 0.1[1 - 0] = 0.1$
>
> $Q_3 = Q_2 + \alpha [R_2 - Q_2] = 0.1 + 0.1[2 - 0.1] = 0.1 + 0.19 = 0.29$
>
> $Q_4 = Q_3 + \alpha [R_3 - Q_3] = 0.29 + 0.1[3 - 0.29] = 0.29 + 0.271 = 0.561$
>
> Usando a f√≥rmula direta:
>
> $Q_4 = (1 - \alpha)^3 Q_1 + \sum_{i=1}^{3} \alpha (1 - \alpha)^{3-i} R_i = (0.9)^3 \cdot 0 + \alpha (0.9)^2 R_1 + \alpha (0.9)^1 R_2 + \alpha (0.9)^0 R_3 = 0 + 0.1(0.81)(1) + 0.1(0.9)(2) + 0.1(1)(3) = 0.081 + 0.18 + 0.3 = 0.561$

Para demonstrar a validade desta express√£o para $Q_{n+1}$, podemos proceder por indu√ß√£o:
*Prova:*

I. Caso base (n = 1):
   $$Q_2 = Q_1 + \alpha [R_1 - Q_1] = (1 - \alpha) Q_1 + \alpha R_1$$
   A f√≥rmula dada se reduz a:
   $$(1 - \alpha)^1 Q_1 + \sum_{i=1}^{1} \alpha (1 - \alpha)^{1-i} R_i = (1 - \alpha) Q_1 + \alpha (1 - \alpha)^0 R_1 = (1 - \alpha) Q_1 + \alpha R_1$$
   Portanto, a f√≥rmula √© v√°lida para n = 1.

II. Hip√≥tese indutiva:
    Assumimos que a f√≥rmula √© v√°lida para algum n = k:
    $$Q_{k+1} = (1 - \alpha)^k Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i$$

III. Passo indutivo:
     Precisamos mostrar que a f√≥rmula √© v√°lida para n = k + 1. Usando a atualiza√ß√£o incremental:
     $$Q_{k+2} = Q_{k+1} + \alpha [R_{k+1} - Q_{k+1}]$$
     Substitu√≠mos $Q_{k+1}$ pela hip√≥tese indutiva:
     $$Q_{k+2} = (1 - \alpha)^k Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i + \alpha \left[ R_{k+1} - \left((1 - \alpha)^k Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i \right) \right]$$
     $$Q_{k+2} = (1 - \alpha)^k Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i + \alpha R_{k+1} - \alpha (1 - \alpha)^k Q_1 - \alpha \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i $$
     $$Q_{k+2} = (1 - \alpha)^k Q_1 - \alpha (1 - \alpha)^k Q_1+ \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i} R_i -  \alpha \sum_{i=1}^{k} (1 - \alpha)^{k-i} R_i+ \alpha R_{k+1} $$
     $$Q_{k+2} = (1 - \alpha)^{k+1} Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{k-i+1-1} R_i  + \alpha R_{k+1} $$
     $$Q_{k+2} = (1 - \alpha)^{k+1} Q_1 + \sum_{i=1}^{k} \alpha (1 - \alpha)^{(k+1)-i-1} R_i  + \alpha R_{k+1} $$
    $$Q_{k+2} = (1 - \alpha)^{k+1} Q_1 + \sum_{i=1}^{k+1} \alpha (1 - \alpha)^{(k+1)-i} R_i $$

IV. Conclus√£o:
    Portanto, a f√≥rmula √© v√°lida para n = k + 1. Pelo princ√≠pio da indu√ß√£o matem√°tica, a f√≥rmula √© v√°lida para todos os valores de n >= 1. $\blacksquare$

Podemos quantificar esse vi√©s.

**Teorema 1** Em um ambiente n√£o-estacion√°rio, usando um tamanho de passo constante $\alpha$, o vi√©s na estimativa $Q_{n+1}$ √© dado por $E[Q_{n+1}] - q_*(a)$, onde $q_*(a)$ √© o valor verdadeiro da a√ß√£o no instante $n+1$.

*Prova (Esbo√ßo):*  A prova envolve tomar o valor esperado da equa√ß√£o $Q_{n+1} = Q_n + \alpha [R_n - Q_n]$ e iterativamente expandir $Q_n$ em termos de recompensas anteriores e $Q_1$. Sob a condi√ß√£o de n√£o-estacionariedade, $E[R_i]$ n√£o √© constante e, portanto, a express√£o resultante n√£o se simplifica para $q_*(a)$. A diferen√ßa entre $E[Q_{n+1}]$ e $q_*(a)$ representa o vi√©s. Uma an√°lise mais aprofundada requer modelar a evolu√ß√£o de $q_*(a)$ ao longo do tempo.

### Conclus√£o

Este cap√≠tulo apresentou os conceitos fundamentais para o aprendizado em problemas *k-armed bandit*, incluindo a **estimativa de valores de a√ß√£o** e **estrat√©gias de sele√ß√£o de a√ß√£o**. M√©todos como o Œµ-greedy permitem um equil√≠brio entre explora√ß√£o e explota√ß√£o, e a implementa√ß√£o incremental torna o c√°lculo das m√©dias mais eficiente [^4]. Adicionalmente, em problemas n√£o estacion√°rios, √© importante dar maior peso para as recompensas mais recentes [^6]. A inicializa√ß√£o otimista de valores de a√ß√£o e a estrat√©gia softmax s√£o alternativas importantes para lidar com o dilema explora√ß√£o-explota√ß√£o.

![Comparative performance of optimistic vs. realistic action-value initialization in a 10-armed testbed, illustrating the impact on optimal action selection over time.](./../images/image3.png)

![Average performance comparison of UCB and Œµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

![Parameter study comparing bandit algorithms, showing average reward over 1000 steps as a function of algorithm-specific parameters.](./../images/image2.png)

![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 bra√ßos.](./../images/image1.png)

### Refer√™ncias
[^1]: Chapter 2: Multi-armed Bandits
[^2]: 2.1 A k-armed Bandit Problem
[^3]: 2.1 A k-armed Bandit Problem
[^4]: 2.2 Action-value Methods
[^5]: 2.4 Incremental Implementation
[^6]: 2.5 Tracking a Nonstationary Problem
<!-- END -->