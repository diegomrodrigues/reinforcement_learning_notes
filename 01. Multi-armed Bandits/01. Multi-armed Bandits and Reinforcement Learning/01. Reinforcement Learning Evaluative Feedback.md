## Feedback Avaliativo e a Necessidade de Explora√ß√£o em Multi-Armed Bandits

### Introdu√ß√£o
O aprendizado por refor√ßo (RL) se distingue de outras formas de aprendizado pela utiliza√ß√£o de informa√ß√£o de treinamento que *avalia* as a√ß√µes tomadas, em vez de *instruir* fornecendo as a√ß√µes corretas [^1]. Esta caracter√≠stica fundamental impulsiona a necessidade de **explora√ß√£o ativa**, um processo de busca expl√≠cita por comportamentos eficazes [^1]. Em contraste com o aprendizado supervisionado, onde o feedback instrutivo indica a a√ß√£o correta independentemente do que foi feito, o RL lida com feedback avaliativo, que apenas indica a qualidade da a√ß√£o tomada, sem indicar se foi a melhor ou a pior a√ß√£o poss√≠vel [^1].

### Conceitos Fundamentais

**Feedback Avaliativo vs. Feedback Instrutivo:** A distin√ß√£o crucial entre feedback avaliativo e instrutivo reside em sua depend√™ncia da a√ß√£o tomada. O feedback avaliativo depende inteiramente da a√ß√£o executada, enquanto o feedback instrutivo √© independente dela [^1].
*Feedback avaliativo* indica qu√£o boa foi a a√ß√£o tomada, mas n√£o se foi a melhor ou a pior a√ß√£o poss√≠vel [^1].
*Feedback instrutivo*, por outro lado, indica a a√ß√£o correta a ser tomada, independentemente da a√ß√£o realmente tomada [^1]. Este √∫ltimo √© a base do aprendizado supervisionado, que inclui grandes partes de classifica√ß√£o de padr√µes, redes neurais artificiais e identifica√ß√£o de sistemas [^1].

**A Necessidade de Explora√ß√£o Ativa:** No contexto do aprendizado por refor√ßo, a aus√™ncia de feedback instrutivo direto imp√µe a necessidade de **explora√ß√£o ativa**. Dado que o agente n√£o √© informado sobre qual a√ß√£o √© a correta, ele deve, ao inv√©s disso, experimentar diferentes a√ß√µes para descobrir quais levam √†s maiores recompensas a longo prazo [^1]. Essa explora√ß√£o pode ser feita de forma aleat√≥ria ou guiada por heur√≠sticas que incentivam a tentativa de a√ß√µes pouco conhecidas ou promissoras [^1].

> üí° **Exemplo Num√©rico:** Imagine um rob√¥ aprendendo a cozinhar. Se o rob√¥ tenta adicionar sal e a comida fica boa (feedback avaliativo positivo), ele sabe que adicionar sal *pode* ser uma boa a√ß√£o. No entanto, ele n√£o sabe se adicionar pimenta *teria sido melhor* (feedback instrutivo). Para descobrir isso, o rob√¥ precisa *explorar* e tentar adicionar pimenta, mesmo que adicionar sal tenha funcionado bem antes. Se a receita viesse com instru√ß√µes detalhadas (feedback instrutivo), o rob√¥ n√£o precisaria explorar tanto, pois j√° saberia quais a√ß√µes tomar.

**O Problema do *k*-Armed Bandit:** O problema do *k*-armed bandit oferece um cen√°rio simplificado para estudar a explora√ß√£o e explota√ß√£o em RL. Neste problema, um agente √© repetidamente confrontado com uma escolha entre *k* diferentes op√ß√µes ou a√ß√µes. Ap√≥s cada escolha, o agente recebe uma recompensa num√©rica amostrada de uma distribui√ß√£o de probabilidade estacion√°ria que depende da a√ß√£o selecionada [^1]. O objetivo do agente √© maximizar a recompensa total esperada ao longo do tempo [^2].

> üí° **Exemplo Num√©rico:** Considere um ca√ßa-n√≠queis com *k* = 5 bra√ßos. Cada bra√ßo paga com uma probabilidade diferente e um valor diferente. O jogador (agente) n√£o sabe as probabilidades e valores de cada bra√ßo. O objetivo do jogador √© maximizar seus ganhos puxando os bra√ßos ao longo de muitos jogos. Isso for√ßa o jogador a equilibrar a explora√ß√£o (testar diferentes bra√ßos para descobrir quais s√£o os melhores) e a explota√ß√£o (puxar o bra√ßo que ele acredita ser o melhor com base em sua experi√™ncia at√© o momento).

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

**Valor da A√ß√£o:** Em um problema *k*-armed bandit, cada a√ß√£o *a* possui um valor esperado ou recompensa m√©dia, denotado por $q_*(a)$ [^2]. Este valor representa a recompensa m√©dia que o agente receber√° ao selecionar a a√ß√£o *a* repetidamente [^2]. Formalmente, $q_*(a)$ √© definido como:
$$
q_*(a) = E[R_t | A_t = a]
$$
onde $A_t$ √© a a√ß√£o selecionada no passo *t*, e $R_t$ √© a recompensa correspondente [^2].

> üí° **Exemplo Num√©rico:** Suponha que tenhamos 3 a√ß√µes (bra√ßos de um ca√ßa-n√≠queis). A√ß√£o 1 sempre retorna 1 unidade de recompensa. A√ß√£o 2 retorna 0 unidades 50% das vezes e 2 unidades 50% das vezes. A√ß√£o 3 retorna -1 unidade 25% das vezes e 3 unidades 75% das vezes. Ent√£o, $q_*(1) = 1$, $q_*(2) = (0.5 * 0) + (0.5 * 2) = 1$, e $q_*(3) = (0.25 * -1) + (0.75 * 3) = 2$. A√ß√£o 3 tem o maior valor esperado.

**Estimativa do Valor da A√ß√£o:** Como os valores verdadeiros das a√ß√µes $q_*(a)$ s√£o desconhecidos, o agente deve estim√°-los com base em sua experi√™ncia. Denotamos a estimativa do valor da a√ß√£o *a* no passo *t* por $Q_t(a)$ [^2]. O objetivo do agente √© fazer com que $Q_t(a)$ seja o mais pr√≥ximo poss√≠vel de $q_*(a)$ [^2]. Uma maneira comum de atualizar $Q_t(a)$ √© usando a m√©dia amostral.

> üí° **Exemplo Num√©rico:** Ap√≥s 5 tentativas, um agente puxou o bra√ßo 1 do ca√ßa-n√≠queis 2 vezes, recebendo recompensas de 0 e 1. Portanto, a estimativa do valor da A√ß√£o 1 √© $Q_5(1) = (0+1)/2 = 0.5$. Isso significa que, com base na experi√™ncia do agente at√© o momento, ele espera receber uma recompensa m√©dia de 0.5 ao puxar o bra√ßo 1.

**Atualiza√ß√£o da Estimativa do Valor da A√ß√£o:** A estimativa do valor da a√ß√£o $Q_t(a)$ pode ser atualizada iterativamente √† medida que o agente interage com o ambiente. Uma abordagem comum √© usar a m√©dia amostral das recompensas obtidas ao selecionar a a√ß√£o *a*. Seja $N_t(a)$ o n√∫mero de vezes que a a√ß√£o *a* foi selecionada at√© o tempo *t*. Ent√£o, a estimativa do valor da a√ß√£o pode ser atualizada da seguinte forma:
$$
Q_{t+1}(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t} R_i \mathbb{1}(A_i = a)
$$
onde $\mathbb{1}(A_i = a)$ √© uma fun√ß√£o indicadora que √© 1 se a a√ß√£o $A_i$ foi igual a *a* no tempo *i*, e 0 caso contr√°rio. Essa atualiza√ß√£o representa uma m√©dia das recompensas recebidas cada vez que a a√ß√£o *a* foi escolhida.

> üí° **Exemplo Num√©rico:** Suponha que, no tempo *t* = 10, a a√ß√£o 'A' foi selecionada 3 vezes, resultando em recompensas de 2, 3 e 4. Assim, $N_{10}(A) = 3$ e $\sum_{i=1}^{10} R_i \mathbb{1}(A_i = A) = 2 + 3 + 4 = 9$. A estimativa do valor da a√ß√£o A no tempo *t+1* = 11 √© $Q_{11}(A) = \frac{9}{3} = 3$. Se, no tempo *t+1* = 11, a a√ß√£o A for selecionada novamente e resultar em uma recompensa de 5, ent√£o $N_{11}(A) = 4$ e a nova estimativa ser√° $Q_{12}(A) = \frac{9 + 5}{4} = \frac{14}{4} = 3.5$.

**Teorema 1:** *A atualiza√ß√£o da estimativa do valor da a√ß√£o converge para o valor real da a√ß√£o √† medida que o n√∫mero de amostras tende ao infinito, assumindo que a recompensa tem vari√¢ncia finita.*

*Prova:* Pela lei forte dos grandes n√∫meros, a m√©dia amostral converge para o valor esperado √† medida que o n√∫mero de amostras tende ao infinito. Portanto, se $N_t(a) \rightarrow \infty$, ent√£o $Q_{t+1}(a) \rightarrow q_*(a)$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Imagine que a verdadeira recompensa m√©dia de uma a√ß√£o √© 2.5 (ou seja, $q_*(a) = 2.5$). No in√≠cio, a estimativa $Q_t(a)$ pode estar longe desse valor (por exemplo, $Q_1(a) = 1$). No entanto, √† medida que coletamos mais amostras (isto √©, selecionamos a a√ß√£o *a* mais vezes), a lei dos grandes n√∫meros garante que nossa estimativa $Q_t(a)$ se aproximar√° cada vez mais de 2.5. Ap√≥s 1000 amostras, $Q_{1000}(a)$ estar√° muito pr√≥ximo de 2.5.

**Estimativa do Valor da A√ß√£o Incremental:** A atualiza√ß√£o por m√©dia amostral pode ser computacionalmente custosa, especialmente se o n√∫mero de intera√ß√µes *t* for grande. Uma forma mais eficiente de atualizar a estimativa do valor da a√ß√£o √© usando uma atualiza√ß√£o incremental:
$$
Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]
$$
onde $\alpha$ √© uma taxa de aprendizado que determina o quanto a nova recompensa $R_t$ influencia a estimativa anterior $Q_t(a)$. Esta f√≥rmula √© uma m√©dia ponderada da estimativa anterior e da nova recompensa.

> üí° **Exemplo Num√©rico:** Suponha que $Q_t(a) = 2$ e recebemos uma recompensa $R_t = 3$ ap√≥s selecionar a a√ß√£o *a*. Se usarmos uma taxa de aprendizado $\alpha = 0.1$, ent√£o a nova estimativa ser√° $Q_{t+1}(a) = 2 + 0.1 * (3 - 2) = 2 + 0.1 = 2.1$. Se $\alpha = 1$, ent√£o $Q_{t+1}(a) = 2 + 1 * (3 - 2) = 3$. Um valor maior de $\alpha$ faz com que a estimativa se mova mais rapidamente em dire√ß√£o √† recompensa recente.

**Lema 1:** *A atualiza√ß√£o incremental √© equivalente √† m√©dia amostral quando a taxa de aprendizado $\alpha$ √© definida como $\alpha = \frac{1}{N_t(a)}$.*

*Prova:*
I. Queremos provar que $Q_{t+1}(a) = Q_t(a) + \alpha [R_t - Q_t(a)]$ √© equivalente a $Q_{t+1}(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t} R_i \mathbb{1}(A_i = a)$ quando $\alpha = \frac{1}{N_t(a)}$.

II. Substituindo $\alpha = \frac{1}{N_t(a)}$ na equa√ß√£o incremental, obtemos:
$$Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)} [R_t - Q_t(a)]$$

III. Multiplicando ambos os lados por $N_t(a)$, obtemos:
$$N_t(a)Q_{t+1}(a) = N_t(a)Q_t(a) + R_t - Q_t(a)$$

IV. Reorganizando os termos, temos:
$$N_t(a)Q_{t+1}(a) = N_t(a)Q_t(a) - Q_t(a) + R_t$$

V. Note que $N_t(a) = N_{t-1}(a) + 1$ se $A_t = a$, e $N_t(a) = N_{t-1}(a)$ caso contr√°rio. Assumindo que $A_t = a$, ent√£o $N_t(a) = N_{t-1}(a) + 1$. Podemos expressar $Q_t(a)$ como a m√©dia amostral at√© o tempo $t-1$:
$$Q_t(a) = \frac{1}{N_{t-1}(a)} \sum_{i=1}^{t-1} R_i \mathbb{1}(A_i = a)$$
Portanto,
$$N_{t-1}(a)Q_t(a) = \sum_{i=1}^{t-1} R_i \mathbb{1}(A_i = a)$$

VI. Substituindo $N_t(a) = N_{t-1}(a) + 1$ na equa√ß√£o do passo III, e usando a express√£o para $Q_t(a)$ do passo V, obtemos:
$$N_t(a)Q_{t+1}(a) = (N_{t-1}(a) + 1)Q_t(a) - Q_t(a) + R_t = N_{t-1}(a)Q_t(a) + R_t$$

VII. Substituindo $N_{t-1}(a)Q_t(a)$ pela soma das recompensas at√© $t-1$:
$$N_t(a)Q_{t+1}(a) = \sum_{i=1}^{t-1} R_i \mathbb{1}(A_i = a) + R_t$$

VIII. Finalmente, combinando a soma at√© $t-1$ com a recompensa no tempo $t$, obtemos:
$$N_t(a)Q_{t+1}(a) = \sum_{i=1}^{t} R_i \mathbb{1}(A_i = a)$$

IX. Dividindo ambos os lados por $N_t(a)$, obtemos a m√©dia amostral:
$$Q_{t+1}(a) = \frac{1}{N_t(a)} \sum_{i=1}^{t} R_i \mathbb{1}(A_i = a)$$
Portanto, a atualiza√ß√£o incremental √© equivalente √† m√©dia amostral quando $\alpha = \frac{1}{N_t(a)}$. ‚ñ†

**Explora√ß√£o vs. Explota√ß√£o:** O dilema fundamental no problema do *k*-armed bandit √© a escolha entre **explora√ß√£o** e **explota√ß√£o**.

*   **Explota√ß√£o** significa selecionar a a√ß√£o com a maior estimativa de valor $Q_t(a)$ no momento [^2]. Isso √© conhecido como uma a√ß√£o *greedy* [^2]. Ao explorar, o agente busca maximizar a recompensa imediata com base no seu conhecimento atual [^2].

*   **Explora√ß√£o**, por outro lado, significa selecionar uma a√ß√£o n√£o-greedy na esperan√ßa de melhorar a estimativa de seu valor [^2]. Ao explorar, o agente sacrifica a recompensa imediata para obter mais informa√ß√µes e potencialmente descobrir a√ß√µes com valores mais elevados a longo prazo [^2].

A decis√£o de explorar ou explotar depende de uma s√©rie de fatores, incluindo a precis√£o das estimativas de valor, as incertezas associadas e o n√∫mero de passos restantes [^2]. M√©todos sofisticados buscam um equil√≠brio ideal entre explora√ß√£o e explota√ß√£o para formula√ß√µes matem√°ticas espec√≠ficas do problema *k*-armed bandit e problemas relacionados [^2].

Uma estrat√©gia comum para equilibrar explora√ß√£o e explota√ß√£o √© a $\epsilon$-greedy, onde com probabilidade $\epsilon$, o agente escolhe uma a√ß√£o aleat√≥ria (explora√ß√£o) e com probabilidade $1-\epsilon$ escolhe a a√ß√£o greedy (explota√ß√£o).

> üí° **Exemplo Num√©rico:** Considere um agente usando uma estrat√©gia $\epsilon$-greedy com $\epsilon = 0.1$. Isso significa que 10% das vezes, o agente escolher√° uma a√ß√£o aleat√≥ria, independentemente das estimativas atuais. Os outros 90% das vezes, o agente escolher√° a a√ß√£o com a maior estimativa de valor atual. Se o agente tem 4 a√ß√µes e as estimativas de valor s√£o Q(A) = 2, Q(B) = 3, Q(C) = 1, Q(D) = 2.5, ent√£o 10% das vezes o agente escolher√° A, B, C ou D com igual probabilidade (2.5% cada). Os outros 90% das vezes, o agente escolher√° a a√ß√£o B porque tem a maior estimativa de valor (3).

**Estrat√©gia $\epsilon$-Greedy:** A estrat√©gia $\epsilon$-greedy √© uma abordagem simples e popular para equilibrar explora√ß√£o e explota√ß√£o. Com probabilidade $\epsilon$, o agente seleciona uma a√ß√£o aleatoriamente, independentemente das estimativas de valor atuais. Com probabilidade $1 - \epsilon$, o agente seleciona a a√ß√£o com a maior estimativa de valor (a√ß√£o greedy).

![Average performance of Œµ-greedy action-value methods on a 10-armed testbed, demonstrating the exploration-exploitation trade-off.](./../images/image6.png)

> üí° **Exemplo Num√©rico:** Aqui est√° um exemplo de c√≥digo em Python para simular a estrat√©gia $\epsilon$-Greedy em um ambiente k-armed bandit:

![Pseudoc√≥digo de um algoritmo de bandit simples com estrat√©gia Œµ-greedy para explora√ß√£o e explota√ß√£o.](./../images/image4.png)

**Corol√°rio 1:** *A estrat√©gia $\epsilon$-greedy garante que todas as a√ß√µes sejam exploradas infinitamente em um ambiente estacion√°rio, desde que $\epsilon > 0$.*

*Prova:*
I. Seja *k* o n√∫mero de a√ß√µes dispon√≠veis.
II. Na estrat√©gia $\epsilon$-greedy, cada a√ß√£o tem uma probabilidade de $\frac{\epsilon}{k}$ de ser selecionada aleatoriamente.
III. Como $\epsilon > 0$, ent√£o $\frac{\epsilon}{k} > 0$ para todas as a√ß√µes.
IV. Seja $N_t(a)$ o n√∫mero de vezes que a a√ß√£o *a* √© selecionada at√© o tempo *t*.
V. √Ä medida que $t \rightarrow \infty$, o n√∫mero esperado de vezes que a a√ß√£o *a* √© selecionada devido √† explora√ß√£o √©:
$E[N_t(a)] = \sum_{i=1}^{t} P(\text{a√ß√£o a √© selecionada no tempo i}) \geq \sum_{i=1}^{t} \frac{\epsilon}{k} = \frac{\epsilon}{k}t$
VI. Portanto, $\lim_{t \to \infty} E[N_t(a)] \geq \lim_{t \to \infty} \frac{\epsilon}{k}t = \infty$
VII. Isso significa que cada a√ß√£o ser√° selecionada um n√∫mero infinito de vezes √† medida que o tempo tende ao infinito. ‚ñ†

> üí° **Exemplo Num√©rico:** Se temos um problema com k=5 bra√ßos e usamos $\epsilon = 0.2$, cada bra√ßo tem uma probabilidade de pelo menos $\frac{0.2}{5} = 0.04$ de ser puxado em qualquer passo. Isso garante que, ao longo de um n√∫mero infinito de passos, todos os bra√ßos ser√£o amostrados um n√∫mero infinito de vezes. Mesmo que um bra√ßo pare√ßa ruim no in√≠cio, ele ainda ser√° experimentado ocasionalmente, dando-lhe a chance de revelar seu verdadeiro potencial.

### Conclus√£o

Em resumo, a caracter√≠stica distintiva do aprendizado por refor√ßo √© o uso de feedback avaliativo, que avalia a qualidade das a√ß√µes em vez de instruir sobre as a√ß√µes corretas. Isso cria a necessidade de explora√ß√£o ativa, que √© essencial para descobrir comportamentos eficazes. O problema do *k*-armed bandit serve como um modelo simplificado para estudar o *trade-off* entre explora√ß√£o e explota√ß√£o, destacando os desafios √∫nicos que surgem no aprendizado por refor√ßo.

### Refer√™ncias
[^1]: Chapter 2, Multi-armed Bandits, page 25
[^2]: Chapter 2, Multi-armed Bandits, page 26
<!-- END -->