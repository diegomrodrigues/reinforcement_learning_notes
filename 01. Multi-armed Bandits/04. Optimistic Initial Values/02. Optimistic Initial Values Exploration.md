## Valores Iniciais Otimistas e Explora√ß√£o em Bandit Problems

### Introdu√ß√£o
O dilema da **explora√ß√£o** versus **explota√ß√£o** √© central em *Reinforcement Learning* (RL). Como vimos anteriormente, m√©todos como o $\epsilon$-greedy e *Upper Confidence Bound* (UCB) [^27] abordam esse dilema de maneiras distintas. Nesta se√ß√£o, exploraremos uma t√©cnica alternativa para incentivar a explora√ß√£o: o uso de **valores iniciais otimistas** [^34]. Essa abordagem, embora simples, pode ser eficaz em certas classes de problemas *k-armed bandit* [^29].

### Conceitos Fundamentais
A maioria dos m√©todos discutidos at√© agora depende, em certa medida, das estimativas iniciais dos valores de a√ß√£o, denotadas por $Q_1(a)$ [^34]. Estatisticamente, esses m√©todos s√£o *biased* por essas estimativas iniciais. No caso dos m√©todos de *sample-average*, esse *bias* desaparece assim que todas as a√ß√µes s√£o selecionadas pelo menos uma vez. Contudo, para m√©todos com taxa de aprendizado constante ($\alpha$), o *bias* √© permanente, embora diminua ao longo do tempo, conforme a equa√ß√£o (2.6) [^32].

> üí° **Exemplo Num√©rico:** Suponha que tenhamos um bandit de duas armas (k=2). Inicialmente, $Q_1(1) = 0$ e $Q_1(2) = 0$. Se usarmos um m√©todo de *sample-average* e a primeira a√ß√£o (a=1) nos der uma recompensa de 1, ent√£o $Q_2(1) = 1/1 = 1$. Se a segunda a√ß√£o (a=2) nos der uma recompensa de 0, ent√£o $Q_2(2) = 0/1 = 0$. O *bias* inicial desaparece rapidamente a medida que coletamos mais amostras. Agora, suponha que usamos uma taxa de aprendizado constante $\alpha = 0.1$. Ent√£o, $Q_2(1) = Q_1(1) + \alpha(r_1 - Q_1(1)) = 0 + 0.1(1 - 0) = 0.1$. Mesmo ap√≥s uma recompensa de 1, o valor da a√ß√£o 1 ainda est√° *biased* em dire√ß√£o ao valor inicial de 0. Este *bias* persiste, embora diminua a cada nova atualiza√ß√£o.

Em geral, esse *bias* n√£o √© problem√°tico e pode at√© ser √∫til. O ponto negativo √© que as estimativas iniciais se tornam um conjunto de par√¢metros que o usu√°rio deve escolher, mesmo que seja para configur√°-los como zero. O ponto positivo √© que eles fornecem uma maneira f√°cil de fornecer algum conhecimento pr√©vio sobre o n√≠vel de recompensas que podem ser esperadas [^34].

Os **valores iniciais das a√ß√µes** tamb√©m podem ser usados como uma maneira simples de estimular a explora√ß√£o. Em vez de definir os valores iniciais das a√ß√µes como zero, como foi feito no *10-armed testbed*, podemos configur√°-los como +5 [^34]. Considerando que os valores $q_*(a)$ nesse problema s√£o selecionados a partir de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1, uma estimativa inicial de +5 √© excessivamente otimista.

> üí° **Exemplo Num√©rico:** No *10-armed testbed*, os verdadeiros valores das a√ß√µes, $q_*(a)$, s√£o amostrados de $\mathcal{N}(0, 1)$. Isso significa que a maioria dos valores estar√° entre -3 e +3. Definir $Q_1(a) = +5$ para todas as a√ß√µes significa que o algoritmo *greedy* inicialmente superestima o valor de todas as a√ß√µes.

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

Essa estrat√©gia otimista incentiva os m√©todos de *action-value* a explorar. Quaisquer que sejam as a√ß√µes inicialmente selecionadas, a recompensa ser√° menor do que as estimativas iniciais; o *learner* muda para outras a√ß√µes, ficando ‚Äúdesapontado‚Äù com as recompensas que est√° recebendo. Como resultado, todas as a√ß√µes s√£o experimentadas diversas vezes antes que as estimativas de valor convergam. O sistema realiza uma boa quantidade de explora√ß√£o, mesmo se as a√ß√µes *greedy* forem selecionadas o tempo todo [^34].

A Figura 2.3 [^34] compara o desempenho no *10-armed bandit testbed* de um m√©todo *greedy* utilizando $Q_1(a) = +5$ para toda a√ß√£o $a$, com um m√©todo $\epsilon$-greedy com $Q_1(a) = 0$. Inicialmente, o m√©todo otimista tem um desempenho inferior por explorar mais. No entanto, eventualmente, ele supera o m√©todo $\epsilon$-greedy, pois sua explora√ß√£o diminui com o tempo. Essa t√©cnica de incentivar a explora√ß√£o √© chamada de **valores iniciais otimistas**.

> üí° **Exemplo Num√©rico:** Suponha que ap√≥s 1000 passos no *10-armed bandit testbed*, o m√©todo *greedy* com valores iniciais otimistas tenha uma recompensa m√©dia de 1.4, enquanto o m√©todo $\epsilon$-greedy tenha uma recompensa m√©dia de 1.2. Inicialmente, o m√©todo otimista pode ter tido uma recompensa m√©dia de 0.5 nos primeiros 100 passos, enquanto o $\epsilon$-greedy teve 1.0. Essa diferen√ßa inicial demonstra o custo da explora√ß√£o otimista, que √© pago no in√≠cio, mas compensado a longo prazo.

![Comparative performance of optimistic vs. realistic action-value initialization in a 10-armed testbed, illustrating the impact on optimal action selection over time.](./../images/image3.png)

Para complementar a discuss√£o sobre valores iniciais otimistas, podemos analisar a sua rela√ß√£o com a vari√¢ncia das recompensas.

**Teorema 1** *Em um problema k-armed bandit estacion√°rio, com recompensas limitadas no intervalo $[0, 1]$, o desempenho assint√≥tico de um m√©todo greedy com valores iniciais otimistas $Q_1(a) = V > 1$ √© limitado pela vari√¢ncia das recompensas das a√ß√µes √≥timas. Especificamente, quanto menor a vari√¢ncia das recompensas √≥timas, mais r√°pido o m√©todo converge para a a√ß√£o √≥tima.*

*Prova (Esbo√ßo)*: A intui√ß√£o por tr√°s desse teorema reside no fato de que, com valores iniciais excessivamente otimistas, o agente explora at√© encontrar uma a√ß√£o com recompensa m√©dia pr√≥xima do valor real. Se a vari√¢ncia das recompensas da a√ß√£o √≥tima for baixa, o agente rapidamente obter√° estimativas precisas do valor dessa a√ß√£o e, portanto, convergir√£o rapidamente. Por outro lado, se a vari√¢ncia for alta, o agente continuar√° explorando por mais tempo, buscando uma a√ß√£o que consistentemente forne√ßa recompensas pr√≥ximas ao valor inicial otimista, retardando a converg√™ncia.

*Prova (Mais Detalhada):*

Provaremos que em um problema k-armed bandit estacion√°rio, com recompensas limitadas no intervalo $[0, 1]$, o desempenho assint√≥tico de um m√©todo greedy com valores iniciais otimistas $Q_1(a) = V > 1$ √© limitado pela vari√¢ncia das recompensas das a√ß√µes √≥timas.

I. **Configura√ß√£o Inicial:** Assumimos um ambiente k-armed bandit estacion√°rio onde as recompensas $r_a$ para cada a√ß√£o $a$ est√£o no intervalo $[0, 1]$. O algoritmo usa um m√©todo *greedy* com valores iniciais otimistas $Q_1(a) = V > 1$ para todas as a√ß√µes $a$.

II. **Explora√ß√£o Inicial:** Devido aos valores iniciais otimistas, o agente explora inicialmente, pois as recompensas observadas s√£o tipicamente menores que $V$. Seja $a^*$ a a√ß√£o √≥tima, com valor real $q_*(a^*)$.

III. **Converg√™ncia para a A√ß√£o √ìtima:** O agente converge para a a√ß√£o √≥tima $a^*$ quando sua estimativa de valor $Q_t(a^*)$ se aproxima de $q_*(a^*)$. A velocidade desta converg√™ncia depende da vari√¢ncia das recompensas da a√ß√£o $a^*$, denotada por $\sigma^2(a^*)$.

IV. **An√°lise da Vari√¢ncia:**
    - **Baixa Vari√¢ncia:** Se $\sigma^2(a^*)$ √© baixa, as recompensas $r_{a^*}$ observadas para a a√ß√£o $a^*$ s√£o consistentes e pr√≥ximas de $q_*(a^*)$. O agente rapidamente ajusta $Q_t(a^*)$ para perto de $q_*(a^*)$, cessando a explora√ß√£o e explorando $a^*$ com alta frequ√™ncia.
    - **Alta Vari√¢ncia:** Se $\sigma^2(a^*)$ √© alta, as recompensas $r_{a^*}$ observadas para a a√ß√£o $a^*$ variam amplamente. O agente pode demorar mais para obter uma estimativa precisa de $q_*(a^*)$, prolongando a fase de explora√ß√£o.

V. **Formaliza√ß√£o da Conex√£o:** Considere a atualiza√ß√£o do valor da a√ß√£o usando uma m√©dia amostral:
    $$Q_{t+1}(a) = Q_t(a) + \frac{1}{n_a}(r_t - Q_t(a))$$
    onde $n_a$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada. A taxa de converg√™ncia de $Q_t(a^*)$ para $q_*(a^*)$ √© influenciada pela variabilidade de $r_t$ em torno de $q_*(a^*)$, que √© medida por $\sigma^2(a^*)$.

VI. **Limite Assint√≥tico:** No limite, √† medida que $t \to \infty$, o desempenho do m√©todo *greedy* √© dominado pela precis√£o da estimativa de $q_*(a^*)$. Uma baixa $\sigma^2(a^*)$ leva a uma estimativa mais precisa e a uma explora√ß√£o mais r√°pida, resultando em um desempenho assint√≥tico superior.

VII. **Conclus√£o:** Portanto, o desempenho assint√≥tico de um m√©todo *greedy* com valores iniciais otimistas √© limitado pela vari√¢ncia das recompensas da a√ß√£o √≥tima. Uma vari√¢ncia menor resulta em converg√™ncia mais r√°pida e melhor desempenho. ‚ñ†

> üí° **Exemplo Num√©rico:** Consideremos duas a√ß√µes √≥timas, $a_1$ e $a_2$. Ambas t√™m um valor esperado de 0.8. No entanto, as recompensas de $a_1$ t√™m uma vari√¢ncia de 0.01 (recompensas est√£o geralmente entre 0.7 e 0.9), enquanto as recompensas de $a_2$ t√™m uma vari√¢ncia de 0.2 (recompensas est√£o frequentemente entre 0.4 e 1.2). Com valores iniciais otimistas de 1.5, o agente rapidamente aprende que $a_1$ tem um valor pr√≥ximo de 0.8 e converge para ele. Para $a_2$, devido a alta vari√¢ncia, o agente pode experimentar recompensas baixas no in√≠cio e continuar explorando outras a√ß√µes por mais tempo.

> Consideramos essa abordagem como um truque simples que pode ser eficaz em problemas estacion√°rios. No entanto, ela n√£o √© uma abordagem geralmente √∫til para incentivar a explora√ß√£o [^35].

Se a tarefa mudar, criando uma necessidade renovada de explora√ß√£o, este m√©todo n√£o consegue ajudar. De fato, qualquer m√©todo que se concentre nas condi√ß√µes iniciais de uma forma especial dificilmente ajudar√° no caso geral n√£o estacion√°rio. O in√≠cio dos tempos ocorre apenas uma vez e, portanto, n√£o devemos nos concentrar muito nisso. Essa cr√≠tica tamb√©m se aplica aos m√©todos de *sample-average*, que tamb√©m tratam o in√≠cio dos tempos como um evento especial, fazendo a m√©dia de todas as recompensas subsequentes com pesos iguais. No entanto, todos esses m√©todos s√£o muito simples, e um deles - ou alguma combina√ß√£o simples deles - √© frequentemente adequado na pr√°tica [^35].

Al√©m disso, podemos considerar uma estrat√©gia para mitigar a limita√ß√£o dos valores iniciais otimistas em ambientes n√£o-estacion√°rios.

**Proposi√ß√£o 1.1** *Para adaptar valores iniciais otimistas a ambientes n√£o-estacion√°rios, podemos combinar essa t√©cnica com um mecanismo de "reset" peri√≥dico dos valores de a√ß√£o. A cada $N$ passos, os valores de a√ß√£o $Q_t(a)$ s√£o redefinidos para o valor otimista inicial $V$.*

Essa abordagem introduz uma forma de "esquecimento" no algoritmo, permitindo que ele se readapte a mudan√ßas no ambiente. O par√¢metro $N$ controla a frequ√™ncia com que a explora√ß√£o √© reiniciada.

**Lema 1.1** *A escolha ideal de $N$ na Proposi√ß√£o 1.1 depende da taxa de mudan√ßa do ambiente. Ambientes com mudan√ßas mais frequentes exigem valores menores de $N$, enquanto ambientes mais est√°veis podem tolerar valores maiores de $N$.*

*Prova:*

Provaremos que a escolha ideal de $N$ na Proposi√ß√£o 1.1 depende da taxa de mudan√ßa do ambiente. Ambientes com mudan√ßas mais frequentes exigem valores menores de $N$, enquanto ambientes mais est√°veis podem tolerar valores maiores de $N$.

I. **Defini√ß√£o de N√£o-Estacionariedade:** Em um ambiente n√£o-estacion√°rio, as recompensas associadas a cada a√ß√£o mudam ao longo do tempo. Seja $\tau$ a escala de tempo caracter√≠stica dessas mudan√ßas; mudan√ßas significativas ocorrem em intervalos de tempo da ordem de $\tau$.

II. **Impacto de $N$:** O par√¢metro $N$ determina a frequ√™ncia com que os valores de a√ß√£o s√£o redefinidos para o valor inicial otimista $V$. Se $N$ √© muito grande em rela√ß√£o a $\tau$, o agente pode n√£o conseguir se adaptar √†s mudan√ßas no ambiente a tempo. Se $N$ √© muito pequeno, o agente pode redefinir seus valores de a√ß√£o com muita frequ√™ncia, impedindo a converg√™ncia para as a√ß√µes √≥timas atuais.

III. **Ambientes com Mudan√ßas Frequentes ($\tau$ pequeno):** Nesses ambientes, os valores de a√ß√£o precisam ser redefinidos com mais frequ√™ncia para acompanhar as mudan√ßas. Portanto, $N$ deve ser pequeno para permitir uma adapta√ß√£o r√°pida. Se $N >> \tau$, o agente estar√° explorando a√ß√µes baseadas em informa√ß√µes desatualizadas, levando a um desempenho ruim.

IV. **Ambientes Est√°veis ($\tau$ grande):** Em ambientes mais est√°veis, os valores de a√ß√£o n√£o precisam ser redefinidos com tanta frequ√™ncia. Um valor maior de $N$ permite que o agente explore e explote as a√ß√µes atuais por um per√≠odo mais longo, levando a uma melhor converg√™ncia. Se $N << \tau$, o agente estar√° redefinindo seus valores de a√ß√£o desnecessariamente, perdendo oportunidades de explora√ß√£o.

V. **Justificativa Matem√°tica (Qualitativa):** Podemos pensar em $N$ como o comprimento de uma janela deslizante sobre o tempo. Se a janela for muito longa, ela suaviza as mudan√ßas no ambiente; se for muito curta, ela introduz ru√≠do. O valor ideal de $N$ equilibra esses dois efeitos.

VI. **Conclus√£o:** Portanto, a escolha ideal de $N$ depende da taxa de mudan√ßa do ambiente, medida por $\tau$. Ambientes com mudan√ßas mais frequentes requerem valores menores de $N$, enquanto ambientes mais est√°veis podem tolerar valores maiores de $N$. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um ambiente onde a a√ß√£o √≥tima muda a cada 200 passos ( $\tau = 200$). Se definirmos $N = 500$, o algoritmo estar√° frequentemente usando informa√ß√µes desatualizadas sobre as a√ß√µes. Se definirmos $N = 50$, o algoritmo estar√° reiniciando a explora√ß√£o muito frequentemente, perdendo oportunidades de aprender a longo prazo. Um valor de $N$ pr√≥ximo a $\tau$, como $N = 200$, seria mais adequado, permitindo que o algoritmo se adapte √†s mudan√ßas sem redefinir a explora√ß√£o prematuramente.

### Conclus√£o
O uso de **valores iniciais otimistas** oferece uma abordagem interessante para equilibrar a explora√ß√£o e a explota√ß√£o nos problemas *k-armed bandit* [^29]. Embora n√£o seja uma solu√ß√£o universal, sua simplicidade e efic√°cia em cen√°rios estacion√°rios a tornam uma ferramenta valiosa no arsenal de um agente de *Reinforcement Learning* [^34]. √â importante notar que essa t√©cnica apresenta limita√ß√µes em ambientes n√£o estacion√°rios, onde a necessidade de explora√ß√£o pode ressurgir ao longo do tempo [^35]. No entanto, combina√ß√µes com outras t√©cnicas, como resets peri√≥dicos, podem mitigar essas limita√ß√µes.

### Refer√™ncias
[^27]: Se√ß√£o 2.7 do documento original, "Upper-Confidence-Bound Action Selection".
[^29]: Se√ß√£o 2.1 do documento original, "A k-armed Bandit Problem".
[^32]: Se√ß√£o 2.5 do documento original, "Tracking a Nonstationary Problem".
[^34]: Se√ß√£o 2.6 do documento original, "Optimistic Initial Values".
[^35]: Se√ß√£o 2.7 do documento original, "Upper-Confidence-Bound Action Selection".
<!-- END -->