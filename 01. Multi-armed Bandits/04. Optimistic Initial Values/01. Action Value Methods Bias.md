## Optimistic Initial Values in Action-Value Methods

### Introdu√ß√£o
Como discutido anteriormente, os m√©todos de **action-value** estimam os valores das a√ß√µes para tomar decis√µes [^27]. Uma caracter√≠stica importante desses m√©todos √© a influ√™ncia das estimativas iniciais dos valores das a√ß√µes, denotadas como $Q_1(a)$ [^34]. Este cap√≠tulo explora como essas estimativas iniciais afetam o aprendizado e como elas podem ser usadas de forma otimista para incentivar a explora√ß√£o. Em particular, vamos discutir o conceito de *optimistic initial values* como uma t√©cnica simples para incentivar a explora√ß√£o em problemas de **k-armed bandit**.

### O Impacto das Estimativas Iniciais
As estimativas iniciais dos valores das a√ß√µes introduzem um **bias** nos m√©todos de action-value [^34]. Em termos estat√≠sticos, esses m√©todos s√£o enviesados por suas estimativas iniciais. O comportamento desse bias varia dependendo do m√©todo de estimativa usado:

*   **Sample-Average Methods**: O bias desaparece assim que todas as a√ß√µes s√£o selecionadas pelo menos uma vez [^34]. Isso ocorre porque, √† medida que o n√∫mero de amostras para cada a√ß√£o aumenta, a m√©dia amostral converge para o valor real da a√ß√£o, conforme demonstrado pela lei dos grandes n√∫meros [^3].

    **Prova:**
    Seja $Q_n(a)$ a estimativa do valor da a√ß√£o $a$ ap√≥s $n$ vezes que ela foi selecionada e $R_i$ a $i$-√©sima recompensa recebida ap√≥s selecionar a a√ß√£o $a$. Ent√£o:

    I. A estimativa do valor usando o m√©todo sample-average √© dada por:
    $$Q_n(a) = \frac{1}{n} \sum_{i=1}^{n} R_i$$

    II. Pela Lei Forte dos Grandes N√∫meros, se as recompensas $R_i$ s√£o independentes e identicamente distribu√≠das com valor esperado $q_*(a)$, ent√£o:
    $$\lim_{n \to \infty} Q_n(a) = q_*(a) \quad \text{com probabilidade 1}$$

    III. Portanto, √† medida que $n$ tende ao infinito, a estimativa $Q_n(a)$ converge para o valor real da a√ß√£o $q_*(a)$, eliminando o bias inicial. ‚ñ†

    > üí° **Exemplo Num√©rico:**
    >
    > Considere um problema de 3-armed bandit com valores reais de a√ß√µes $q_*(1) = 1$, $q_*(2) = 2$, e $q_*(3) = 3$. Inicializamos $Q_1(a) = 0$ para todas as a√ß√µes. Ap√≥s selecionar a a√ß√£o 1 por 10 vezes e receber recompensas de {0.5, 0.8, 1.2, 0.9, 1.1, 0.7, 1.0, 0.6, 1.3, 0.9}, a estimativa do valor da a√ß√£o 1 ser√°:
    >
    > $Q_{10}(1) = \frac{0.5 + 0.8 + 1.2 + 0.9 + 1.1 + 0.7 + 1.0 + 0.6 + 1.3 + 0.9}{10} = \frac{9}{10} = 0.9$.
    >
    > Ap√≥s selecionar a a√ß√£o 1 por 100 vezes e receber recompensas cuja m√©dia √© 0.98, a estimativa do valor da a√ß√£o 1 ser√°:
    > $Q_{100}(1) = 0.98$.
    >
    > Observe que a estimativa se aproxima do valor real da a√ß√£o, $q_*(1) = 1$, demonstrando a converg√™ncia pelo m√©todo sample-average.

*   **Constant-Œ± Methods**: O bias √© permanente, embora diminua ao longo do tempo [^34]. A equa√ß√£o (2.6) [^8] mostra que o valor de $Q_{n+1}$ √© uma m√©dia ponderada de recompensas passadas e da estimativa inicial $Q_1$. O peso de $Q_1$ diminui exponencialmente, mas nunca se torna zero.

    **Prova:**
    A atualiza√ß√£o para o m√©todo constant-$\alpha$ √© dada por:

    I. $Q_{n+1}(a) = Q_n(a) + \alpha [R_n - Q_n(a)]$, onde $\alpha$ √© a taxa de aprendizado constante.

    II. Expandindo recursivamente, obtemos:
    $Q_{n+1}(a) = (1-\alpha)^n Q_1(a) + \sum_{i=1}^{n} \alpha (1-\alpha)^{n-i} R_i$

    III. Observe que o peso de $Q_1(a)$ √© $(1-\alpha)^n$, que se aproxima de 0 √† medida que $n$ tende ao infinito, mas nunca √© exatamente 0 para $\alpha > 0$ e $n < \infty$. Portanto, a estimativa inicial $Q_1(a)$ sempre ter√° alguma influ√™ncia, mesmo que pequena, em $Q_{n+1}(a)$.

    IV. Isso demonstra que o bias introduzido pela estimativa inicial persiste, embora seu impacto diminua ao longo do tempo. ‚ñ†

    > üí° **Exemplo Num√©rico:**
    >
    > Considere uma a√ß√£o com valor real $q_*(a) = 2$. Inicializamos $Q_1(a) = 0$ e usamos $\alpha = 0.1$. Recebemos uma recompensa $R_1 = 1$.
    >
    > $Q_2(a) = Q_1(a) + \alpha [R_1 - Q_1(a)] = 0 + 0.1 * (1 - 0) = 0.1$.
    >
    > Agora, recebemos uma recompensa $R_2 = 3$.
    >
    > $Q_3(a) = Q_2(a) + \alpha [R_2 - Q_2(a)] = 0.1 + 0.1 * (3 - 0.1) = 0.1 + 0.29 = 0.39$.
    >
    > Ap√≥s 100 itera√ß√µes com recompensas aleat√≥rias em torno de $q_*(a) = 2$, a estimativa $Q_{101}(a)$ se aproximar√° de 2, mas nunca ser√° exatamente 2 devido √† influ√™ncia persistente de $Q_1(a) = 0$.  Para ilustrar, digamos que $Q_{100}(a) = 1.9$. Se $R_{100} = 2$, ent√£o $Q_{101}(a) = 1.9 + 0.1(2 - 1.9) = 1.9 + 0.01 = 1.91$. A estimativa se move em dire√ß√£o ao valor real, mas a taxa de aprendizado $\alpha$ controla a velocidade dessa converg√™ncia.

Em ambos os casos, as estimativas iniciais dos valores das a√ß√µes servem como um conjunto de par√¢metros que devem ser escolhidos pelo usu√°rio [^34]. Mesmo definir todas as estimativas para zero implica uma escolha. No entanto, essa influ√™ncia pode ser aproveitada para injetar conhecimento pr√©vio sobre o n√≠vel de recompensas esperado [^34].

**Proposi√ß√£o 1** A escolha de $Q_1(a)$ impacta diretamente a vari√¢ncia das estimativas iniciais de valor.

*Proof:*
Se $Q_1(a)$ √© definido como um valor constante para todas as a√ß√µes, a vari√¢ncia inicial √© zero. Se $Q_1(a)$ √© amostrado de uma distribui√ß√£o, a vari√¢ncia inicial ser√° maior, influenciando a explora√ß√£o inicial.

I. Se $Q_1(a) = c$ para todo $a$, onde $c$ √© uma constante, ent√£o $Var[Q_1(a)] = E[(Q_1(a) - E[Q_1(a)])^2] = E[(c - c)^2] = 0$.
II. Se $Q_1(a)$ √© amostrado de uma distribui√ß√£o com vari√¢ncia $\sigma^2$, ent√£o $Var[Q_1(a)] = \sigma^2 > 0$.
III. Portanto, a escolha de como $Q_1(a)$ √© definido impacta diretamente a vari√¢ncia das estimativas iniciais de valor. ‚ñ†

> üí° **Exemplo Num√©rico:**
    >
    > Considere que temos 5 a√ß√µes.
    > Caso 1: $Q_1(a) = 0$ para todas as a√ß√µes $a$. A vari√¢ncia √© 0.
    > Caso 2: $Q_1(a)$ √© amostrado de uma distribui√ß√£o normal com m√©dia 0 e desvio padr√£o 1 ($N(0, 1)$). Podemos amostrar os seguintes valores: -0.5, 0.2, 1.1, -0.8, 0.4. A vari√¢ncia desses valores amostrados ser√° diferente de 0, influenciando a explora√ß√£o inicial.
    >
    > ```python
    > import numpy as np
    >
    > # Caso 1: Q_1(a) = 0 para todas as a√ß√µes
    > q1_caso1 = np.zeros(5)
    > variancia_caso1 = np.var(q1_caso1)
    > print(f"Vari√¢ncia no Caso 1: {variancia_caso1}")
    >
    > # Caso 2: Q_1(a) amostrado de N(0, 1)
    > np.random.seed(42)  # Para reproducibilidade
    > q1_caso2 = np.random.normal(0, 1, 5)
    > variancia_caso2 = np.var(q1_caso2)
    > print(f"Vari√¢ncia no Caso 2: {variancia_caso2}")
    > ```
    >
    > **Interpreta√ß√£o:** A vari√¢ncia no Caso 1 √© 0, significando que todas as a√ß√µes come√ßam com a mesma estimativa inicial. No Caso 2, a vari√¢ncia √© maior que 0, o que significa que as a√ß√µes t√™m estimativas iniciais diferentes, o que incentiva a explora√ß√£o de a√ß√µes com valores iniciais mais altos.

### Usando Estimativas Otimistas para Incentivar a Explora√ß√£o

As estimativas iniciais dos valores das a√ß√µes tamb√©m podem ser usadas para incentivar a explora√ß√£o [^34]. Em vez de definir os valores iniciais das a√ß√µes para zero, como fizemos no 10-armed testbed [^34], podemos defini-los para um valor alto, como +5 [^34]. Sabendo que os $q_*(a)$ neste problema s√£o selecionados a partir de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1 [^34], uma estimativa inicial de +5 √© considerada excessivamente otimista [^34].

Este otimismo incentiva os m√©todos de action-value a explorar:

1.  As a√ß√µes s√£o inicialmente selecionadas porque suas estimativas de valor s√£o as mais altas [^34].
2.  Quando uma a√ß√£o √© selecionada, a recompensa √© geralmente menor que a estimativa inicial [^34].
3.  O *learner* fica "desapontado" com a recompensa e muda para outras a√ß√µes, reduzindo assim o impacto das recompensas iniciais [^34].

Como resultado, todas as a√ß√µes s√£o tentadas v√°rias vezes antes que as estimativas de valor converjam [^34]. O sistema faz uma quantidade razo√°vel de explora√ß√£o, mesmo se a√ß√µes *greedy* forem sempre selecionadas [^34].

A Figura 2.3 [^10] ilustra o desempenho de um m√©todo *greedy* usando $Q_1(a) = +5$ para todas as a√ß√µes em um *10-armed bandit testbed*. Para fins de compara√ß√£o, tamb√©m √© mostrado um m√©todo $\epsilon$-*greedy* com $Q_1(a) = 0$ [^10]. Inicialmente, o m√©todo otimista tem um desempenho pior, pois explora mais. No entanto, acaba superando o m√©todo $\epsilon$-*greedy* porque sua explora√ß√£o diminui com o tempo [^10]. Essa t√©cnica √© conhecida como **optimistic initial values** [^10].

![Comparative performance of optimistic vs. realistic action-value initialization in a 10-armed testbed, illustrating the impact on optimal action selection over time.](./../images/image3.png)

> **Optimistic Initial Values**
>
> Uma t√©cnica para incentivar a explora√ß√£o em problemas de *k-armed bandit* definindo os valores iniciais da a√ß√£o para um valor irrealisticamente alto, o que leva o agente a explorar diferentes a√ß√µes no in√≠cio, pois recebe recompensas menores do que o esperado [^34].

Consideramos isso um truque simples que pode ser bastante eficaz em problemas estacion√°rios [^34]. No entanto, est√° longe de ser uma abordagem geralmente √∫til para incentivar a explora√ß√£o [^34]. Por exemplo, n√£o √© adequado para problemas n√£o estacion√°rios porque seu *drive* para explora√ß√£o √© inerentemente tempor√°rio [^34]. Se a tarefa mudar, criando uma necessidade renovada de explora√ß√£o, este m√©todo n√£o pode ajudar [^34].

> üí° **Exemplo Num√©rico:**
>
> Considere um 2-armed bandit com $q_*(1) = 1$ e $q_*(2) = 2$. Inicializamos $Q_1(1) = Q_1(2) = 5$.
>
> 1.  Inicialmente, ambos os bra√ßos parecem igualmente atraentes (devido aos valores iniciais otimistas). O agente pode escolher aleatoriamente um bra√ßo.
> 2.  Suponha que o agente escolha o bra√ßo 1 e receba uma recompensa de 0.5.
> 3.  $Q_2(1) = 5 + \alpha (0.5 - 5) = 5 + \alpha (-4.5)$. Se $\alpha = 0.1$, $Q_2(1) = 5 - 0.45 = 4.55$.
> 4.  O agente ent√£o escolhe o bra√ßo 2 e recebe uma recompensa de 1.5.
> 5.  $Q_2(2) = 5 + \alpha (1.5 - 5) = 5 + \alpha (-3.5)$. Se $\alpha = 0.1$, $Q_2(2) = 5 - 0.35 = 4.65$.
>
> O agente continuar√° explorando ambos os bra√ßos, mas com o tempo, $Q(1)$ diminuir√° em dire√ß√£o a 1 e $Q(2)$ diminuir√° em dire√ß√£o a 2. No in√≠cio, o agente √© "desapontado" pelas recompensas mais baixas do que o esperado, incentivando a explora√ß√£o. Eventualmente, o agente aprender√° os valores reais dos bra√ßos e explorar√° menos.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Configura√ß√£o do problema
> q_star = [1, 2]  # Valores reais dos bra√ßos
> n_arms = len(q_star)
> n_steps = 100
> alpha = 0.1
>
> # Inicializa√ß√£o otimista
> Q = np.array([5.0, 5.0])
>
> # Armazenamento de resultados
> rewards = np.zeros(n_steps)
> actions = np.zeros(n_steps)
>
> # Loop principal
> np.random.seed(42)
> for t in range(n_steps):
>     # Escolha da a√ß√£o (greedy)
>     action = np.argmax(Q)
>     actions[t] = action
>
>     # Recebimento da recompensa (amostra de uma normal com m√©dia q_star[action])
>     reward = np.random.normal(q_star[action], 1)
>     rewards[t] = reward
>
>     # Atualiza√ß√£o do valor da a√ß√£o
>     Q[action] = Q[action] + alpha * (reward - Q[action])
>
> # Plotagem dos valores estimados das a√ß√µes ao longo do tempo
> plt.figure(figsize=(10, 6))
> plt.plot(range(n_steps), [5 + alpha * np.sum(rewards[:i] - 5) for i in range(n_steps)], label='Q(1)')
> plt.plot(range(n_steps), [5 + alpha * np.sum(rewards[:i] - 5) for i in range(n_steps)], label='Q(2)')
> plt.xlabel('Passos')
> plt.ylabel('Valor Estimado')
> plt.title('Estimativas de Valor com Valores Iniciais Otimistas')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

Uma alternativa para definir um valor fixo para as estimativas iniciais √© amostr√°-las de uma distribui√ß√£o. Isso introduz uma explora√ß√£o estoc√°stica que pode ser ben√©fica em certos cen√°rios.

**Teorema 1** Se $Q_1(a)$ s√£o amostrados de uma distribui√ß√£o $D$ com m√©dia $\mu$ e vari√¢ncia $\sigma^2$, a taxa de explora√ß√£o inicial √© proporcional a $\sigma$.

*Proof:*
A vari√¢ncia $\sigma^2$ representa a dispers√£o dos valores iniciais. Uma vari√¢ncia maior implica uma maior probabilidade de que algumas a√ß√µes tenham valores iniciais significativamente diferentes, incentivando o agente a explorar essas a√ß√µes inicialmente. A m√©dia $\mu$ influencia o n√≠vel geral de otimismo, com valores maiores de $\mu$ incentivando mais explora√ß√£o.

I. Seja $Q_1(a) \sim D(\mu, \sigma^2)$.
II. A probabilidade de selecionar uma a√ß√£o $a$ no in√≠cio depende da magnitude de $Q_1(a)$ em rela√ß√£o √†s outras a√ß√µes.
III. Uma vari√¢ncia maior $\sigma^2$ implica uma maior probabilidade de que alguma a√ß√£o tenha um valor inicial significativamente maior que a m√©dia $\mu$, levando a uma maior probabilidade de ser explorada.
IV. Portanto, a taxa de explora√ß√£o inicial √© proporcional √† vari√¢ncia $\sigma^2$ da distribui√ß√£o $D$. Um valor maior de $\sigma$ significa mais variabilidade nos valores iniciais, levando a uma maior explora√ß√£o. ‚ñ†

> üí° **Exemplo Num√©rico:**
    >
    > Considere um problema de 3-armed bandit. Em vez de inicializar $Q_1(a)$ com um valor fixo, amostramos de uma distribui√ß√£o normal com m√©dia 3 e desvio padr√£o 2 ($N(3, 2)$). Isso significa que $Q_1(a) \sim N(3, 4)$ para cada a√ß√£o $a$.
    >
    > Digamos que os valores amostrados sejam:
    > $Q_1(1) = 1.5$, $Q_1(2) = 4.2$, $Q_1(3) = 2.8$.
    >
    > Inicialmente, a a√ß√£o 2 (com $Q_1(2) = 4.2$) tem a maior estimativa inicial e, portanto, √© mais propensa a ser explorada primeiro. A vari√¢ncia da distribui√ß√£o afeta a probabilidade de um bra√ßo ter um valor inicial muito alto, incentivando a explora√ß√£o. Uma vari√¢ncia maior significa que √© mais prov√°vel que uma a√ß√£o tenha um valor inicial significativamente maior que a m√©dia, incentivando a explora√ß√£o dessa a√ß√£o.
    >
    > ```python
    > import numpy as np
    >
    > # Configura√ß√£o
    > n_arms = 3
    > mu = 3
    > sigma = 2
    >
    > # Amostrando valores iniciais
    > np.random.seed(42)
    > Q_initial = np.random.normal(mu, sigma, n_arms)
    >
    > print(f"Valores iniciais amostrados: {Q_initial}")
    > print(f"Vari√¢ncia da distribui√ß√£o: {sigma**2}")
    > ```
    >
    > **Interpreta√ß√£o:** A vari√¢ncia maior significa que os valores iniciais s√£o mais dispersos, levando a uma maior probabilidade de que alguma a√ß√£o tenha um valor inicial significativamente maior, resultando em maior explora√ß√£o inicial.

### Limita√ß√µes e Considera√ß√µes
Qualquer m√©todo que se concentre nas condi√ß√µes iniciais de forma especial √© improv√°vel que ajude no caso n√£o estacion√°rio geral [^34]. O in√≠cio do tempo ocorre apenas uma vez e, portanto, n√£o devemos nos concentrar muito nisso [^34]. Essa cr√≠tica tamb√©m se aplica aos m√©todos *sample-average*, que tamb√©m tratam o in√≠cio do tempo como um evento especial, calculando a m√©dia de todas as recompensas subsequentes com pesos iguais [^34]. No entanto, todos esses m√©todos s√£o muito simples e um deles ‚Äì ou alguma combina√ß√£o simples deles ‚Äì geralmente √© adequado na pr√°tica [^34]. No restante deste livro, faremos uso frequente de v√°rias dessas t√©cnicas de explora√ß√£o simples [^34].

Uma maneira de mitigar a limita√ß√£o da n√£o-estacionariedade √© combinar estimativas iniciais otimistas com uma taxa de aprendizado constante $\alpha$ e um mecanismo de reinicializa√ß√£o.

**Teorema 1.1** Em um ambiente n√£o estacion√°rio, combinar *optimistic initial values* com uma taxa de aprendizado constante $\alpha$ e reinicializa√ß√£o peri√≥dica das estimativas de valor pode melhorar o desempenho em compara√ß√£o com o uso isolado de *optimistic initial values*.

*Proof (Outline):* A taxa de aprendizado constante $\alpha$ permite que o agente se adapte √†s mudan√ßas no ambiente ao longo do tempo. A reinicializa√ß√£o peri√≥dica das estimativas de valor, possivelmente para os valores otimistas iniciais, garante que o agente possa "recome√ßar" a explorar quando o ambiente muda significativamente. A combina√ß√£o desses mecanismos permite que o agente equilibre a explora√ß√£o inicial impulsionada pelo otimismo com a adapta√ß√£o cont√≠nua √†s mudan√ßas no ambiente. Um ciclo de reinicializa√ß√£o pode ser disparado por uma queda repentina na recompensa m√©dia, indicando uma mudan√ßa no ambiente.

Essa prova √© um outline, portanto, aqui est√° uma elabora√ß√£o mais formal:

I. Seja $Q_t(a)$ a estimativa do valor da a√ß√£o $a$ no tempo $t$.
II. Em um ambiente n√£o estacion√°rio, a recompensa esperada $q_*(a, t)$ para a a√ß√£o $a$ varia com o tempo $t$.
III. Usando apenas estimativas iniciais otimistas, $Q_1(a)$ √© alto, incentivando a explora√ß√£o inicial, mas a taxa de aprendizado est√°tica n√£o se adapta a mudan√ßas subsequentes.
IV. Com uma taxa de aprendizado constante $\alpha$, a atualiza√ß√£o √© $Q_{t+1}(a) = Q_t(a) + \alpha(R_t - Q_t(a))$, permitindo adapta√ß√£o cont√≠nua.
V. A reinicializa√ß√£o peri√≥dica redefine $Q_t(a)$ para um valor otimista $Q_{initial}$, for√ßando a reexplora√ß√£o.
VI. A combina√ß√£o desses m√©todos permite que o agente equilibre a explora√ß√£o inicial (valores iniciais otimistas), adapta√ß√£o cont√≠nua (taxa de aprendizado $\alpha$) e reexplora√ß√£o (reinicializa√ß√£o peri√≥dica).
VII. Portanto, combinar estimativas iniciais otimistas com uma taxa de aprendizado constante e reinicializa√ß√£o peri√≥dica pode melhorar o desempenho em compara√ß√£o com o uso isolado de estimativas iniciais otimistas em ambientes n√£o estacion√°rios. ‚ñ†

> üí° **Exemplo Num√©rico:**
    >
    > Imagine um ambiente 2-armed bandit n√£o estacion√°rio. Inicialmente, $q_*(1) = 1$ e $q_*(2) = 2$. Ap√≥s 1000 passos, os valores mudam para $q_*(1) = 3$ e $q_*(2) = 0.5$.
    >
    > Usamos $Q_1(1) = Q_1(2) = 5$ (valores iniciais otimistas) e $\alpha = 0.1$. Al√©m disso, monitoramos a recompensa m√©dia. Se a recompensa m√©dia cair abaixo de um limiar, reinicializamos as estimativas de valor para 5.
    >
    > 1.  **Fase 1 (Estacion√°ria):** O agente explora inicialmente devido aos valores otimistas e eventualmente converge para $Q(1) \approx 1$ e $Q(2) \approx 2$.
    > 2.  **Mudan√ßa Ambiental:** No passo 1001, os valores mudam repentinamente. A recompensa m√©dia come√ßa a cair porque o agente ainda est√° explorando principalmente o bra√ßo 2 (que agora tem um valor baixo).
    > 3.  **Reinicializa√ß√£o:** Quando a recompensa m√©dia cai abaixo do limiar, reinicializamos $Q(1) = Q(2) = 5$.
    > 4.  **Nova Explora√ß√£o:** O agente √© for√ßado a explorar novamente devido aos valores otimistas. Ele rapidamente aprende os novos valores $Q(1) \approx 3$ e $Q(2) \approx 0.5$.
    >
    > A reinicializa√ß√£o peri√≥dica permite que o agente se recupere das mudan√ßas ambientais e se readapte, mostrando os benef√≠cios de combinar estimativas iniciais otimistas com uma taxa de aprendizado constante e reinicializa√ß√£o em ambientes n√£o estacion√°rios.

### Conclus√£o
As estimativas iniciais dos valores das a√ß√µes t√™m um impacto significativo no comportamento dos m√©todos de **action-value** [^34]. Definir estimativas iniciais para valores otimistas pode ser uma maneira eficaz de incentivar a explora√ß√£o em problemas estacion√°rios [^34]. No entanto, essa abordagem tem limita√ß√µes e pode n√£o ser adequada para problemas n√£o estacion√°rios [^34]. Compreender o impacto das estimativas iniciais √© essencial para projetar algoritmos de **reinforcement learning** eficazes [^34].

### Refer√™ncias
[^34]: Cap√≠tulo 2 do texto fornecido.
[^27]: Cap√≠tulo 2 do texto fornecido.
[^8]: Cap√≠tulo 2 do texto fornecido.
[^10]: Cap√≠tulo 2 do texto fornecido.
[^3]: Cap√≠tulo 2 do texto fornecido.
<!-- END -->