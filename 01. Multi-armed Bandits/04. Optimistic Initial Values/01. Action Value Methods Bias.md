## Optimistic Initial Values in Action-Value Methods

### IntroduÃ§Ã£o
Como discutido anteriormente, os mÃ©todos de **action-value** estimam os valores das aÃ§Ãµes para tomar decisÃµes [^27]. Uma caracterÃ­stica importante desses mÃ©todos Ã© a influÃªncia das estimativas iniciais dos valores das aÃ§Ãµes, denotadas como $Q_1(a)$ [^34]. Este capÃ­tulo explora como essas estimativas iniciais afetam o aprendizado e como elas podem ser usadas de forma otimista para incentivar a exploraÃ§Ã£o. Em particular, vamos discutir o conceito de *optimistic initial values* como uma tÃ©cnica simples para incentivar a exploraÃ§Ã£o em problemas de **k-armed bandit**.

### O Impacto das Estimativas Iniciais
As estimativas iniciais dos valores das aÃ§Ãµes introduzem um **bias** nos mÃ©todos de action-value [^34]. Em termos estatÃ­sticos, esses mÃ©todos sÃ£o enviesados por suas estimativas iniciais. O comportamento desse bias varia dependendo do mÃ©todo de estimativa usado:

*   **Sample-Average Methods**: O bias desaparece assim que todas as aÃ§Ãµes sÃ£o selecionadas pelo menos uma vez [^34]. Isso ocorre porque, Ã  medida que o nÃºmero de amostras para cada aÃ§Ã£o aumenta, a mÃ©dia amostral converge para o valor real da aÃ§Ã£o, conforme demonstrado pela lei dos grandes nÃºmeros [^3].

    **Prova:**
    Seja $Q_n(a)$ a estimativa do valor da aÃ§Ã£o $a$ apÃ³s $n$ vezes que ela foi selecionada e $R_i$ a $i$-Ã©sima recompensa recebida apÃ³s selecionar a aÃ§Ã£o $a$. EntÃ£o:

    I. A estimativa do valor usando o mÃ©todo sample-average Ã© dada por:
    $$Q_n(a) = \frac{1}{n} \sum_{i=1}^{n} R_i$$

    II. Pela Lei Forte dos Grandes NÃºmeros, se as recompensas $R_i$ sÃ£o independentes e identicamente distribuÃ­das com valor esperado $q_*(a)$, entÃ£o:
    $$\lim_{n \to \infty} Q_n(a) = q_*(a) \quad \text{com probabilidade 1}$$

    III. Portanto, Ã  medida que $n$ tende ao infinito, a estimativa $Q_n(a)$ converge para o valor real da aÃ§Ã£o $q_*(a)$, eliminando o bias inicial. â– 

    > ðŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Considere um problema de 3-armed bandit com valores reais de aÃ§Ãµes $q_*(1) = 1$, $q_*(2) = 2$, e $q_*(3) = 3$. Inicializamos $Q_1(a) = 0$ para todas as aÃ§Ãµes. ApÃ³s selecionar a aÃ§Ã£o 1 por 10 vezes e receber recompensas de {0.5, 0.8, 1.2, 0.9, 1.1, 0.7, 1.0, 0.6, 1.3, 0.9}, a estimativa do valor da aÃ§Ã£o 1 serÃ¡:
    >
    > $Q_{10}(1) = \frac{0.5 + 0.8 + 1.2 + 0.9 + 1.1 + 0.7 + 1.0 + 0.6 + 1.3 + 0.9}{10} = \frac{9}{10} = 0.9$.
    >
    > ApÃ³s selecionar a aÃ§Ã£o 1 por 100 vezes e receber recompensas cuja mÃ©dia Ã© 0.98, a estimativa do valor da aÃ§Ã£o 1 serÃ¡:
    > $Q_{100}(1) = 0.98$.
    >
    > Observe que a estimativa se aproxima do valor real da aÃ§Ã£o, $q_*(1) = 1$, demonstrando a convergÃªncia pelo mÃ©todo sample-average.

*   **Constant-Î± Methods**: O bias Ã© permanente, embora diminua ao longo do tempo [^34]. A equaÃ§Ã£o (2.6) [^8] mostra que o valor de $Q_{n+1}$ Ã© uma mÃ©dia ponderada de recompensas passadas e da estimativa inicial $Q_1$. O peso de $Q_1$ diminui exponencialmente, mas nunca se torna zero.

    **Prova:**
    A atualizaÃ§Ã£o para o mÃ©todo constant-$\alpha$ Ã© dada por:

    I. $Q_{n+1}(a) = Q_n(a) + \alpha [R_n - Q_n(a)]$, onde $\alpha$ Ã© a taxa de aprendizado constante.

    II. Expandindo recursivamente, obtemos:
    $Q_{n+1}(a) = (1-\alpha)^n Q_1(a) + \sum_{i=1}^{n} \alpha (1-\alpha)^{n-i} R_i$

    III. Observe que o peso de $Q_1(a)$ Ã© $(1-\alpha)^n$, que se aproxima de 0 Ã  medida que $n$ tende ao infinito, mas nunca Ã© exatamente 0 para $\alpha > 0$ e $n < \infty$. Portanto, a estimativa inicial $Q_1(a)$ sempre terÃ¡ alguma influÃªncia, mesmo que pequena, em $Q_{n+1}(a)$.

    IV. Isso demonstra que o bias introduzido pela estimativa inicial persiste, embora seu impacto diminua ao longo do tempo. â– 

    > ðŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Considere uma aÃ§Ã£o com valor real $q_*(a) = 2$. Inicializamos $Q_1(a) = 0$ e usamos $\alpha = 0.1$. Recebemos uma recompensa $R_1 = 1$.
    >
    > $Q_2(a) = Q_1(a) + \alpha [R_1 - Q_1(a)] = 0 + 0.1 * (1 - 0) = 0.1$.
    >
    > Agora, recebemos uma recompensa $R_2 = 3$.
    >
    > $Q_3(a) = Q_2(a) + \alpha [R_2 - Q_2(a)] = 0.1 + 0.1 * (3 - 0.1) = 0.1 + 0.29 = 0.39$.
    >
    > ApÃ³s 100 iteraÃ§Ãµes com recompensas aleatÃ³rias em torno de $q_*(a) = 2$, a estimativa $Q_{101}(a)$ se aproximarÃ¡ de 2, mas nunca serÃ¡ exatamente 2 devido Ã  influÃªncia persistente de $Q_1(a) = 0$.  Para ilustrar, digamos que $Q_{100}(a) = 1.9$. Se $R_{100} = 2$, entÃ£o $Q_{101}(a) = 1.9 + 0.1(2 - 1.9) = 1.9 + 0.01 = 1.91$. A estimativa se move em direÃ§Ã£o ao valor real, mas a taxa de aprendizado $\alpha$ controla a velocidade dessa convergÃªncia.

Em ambos os casos, as estimativas iniciais dos valores das aÃ§Ãµes servem como um conjunto de parÃ¢metros que devem ser escolhidos pelo usuÃ¡rio [^34]. Mesmo definir todas as estimativas para zero implica uma escolha. No entanto, essa influÃªncia pode ser aproveitada para injetar conhecimento prÃ©vio sobre o nÃ­vel de recompensas esperado [^34].

**ProposiÃ§Ã£o 1** A escolha de $Q_1(a)$ impacta diretamente a variÃ¢ncia das estimativas iniciais de valor.

*Proof:*
Se $Q_1(a)$ Ã© definido como um valor constante para todas as aÃ§Ãµes, a variÃ¢ncia inicial Ã© zero. Se $Q_1(a)$ Ã© amostrado de uma distribuiÃ§Ã£o, a variÃ¢ncia inicial serÃ¡ maior, influenciando a exploraÃ§Ã£o inicial.

I. Se $Q_1(a) = c$ para todo $a$, onde $c$ Ã© uma constante, entÃ£o $Var[Q_1(a)] = E[(Q_1(a) - E[Q_1(a)])^2] = E[(c - c)^2] = 0$.
II. Se $Q_1(a)$ Ã© amostrado de uma distribuiÃ§Ã£o com variÃ¢ncia $\sigma^2$, entÃ£o $Var[Q_1(a)] = \sigma^2 > 0$.
III. Portanto, a escolha de como $Q_1(a)$ Ã© definido impacta diretamente a variÃ¢ncia das estimativas iniciais de valor. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Considere que temos 5 aÃ§Ãµes.
    > Caso 1: $Q_1(a) = 0$ para todas as aÃ§Ãµes $a$. A variÃ¢ncia Ã© 0.
    > Caso 2: $Q_1(a)$ Ã© amostrado de uma distribuiÃ§Ã£o normal com mÃ©dia 0 e desvio padrÃ£o 1 ($N(0, 1)$). Podemos amostrar os seguintes valores: -0.5, 0.2, 1.1, -0.8, 0.4. A variÃ¢ncia desses valores amostrados serÃ¡ diferente de 0, influenciando a exploraÃ§Ã£o inicial.
    >
    > ```python
    > import numpy as np
    >
    > # Caso 1: Q_1(a) = 0 para todas as aÃ§Ãµes
    > q1_caso1 = np.zeros(5)
    > variancia_caso1 = np.var(q1_caso1)
    > print(f"VariÃ¢ncia no Caso 1: {variancia_caso1}")
    >
    > # Caso 2: Q_1(a) amostrado de N(0, 1)
    > np.random.seed(42)  # Para reproducibilidade
    > q1_caso2 = np.random.normal(0, 1, 5)
    > variancia_caso2 = np.var(q1_caso2)
    > print(f"VariÃ¢ncia no Caso 2: {variancia_caso2}")
    > ```
    >
    > **InterpretaÃ§Ã£o:** A variÃ¢ncia no Caso 1 Ã© 0, significando que todas as aÃ§Ãµes comeÃ§am com a mesma estimativa inicial. No Caso 2, a variÃ¢ncia Ã© maior que 0, o que significa que as aÃ§Ãµes tÃªm estimativas iniciais diferentes, o que incentiva a exploraÃ§Ã£o de aÃ§Ãµes com valores iniciais mais altos.

### Usando Estimativas Otimistas para Incentivar a ExploraÃ§Ã£o

As estimativas iniciais dos valores das aÃ§Ãµes tambÃ©m podem ser usadas para incentivar a exploraÃ§Ã£o [^34]. Em vez de definir os valores iniciais das aÃ§Ãµes para zero, como fizemos no 10-armed testbed [^34], podemos defini-los para um valor alto, como +5 [^34]. Sabendo que os $q_*(a)$ neste problema sÃ£o selecionados a partir de uma distribuiÃ§Ã£o normal com mÃ©dia 0 e variÃ¢ncia 1 [^34], uma estimativa inicial de +5 Ã© considerada excessivamente otimista [^34].

Este otimismo incentiva os mÃ©todos de action-value a explorar:

1.  As aÃ§Ãµes sÃ£o inicialmente selecionadas porque suas estimativas de valor sÃ£o as mais altas [^34].
2.  Quando uma aÃ§Ã£o Ã© selecionada, a recompensa Ã© geralmente menor que a estimativa inicial [^34].
3.  O *learner* fica "desapontado" com a recompensa e muda para outras aÃ§Ãµes, reduzindo assim o impacto das recompensas iniciais [^34].

Como resultado, todas as aÃ§Ãµes sÃ£o tentadas vÃ¡rias vezes antes que as estimativas de valor converjam [^34]. O sistema faz uma quantidade razoÃ¡vel de exploraÃ§Ã£o, mesmo se aÃ§Ãµes *greedy* forem sempre selecionadas [^34].

A Figura 2.3 [^10] ilustra o desempenho de um mÃ©todo *greedy* usando $Q_1(a) = +5$ para todas as aÃ§Ãµes em um *10-armed bandit testbed*. Para fins de comparaÃ§Ã£o, tambÃ©m Ã© mostrado um mÃ©todo $\epsilon$-*greedy* com $Q_1(a) = 0$ [^10]. Inicialmente, o mÃ©todo otimista tem um desempenho pior, pois explora mais. No entanto, acaba superando o mÃ©todo $\epsilon$-*greedy* porque sua exploraÃ§Ã£o diminui com o tempo [^10]. Essa tÃ©cnica Ã© conhecida como **optimistic initial values** [^10].

![Comparative performance of optimistic vs. realistic action-value initialization in a 10-armed testbed, illustrating the impact on optimal action selection over time.](./../images/image3.png)

> **Optimistic Initial Values**
>
> Uma tÃ©cnica para incentivar a exploraÃ§Ã£o em problemas de *k-armed bandit* definindo os valores iniciais da aÃ§Ã£o para um valor irrealisticamente alto, o que leva o agente a explorar diferentes aÃ§Ãµes no inÃ­cio, pois recebe recompensas menores do que o esperado [^34].

Consideramos isso um truque simples que pode ser bastante eficaz em problemas estacionÃ¡rios [^34]. No entanto, estÃ¡ longe de ser uma abordagem geralmente Ãºtil para incentivar a exploraÃ§Ã£o [^34]. Por exemplo, nÃ£o Ã© adequado para problemas nÃ£o estacionÃ¡rios porque seu *drive* para exploraÃ§Ã£o Ã© inerentemente temporÃ¡rio [^34]. Se a tarefa mudar, criando uma necessidade renovada de exploraÃ§Ã£o, este mÃ©todo nÃ£o pode ajudar [^34].

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um 2-armed bandit com $q_*(1) = 1$ e $q_*(2) = 2$. Inicializamos $Q_1(1) = Q_1(2) = 5$.
>
> 1.  Inicialmente, ambos os braÃ§os parecem igualmente atraentes (devido aos valores iniciais otimistas). O agente pode escolher aleatoriamente um braÃ§o.
> 2.  Suponha que o agente escolha o braÃ§o 1 e receba uma recompensa de 0.5.
> 3.  $Q_2(1) = 5 + \alpha (0.5 - 5) = 5 + \alpha (-4.5)$. Se $\alpha = 0.1$, $Q_2(1) = 5 - 0.45 = 4.55$.
> 4.  O agente entÃ£o escolhe o braÃ§o 2 e recebe uma recompensa de 1.5.
> 5.  $Q_2(2) = 5 + \alpha (1.5 - 5) = 5 + \alpha (-3.5)$. Se $\alpha = 0.1$, $Q_2(2) = 5 - 0.35 = 4.65$.
>
> O agente continuarÃ¡ explorando ambos os braÃ§os, mas com o tempo, $Q(1)$ diminuirÃ¡ em direÃ§Ã£o a 1 e $Q(2)$ diminuirÃ¡ em direÃ§Ã£o a 2. No inÃ­cio, o agente Ã© "desapontado" pelas recompensas mais baixas do que o esperado, incentivando a exploraÃ§Ã£o. Eventualmente, o agente aprenderÃ¡ os valores reais dos braÃ§os e explorarÃ¡ menos.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # ConfiguraÃ§Ã£o do problema
> q_star = [1, 2]  # Valores reais dos braÃ§os
> n_arms = len(q_star)
> n_steps = 100
> alpha = 0.1
>
> # InicializaÃ§Ã£o otimista
> Q = np.array([5.0, 5.0])
>
> # Armazenamento de resultados
> rewards = np.zeros(n_steps)
> actions = np.zeros(n_steps)
>
> # Loop principal
> np.random.seed(42)
> for t in range(n_steps):
>     # Escolha da aÃ§Ã£o (greedy)
>     action = np.argmax(Q)
>     actions[t] = action
>
>     # Recebimento da recompensa (amostra de uma normal com mÃ©dia q_star[action])
>     reward = np.random.normal(q_star[action], 1)
>     rewards[t] = reward
>
>     # AtualizaÃ§Ã£o do valor da aÃ§Ã£o
>     Q[action] = Q[action] + alpha * (reward - Q[action])
>
> # Plotagem dos valores estimados das aÃ§Ãµes ao longo do tempo
> plt.figure(figsize=(10, 6))
> plt.plot(range(n_steps), [5 + alpha * np.sum(rewards[:i] - 5) for i in range(n_steps)], label='Q(1)')
> plt.plot(range(n_steps), [5 + alpha * np.sum(rewards[:i] - 5) for i in range(n_steps)], label='Q(2)')
> plt.xlabel('Passos')
> plt.ylabel('Valor Estimado')
> plt.title('Estimativas de Valor com Valores Iniciais Otimistas')
> plt.legend()
> plt.grid(True)
> plt.show()
> ```

Uma alternativa para definir um valor fixo para as estimativas iniciais Ã© amostrÃ¡-las de uma distribuiÃ§Ã£o. Isso introduz uma exploraÃ§Ã£o estocÃ¡stica que pode ser benÃ©fica em certos cenÃ¡rios.

**Teorema 1** Se $Q_1(a)$ sÃ£o amostrados de uma distribuiÃ§Ã£o $D$ com mÃ©dia $\mu$ e variÃ¢ncia $\sigma^2$, a taxa de exploraÃ§Ã£o inicial Ã© proporcional a $\sigma$.

*Proof:*
A variÃ¢ncia $\sigma^2$ representa a dispersÃ£o dos valores iniciais. Uma variÃ¢ncia maior implica uma maior probabilidade de que algumas aÃ§Ãµes tenham valores iniciais significativamente diferentes, incentivando o agente a explorar essas aÃ§Ãµes inicialmente. A mÃ©dia $\mu$ influencia o nÃ­vel geral de otimismo, com valores maiores de $\mu$ incentivando mais exploraÃ§Ã£o.

I. Seja $Q_1(a) \sim D(\mu, \sigma^2)$.
II. A probabilidade de selecionar uma aÃ§Ã£o $a$ no inÃ­cio depende da magnitude de $Q_1(a)$ em relaÃ§Ã£o Ã s outras aÃ§Ãµes.
III. Uma variÃ¢ncia maior $\sigma^2$ implica uma maior probabilidade de que alguma aÃ§Ã£o tenha um valor inicial significativamente maior que a mÃ©dia $\mu$, levando a uma maior probabilidade de ser explorada.
IV. Portanto, a taxa de exploraÃ§Ã£o inicial Ã© proporcional Ã  variÃ¢ncia $\sigma^2$ da distribuiÃ§Ã£o $D$. Um valor maior de $\sigma$ significa mais variabilidade nos valores iniciais, levando a uma maior exploraÃ§Ã£o. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Considere um problema de 3-armed bandit. Em vez de inicializar $Q_1(a)$ com um valor fixo, amostramos de uma distribuiÃ§Ã£o normal com mÃ©dia 3 e desvio padrÃ£o 2 ($N(3, 2)$). Isso significa que $Q_1(a) \sim N(3, 4)$ para cada aÃ§Ã£o $a$.
    >
    > Digamos que os valores amostrados sejam:
    > $Q_1(1) = 1.5$, $Q_1(2) = 4.2$, $Q_1(3) = 2.8$.
    >
    > Inicialmente, a aÃ§Ã£o 2 (com $Q_1(2) = 4.2$) tem a maior estimativa inicial e, portanto, Ã© mais propensa a ser explorada primeiro. A variÃ¢ncia da distribuiÃ§Ã£o afeta a probabilidade de um braÃ§o ter um valor inicial muito alto, incentivando a exploraÃ§Ã£o. Uma variÃ¢ncia maior significa que Ã© mais provÃ¡vel que uma aÃ§Ã£o tenha um valor inicial significativamente maior que a mÃ©dia, incentivando a exploraÃ§Ã£o dessa aÃ§Ã£o.
    >
    > ```python
    > import numpy as np
    >
    > # ConfiguraÃ§Ã£o
    > n_arms = 3
    > mu = 3
    > sigma = 2
    >
    > # Amostrando valores iniciais
    > np.random.seed(42)
    > Q_initial = np.random.normal(mu, sigma, n_arms)
    >
    > print(f"Valores iniciais amostrados: {Q_initial}")
    > print(f"VariÃ¢ncia da distribuiÃ§Ã£o: {sigma**2}")
    > ```
    >
    > **InterpretaÃ§Ã£o:** A variÃ¢ncia maior significa que os valores iniciais sÃ£o mais dispersos, levando a uma maior probabilidade de que alguma aÃ§Ã£o tenha um valor inicial significativamente maior, resultando em maior exploraÃ§Ã£o inicial.

### LimitaÃ§Ãµes e ConsideraÃ§Ãµes
Qualquer mÃ©todo que se concentre nas condiÃ§Ãµes iniciais de forma especial Ã© improvÃ¡vel que ajude no caso nÃ£o estacionÃ¡rio geral [^34]. O inÃ­cio do tempo ocorre apenas uma vez e, portanto, nÃ£o devemos nos concentrar muito nisso [^34]. Essa crÃ­tica tambÃ©m se aplica aos mÃ©todos *sample-average*, que tambÃ©m tratam o inÃ­cio do tempo como um evento especial, calculando a mÃ©dia de todas as recompensas subsequentes com pesos iguais [^34]. No entanto, todos esses mÃ©todos sÃ£o muito simples e um deles â€“ ou alguma combinaÃ§Ã£o simples deles â€“ geralmente Ã© adequado na prÃ¡tica [^34]. No restante deste livro, faremos uso frequente de vÃ¡rias dessas tÃ©cnicas de exploraÃ§Ã£o simples [^34].

Uma maneira de mitigar a limitaÃ§Ã£o da nÃ£o-estacionariedade Ã© combinar estimativas iniciais otimistas com uma taxa de aprendizado constante $\alpha$ e um mecanismo de reinicializaÃ§Ã£o.

**Teorema 1.1** Em um ambiente nÃ£o estacionÃ¡rio, combinar *optimistic initial values* com uma taxa de aprendizado constante $\alpha$ e reinicializaÃ§Ã£o periÃ³dica das estimativas de valor pode melhorar o desempenho em comparaÃ§Ã£o com o uso isolado de *optimistic initial values*.

*Proof (Outline):* A taxa de aprendizado constante $\alpha$ permite que o agente se adapte Ã s mudanÃ§as no ambiente ao longo do tempo. A reinicializaÃ§Ã£o periÃ³dica das estimativas de valor, possivelmente para os valores otimistas iniciais, garante que o agente possa "recomeÃ§ar" a explorar quando o ambiente muda significativamente. A combinaÃ§Ã£o desses mecanismos permite que o agente equilibre a exploraÃ§Ã£o inicial impulsionada pelo otimismo com a adaptaÃ§Ã£o contÃ­nua Ã s mudanÃ§as no ambiente. Um ciclo de reinicializaÃ§Ã£o pode ser disparado por uma queda repentina na recompensa mÃ©dia, indicando uma mudanÃ§a no ambiente.

Essa prova Ã© um outline, portanto, aqui estÃ¡ uma elaboraÃ§Ã£o mais formal:

I. Seja $Q_t(a)$ a estimativa do valor da aÃ§Ã£o $a$ no tempo $t$.
II. Em um ambiente nÃ£o estacionÃ¡rio, a recompensa esperada $q_*(a, t)$ para a aÃ§Ã£o $a$ varia com o tempo $t$.
III. Usando apenas estimativas iniciais otimistas, $Q_1(a)$ Ã© alto, incentivando a exploraÃ§Ã£o inicial, mas a taxa de aprendizado estÃ¡tica nÃ£o se adapta a mudanÃ§as subsequentes.
IV. Com uma taxa de aprendizado constante $\alpha$, a atualizaÃ§Ã£o Ã© $Q_{t+1}(a) = Q_t(a) + \alpha(R_t - Q_t(a))$, permitindo adaptaÃ§Ã£o contÃ­nua.
V. A reinicializaÃ§Ã£o periÃ³dica redefine $Q_t(a)$ para um valor otimista $Q_{initial}$, forÃ§ando a reexploraÃ§Ã£o.
VI. A combinaÃ§Ã£o desses mÃ©todos permite que o agente equilibre a exploraÃ§Ã£o inicial (valores iniciais otimistas), adaptaÃ§Ã£o contÃ­nua (taxa de aprendizado $\alpha$) e reexploraÃ§Ã£o (reinicializaÃ§Ã£o periÃ³dica).
VII. Portanto, combinar estimativas iniciais otimistas com uma taxa de aprendizado constante e reinicializaÃ§Ã£o periÃ³dica pode melhorar o desempenho em comparaÃ§Ã£o com o uso isolado de estimativas iniciais otimistas em ambientes nÃ£o estacionÃ¡rios. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**
    >
    > Imagine um ambiente 2-armed bandit nÃ£o estacionÃ¡rio. Inicialmente, $q_*(1) = 1$ e $q_*(2) = 2$. ApÃ³s 1000 passos, os valores mudam para $q_*(1) = 3$ e $q_*(2) = 0.5$.
    >
    > Usamos $Q_1(1) = Q_1(2) = 5$ (valores iniciais otimistas) e $\alpha = 0.1$. AlÃ©m disso, monitoramos a recompensa mÃ©dia. Se a recompensa mÃ©dia cair abaixo de um limiar, reinicializamos as estimativas de valor para 5.
    >
    > 1.  **Fase 1 (EstacionÃ¡ria):** O agente explora inicialmente devido aos valores otimistas e eventualmente converge para $Q(1) \approx 1$ e $Q(2) \approx 2$.
    > 2.  **MudanÃ§a Ambiental:** No passo 1001, os valores mudam repentinamente. A recompensa mÃ©dia comeÃ§a a cair porque o agente ainda estÃ¡ explorando principalmente o braÃ§o 2 (que agora tem um valor baixo).
    > 3.  **ReinicializaÃ§Ã£o:** Quando a recompensa mÃ©dia cai abaixo do limiar, reinicializamos $Q(1) = Q(2) = 5$.
    > 4.  **Nova ExploraÃ§Ã£o:** O agente Ã© forÃ§ado a explorar novamente devido aos valores otimistas. Ele rapidamente aprende os novos valores $Q(1) \approx 3$ e $Q(2) \approx 0.5$.
    >
    > A reinicializaÃ§Ã£o periÃ³dica permite que o agente se recupere das mudanÃ§as ambientais e se readapte, mostrando os benefÃ­cios de combinar estimativas iniciais otimistas com uma taxa de aprendizado constante e reinicializaÃ§Ã£o em ambientes nÃ£o estacionÃ¡rios.

### ConclusÃ£o
As estimativas iniciais dos valores das aÃ§Ãµes tÃªm um impacto significativo no comportamento dos mÃ©todos de **action-value** [^34]. Definir estimativas iniciais para valores otimistas pode ser uma maneira eficaz de incentivar a exploraÃ§Ã£o em problemas estacionÃ¡rios [^34]. No entanto, essa abordagem tem limitaÃ§Ãµes e pode nÃ£o ser adequada para problemas nÃ£o estacionÃ¡rios [^34]. Compreender o impacto das estimativas iniciais Ã© essencial para projetar algoritmos de **reinforcement learning** eficazes [^34].

### ReferÃªncias
[^34]: CapÃ­tulo 2 do texto fornecido.
[^27]: CapÃ­tulo 2 do texto fornecido.
[^8]: CapÃ­tulo 2 do texto fornecido.
[^10]: CapÃ­tulo 2 do texto fornecido.
[^3]: CapÃ­tulo 2 do texto fornecido.
<!-- END -->