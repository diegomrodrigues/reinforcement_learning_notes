## Otimismo Inicial e DistribuiÃ§Ã£o de Recompensas em Bandit Problems

### IntroduÃ§Ã£o
O uso de **valores iniciais otimistas** Ã© uma tÃ©cnica para incentivar a exploraÃ§Ã£o em problemas *k*-armed bandit [^34]. A ideia central Ã© inicializar as estimativas de valor das aÃ§Ãµes, $Q_1(a)$, com um valor significativamente superior ao que se espera receber como recompensa. Este mÃ©todo explora a **natureza transitÃ³ria do otimismo**, incentivando o agente a experimentar diferentes aÃ§Ãµes atÃ© que suas estimativas de valor convirjam. No entanto, a eficÃ¡cia desta tÃ©cnica pode depender da distribuiÃ§Ã£o subjacente das recompensas [^34].

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um problema 3-armed bandit. Inicializamos $Q_1(1) = Q_1(2) = Q_1(3) = 5$. Suponha que os valores reais das aÃ§Ãµes sejam $q_*(1) = 1$, $q_*(2) = 2$, e $q_*(3) = 3$. O otimismo inicial de 5 incentivarÃ¡ o agente a experimentar todas as aÃ§Ãµes, mesmo que as primeiras recompensas sejam menores que 5.

### Conceitos Fundamentais
Em problemas *k*-armed bandit, o objetivo Ã© maximizar a recompensa total esperada ao longo do tempo, repetidamente selecionando aÃ§Ãµes entre *k* opÃ§Ãµes disponÃ­veis [^25]. Cada aÃ§Ã£o *a* tem um valor verdadeiro, $q_*(a)$, que representa a recompensa mÃ©dia esperada ao selecionar essa aÃ§Ã£o [^25].

![DistribuiÃ§Ãµes de recompensa para um problema de bandit de 10 braÃ§os.](./../images/image5.png)

A estimativa do valor de uma aÃ§Ã£o no tempo *t* Ã© denotada por $Q_t(a)$ [^26]. O **objetivo do aprendizado** Ã© fazer com que $Q_t(a)$ se aproxime de $q_*(a)$ o mais rÃ¡pido possÃ­vel. Para alcanÃ§ar este objetivo, o agente deve equilibrar *exploraÃ§Ã£o* (experimentar aÃ§Ãµes diferentes para melhorar as estimativas) e *explotaÃ§Ã£o* (selecionar a aÃ§Ã£o com a maior estimativa atual) [^26].

Otimismo inicial Ã© uma tÃ©cnica que pode ser usada para incentivar a exploraÃ§Ã£o. Ao inicializar $Q_1(a)$ com um valor alto, o agente Ã© inicialmente "otimista" sobre o potencial de cada aÃ§Ã£o. Essa abordagem induz o agente a explorar as aÃ§Ãµes de forma mais extensa, pois as recompensas reais serÃ£o, inicialmente, menores do que a estimativa inicial [^34].

**Teorema 1** Seja $Q_1(a) = V, \forall a$, onde $V$ Ã© um valor inicial otimista. Se existe uma aÃ§Ã£o $a^*$ tal que $q_*(a^*) > \mathbb{E}[R_t | A_t \neq a^*]$, entÃ£o o agente irÃ¡, em algum momento, explorar $a^*$.

*Prova.* O otimismo inicial garante que todas as aÃ§Ãµes serÃ£o tentadas em algum momento. Se a aÃ§Ã£o $a^*$ tem um valor esperado maior que a mÃ©dia das outras aÃ§Ãµes, eventualmente sua estimativa de valor $Q_t(a^*)$ irÃ¡ superar as demais, levando o agente a explorÃ¡-la mais frequentemente.

*Prova Detalhada.*

I. Inicialmente, $Q_1(a) = V$ para todas as aÃ§Ãµes $a$. Isso significa que todas as aÃ§Ãµes sÃ£o consideradas igualmente "boas" no inÃ­cio.

II. Como o agente explora, ele observa as recompensas $R_t$ para cada aÃ§Ã£o $a$ selecionada $A_t$. A estimativa de valor $Q_t(a)$ Ã© atualizada com base nessas recompensas.

III. Seja $a^*$ a aÃ§Ã£o Ã³tima, tal que $q_*(a^*) > \mathbb{E}[R_t | A_t \neq a^*]$. Isso significa que o valor esperado da aÃ§Ã£o $a^*$ Ã© maior do que o valor esperado mÃ©dio de todas as outras aÃ§Ãµes.

IV. Como todas as aÃ§Ãµes sÃ£o exploradas inicialmente (devido ao otimismo inicial), $a^*$ serÃ¡ selecionada pelo menos uma vez.

V. ApÃ³s a seleÃ§Ã£o de $a^*$, a estimativa $Q_t(a^*)$ serÃ¡ atualizada com base na recompensa observada. Como $q_*(a^*) > \mathbb{E}[R_t | A_t \neq a^*]$, a estimativa $Q_t(a^*)$ tenderÃ¡ a aumentar em relaÃ§Ã£o Ã s estimativas das outras aÃ§Ãµes.

VI. Com o tempo, $Q_t(a^*)$ se aproximarÃ¡ de $q_*(a^*)$, enquanto as estimativas das aÃ§Ãµes sub-Ã³timas se aproximarÃ£o de seus valores esperados, que sÃ£o menores que $q_*(a^*)$.

VII. Portanto, eventualmente, $Q_t(a^*) > Q_t(a)$ para todas as outras aÃ§Ãµes $a \neq a^*$, e o agente explorarÃ¡ $a^*$ mais frequentemente. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um 2-armed bandit com $q_*(1) = 1$ e $q_*(2) = 2$. Inicializamos $Q_1(1) = Q_1(2) = 5$. Na primeira iteraÃ§Ã£o, o agente escolhe a aÃ§Ã£o 1 e recebe uma recompensa de 0. A nova estimativa para a aÃ§Ã£o 1 Ã© $Q_2(1) = 0$. Na segunda iteraÃ§Ã£o, o agente escolhe a aÃ§Ã£o 2 e recebe uma recompensa de 2. A nova estimativa para a aÃ§Ã£o 2 Ã© $Q_2(2) = 2$. O agente continuarÃ¡ explorando atÃ© que $Q_t(2)$ seja significativamente maior que $Q_t(1)$.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # ParÃ¢metros
> k = 2  # NÃºmero de braÃ§os
> q_true = [1, 2]  # Valores verdadeiros das aÃ§Ãµes
> initial_value = 5  # Valor inicial otimista
> n_steps = 100  # NÃºmero de passos
>
> # InicializaÃ§Ã£o
> Q = [initial_value, initial_value]  # Estimativas de valor
> N = [0, 0]  # Contagem de vezes que cada aÃ§Ã£o foi selecionada
> rewards = []
>
> # Loop principal
> for t in range(n_steps):
>     # Selecionar aÃ§Ã£o (epsilon-greedy com epsilon=0.1)
>     if np.random.rand() < 0.1:
>         action = np.random.choice(k)  # ExploraÃ§Ã£o
>     else:
>         action = np.argmax(Q)  # ExplotaÃ§Ã£o
>
>     # Receber recompensa
>     reward = np.random.normal(q_true[action], 1)
>     rewards.append(reward)
>
>     # Atualizar estimativas de valor
>     N[action] += 1
>     Q[action] = Q[action] + (1/N[action]) * (reward - Q[action])
>
> # Plotar as estimativas de valor ao longo do tempo
> plt.figure(figsize=(10, 6))
> plt.plot(np.cumsum(np.array(rewards)) / (np.arange(n_steps) + 1))
> plt.xlabel("Passos")
> plt.ylabel("Recompensa mÃ©dia")
> plt.title("Recompensa MÃ©dia ao Longo do Tempo com Otimismo Inicial")
> plt.grid(True)
> plt.show()
> ```

### DistribuiÃ§Ã£o Normal das Recompensas
Em muitos problemas *k*-armed bandit, os valores verdadeiros das aÃ§Ãµes, $q_*(a)$, sÃ£o selecionados a partir de uma distribuiÃ§Ã£o normal (Gaussiana) com mÃ©dia 0 e variÃ¢ncia 1 [^29]. A recompensa real, $R_t$, obtida ao selecionar a aÃ§Ã£o $A_t$ no tempo *t*, Ã© entÃ£o selecionada a partir de uma distribuiÃ§Ã£o normal com mÃ©dia $q_*(A_t)$ e variÃ¢ncia 1 [^29].

Quando as recompensas sÃ£o distribuÃ­das normalmente, um valor inicial otimista pode ser eficaz para incentivar a exploraÃ§Ã£o [^34]. A inicializaÃ§Ã£o $Q_1(a)$ para um valor alto (por exemplo, +5) garante que todas as aÃ§Ãµes serÃ£o exploradas pelo menos uma vez, pois o agente ficarÃ¡ inicialmente "desapontado" com as recompensas obtidas [^34].

**ProposiÃ§Ã£o 1** Em um problema *k*-armed bandit com recompensas normalmente distribuÃ­das, a probabilidade de selecionar uma aÃ§Ã£o sub-Ã³tima diminui com o aumento do nÃºmero de vezes que a aÃ§Ã£o Ã³tima Ã© selecionada.

*Prova.* Com cada seleÃ§Ã£o da aÃ§Ã£o Ã³tima, a estimativa de seu valor se torna mais precisa e, portanto, mais provÃ¡vel de ser selecionada em futuras iteraÃ§Ãµes. As aÃ§Ãµes sub-Ã³timas, por outro lado, terÃ£o estimativas de valor menos precisas e, eventualmente, serÃ£o superadas pela aÃ§Ã£o Ã³tima.

*Prova Detalhada.*

I. Seja $a^*$ a aÃ§Ã£o Ã³tima e $a_i$ uma aÃ§Ã£o sub-Ã³tima, onde $i$ varia de 1 a $k-1$.
II. A estimativa de valor da aÃ§Ã£o Ã³tima Ã© dada por: $Q_t(a^*) = \frac{1}{n_{a^*}} \sum_{j=1}^{n_{a^*}} R_j$, onde $n_{a^*}$ Ã© o nÃºmero de vezes que $a^*$ foi selecionada e $R_j$ sÃ£o as recompensas obtidas ao selecionar $a^*$.
III. Similarmente, a estimativa de valor da aÃ§Ã£o sub-Ã³tima $a_i$ Ã© dada por: $Q_t(a_i) = \frac{1}{n_{a_i}} \sum_{j=1}^{n_{a_i}} R_j$, onde $n_{a_i}$ Ã© o nÃºmero de vezes que $a_i$ foi selecionada.
IV. Pela Lei dos Grandes NÃºmeros, Ã  medida que $n_{a^*}$ aumenta, $Q_t(a^*)$ converge para $q_*(a^*)$. Similarmente, Ã  medida que $n_{a_i}$ aumenta, $Q_t(a_i)$ converge para $q_*(a_i)$.
V. Como $a^*$ Ã© a aÃ§Ã£o Ã³tima, $q_*(a^*) > q_*(a_i)$ para todas as aÃ§Ãµes sub-Ã³timas $a_i$.
VI. A probabilidade de selecionar uma aÃ§Ã£o sub-Ã³tima $a_i$ em vez de $a^*$ diminui Ã  medida que $Q_t(a^*)$ se torna uma estimativa mais precisa de $q_*(a^*)$ e $Q_t(a_i)$ se torna uma estimativa mais precisa de $q_*(a_i)$.
VII. Portanto, a probabilidade de selecionar uma aÃ§Ã£o sub-Ã³tima diminui com o aumento do nÃºmero de vezes que a aÃ§Ã£o Ã³tima Ã© selecionada. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um problema com 3 aÃ§Ãµes, onde $q_*(1) = 0.1$, $q_*(2) = 0.2$, e $q_*(3) = 0.3$. As recompensas sÃ£o amostradas de uma distribuiÃ§Ã£o normal com variÃ¢ncia 1. Inicializamos $Q_1(a) = 5$ para todas as aÃ§Ãµes. ApÃ³s 1000 iteraÃ§Ãµes, a aÃ§Ã£o 3 serÃ¡ selecionada com maior frequÃªncia do que as outras aÃ§Ãµes. Podemos verificar isso empiricamente simulando o problema.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # ParÃ¢metros
> k = 3  # NÃºmero de braÃ§os
> q_true = [0.1, 0.2, 0.3]  # Valores verdadeiros das aÃ§Ãµes
> initial_value = 5  # Valor inicial otimista
> n_steps = 1000  # NÃºmero de passos
>
> # InicializaÃ§Ã£o
> Q = [initial_value] * k  # Estimativas de valor
> N = [0] * k  # Contagem de vezes que cada aÃ§Ã£o foi selecionada
> actions_selected = []
>
> # Loop principal
> for t in range(n_steps):
>     # Selecionar aÃ§Ã£o (epsilon-greedy com epsilon=0.1)
>     if np.random.rand() < 0.1:
>         action = np.random.choice(k)  # ExploraÃ§Ã£o
>     else:
>         action = np.argmax(Q)  # ExplotaÃ§Ã£o
>
>     actions_selected.append(action)
>
>     # Receber recompensa
>     reward = np.random.normal(q_true[action], 1)
>
>     # Atualizar estimativas de valor
>     N[action] += 1
>     Q[action] = Q[action] + (1/N[action]) * (reward - Q[action])
>
> # Contar a frequÃªncia de cada aÃ§Ã£o
> unique_actions, counts = np.unique(actions_selected, return_counts=True)
> action_counts = dict(zip(unique_actions, counts))
>
> print("FrequÃªncia das aÃ§Ãµes selecionadas:", action_counts)
>
> # Plotar a frequÃªncia das aÃ§Ãµes selecionadas
> plt.figure(figsize=(8, 6))
> plt.bar(action_counts.keys(), action_counts.values())
> plt.xlabel("AÃ§Ã£o")
> plt.ylabel("FrequÃªncia")
> plt.title("FrequÃªncia das AÃ§Ãµes Selecionadas apÃ³s 1000 Passos com Otimismo Inicial")
> plt.xticks(unique_actions)
> plt.grid(axis='y')
> plt.show()
> ```
> Este cÃ³digo simula o problema e imprime a frequÃªncia com que cada aÃ§Ã£o Ã© selecionada. Devemos observar que a aÃ§Ã£o 3 (Ã­ndice 2) Ã© selecionada com maior frequÃªncia.

### Impacto da VariÃ¢ncia da Recompensa
A eficÃ¡cia dos valores iniciais otimistas pode ser afetada pela **variÃ¢ncia das recompensas**. Se a variÃ¢ncia for alta, as recompensas observadas podem variar amplamente, tornando mais difÃ­cil para o agente determinar rapidamente o verdadeiro valor de cada aÃ§Ã£o. Nestas situaÃ§Ãµes, a exploraÃ§Ã£o incentivada pelo otimismo inicial pode nÃ£o ser suficiente para superar a variabilidade das recompensas.

Para ilustrar, considere dois cenÃ¡rios:

1.  **Baixa VariÃ¢ncia:** As recompensas estÃ£o distribuÃ­das normalmente em torno de $q_*(a)$ com uma pequena variÃ¢ncia (por exemplo, 1). Nesse caso, o agente pode obter rapidamente uma estimativa precisa de $Q_t(a)$ para cada aÃ§Ã£o.

2.  **Alta VariÃ¢ncia:** As recompensas estÃ£o distribuÃ­das normalmente em torno de $q_*(a)$ com uma grande variÃ¢ncia (por exemplo, 10). Nesse caso, o agente pode demorar muito mais para obter uma estimativa precisa de $Q_t(a)$ para cada aÃ§Ã£o, pois as recompensas observadas sÃ£o mais ruidosas [^30].

No cenÃ¡rio de alta variÃ¢ncia, o otimismo inicial pode ser menos eficaz. O agente ainda serÃ¡ incentivado a explorar inicialmente, mas a alta variabilidade das recompensas pode dificultar a identificaÃ§Ã£o das aÃ§Ãµes verdadeiramente Ã³timas [^30].

**Lema 1** Seja $\sigma^2$ a variÃ¢ncia da recompensa. Quanto maior $\sigma^2$, maior o nÃºmero de amostras necessÃ¡rias para convergir para uma estimativa precisa de $q_*(a)$.

*Prova.* A precisÃ£o da estimativa de $q_*(a)$ Ã© proporcional a $\frac{\sigma}{\sqrt{n}}$, onde $n$ Ã© o nÃºmero de amostras. Para alcanÃ§ar uma precisÃ£o desejada, $n$ deve aumentar com o aumento de $\sigma^2$.

*Prova Detalhada.*

I. Seja $q_*(a)$ o valor verdadeiro da aÃ§Ã£o $a$. Queremos estimar $q_*(a)$ usando a mÃ©dia amostral $Q_t(a) = \frac{1}{n} \sum_{i=1}^{n} R_i$, onde $R_i$ sÃ£o as recompensas observadas ao selecionar a aÃ§Ã£o $a$ e $n$ Ã© o nÃºmero de amostras.

II. O erro da estimativa Ã© dado por $|Q_t(a) - q_*(a)|$. Queremos que esse erro seja menor que um limiar $\epsilon > 0$ com alta probabilidade.

III. Pelo Teorema do Limite Central, a distribuiÃ§Ã£o de $Q_t(a)$ se aproxima de uma distribuiÃ§Ã£o normal com mÃ©dia $q_*(a)$ e variÃ¢ncia $\frac{\sigma^2}{n}$.

IV. Portanto, podemos escrever: $P(|Q_t(a) - q_*(a)| > \epsilon) \approx 2Q(-\frac{\epsilon}{\sigma/\sqrt{n}})$, onde $Q(x)$ Ã© a funÃ§Ã£o Q padrÃ£o normal.

V. Para garantir que $P(|Q_t(a) - q_*(a)| > \epsilon) < \alpha$ para algum nÃ­vel de confianÃ§a $\alpha$, precisamos que $\frac{\epsilon}{\sigma/\sqrt{n}} > Q^{-1}(\alpha/2)$.

VI. Resolvendo para $n$, obtemos $n > (\frac{\sigma}{\epsilon} Q^{-1}(\alpha/2))^2$.

VII. Isso mostra que o nÃºmero de amostras $n$ necessÃ¡rio para alcanÃ§ar uma precisÃ£o $\epsilon$ com um nÃ­vel de confianÃ§a $1-\alpha$ Ã© proporcional a $\sigma^2$. Portanto, quanto maior $\sigma^2$, maior o nÃºmero de amostras necessÃ¡rias para convergir para uma estimativa precisa de $q_*(a)$. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Vamos comparar dois cenÃ¡rios com diferentes variÃ¢ncias. Suponha que $q_*(a) = 1$ e queremos estimar esse valor com um erro $\epsilon = 0.1$ e um nÃ­vel de confianÃ§a de 95% ($\alpha = 0.05$). Usando a fÃ³rmula derivada na prova do Lema 1: $n > (\frac{\sigma}{\epsilon} Q^{-1}(\alpha/2))^2$. Para $\alpha = 0.05$, $Q^{-1}(\alpha/2) \approx 1.96$.
>
> 1.  **Baixa VariÃ¢ncia:** $\sigma^2 = 1$, entÃ£o $\sigma = 1$. $n > (\frac{1}{0.1} \times 1.96)^2 \approx 384.16$. Precisamos de aproximadamente 385 amostras.
> 2.  **Alta VariÃ¢ncia:** $\sigma^2 = 10$, entÃ£o $\sigma = \sqrt{10} \approx 3.16$. $n > (\frac{3.16}{0.1} \times 1.96)^2 \approx 3841.47$. Precisamos de aproximadamente 3842 amostras.
>
> Este exemplo numÃ©rico ilustra que, com alta variÃ¢ncia, precisamos de significativamente mais amostras para obter a mesma precisÃ£o na estimativa do valor da aÃ§Ã£o.

### EstratÃ©gias de MitigaÃ§Ã£o
Para mitigar o impacto da alta variÃ¢ncia, vÃ¡rias estratÃ©gias podem ser empregadas:

*   **ReduÃ§Ã£o do passo de aprendizado:** Usar um passo de aprendizado menor pode tornar as estimativas de valor menos sensÃ­veis a recompensas individuais ruidosas [^31]. No entanto, isso tambÃ©m pode tornar o aprendizado mais lento.
*   **MÃ©todos de mÃ©dia ponderada:** Dar mais peso Ã s recompensas recentes pode ajudar o agente a se adaptar a mudanÃ§as na distribuiÃ§Ã£o de recompensas, embora isso possa nÃ£o ser ideal em ambientes estacionÃ¡rios [^32].
*   **Upper Confidence Bound (UCB):** Este mÃ©todo equilibra a exploraÃ§Ã£o e a explotaÃ§Ã£o, selecionando aÃ§Ãµes com base em uma estimativa superior do seu verdadeiro valor, levando em consideraÃ§Ã£o a incerteza na estimativa [^35].

![Average performance comparison of UCB and Îµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

*   **Bandit de Gradiente:** Esse mÃ©todo aprende uma preferÃªncia numÃ©rica para cada aÃ§Ã£o e favorece as aÃ§Ãµes mais preferidas, tornando a exploraÃ§Ã£o mais direcionada [^37].

![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 braÃ§os.](./../images/image1.png)

**Teorema 1.1** (Adaptado de UCB) A estratÃ©gia UCB garante que, com alta probabilidade, o agente irÃ¡ selecionar a aÃ§Ã£o Ã³tima com uma frequÃªncia que converge para 1 Ã  medida que o tempo tende ao infinito.

*Prova.* A estratÃ©gia UCB mantÃ©m um limite superior de confianÃ§a para o valor de cada aÃ§Ã£o. Esse limite superior diminui Ã  medida que a aÃ§Ã£o Ã© amostrada mais vezes. Eventualmente, o limite superior da aÃ§Ã£o Ã³tima ultrapassarÃ¡ os limites superiores das aÃ§Ãµes sub-Ã³timas, levando Ã  sua seleÃ§Ã£o cada vez mais frequente.

*Prova Detalhada.*

I. A estratÃ©gia UCB seleciona a aÃ§Ã£o $A_t$ no tempo $t$ de acordo com: $A_t = \underset{a}{\mathrm{argmax}} \left[Q_t(a) + c \sqrt{\frac{\ln t}{n_a(t)}}\right]$, onde $Q_t(a)$ Ã© a estimativa do valor da aÃ§Ã£o $a$, $n_a(t)$ Ã© o nÃºmero de vezes que a aÃ§Ã£o $a$ foi selecionada atÃ© o tempo $t$, e $c > 0$ Ã© um parÃ¢metro que controla o nÃ­vel de exploraÃ§Ã£o.

II. O termo $c \sqrt{\frac{\ln t}{n_a(t)}}$ representa o limite superior de confianÃ§a da estimativa de valor da aÃ§Ã£o $a$. Esse termo diminui Ã  medida que $n_a(t)$ aumenta, o que significa que a incerteza sobre o valor da aÃ§Ã£o diminui Ã  medida que ela Ã© amostrada mais vezes.

III. Seja $a^*$ a aÃ§Ã£o Ã³tima e $a$ uma aÃ§Ã£o sub-Ã³tima. Suponha que em algum momento $t$, a aÃ§Ã£o sub-Ã³tima $a$ seja selecionada em vez da aÃ§Ã£o Ã³tima $a^*$. Isso significa que:
    $Q_t(a) + c \sqrt{\frac{\ln t}{n_a(t)}} > Q_t(a^*) + c \sqrt{\frac{\ln t}{n_{a^*}(t)}}$.

IV. Ã€ medida que $t$ aumenta, $Q_t(a^*)$ converge para $q_*(a^*)$ e $Q_t(a)$ converge para $q_*(a)$. Como $a^*$ Ã© a aÃ§Ã£o Ã³tima, $q_*(a^*) > q_*(a)$.

V. Para que a aÃ§Ã£o sub-Ã³tima $a$ continue a ser selecionada em vez de $a^*$, o termo de incerteza $c \sqrt{\frac{\ln t}{n_a(t)}}$ deve ser suficientemente grande para compensar a diferenÃ§a entre $Q_t(a^*)$ e $Q_t(a)$.

VI. No entanto, Ã  medida que $t$ tende ao infinito, o termo $\ln t$ cresce muito mais lentamente do que $n_a(t)$ se a aÃ§Ã£o $a$ for selecionada repetidamente. Isso significa que o termo $c \sqrt{\frac{\ln t}{n_a(t)}}$ eventualmente se tornarÃ¡ muito pequeno para compensar a diferenÃ§a entre $Q_t(a^*)$ e $Q_t(a)$.

VII. Portanto, com alta probabilidade, a estratÃ©gia UCB garante que o agente irÃ¡ selecionar a aÃ§Ã£o Ã³tima $a^*$ com uma frequÃªncia que converge para 1 Ã  medida que o tempo tende ao infinito. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um 2-armed bandit com $q_*(1) = 1$ e $q_*(2) = 2$. Inicializamos $Q_1(1) = 0$ e $Q_1(2) = 0$. Usamos UCB com $c = 2$.
>
> Na primeira iteraÃ§Ã£o ($t=1$), $n_1(1) = 0$ e $n_2(1) = 0$. Usando a fÃ³rmula UCB:
> $A_1 = \underset{a}{\mathrm{argmax}} \left[Q_1(a) + 2 \sqrt{\frac{\ln 1}{n_a(1)}}\right]$. Como $\ln 1 = 0$ e $n_a(1)=0$, temos que definir uma regra para $n_a(t) = 0$. Podemos definir que a primeira vez todas as acoes sÃ£o selecionadas.
> $A_1 = 1$, $A_2 = 2$.
>
> Na segunda iteraÃ§Ã£o ($t=2$):
> Suponha que a recompensa da aÃ§Ã£o 1 seja 1 e da aÃ§Ã£o 2 seja 2.
> $Q_2(1) = 1$, $Q_2(2) = 2$, $n_1(2) = 1$, $n_2(2) = 1$.
> $UCB(1) = 1 + 2 \sqrt{\frac{\ln 2}{1}} \approx 1 + 2 \times 0.83 = 2.66$.
> $UCB(2) = 2 + 2 \sqrt{\frac{\ln 2}{1}} \approx 2 + 2 \times 0.83 = 3.66$.
> A aÃ§Ã£o 2 serÃ¡ selecionada.
>
> Este exemplo demonstra como o UCB incentiva a exploraÃ§Ã£o, mesmo quando uma aÃ§Ã£o parece ser melhor inicialmente.

### ConclusÃ£o
O uso de valores iniciais otimistas Ã© uma tÃ©cnica simples e eficaz para incentivar a exploraÃ§Ã£o em problemas *k*-armed bandit [^34]. No entanto, a eficÃ¡cia desta tÃ©cnica pode depender da distribuiÃ§Ã£o das recompensas, particularmente da variÃ¢ncia [^30]. Em problemas com alta variÃ¢ncia de recompensa, estratÃ©gias adicionais podem ser necessÃ¡rias para garantir uma exploraÃ§Ã£o adequada e um aprendizado eficiente.

### ReferÃªncias
[^25]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^26]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^29]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^30]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^31]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^32]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^34]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^35]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^37]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
<!-- END -->