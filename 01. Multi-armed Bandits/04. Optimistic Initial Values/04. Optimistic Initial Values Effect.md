## Otimismo Inicial e Distribui√ß√£o de Recompensas em Bandit Problems

### Introdu√ß√£o
O uso de **valores iniciais otimistas** √© uma t√©cnica para incentivar a explora√ß√£o em problemas *k*-armed bandit [^34]. A ideia central √© inicializar as estimativas de valor das a√ß√µes, $Q_1(a)$, com um valor significativamente superior ao que se espera receber como recompensa. Este m√©todo explora a **natureza transit√≥ria do otimismo**, incentivando o agente a experimentar diferentes a√ß√µes at√© que suas estimativas de valor convirjam. No entanto, a efic√°cia desta t√©cnica pode depender da distribui√ß√£o subjacente das recompensas [^34].

> üí° **Exemplo Num√©rico:** Considere um problema 3-armed bandit. Inicializamos $Q_1(1) = Q_1(2) = Q_1(3) = 5$. Suponha que os valores reais das a√ß√µes sejam $q_*(1) = 1$, $q_*(2) = 2$, e $q_*(3) = 3$. O otimismo inicial de 5 incentivar√° o agente a experimentar todas as a√ß√µes, mesmo que as primeiras recompensas sejam menores que 5.

### Conceitos Fundamentais
Em problemas *k*-armed bandit, o objetivo √© maximizar a recompensa total esperada ao longo do tempo, repetidamente selecionando a√ß√µes entre *k* op√ß√µes dispon√≠veis [^25]. Cada a√ß√£o *a* tem um valor verdadeiro, $q_*(a)$, que representa a recompensa m√©dia esperada ao selecionar essa a√ß√£o [^25].

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

A estimativa do valor de uma a√ß√£o no tempo *t* √© denotada por $Q_t(a)$ [^26]. O **objetivo do aprendizado** √© fazer com que $Q_t(a)$ se aproxime de $q_*(a)$ o mais r√°pido poss√≠vel. Para alcan√ßar este objetivo, o agente deve equilibrar *explora√ß√£o* (experimentar a√ß√µes diferentes para melhorar as estimativas) e *explota√ß√£o* (selecionar a a√ß√£o com a maior estimativa atual) [^26].

Otimismo inicial √© uma t√©cnica que pode ser usada para incentivar a explora√ß√£o. Ao inicializar $Q_1(a)$ com um valor alto, o agente √© inicialmente "otimista" sobre o potencial de cada a√ß√£o. Essa abordagem induz o agente a explorar as a√ß√µes de forma mais extensa, pois as recompensas reais ser√£o, inicialmente, menores do que a estimativa inicial [^34].

**Teorema 1** Seja $Q_1(a) = V, \forall a$, onde $V$ √© um valor inicial otimista. Se existe uma a√ß√£o $a^*$ tal que $q_*(a^*) > \mathbb{E}[R_t | A_t \neq a^*]$, ent√£o o agente ir√°, em algum momento, explorar $a^*$.

*Prova.* O otimismo inicial garante que todas as a√ß√µes ser√£o tentadas em algum momento. Se a a√ß√£o $a^*$ tem um valor esperado maior que a m√©dia das outras a√ß√µes, eventualmente sua estimativa de valor $Q_t(a^*)$ ir√° superar as demais, levando o agente a explor√°-la mais frequentemente.

*Prova Detalhada.*

I. Inicialmente, $Q_1(a) = V$ para todas as a√ß√µes $a$. Isso significa que todas as a√ß√µes s√£o consideradas igualmente "boas" no in√≠cio.

II. Como o agente explora, ele observa as recompensas $R_t$ para cada a√ß√£o $a$ selecionada $A_t$. A estimativa de valor $Q_t(a)$ √© atualizada com base nessas recompensas.

III. Seja $a^*$ a a√ß√£o √≥tima, tal que $q_*(a^*) > \mathbb{E}[R_t | A_t \neq a^*]$. Isso significa que o valor esperado da a√ß√£o $a^*$ √© maior do que o valor esperado m√©dio de todas as outras a√ß√µes.

IV. Como todas as a√ß√µes s√£o exploradas inicialmente (devido ao otimismo inicial), $a^*$ ser√° selecionada pelo menos uma vez.

V. Ap√≥s a sele√ß√£o de $a^*$, a estimativa $Q_t(a^*)$ ser√° atualizada com base na recompensa observada. Como $q_*(a^*) > \mathbb{E}[R_t | A_t \neq a^*]$, a estimativa $Q_t(a^*)$ tender√° a aumentar em rela√ß√£o √†s estimativas das outras a√ß√µes.

VI. Com o tempo, $Q_t(a^*)$ se aproximar√° de $q_*(a^*)$, enquanto as estimativas das a√ß√µes sub-√≥timas se aproximar√£o de seus valores esperados, que s√£o menores que $q_*(a^*)$.

VII. Portanto, eventualmente, $Q_t(a^*) > Q_t(a)$ para todas as outras a√ß√µes $a \neq a^*$, e o agente explorar√° $a^*$ mais frequentemente. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um 2-armed bandit com $q_*(1) = 1$ e $q_*(2) = 2$. Inicializamos $Q_1(1) = Q_1(2) = 5$. Na primeira itera√ß√£o, o agente escolhe a a√ß√£o 1 e recebe uma recompensa de 0. A nova estimativa para a a√ß√£o 1 √© $Q_2(1) = 0$. Na segunda itera√ß√£o, o agente escolhe a a√ß√£o 2 e recebe uma recompensa de 2. A nova estimativa para a a√ß√£o 2 √© $Q_2(2) = 2$. O agente continuar√° explorando at√© que $Q_t(2)$ seja significativamente maior que $Q_t(1)$.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> k = 2  # N√∫mero de bra√ßos
> q_true = [1, 2]  # Valores verdadeiros das a√ß√µes
> initial_value = 5  # Valor inicial otimista
> n_steps = 100  # N√∫mero de passos
>
> # Inicializa√ß√£o
> Q = [initial_value, initial_value]  # Estimativas de valor
> N = [0, 0]  # Contagem de vezes que cada a√ß√£o foi selecionada
> rewards = []
>
> # Loop principal
> for t in range(n_steps):
>     # Selecionar a√ß√£o (epsilon-greedy com epsilon=0.1)
>     if np.random.rand() < 0.1:
>         action = np.random.choice(k)  # Explora√ß√£o
>     else:
>         action = np.argmax(Q)  # Explota√ß√£o
>
>     # Receber recompensa
>     reward = np.random.normal(q_true[action], 1)
>     rewards.append(reward)
>
>     # Atualizar estimativas de valor
>     N[action] += 1
>     Q[action] = Q[action] + (1/N[action]) * (reward - Q[action])
>
> # Plotar as estimativas de valor ao longo do tempo
> plt.figure(figsize=(10, 6))
> plt.plot(np.cumsum(np.array(rewards)) / (np.arange(n_steps) + 1))
> plt.xlabel("Passos")
> plt.ylabel("Recompensa m√©dia")
> plt.title("Recompensa M√©dia ao Longo do Tempo com Otimismo Inicial")
> plt.grid(True)
> plt.show()
> ```

### Distribui√ß√£o Normal das Recompensas
Em muitos problemas *k*-armed bandit, os valores verdadeiros das a√ß√µes, $q_*(a)$, s√£o selecionados a partir de uma distribui√ß√£o normal (Gaussiana) com m√©dia 0 e vari√¢ncia 1 [^29]. A recompensa real, $R_t$, obtida ao selecionar a a√ß√£o $A_t$ no tempo *t*, √© ent√£o selecionada a partir de uma distribui√ß√£o normal com m√©dia $q_*(A_t)$ e vari√¢ncia 1 [^29].

Quando as recompensas s√£o distribu√≠das normalmente, um valor inicial otimista pode ser eficaz para incentivar a explora√ß√£o [^34]. A inicializa√ß√£o $Q_1(a)$ para um valor alto (por exemplo, +5) garante que todas as a√ß√µes ser√£o exploradas pelo menos uma vez, pois o agente ficar√° inicialmente "desapontado" com as recompensas obtidas [^34].

**Proposi√ß√£o 1** Em um problema *k*-armed bandit com recompensas normalmente distribu√≠das, a probabilidade de selecionar uma a√ß√£o sub-√≥tima diminui com o aumento do n√∫mero de vezes que a a√ß√£o √≥tima √© selecionada.

*Prova.* Com cada sele√ß√£o da a√ß√£o √≥tima, a estimativa de seu valor se torna mais precisa e, portanto, mais prov√°vel de ser selecionada em futuras itera√ß√µes. As a√ß√µes sub-√≥timas, por outro lado, ter√£o estimativas de valor menos precisas e, eventualmente, ser√£o superadas pela a√ß√£o √≥tima.

*Prova Detalhada.*

I. Seja $a^*$ a a√ß√£o √≥tima e $a_i$ uma a√ß√£o sub-√≥tima, onde $i$ varia de 1 a $k-1$.
II. A estimativa de valor da a√ß√£o √≥tima √© dada por: $Q_t(a^*) = \frac{1}{n_{a^*}} \sum_{j=1}^{n_{a^*}} R_j$, onde $n_{a^*}$ √© o n√∫mero de vezes que $a^*$ foi selecionada e $R_j$ s√£o as recompensas obtidas ao selecionar $a^*$.
III. Similarmente, a estimativa de valor da a√ß√£o sub-√≥tima $a_i$ √© dada por: $Q_t(a_i) = \frac{1}{n_{a_i}} \sum_{j=1}^{n_{a_i}} R_j$, onde $n_{a_i}$ √© o n√∫mero de vezes que $a_i$ foi selecionada.
IV. Pela Lei dos Grandes N√∫meros, √† medida que $n_{a^*}$ aumenta, $Q_t(a^*)$ converge para $q_*(a^*)$. Similarmente, √† medida que $n_{a_i}$ aumenta, $Q_t(a_i)$ converge para $q_*(a_i)$.
V. Como $a^*$ √© a a√ß√£o √≥tima, $q_*(a^*) > q_*(a_i)$ para todas as a√ß√µes sub-√≥timas $a_i$.
VI. A probabilidade de selecionar uma a√ß√£o sub-√≥tima $a_i$ em vez de $a^*$ diminui √† medida que $Q_t(a^*)$ se torna uma estimativa mais precisa de $q_*(a^*)$ e $Q_t(a_i)$ se torna uma estimativa mais precisa de $q_*(a_i)$.
VII. Portanto, a probabilidade de selecionar uma a√ß√£o sub-√≥tima diminui com o aumento do n√∫mero de vezes que a a√ß√£o √≥tima √© selecionada. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um problema com 3 a√ß√µes, onde $q_*(1) = 0.1$, $q_*(2) = 0.2$, e $q_*(3) = 0.3$. As recompensas s√£o amostradas de uma distribui√ß√£o normal com vari√¢ncia 1. Inicializamos $Q_1(a) = 5$ para todas as a√ß√µes. Ap√≥s 1000 itera√ß√µes, a a√ß√£o 3 ser√° selecionada com maior frequ√™ncia do que as outras a√ß√µes. Podemos verificar isso empiricamente simulando o problema.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros
> k = 3  # N√∫mero de bra√ßos
> q_true = [0.1, 0.2, 0.3]  # Valores verdadeiros das a√ß√µes
> initial_value = 5  # Valor inicial otimista
> n_steps = 1000  # N√∫mero de passos
>
> # Inicializa√ß√£o
> Q = [initial_value] * k  # Estimativas de valor
> N = [0] * k  # Contagem de vezes que cada a√ß√£o foi selecionada
> actions_selected = []
>
> # Loop principal
> for t in range(n_steps):
>     # Selecionar a√ß√£o (epsilon-greedy com epsilon=0.1)
>     if np.random.rand() < 0.1:
>         action = np.random.choice(k)  # Explora√ß√£o
>     else:
>         action = np.argmax(Q)  # Explota√ß√£o
>
>     actions_selected.append(action)
>
>     # Receber recompensa
>     reward = np.random.normal(q_true[action], 1)
>
>     # Atualizar estimativas de valor
>     N[action] += 1
>     Q[action] = Q[action] + (1/N[action]) * (reward - Q[action])
>
> # Contar a frequ√™ncia de cada a√ß√£o
> unique_actions, counts = np.unique(actions_selected, return_counts=True)
> action_counts = dict(zip(unique_actions, counts))
>
> print("Frequ√™ncia das a√ß√µes selecionadas:", action_counts)
>
> # Plotar a frequ√™ncia das a√ß√µes selecionadas
> plt.figure(figsize=(8, 6))
> plt.bar(action_counts.keys(), action_counts.values())
> plt.xlabel("A√ß√£o")
> plt.ylabel("Frequ√™ncia")
> plt.title("Frequ√™ncia das A√ß√µes Selecionadas ap√≥s 1000 Passos com Otimismo Inicial")
> plt.xticks(unique_actions)
> plt.grid(axis='y')
> plt.show()
> ```
> Este c√≥digo simula o problema e imprime a frequ√™ncia com que cada a√ß√£o √© selecionada. Devemos observar que a a√ß√£o 3 (√≠ndice 2) √© selecionada com maior frequ√™ncia.

### Impacto da Vari√¢ncia da Recompensa
A efic√°cia dos valores iniciais otimistas pode ser afetada pela **vari√¢ncia das recompensas**. Se a vari√¢ncia for alta, as recompensas observadas podem variar amplamente, tornando mais dif√≠cil para o agente determinar rapidamente o verdadeiro valor de cada a√ß√£o. Nestas situa√ß√µes, a explora√ß√£o incentivada pelo otimismo inicial pode n√£o ser suficiente para superar a variabilidade das recompensas.

Para ilustrar, considere dois cen√°rios:

1.  **Baixa Vari√¢ncia:** As recompensas est√£o distribu√≠das normalmente em torno de $q_*(a)$ com uma pequena vari√¢ncia (por exemplo, 1). Nesse caso, o agente pode obter rapidamente uma estimativa precisa de $Q_t(a)$ para cada a√ß√£o.

2.  **Alta Vari√¢ncia:** As recompensas est√£o distribu√≠das normalmente em torno de $q_*(a)$ com uma grande vari√¢ncia (por exemplo, 10). Nesse caso, o agente pode demorar muito mais para obter uma estimativa precisa de $Q_t(a)$ para cada a√ß√£o, pois as recompensas observadas s√£o mais ruidosas [^30].

No cen√°rio de alta vari√¢ncia, o otimismo inicial pode ser menos eficaz. O agente ainda ser√° incentivado a explorar inicialmente, mas a alta variabilidade das recompensas pode dificultar a identifica√ß√£o das a√ß√µes verdadeiramente √≥timas [^30].

**Lema 1** Seja $\sigma^2$ a vari√¢ncia da recompensa. Quanto maior $\sigma^2$, maior o n√∫mero de amostras necess√°rias para convergir para uma estimativa precisa de $q_*(a)$.

*Prova.* A precis√£o da estimativa de $q_*(a)$ √© proporcional a $\frac{\sigma}{\sqrt{n}}$, onde $n$ √© o n√∫mero de amostras. Para alcan√ßar uma precis√£o desejada, $n$ deve aumentar com o aumento de $\sigma^2$.

*Prova Detalhada.*

I. Seja $q_*(a)$ o valor verdadeiro da a√ß√£o $a$. Queremos estimar $q_*(a)$ usando a m√©dia amostral $Q_t(a) = \frac{1}{n} \sum_{i=1}^{n} R_i$, onde $R_i$ s√£o as recompensas observadas ao selecionar a a√ß√£o $a$ e $n$ √© o n√∫mero de amostras.

II. O erro da estimativa √© dado por $|Q_t(a) - q_*(a)|$. Queremos que esse erro seja menor que um limiar $\epsilon > 0$ com alta probabilidade.

III. Pelo Teorema do Limite Central, a distribui√ß√£o de $Q_t(a)$ se aproxima de uma distribui√ß√£o normal com m√©dia $q_*(a)$ e vari√¢ncia $\frac{\sigma^2}{n}$.

IV. Portanto, podemos escrever: $P(|Q_t(a) - q_*(a)| > \epsilon) \approx 2Q(-\frac{\epsilon}{\sigma/\sqrt{n}})$, onde $Q(x)$ √© a fun√ß√£o Q padr√£o normal.

V. Para garantir que $P(|Q_t(a) - q_*(a)| > \epsilon) < \alpha$ para algum n√≠vel de confian√ßa $\alpha$, precisamos que $\frac{\epsilon}{\sigma/\sqrt{n}} > Q^{-1}(\alpha/2)$.

VI. Resolvendo para $n$, obtemos $n > (\frac{\sigma}{\epsilon} Q^{-1}(\alpha/2))^2$.

VII. Isso mostra que o n√∫mero de amostras $n$ necess√°rio para alcan√ßar uma precis√£o $\epsilon$ com um n√≠vel de confian√ßa $1-\alpha$ √© proporcional a $\sigma^2$. Portanto, quanto maior $\sigma^2$, maior o n√∫mero de amostras necess√°rias para convergir para uma estimativa precisa de $q_*(a)$. ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos comparar dois cen√°rios com diferentes vari√¢ncias. Suponha que $q_*(a) = 1$ e queremos estimar esse valor com um erro $\epsilon = 0.1$ e um n√≠vel de confian√ßa de 95% ($\alpha = 0.05$). Usando a f√≥rmula derivada na prova do Lema 1: $n > (\frac{\sigma}{\epsilon} Q^{-1}(\alpha/2))^2$. Para $\alpha = 0.05$, $Q^{-1}(\alpha/2) \approx 1.96$.
>
> 1.  **Baixa Vari√¢ncia:** $\sigma^2 = 1$, ent√£o $\sigma = 1$. $n > (\frac{1}{0.1} \times 1.96)^2 \approx 384.16$. Precisamos de aproximadamente 385 amostras.
> 2.  **Alta Vari√¢ncia:** $\sigma^2 = 10$, ent√£o $\sigma = \sqrt{10} \approx 3.16$. $n > (\frac{3.16}{0.1} \times 1.96)^2 \approx 3841.47$. Precisamos de aproximadamente 3842 amostras.
>
> Este exemplo num√©rico ilustra que, com alta vari√¢ncia, precisamos de significativamente mais amostras para obter a mesma precis√£o na estimativa do valor da a√ß√£o.

### Estrat√©gias de Mitiga√ß√£o
Para mitigar o impacto da alta vari√¢ncia, v√°rias estrat√©gias podem ser empregadas:

*   **Redu√ß√£o do passo de aprendizado:** Usar um passo de aprendizado menor pode tornar as estimativas de valor menos sens√≠veis a recompensas individuais ruidosas [^31]. No entanto, isso tamb√©m pode tornar o aprendizado mais lento.
*   **M√©todos de m√©dia ponderada:** Dar mais peso √†s recompensas recentes pode ajudar o agente a se adaptar a mudan√ßas na distribui√ß√£o de recompensas, embora isso possa n√£o ser ideal em ambientes estacion√°rios [^32].
*   **Upper Confidence Bound (UCB):** Este m√©todo equilibra a explora√ß√£o e a explota√ß√£o, selecionando a√ß√µes com base em uma estimativa superior do seu verdadeiro valor, levando em considera√ß√£o a incerteza na estimativa [^35].

![Average performance comparison of UCB and Œµ-greedy action selection on a 10-armed testbed.](./../images/image7.png)

*   **Bandit de Gradiente:** Esse m√©todo aprende uma prefer√™ncia num√©rica para cada a√ß√£o e favorece as a√ß√µes mais preferidas, tornando a explora√ß√£o mais direcionada [^37].

![Comparativo do desempenho do algoritmo gradient bandit com e sem baseline de recompensa no teste de 10 bra√ßos.](./../images/image1.png)

**Teorema 1.1** (Adaptado de UCB) A estrat√©gia UCB garante que, com alta probabilidade, o agente ir√° selecionar a a√ß√£o √≥tima com uma frequ√™ncia que converge para 1 √† medida que o tempo tende ao infinito.

*Prova.* A estrat√©gia UCB mant√©m um limite superior de confian√ßa para o valor de cada a√ß√£o. Esse limite superior diminui √† medida que a a√ß√£o √© amostrada mais vezes. Eventualmente, o limite superior da a√ß√£o √≥tima ultrapassar√° os limites superiores das a√ß√µes sub-√≥timas, levando √† sua sele√ß√£o cada vez mais frequente.

*Prova Detalhada.*

I. A estrat√©gia UCB seleciona a a√ß√£o $A_t$ no tempo $t$ de acordo com: $A_t = \underset{a}{\mathrm{argmax}} \left[Q_t(a) + c \sqrt{\frac{\ln t}{n_a(t)}}\right]$, onde $Q_t(a)$ √© a estimativa do valor da a√ß√£o $a$, $n_a(t)$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© o tempo $t$, e $c > 0$ √© um par√¢metro que controla o n√≠vel de explora√ß√£o.

II. O termo $c \sqrt{\frac{\ln t}{n_a(t)}}$ representa o limite superior de confian√ßa da estimativa de valor da a√ß√£o $a$. Esse termo diminui √† medida que $n_a(t)$ aumenta, o que significa que a incerteza sobre o valor da a√ß√£o diminui √† medida que ela √© amostrada mais vezes.

III. Seja $a^*$ a a√ß√£o √≥tima e $a$ uma a√ß√£o sub-√≥tima. Suponha que em algum momento $t$, a a√ß√£o sub-√≥tima $a$ seja selecionada em vez da a√ß√£o √≥tima $a^*$. Isso significa que:
    $Q_t(a) + c \sqrt{\frac{\ln t}{n_a(t)}} > Q_t(a^*) + c \sqrt{\frac{\ln t}{n_{a^*}(t)}}$.

IV. √Ä medida que $t$ aumenta, $Q_t(a^*)$ converge para $q_*(a^*)$ e $Q_t(a)$ converge para $q_*(a)$. Como $a^*$ √© a a√ß√£o √≥tima, $q_*(a^*) > q_*(a)$.

V. Para que a a√ß√£o sub-√≥tima $a$ continue a ser selecionada em vez de $a^*$, o termo de incerteza $c \sqrt{\frac{\ln t}{n_a(t)}}$ deve ser suficientemente grande para compensar a diferen√ßa entre $Q_t(a^*)$ e $Q_t(a)$.

VI. No entanto, √† medida que $t$ tende ao infinito, o termo $\ln t$ cresce muito mais lentamente do que $n_a(t)$ se a a√ß√£o $a$ for selecionada repetidamente. Isso significa que o termo $c \sqrt{\frac{\ln t}{n_a(t)}}$ eventualmente se tornar√° muito pequeno para compensar a diferen√ßa entre $Q_t(a^*)$ e $Q_t(a)$.

VII. Portanto, com alta probabilidade, a estrat√©gia UCB garante que o agente ir√° selecionar a a√ß√£o √≥tima $a^*$ com uma frequ√™ncia que converge para 1 √† medida que o tempo tende ao infinito. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um 2-armed bandit com $q_*(1) = 1$ e $q_*(2) = 2$. Inicializamos $Q_1(1) = 0$ e $Q_1(2) = 0$. Usamos UCB com $c = 2$.
>
> Na primeira itera√ß√£o ($t=1$), $n_1(1) = 0$ e $n_2(1) = 0$. Usando a f√≥rmula UCB:
> $A_1 = \underset{a}{\mathrm{argmax}} \left[Q_1(a) + 2 \sqrt{\frac{\ln 1}{n_a(1)}}\right]$. Como $\ln 1 = 0$ e $n_a(1)=0$, temos que definir uma regra para $n_a(t) = 0$. Podemos definir que a primeira vez todas as acoes s√£o selecionadas.
> $A_1 = 1$, $A_2 = 2$.
>
> Na segunda itera√ß√£o ($t=2$):
> Suponha que a recompensa da a√ß√£o 1 seja 1 e da a√ß√£o 2 seja 2.
> $Q_2(1) = 1$, $Q_2(2) = 2$, $n_1(2) = 1$, $n_2(2) = 1$.
> $UCB(1) = 1 + 2 \sqrt{\frac{\ln 2}{1}} \approx 1 + 2 \times 0.83 = 2.66$.
> $UCB(2) = 2 + 2 \sqrt{\frac{\ln 2}{1}} \approx 2 + 2 \times 0.83 = 3.66$.
> A a√ß√£o 2 ser√° selecionada.
>
> Este exemplo demonstra como o UCB incentiva a explora√ß√£o, mesmo quando uma a√ß√£o parece ser melhor inicialmente.

### Conclus√£o
O uso de valores iniciais otimistas √© uma t√©cnica simples e eficaz para incentivar a explora√ß√£o em problemas *k*-armed bandit [^34]. No entanto, a efic√°cia desta t√©cnica pode depender da distribui√ß√£o das recompensas, particularmente da vari√¢ncia [^30]. Em problemas com alta vari√¢ncia de recompensa, estrat√©gias adicionais podem ser necess√°rias para garantir uma explora√ß√£o adequada e um aprendizado eficiente.

### Refer√™ncias
[^25]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^26]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^29]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^30]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^31]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^32]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^34]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^35]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^37]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
<!-- END -->