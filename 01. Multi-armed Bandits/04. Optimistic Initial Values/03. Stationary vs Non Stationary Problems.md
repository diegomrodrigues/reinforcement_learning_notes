## Otimismo Inicial em Detalhe: Estacionariedade e Limita√ß√µes Temporais

### Introdu√ß√£o
Como vimos anteriormente, os m√©todos de **valor de a√ß√£o** dependem das estimativas iniciais $Q_1(a)$ [^34]. A escolha dessas estimativas influencia o comportamento inicial do agente, introduzindo um vi√©s que pode ser tanto ben√©fico quanto prejudicial. Exploramos agora em detalhes como a t√©cnica de **valores iniciais otimistas** interage com ambientes estacion√°rios e n√£o estacion√°rios, destacando suas limita√ß√µes inerentes em cen√°rios din√¢micos.

### Conceitos Fundamentais
A t√©cnica de **valores iniciais otimistas** √© uma estrat√©gia simples para incentivar a explora√ß√£o. Em vez de inicializar os valores de a√ß√£o com zero, como feito no *10-armed testbed* [^34], atribu√≠mos um valor alto e irrealisticamente otimista (por exemplo, +5) a todas as a√ß√µes. Dado que as recompensas $q_*(a)$ s√£o normalmente selecionadas de uma distribui√ß√£o normal com m√©dia 0 e vari√¢ncia 1 [^29], essa inicializa√ß√£o induz o agente a explorar.

> üí° **Exemplo Num√©rico:** Imagine um problema de 3 bra√ßos ($k=3$) com valores verdadeiros $q_*(1) = 1$, $q_*(2) = 0.5$ e $q_*(3) = 0$. Inicializamos as estimativas de valor com $Q_1(1) = Q_1(2) = Q_1(3) = 5$. Na primeira itera√ß√£o, o agente escolher√° uma a√ß√£o aleatoriamente (digamos, a a√ß√£o 1). Se a recompensa obtida for, por exemplo, $R_1 = 0.8$ (retirada de uma distribui√ß√£o normal em torno de $q_*(1) = 1$), a estimativa de valor para a a√ß√£o 1 ser√° atualizada usando a m√©dia amostral. Se usarmos a m√©dia simples ($N_t(a)$), teremos $Q_2(1) = 0.8$. A diferen√ßa entre a estimativa inicial e a recompensa real incentiva o agente a explorar as outras a√ß√µes.

A intui√ß√£o por tr√°s dessa abordagem √© que, ao selecionar inicialmente uma a√ß√£o, a recompensa obtida ser√° *inferior* √† estimativa inicial. Essa "decep√ß√£o" motiva o agente a mudar para outras a√ß√µes, na esperan√ßa de encontrar uma recompensa maior. O resultado √© que todas as a√ß√µes s√£o tentadas v√°rias vezes antes que as estimativas de valor converjam [^34].

![Comparative performance of optimistic vs. realistic action-value initialization in a 10-armed testbed, illustrating the impact on optimal action selection over time.](./../images/image3.png)

**Vantagens em Problemas Estacion√°rios:**
Em problemas **estacion√°rios**, onde as distribui√ß√µes de recompensa das a√ß√µes permanecem constantes ao longo do tempo, os valores iniciais otimistas podem ser eficazes. A explora√ß√£o inicial garante que o agente obtenha uma amostra razo√°vel de cada a√ß√£o, permitindo que ele identifique a a√ß√£o √≥tima [^34].

> üí° **Exemplo Num√©rico:** Considere um problema estacion√°rio de 2 bra√ßos com $q_*(1) = 0.7$ e $q_*(2) = 0.3$. Inicializamos com $Q_1(1) = Q_1(2) = 5$. Ap√≥s algumas itera√ß√µes, as estimativas de valor convergem para perto dos valores verdadeiros, por exemplo, $Q_t(1) \approx 0.7$ e $Q_t(2) \approx 0.3$. O agente ent√£o explora menos e explora mais frequentemente o bra√ßo 1, alcan√ßando uma recompensa m√©dia maior ao longo do tempo.

**Desvantagens em Problemas N√£o Estacion√°rios:**
A t√©cnica de valores iniciais otimistas sofre de limita√ß√µes significativas em problemas **n√£o estacion√°rios**, onde as distribui√ß√µes de recompensa das a√ß√µes mudam ao longo do tempo. A raz√£o fundamental para essa defici√™ncia √© que *a motiva√ß√£o para explorar √© inerentemente tempor√°ria* [^35]. Uma vez que as estimativas de valor convergem, o incentivo √† explora√ß√£o desaparece, mesmo que o ambiente mude.

> üí° **Exemplo Num√©rico:** Imagine um problema n√£o estacion√°rio onde $q_*(1)$ muda de 0.7 para 0.1 no tempo $t=100$, enquanto $q_*(2)$ permanece constante em 0.3. Se o agente j√° tiver convergido para $Q_t(1) \approx 0.7$ antes de $t=100$, ele continuar√° explorando o bra√ßo 1, mesmo que este se torne pior que o bra√ßo 2. A explora√ß√£o inicial n√£o garante adapta√ß√£o cont√≠nua √†s mudan√ßas.

> A cr√≠tica se aplica tamb√©m aos m√©todos de m√©dia amostral, que tratam o in√≠cio do tempo como um evento especial, calculando a m√©dia de todas as recompensas subsequentes com pesos iguais [^35].

Al√©m disso, [^35] menciona que "Qualquer m√©todo que foque nas condi√ß√µes iniciais de uma maneira especial dificilmente ajudar√° no caso geral n√£o estacion√°rio. O come√ßo do tempo ocorre apenas uma vez e, portanto, n√£o devemos focar muito nele".

Em problemas n√£o estacion√°rios, a necessidade de explora√ß√£o √© recorrente. O agente deve continuamente monitorar o ambiente em busca de mudan√ßas e ajustar sua pol√≠tica de acordo. Os valores iniciais otimistas n√£o fornecem um mecanismo para essa explora√ß√£o cont√≠nua. Eles incentivam a explora√ß√£o *apenas* no in√≠cio do aprendizado, falhando em responder a mudan√ßas subsequentes no ambiente [^35].

Para formalizar essa intui√ß√£o, podemos considerar um cen√°rio simplificado.

**Lema 1:** *Converg√™ncia em Ambientes Estacion√°rios.* Em um ambiente estacion√°rio, com probabilidade 1, as estimativas de valor $Q_t(a)$ convergem para os valores verdadeiros $q_*(a)$ para toda a√ß√£o $a$ se cada a√ß√£o for selecionada infinitas vezes.

*Prova (Esbo√ßo):* Sob estacionariedade, a m√©dia amostral converge para o valor verdadeiro, e a atualiza√ß√£o iterativa de $Q_t(a)$ √© uma forma de m√©dia amostral. Se cada a√ß√£o for selecionada infinitas vezes, o n√∫mero de amostras tende ao infinito, e portanto, a estimativa converge para o valor verdadeiro.

Vamos agora fornecer uma prova mais detalhada deste lema.

**Prova do Lema 1:**

I. Seja $Q_t(a)$ a estimativa do valor da a√ß√£o $a$ no tempo $t$, e $q_*(a)$ o valor verdadeiro da a√ß√£o $a$. Definimos o erro como $e_t(a) = Q_t(a) - q_*(a)$.

II. Assumimos um ambiente estacion√°rio, o que significa que $q_*(a)$ √© constante ao longo do tempo. Usamos a regra de atualiza√ß√£o de m√©dia amostral:
   $$Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)} [R_t - Q_t(a)]$$
   onde $N_t(a)$ √© o n√∫mero de vezes que a a√ß√£o $a$ foi selecionada at√© o tempo $t$, e $R_t$ √© a recompensa obtida no tempo $t$ ao selecionar a a√ß√£o $a$.

III. Reescrevendo a regra de atualiza√ß√£o em termos do erro:
    $$e_{t+1}(a) = Q_{t+1}(a) - q_*(a) = Q_t(a) + \frac{1}{N_t(a)}[R_t - Q_t(a)] - q_*(a)$$
    $$e_{t+1}(a) = e_t(a) + \frac{1}{N_t(a)}[R_t - Q_t(a)]$$
    $$e_{t+1}(a) = e_t(a) + \frac{1}{N_t(a)}[R_t - q_*(a) - e_t(a)]$$

IV. Tomando o valor esperado condicional a $Q_t(a)$, e sabendo que $E[R_t | a] = q_*(a)$ sob estacionariedade:
    $$E[e_{t+1}(a) | Q_t(a)] = e_t(a) + \frac{1}{N_t(a)}[E[R_t | a] - q_*(a) - e_t(a)] = e_t(a) - \frac{e_t(a)}{N_t(a)} = e_t(a)\left(1 - \frac{1}{N_t(a)}\right)$$

V. Se a a√ß√£o $a$ √© selecionada infinitas vezes, ent√£o $N_t(a) \rightarrow \infty$ quando $t \rightarrow \infty$.  Assim, o termo $\left(1 - \frac{1}{N_t(a)}\right)$ se aproxima de 1 pela esquerda, mas a cada itera√ß√£o o erro √© multiplicado por um fator menor que 1, o que leva o erro a convergir para 0. Formalmente:
$$\lim_{t \to \infty} E[e_{t+1}(a) | Q_t(a)] = \lim_{t \to \infty} e_t(a)\left(1 - \frac{1}{N_t(a)}\right) = 0$$

VI. Portanto, $Q_t(a)$ converge para $q_*(a)$ com probabilidade 1 quando $t \to \infty$, sob a condi√ß√£o de que a a√ß√£o $a$ seja selecionada infinitas vezes. ‚ñ†

No entanto, a velocidade dessa converg√™ncia depende crucialmente da taxa de aprendizado e da vari√¢ncia das recompensas. Adicionalmente, o otimismo inicial acelera a converg√™ncia inicial, mas n√£o garante otimalidade a longo prazo. Para ilustrar o efeito do otimismo inicial na taxa de converg√™ncia, considere o seguinte:

**Teorema 1:** *Impacto do Otimismo Inicial na Converg√™ncia.* Seja $\Delta Q_0 = \max_a |Q_1(a) - q_*(a)|$ o desvio inicial m√°ximo das estimativas de valor em rela√ß√£o aos valores verdadeiros. Sob condi√ß√µes estacion√°rias e usando uma taxa de aprendizado $\alpha$ constante, o n√∫mero de passos $T$ necess√°rios para que todas as estimativas $Q_t(a)$ estejam a uma dist√¢ncia $\epsilon$ de seus valores verdadeiros $q_*(a)$ √© $O(\frac{\Delta Q_0}{\alpha \epsilon})$.

*Prova (Esbo√ßo):* A atualiza√ß√£o de valor √© dada por $Q_{t+1}(a) = Q_t(a) + \alpha(R_t - Q_t(a))$. O erro em cada passo diminui proporcionalmente a $\alpha$. Portanto, o n√∫mero de passos para reduzir o erro inicial $\Delta Q_0$ para um n√≠vel $\epsilon$ √© inversamente proporcional a $\alpha$ e proporcional a $\Delta Q_0$ e $\frac{1}{\epsilon}$.

Vamos agora fornecer uma prova mais detalhada deste teorema.

**Prova do Teorema 1:**
I. Definimos o erro no tempo $t$ para a a√ß√£o $a$ como $e_t(a) = Q_t(a) - q_*(a)$. Queremos encontrar o n√∫mero de passos $T$ tal que $|e_T(a)| \leq \epsilon$ para todas as a√ß√µes $a$.

II. A regra de atualiza√ß√£o com taxa de aprendizado constante $\alpha$ √©:
$$Q_{t+1}(a) = Q_t(a) + \alpha(R_t - Q_t(a))$$

III. Reescrevendo em termos do erro:
$$e_{t+1}(a) = Q_{t+1}(a) - q_*(a) = Q_t(a) + \alpha(R_t - Q_t(a)) - q_*(a)$$
$$e_{t+1}(a) = e_t(a) + \alpha(R_t - q_*(a) - Q_t(a) + q_*(a) - q_*(a))$$
$$e_{t+1}(a) = e_t(a) + \alpha(R_t - q_*(a) - e_t(a))$$

IV. Tomando o valor absoluto:
$$|e_{t+1}(a)| = |e_t(a) + \alpha(R_t - q_*(a) - e_t(a))|$$

V. Queremos analisar a converg√™ncia no pior caso, que ocorre quando $R_t - q_*(a)$ tem o maior valor absoluto poss√≠vel na dire√ß√£o oposta a $e_t(a)$.  No entanto, para simplificar e obter uma ordem de magnitude, vamos considerar o caso m√©dio, onde $E[R_t] = q_*(a)$. Neste caso,
$$E[e_{t+1}(a)] = E[e_t(a) + \alpha(R_t - q_*(a) - e_t(a))] = e_t(a) - \alpha e_t(a) = (1 - \alpha)e_t(a)$$
Ent√£o, $|e_{t+1}(a)| = (1 - \alpha)|e_t(a)|$.

VI. Ap√≥s $T$ passos, temos:
$$|e_T(a)| = (1 - \alpha)^T |e_0(a)|$$
onde $|e_0(a)| = |Q_1(a) - q_*(a)|$ √© o erro inicial. Queremos que $|e_T(a)| \leq \epsilon$, ent√£o:
$$(1 - \alpha)^T |e_0(a)| \leq \epsilon$$

VII. Tomando o logaritmo natural de ambos os lados:
$$T \ln(1 - \alpha) + \ln(|e_0(a)|) \leq \ln(\epsilon)$$
$$T \ln(1 - \alpha) \leq \ln(\epsilon) - \ln(|e_0(a)|) = \ln\left(\frac{\epsilon}{|e_0(a)|}\right)$$

VIII. Como $\ln(1 - \alpha) \approx -\alpha$ para $\alpha$ pequeno, temos:
$$-T\alpha \leq \ln\left(\frac{\epsilon}{|e_0(a)|}\right)$$
$$T \geq -\frac{1}{\alpha} \ln\left(\frac{\epsilon}{|e_0(a)|}\right) = \frac{1}{\alpha} \ln\left(\frac{|e_0(a)|}{\epsilon}\right)$$

IX. Seja $\Delta Q_0 = \max_a |Q_1(a) - q_*(a)| = \max_a |e_0(a)|$.  Ent√£o, o maior erro inicial poss√≠vel √© $\Delta Q_0$.  Para garantir que todas as estimativas estejam a uma dist√¢ncia $\epsilon$ de seus valores verdadeiros, precisamos:
$$T \geq \frac{1}{\alpha} \ln\left(\frac{\Delta Q_0}{\epsilon}\right)$$

X. Portanto, $T$ √© da ordem de $O\left(\frac{\Delta Q_0}{\alpha \epsilon}\right)$, pois $\ln(x)$ cresce mais lentamente que $x$ para $x > 1$. ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que $\Delta Q_0 = 5$ (o valor inicial otimista menos o valor m√≠nimo poss√≠vel de $q_*(a)$), $\alpha = 0.1$ e $\epsilon = 0.1$. Ent√£o, $T \geq \frac{1}{0.1} \ln(\frac{5}{0.1}) = 10 \ln(50) \approx 39.12$. Isso sugere que levar√° aproximadamente 39 passos para as estimativas de valor convergirem para dentro de $\epsilon = 0.1$ dos valores verdadeiros, assumindo as condi√ß√µes estacion√°rias e a taxa de aprendizado dada.

Esses resultados refor√ßam que, enquanto o otimismo inicial pode acelerar a explora√ß√£o e a converg√™ncia inicial em ambientes estacion√°rios, ele n√£o altera as propriedades assint√≥ticas do algoritmo. Al√©m disso, ele n√£o resolve o problema fundamental de adapta√ß√£o em ambientes n√£o estacion√°rios.

### Conclus√£o
A t√©cnica de valores iniciais otimistas √© uma ferramenta √∫til para incentivar a explora√ß√£o em problemas de *k-armed bandit estacion√°rios* [^28]. No entanto, sua efic√°cia √© limitada em ambientes *n√£o estacion√°rios* devido √† sua natureza *tempor√°ria* [^35]. Para enfrentar os desafios dos problemas *n√£o estacion√°rios*, s√£o necess√°rias estrat√©gias de explora√ß√£o mais sofisticadas que permitam ao agente *monitorar* e *adaptar-se* continuamente √†s mudan√ßas ambientais. Como veremos nas pr√≥ximas se√ß√µes, existem outras abordagens, como UCB (Upper Confidence Bound) [^35] e m√©todos gradientes [^37], que oferecem um melhor desempenho em ambientes din√¢micos.

### Refer√™ncias
[^28]: Se√ß√£o 2.3, "The 10-armed Testbed"

![Distribui√ß√µes de recompensa para um problema de bandit de 10 bra√ßos.](./../images/image5.png)

[^29]: Se√ß√£o 2.3, "The 10-armed Testbed"
[^34]: Se√ß√£o 2.6, "Optimistic Initial Values"
[^35]: Se√ß√£o 2.7, "Upper-Confidence-Bound Action Selection"
[^37]: Se√ß√£o 2.8, "Gradient Bandit Algorithms"
<!-- END -->