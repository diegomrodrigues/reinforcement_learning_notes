## Asynchronous Dynamic Programming: Convergence and Truncated Iteration

### Introdu√ß√£o
O conceito de **Dynamic Programming (DP)**, conforme explorado anteriormente, fornece uma base s√≥lida para o entendimento de algoritmos que buscam pol√≠ticas √≥timas em ambientes modelados como Processos de Decis√£o de Markov (MDPs) [^1]. No entanto, a natureza *s√≠ncrona* dos algoritmos de DP tradicionais, que requerem varreduras completas do espa√ßo de estados, pode se tornar computacionalmente proibitiva para problemas de grande escala. Expandindo o conceito apresentado, esta se√ß√£o se aprofunda em **Asynchronous Dynamic Programming (ADP)**, focando especificamente na condi√ß√£o de converg√™ncia garantida e na possibilidade de intermixar atualiza√ß√µes de policy evaluation e value iteration [^85].

### Converg√™ncia em ADP
Um dos principais atrativos dos algoritmos ADP √© sua flexibilidade em rela√ß√£o √† ordem das atualiza√ß√µes de estado. Diferentemente dos m√©todos s√≠ncronos, que exigem varreduras sistem√°ticas do espa√ßo de estados, os algoritmos ADP atualizam os valores dos estados em qualquer ordem, utilizando os valores dispon√≠veis de outros estados [^85]. Essa abordagem introduz a quest√£o crucial da *converg√™ncia*.

O texto afirma que a converg√™ncia assint√≥tica para a fun√ß√£o de valor √≥tima ($v_*$) √© garantida, desde que todos os estados sejam visitados e atualizados *infinitamente* [^85]. Formalmente, se denotarmos $s_k$ como o estado atualizado no passo *k*, e se $0 \leq \gamma < 1$ (onde $\gamma$ √© o fator de desconto), a converg√™ncia para $v_*$ √© assegurada se todos os estados ocorrerem na sequ√™ncia $\{s_k\}$ um n√∫mero infinito de vezes [^85]. Matematicamente, podemos expressar a atualiza√ß√£o ass√≠ncrona de value iteration da seguinte forma:
$$
V_{k+1}(s_k) = \max_{a \in A(s_k)} \mathbb{E}[R_{t+1} + \gamma V_k(S_{t+1}) | S_t = s_k, A_t = a]
$$
Essa equa√ß√£o representa a atualiza√ß√£o do valor do estado $s_k$ no instante $k+1$, com base na recompensa esperada e no valor descontado do pr√≥ximo estado, maximizado sobre todas as a√ß√µes poss√≠veis em $s_k$. A condi√ß√£o de que todos os estados devem ser atualizados infinitamente garante que a informa√ß√£o sobre as recompensas e as transi√ß√µes se propague por todo o espa√ßo de estados, levando √† converg√™ncia para a pol√≠tica √≥tima.

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP simples com 3 estados ($S = \{s_1, s_2, s_3\}$) e duas a√ß√µes em cada estado ($A = \{a_1, a_2\}$). Suponha que o fator de desconto $\gamma = 0.9$.  Inicializamos os valores dos estados como $V_0(s_1) = 0$, $V_0(s_2) = 0$, $V_0(s_3) = 0$.  As recompensas e probabilidades de transi√ß√£o s√£o definidas para ilustrar o conceito.
>
> *   **Estado $s_1$:**
>     *   A√ß√£o $a_1$: Recompensa esperada $R = 1$, pr√≥ximo estado $s_2$ com probabilidade 1.
>     *   A√ß√£o $a_2$: Recompensa esperada $R = 0$, pr√≥ximo estado $s_3$ com probabilidade 1.
> *   **Estado $s_2$:**
>     *   A√ß√£o $a_1$: Recompensa esperada $R = 2$, pr√≥ximo estado $s_1$ com probabilidade 1.
>     *   A√ß√£o $a_2$: Recompensa esperada $R = 0$, pr√≥ximo estado $s_3$ com probabilidade 1.
> *   **Estado $s_3$:**
>     *   A√ß√£o $a_1$: Recompensa esperada $R = 3$, pr√≥ximo estado $s_1$ com probabilidade 1.
>     *   A√ß√£o $a_2$: Recompensa esperada $R = 1$, pr√≥ximo estado $s_2$ com probabilidade 1.
>
> Agora, vamos realizar algumas atualiza√ß√µes ass√≠ncronas de Value Iteration:
>
> **Itera√ß√£o 1:** Atualizamos o estado $s_1$. Suponha que escolhemos a a√ß√£o $a_1$.
> $$V_1(s_1) = \max(1 + 0.9 * V_0(s_2), 0 + 0.9 * V_0(s_3)) = \max(1 + 0.9 * 0, 0 + 0.9 * 0) = 1$$
>  $V_1(s_2) = 0$, $V_1(s_3) = 0$
>
> **Itera√ß√£o 2:** Atualizamos o estado $s_2$. Suponha que escolhemos a a√ß√£o $a_1$.
> $$V_2(s_2) = \max(2 + 0.9 * V_1(s_1), 0 + 0.9 * V_1(s_3)) = \max(2 + 0.9 * 1, 0 + 0.9 * 0) = 2.9$$
>  $V_2(s_1) = 1$, $V_2(s_3) = 0$
>
> **Itera√ß√£o 3:** Atualizamos o estado $s_3$. Suponha que escolhemos a a√ß√£o $a_1$.
> $$V_3(s_3) = \max(3 + 0.9 * V_2(s_1), 1 + 0.9 * V_2(s_2)) = \max(3 + 0.9 * 1, 1 + 0.9 * 2.9) = \max(3.9, 3.61) = 3.9$$
> $V_3(s_1) = 1$, $V_3(s_2) = 2.9$
>
> **Itera√ß√£o 4:** Atualizamos o estado $s_1$ novamente. Suponha que escolhemos a a√ß√£o $a_1$ novamente.
> $$V_4(s_1) = \max(1 + 0.9 * V_3(s_2), 0 + 0.9 * V_3(s_3)) = \max(1 + 0.9 * 2.9, 0 + 0.9 * 3.9) = \max(3.61, 3.51) = 3.61$$
> $V_4(s_2) = 2.9$, $V_4(s_3) = 3.9$
>
> Observe que os valores dos estados est√£o mudando a cada itera√ß√£o. A converg√™ncia para os valores √≥timos $V_*(s_1)$, $V_*(s_2)$, e $V_*(s_3)$ √© garantida se continuarmos atualizando todos os estados infinitamente. A escolha da a√ß√£o em cada estado afetar√° a velocidade de converg√™ncia.

**Lemma 1:** (Converg√™ncia Ass√≠ncrona com Fator de Desconto)
Seja um MDP com fator de desconto $\gamma \in [0, 1)$. Se um algoritmo ADP garante que cada estado $s \in S$ √© atualizado um n√∫mero infinito de vezes, ent√£o a sequ√™ncia de fun√ß√µes de valor $V_k(s)$ converge assintoticamente para $V_*(s)$ para todo $s \in S$.

**Prova:** (Sketch)
A prova se baseia na propriedade de contra√ß√£o do operador de Bellman e na garantia de que a informa√ß√£o relevante se propaga atrav√©s do espa√ßo de estados devido √†s infinitas atualiza√ß√µes de cada estado. Uma prova formal requereria argumentos de converg√™ncia para processos de contra√ß√£o ass√≠ncronos, que est√£o al√©m do escopo deste cap√≠tulo. $\blacksquare$

Para dar uma ideia mais clara, podemos apresentar a prova da converg√™ncia ass√≠ncrona de forma mais detalhada, embora ainda de forma resumida. A prova completa √© tecnicamente complexa, mas os principais passos podem ser esbo√ßados da seguinte forma:

**Prova (mais detalhada):**
A prova √© baseada na propriedade de contra√ß√£o do operador de Bellman.

I. **Operador de Bellman:** O operador de Bellman para value iteration √© definido como:
   $$(TV)(s) = \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma V(S_{t+1}) | S_t = s, A_t = a]$$

II. **Propriedade de Contra√ß√£o:** O operador de Bellman √© uma contra√ß√£o em rela√ß√£o √† norma do supremo (m√°ximo), ou seja:
   $$||TV - TV'|| \leq \gamma ||V - V'||$$
   onde $||V|| = \max_{s \in S} |V(s)|$. Isso significa que cada aplica√ß√£o do operador de Bellman aproxima as fun√ß√µes de valor mais perto umas das outras.

III. **Atualiza√ß√£o Ass√≠ncrona:** Na atualiza√ß√£o ass√≠ncrona, escolhemos um estado $s_k$ no passo *k* e atualizamos seu valor:
    $$V_{k+1}(s) = \begin{cases} (TV_k)(s) & \text{se } s = s_k \\ V_k(s) & \text{se } s \neq s_k \end{cases}$$

IV. **Erro M√°ximo:** Seja $\epsilon_k = ||V_k - V_*||$ o erro m√°ximo no passo *k*. Queremos mostrar que $\epsilon_k$ converge para 0.

V. **Redu√ß√£o do Erro:** Devido √† propriedade de contra√ß√£o do operador de Bellman, quando atualizamos o estado $s_k$, o erro nesse estado √© reduzido por um fator de $\gamma$:
   $$|V_{k+1}(s_k) - V_*(s_k)| = |(TV_k)(s_k) - (TV_*)(s_k)| \leq \gamma ||V_k - V_*|| = \gamma \epsilon_k$$

VI. **Atualiza√ß√µes Infinitas:** Como cada estado √© atualizado infinitas vezes, o erro em cada estado √© reduzido repetidamente por um fator de $\gamma$.

VII. **Converg√™ncia:** Para qualquer estado $s$, ap√≥s um n√∫mero suficientemente grande de atualiza√ß√µes, o valor $V_k(s)$ estar√° arbitrariamente pr√≥ximo de $V_*(s)$. Isso ocorre porque o erro √© reduzido exponencialmente a cada atualiza√ß√£o.  Formalmente, para qualquer $\epsilon > 0$, existe um $K$ tal que para todo $k > K$, $|V_k(s) - V_*(s)| < \epsilon$.

VIII. **Conclus√£o:** Portanto, a sequ√™ncia de fun√ß√µes de valor $V_k(s)$ converge assintoticamente para $V_*(s)$ para todo $s \in S$. ‚ñ†

**Corol√°rio 1:** Em um MDP epis√≥dico com termina√ß√£o garantida, onde $\gamma < 1$ ou a termina√ß√£o eventual √© garantida a partir de todos os estados sob qualquer pol√≠tica, a converg√™ncia ass√≠ncrona para a fun√ß√£o de valor √≥tima √© garantida se cada estado for visitado e atualizado um n√∫mero infinito de vezes.

Al√©m da converg√™ncia da fun√ß√£o de valor, √© relevante considerar a converg√™ncia da pol√≠tica.

**Teorema 1:** (Converg√™ncia da Pol√≠tica em ADP)
Sob as mesmas condi√ß√µes do Lema 1, se a cada passo *k*, uma pol√≠tica $\pi_k$ √© obtida greedy em rela√ß√£o a $V_k$, e todos os pares estado-a√ß√£o s√£o visitados e atualizados infinitas vezes, ent√£o a sequ√™ncia de pol√≠ticas $\{\pi_k\}$ converge para a pol√≠tica √≥tima $\pi_*$.

**Prova:** (Sketch)
A prova se baseia no fato de que, como $V_k$ converge para $V_*$, tomar a√ß√µes greedy com rela√ß√£o a $V_k$ leva a pol√≠ticas que se aproximam cada vez mais da pol√≠tica √≥tima. A condi√ß√£o de visita√ß√£o infinita garante que todas as a√ß√µes relevantes sejam consideradas ao longo do tempo. $\blacksquare$

Para melhor entendimento, podemos expandir o "sketch" da prova deste teorema:

**Prova (mais detalhada):**
A prova se baseia na converg√™ncia da fun√ß√£o de valor para a fun√ß√£o de valor √≥tima e na explora√ß√£o infinita de todos os pares estado-a√ß√£o.

I. **Converg√™ncia de $V_k$ para $V_*$:** Pelo Lema 1, sabemos que $V_k(s)$ converge para $V_*(s)$ para todo $s \in S$ quando cada estado √© atualizado infinitas vezes.

II. **Pol√≠tica Greedy:** A pol√≠tica $\pi_k$ √© obtida greedy em rela√ß√£o a $V_k$, o que significa que para cada estado $s$:
   $$\pi_k(s) = \arg\max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma V_k(S_{t+1}) | S_t = s, A_t = a]$$

III. **Converg√™ncia da A√ß√£o Greedy:** √Ä medida que $V_k$ se aproxima de $V_*$, a a√ß√£o greedy com rela√ß√£o a $V_k$ se aproxima da a√ß√£o greedy com rela√ß√£o a $V_*$. Formalmente, para qualquer estado $s$, a a√ß√£o selecionada por $\pi_k(s)$ se aproxima da a√ß√£o √≥tima $\pi_*(s)$.

IV. **Visita√ß√£o Infinita de Pares Estado-A√ß√£o:** A condi√ß√£o de que todos os pares estado-a√ß√£o s√£o visitados e atualizados infinitas vezes garante que todas as a√ß√µes poss√≠veis em cada estado sejam eventualmente consideradas na sele√ß√£o da pol√≠tica. Isso √© crucial porque, mesmo que $V_k$ esteja pr√≥ximo de $V_*$, a pol√≠tica pode n√£o convergir se algumas a√ß√µes n√£o forem exploradas o suficiente.

V. **Melhora da Pol√≠tica:** Como $\pi_k$ √© greedy em rela√ß√£o a $V_k$, e $V_k$ est√° se aproximando de $V_*$, a pol√≠tica $\pi_k$ melhora a cada itera√ß√£o ou permanece a mesma.  Se $\pi_k = \pi_*$ para algum *k*, ent√£o a pol√≠tica permanece √≥tima.

VI. **Converg√™ncia para a Pol√≠tica √ìtima:** Dado que a pol√≠tica melhora a cada itera√ß√£o e que todos os pares estado-a√ß√£o s√£o visitados infinitas vezes, a sequ√™ncia de pol√≠ticas $\{\pi_k\}$ converge para a pol√≠tica √≥tima $\pi_*$.  Isso significa que, para um n√∫mero suficientemente grande de itera√ß√µes *k*, $\pi_k(s) = \pi_*(s)$ para todo $s \in S$.

VII. **Conclus√£o:** Portanto, a sequ√™ncia de pol√≠ticas $\{\pi_k\}$ converge para a pol√≠tica √≥tima $\pi_*$. ‚ñ†

### Intermixando Policy Evaluation e Value Iteration
Al√©m da flexibilidade na ordem das atualiza√ß√µes, os algoritmos ADP permitem a combina√ß√£o de policy evaluation e value iteration [^85]. Essa intermixa√ß√£o leva a uma forma de *asynchronous truncated policy iteration*. Em vez de realizar uma policy evaluation completa at√© a converg√™ncia antes de melhorar a pol√≠tica, podemos alternar entre alguns passos de policy evaluation e um passo de policy improvement, tudo de forma ass√≠ncrona.

Essa abordagem oferece vantagens significativas em termos de efici√™ncia computacional. Ao truncar a policy evaluation, evitamos gastar recursos computacionais em estados que podem n√£o ser relevantes para a pol√≠tica √≥tima atual. Al√©m disso, a atualiza√ß√£o ass√≠ncrona permite que a informa√ß√£o se propague mais rapidamente pelo espa√ßo de estados, acelerando a converg√™ncia.

Considerando a policy evaluation iterativa vista em [^74]
$$
v_{k+1}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] = \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s,a)[r + \gamma v_k(s')]
$$

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

Podemos combinar essa atualiza√ß√£o com a value iteration:
$$
V_{k+1}(s) =
\begin{cases}
    \mathbb{E}_{\pi}[R_{t+1} + \gamma V_k(S_{t+1}) | S_t = s] & \text{se } s \in S_{\text{eval}} \\
    \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma V_k(S_{t+1}) | S_t = s, A_t = a] & \text{se } s \in S_{\text{improv}}
\end{cases}
$$

Onde $S_{\text{eval}}$ e $S_{\text{improv}}$ s√£o subconjuntos de estados selecionados para policy evaluation e policy improvement, respectivamente. A escolha desses subconjuntos e a frequ√™ncia com que as atualiza√ß√µes s√£o aplicadas podem ser ajustadas para otimizar o desempenho do algoritmo [^85].

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP com dois estados ($S = \{s_1, s_2\}$) e duas a√ß√µes ($A = \{a_1, a_2\}$). Seja $\gamma = 0.9$. Suponha que a pol√≠tica atual $\pi$ √©: $\pi(a_1|s_1) = 0.7$ e $\pi(a_2|s_1) = 0.3$; $\pi(a_1|s_2) = 0.2$ e $\pi(a_2|s_2) = 0.8$.
>
> **Definindo as recompensas e as transi√ß√µes:**
> *   **Estado $s_1$:**
>     *   A√ß√£o $a_1$: $R = 1$, vai para $s_2$ com probabilidade 0.8, permanece em $s_1$ com probabilidade 0.2.
>     *   A√ß√£o $a_2$: $R = 0$, vai para $s_2$ com probabilidade 0.6, permanece em $s_1$ com probabilidade 0.4.
> *   **Estado $s_2$:**
>     *   A√ß√£o $a_1$: $R = 2$, vai para $s_1$ com probabilidade 0.9, permanece em $s_2$ com probabilidade 0.1.
>     *   A√ß√£o $a_2$: $R = 1$, vai para $s_1$ com probabilidade 0.5, permanece em $s_2$ com probabilidade 0.5.
>
> **Itera√ß√£o 1:** Inicializamos $V_0(s_1) = 0$ e $V_0(s_2) = 0$.
>
> **Atualiza√ß√£o de Policy Evaluation para $s_1$:**
>
> Primeiro, calculamos o valor esperado para cada a√ß√£o no estado $s_1$:
>
> $$q(s_1, a_1) = \mathbb{E}[R + \gamma V_0(S') | s_1, a_1] = 1 + 0.9(0.8*V_0(s_2) + 0.2*V_0(s_1)) = 1 + 0.9(0) = 1$$
> $$q(s_1, a_2) = \mathbb{E}[R + \gamma V_0(S') | s_1, a_2] = 0 + 0.9(0.6*V_0(s_2) + 0.4*V_0(s_1)) = 0 + 0.9(0) = 0$$
>
> Agora, atualizamos $V_1(s_1)$ usando a pol√≠tica $\pi$:
>
> $$V_1(s_1) = \pi(a_1|s_1) * q(s_1, a_1) + \pi(a_2|s_1) * q(s_1, a_2) = 0.7 * 1 + 0.3 * 0 = 0.7$$
>
> **Atualiza√ß√£o de Value Iteration para $s_2$:**
>
> Calculamos o valor m√°ximo esperado para cada a√ß√£o no estado $s_2$:
>
> $$q(s_2, a_1) = \mathbb{E}[R + \gamma V_0(S') | s_2, a_1] = 2 + 0.9(0.9*V_0(s_1) + 0.1*V_0(s_2)) = 2 + 0.9(0) = 2$$
> $$q(s_2, a_2) = \mathbb{E}[R + \gamma V_0(S') | s_2, a_2] = 1 + 0.9(0.5*V_0(s_1) + 0.5*V_0(s_2)) = 1 + 0.9(0) = 1$$
>
> Atualizamos $V_1(s_2)$ usando a a√ß√£o que maximiza o valor esperado:
>
> $$V_1(s_2) = \max(q(s_2, a_1), q(s_2, a_2)) = \max(2, 1) = 2$$
>
> Portanto, ap√≥s uma itera√ß√£o, temos $V_1(s_1) = 0.7$ e $V_1(s_2) = 2$.
>
> Ao alternar entre policy evaluation e value iteration de forma ass√≠ncrona, podemos ajustar dinamicamente os valores dos estados e, eventualmente, convergir para a pol√≠tica √≥tima, explorando e melhorando a pol√≠tica de forma cont√≠nua. Se na pr√≥xima itera√ß√£o aplicarmos a pol√≠tica greedy derivada de $V_1$ para o estado $s_1$ e continuarmos a policy evaluation para $s_2$, teremos uma combina√ß√£o ass√≠ncrona dos dois m√©todos.



![Pseudocode for Value Iteration algorithm, a method for estimating the optimal policy in an MDP.](./../images/image4.png)

Para garantir a converg√™ncia dessa abordagem h√≠brida, precisamos garantir que tanto os estados de avalia√ß√£o quanto os de melhoria sejam visitados infinitas vezes.

**Lema 2:** (Converg√™ncia da Intermixa√ß√£o Ass√≠ncrona)
Se em um algoritmo ADP que intermixa policy evaluation e value iteration, cada estado $s \in S$ √© atualizado infinitas vezes, e se os conjuntos $S_{\text{eval}}$ e $S_{\text{improv}}$ s√£o tais que cada estado pertence a pelo menos um deles infinitas vezes, ent√£o a sequ√™ncia de fun√ß√µes de valor $V_k(s)$ converge assintoticamente para $V_*(s)$ para todo $s \in S$.

**Prova:** (Sketch)
A prova combina os argumentos de converg√™ncia da policy evaluation e value iteration ass√≠ncronas. Como cada estado √© visitado infinitas vezes sob ambas as opera√ß√µes, a informa√ß√£o se propaga adequadamente pelo espa√ßo de estados, garantindo a converg√™ncia. $\blacksquare$

Podemos expandir um pouco o sketch dessa prova para melhorar o entendimento:

**Prova (mais detalhada):**
A prova combina os princ√≠pios de converg√™ncia da policy evaluation e value iteration ass√≠ncronas, garantindo que a intermixa√ß√£o n√£o prejudique a converg√™ncia geral.

I. **Atualiza√ß√µes Infinitas de Cada Estado:** A condi√ß√£o principal √© que cada estado $s \in S$ seja atualizado infinitas vezes. Isso significa que, ao longo do tempo, cada estado recebe atualiza√ß√µes tanto da policy evaluation quanto da value iteration.

II. **Conjuntos $S_{\text{eval}}$ e $S_{\text{improv}}$:** Os conjuntos $S_{\text{eval}}$ e $S_{\text{improv}}$ s√£o definidos de forma que cada estado perten√ßa a pelo menos um deles infinitas vezes. Isso garante que cada estado seja sujeito a ambos os processos de avalia√ß√£o e melhoria ao longo do tempo.

III. **Converg√™ncia da Policy Evaluation:** Quando um estado $s$ √© atualizado usando policy evaluation, o valor $V_k(s)$ se aproxima do valor verdadeiro sob a pol√≠tica atual, ou seja, $V^{\pi_k}(s)$. Isso ocorre porque a policy evaluation itera repetidamente sobre os estados, refinando a estimativa do valor da pol√≠tica.

IV. **Converg√™ncia da Value Iteration:** Quando um estado $s$ √© atualizado usando value iteration, o valor $V_k(s)$ se aproxima do valor √≥timo, ou seja, $V_*(s)$. Isso ocorre porque a value iteration considera todas as a√ß√µes poss√≠veis e seleciona a melhor a√ß√£o para cada estado, convergindo para a pol√≠tica √≥tima.

V. **Intermixa√ß√£o:** A intermixa√ß√£o dos dois processos garante que a fun√ß√£o de valor seja continuamente refinada. A policy evaluation ajuda a estabilizar a fun√ß√£o de valor, enquanto a value iteration empurra a fun√ß√£o de valor em dire√ß√£o √† otimalidade.

VI. **Propaga√ß√£o da Informa√ß√£o:** Como cada estado √© visitado infinitas vezes sob ambas as opera√ß√µes, a informa√ß√£o sobre recompensas e transi√ß√µes se propaga adequadamente pelo espa√ßo de estados. Isso √© crucial para garantir que a fun√ß√£o de valor convirja para a fun√ß√£o de valor √≥tima.

VII. **Conclus√£o:** Portanto, a sequ√™ncia de fun√ß√µes de valor $V_k(s)$ converge assintoticamente para $V_*(s)$ para todo $s \in S$ quando a intermixa√ß√£o √© feita de forma que cada estado seja visitado infinitas vezes sob ambas as opera√ß√µes. ‚ñ†

### Conclus√£o
Os algoritmos Asynchronous Dynamic Programming representam uma evolu√ß√£o significativa em rela√ß√£o aos m√©todos de DP tradicionais, oferecendo maior flexibilidade e efici√™ncia computacional [^85]. A garantia de converg√™ncia, desde que todos os estados sejam atualizados infinitamente, combinada com a possibilidade de intermixar atualiza√ß√µes de policy evaluation e value iteration, abre novas possibilidades para a aplica√ß√£o de DP em problemas de reinforcement learning de grande escala. A escolha dos estados a serem atualizados e a frequ√™ncia dessas atualiza√ß√µes √© crucial para o desempenho do algoritmo, e estrat√©gias para otimizar essas escolhas ser√£o discutidas em cap√≠tulos subsequentes.

### Refer√™ncias
[^1]: Chapter 4: Dynamic Programming
[^74]: Cap√≠tulo 4, Se√ß√£o 4.1: Policy Evaluation (Prediction)
[^85]: Cap√≠tulo 4, Se√ß√£o 4.5: Asynchronous Dynamic Programming
<!-- END -->