## Asynchronous Dynamic Programming: Flexible State Updates

### Introdu√ß√£o
Como introduzido no Cap√≠tulo 4, **Dynamic Programming (DP)**, os m√©todos cl√°ssicos de DP envolvem opera√ß√µes sobre o conjunto completo de estados do MDP, requerendo *sweeps* sistem√°ticos [^1]. Em cen√°rios com espa√ßos de estados vastos, como o jogo de *backgammon* que possui mais de $10^{20}$ estados, mesmo um √∫nico *sweep* pode se tornar invi√°vel computacionalmente [^1]. Para mitigar essa limita√ß√£o, a t√©cnica de **Asynchronous Dynamic Programming (Asynchronous DP)** surge como uma alternativa flex√≠vel e eficiente.

### Conceitos Fundamentais

**Asynchronous DP algorithms** s√£o algoritmos iterativos *in-place* que n√£o seguem *sweeps* sistem√°ticos do conjunto de estados [^1]. Em vez disso, esses algoritmos atualizam os valores dos estados em qualquer ordem, utilizando os valores dispon√≠veis de outros estados no momento da atualiza√ß√£o [^1]. Essa abordagem oferece flexibilidade significativa na sele√ß√£o dos estados a serem atualizados e evita a necessidade de percorrer todo o espa√ßo de estados em cada itera√ß√£o.

> üí° **Exemplo Num√©rico:** Considere um MDP simples com 3 estados: $S = \{s_1, s_2, s_3\}$. Em um *sweep* tradicional de Value Iteration, atualizar√≠amos os valores dos estados na ordem $s_1$, $s_2$, $s_3$, e ent√£o repetir√≠amos. Em Asynchronous DP, poder√≠amos, por exemplo, atualizar na ordem $s_2$, $s_1$, $s_2$, $s_3$, $s_1$, $s_3$, $s_2$, \ldots, ou mesmo focar em atualizar repetidamente apenas o estado $s_2$ por v√°rias itera√ß√µes antes de considerar os outros. Essa flexibilidade pode ser √∫til se, por algum motivo, $s_2$ for considerado mais cr√≠tico para o aprendizado.

√â crucial ressaltar que, para garantir a converg√™ncia correta, um algoritmo ass√≠ncrono deve continuar a atualizar os valores de todos os estados, sem ignorar nenhum estado ao longo da computa√ß√£o [^1]. Essa condi√ß√£o assegura que o algoritmo explore suficientemente o espa√ßo de estados e converja para a solu√ß√£o √≥tima.

Um exemplo de Asynchronous DP √© uma vers√£o de **value iteration** que atualiza o valor de apenas um estado, $s_k$, em cada passo, $k$, usando a atualiza√ß√£o de *value iteration* [^1]:

$$
v_{k+1}(s) = \max_{a} \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a]
$$
ou, equivalentemente,
$$
v_{k+1}(s) = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')]
$$

> üí° **Exemplo Num√©rico:** Suponha que tenhamos um estado $s$ e duas a√ß√µes poss√≠veis, $a_1$ e $a_2$. Ap√≥s aplicar a a√ß√£o $a_1$, o agente transita para o estado $s'$ com probabilidade $p(s'|s, a_1) = 0.7$ e recebe uma recompensa $r_1 = 1$, ou permanece em $s$ com probabilidade $0.3$ e recebe $r_2 = 0$. Ap√≥s aplicar $a_2$, o agente sempre transita para o estado $s'$ e recebe uma recompensa $r_3 = 0.5$. Assumindo $\gamma = 0.9$ e $v_k(s') = 10$, podemos calcular:
>
> $\text{Para } a_1: \sum_{s', r} p(s', r | s, a_1) [r + \gamma v_k(s')] = 0.7 * (1 + 0.9 * 10) + 0.3 * (0 + 0.9 * v_k(s))$
>
> $\text{Para } a_2: \sum_{s', r} p(s', r | s, a_2) [r + \gamma v_k(s')] = 1 * (0.5 + 0.9 * 10) = 9.5$
>
> Se $v_k(s) = 5$, ent√£o
>
> $\text{Para } a_1: 0.7 * (1 + 0.9 * 10) + 0.3 * (0 + 0.9 * 5) = 7.7 + 1.35 = 9.05$
>
> Portanto, $v_{k+1}(s) = \max(9.05, 9.5) = 9.5$. O valor do estado $s$ √© atualizado com base na melhor a√ß√£o e no valor estimado do pr√≥ximo estado.

Se $0 \leq \gamma < 1$, a converg√™ncia assint√≥tica para $v_*$ √© garantida, desde que todos os estados ocorram na sequ√™ncia {$s_k$} um n√∫mero infinito de vezes [^1]. Essa sequ√™ncia pode at√© mesmo ser aleat√≥ria.

**Teorema 1:** (Converg√™ncia Ass√≠ncrona da Value Iteration) Seja $V$ o espa√ßo de fun√ß√µes valor limitadas. Sob a condi√ß√£o de que $0 \leq \gamma < 1$ e que cada estado $s \in \mathcal{S}$ seja visitado um n√∫mero infinito de vezes na sequ√™ncia de atualiza√ß√µes, a Asynchronous Value Iteration converge para a fun√ß√£o valor √≥tima $v_*$.

*Proof Sketch:* A prova segue da aplica√ß√£o do Teorema da Contra√ß√£o de Banach ao operador de Bellman. A condi√ß√£o de que cada estado seja visitado infinitamente garante que o operador de Bellman seja aplicado a todos os estados com o tempo, levando √† converg√™ncia para o √∫nico ponto fixo, que √© a fun√ß√£o valor √≥tima.

Uma prova mais detalhada pode ser fornecida como segue:

Prova:
I. Definimos o operador de Bellman $T$ como:
   $$(Tv)(s) = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')]$$

II.  Mostraremos que $T$ √© uma contra√ß√£o sob a norma do supremo, ou seja, $||T v - T v'||_{\infty} \leq \gamma ||v - v'||_{\infty}$ para quaisquer duas fun√ß√µes valor $v$ e $v'$.

III. Considere quaisquer duas fun√ß√µes valor $v$ e $v'$. Ent√£o:
    $$|(Tv)(s) - (Tv')(s)| = |\max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] - \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v'(s')]|$$

IV. Usando a desigualdade do tri√¢ngulo e o fato de que a diferen√ßa entre os m√°ximos √© menor ou igual ao m√°ximo das diferen√ßas:
    $$|(Tv)(s) - (Tv')(s)| \leq \max_{a} |\sum_{s', r} p(s', r | s, a) \gamma [v(s') - v'(s')]|$$

V. Como $\sum_{s', r} p(s', r | s, a) = 1$:
   $$|(Tv)(s) - (Tv')(s)| \leq \max_{a} \gamma \sum_{s'} p(s' | s, a) |v(s') - v'(s')|$$

VI.  Sabemos que $|v(s') - v'(s')| \leq ||v - v'||_{\infty}$, ent√£o:
    $$|(Tv)(s) - (Tv')(s)| \leq \gamma ||v - v'||_{\infty} \max_{a} \sum_{s'} p(s' | s, a) = \gamma ||v - v'||_{\infty}$$

VII. Portanto, $||Tv - Tv'||_{\infty} \leq \gamma ||v ||v - v'||_{\infty}$. Pelo Teorema da Contra√ß√£o de Banach, $T$ tem um √∫nico ponto fixo $v_*$, e a itera√ß√£o de valor converge para $v_*$ independentemente do ponto de partida $v_0$.

VIII. A condi√ß√£o de que cada estado seja visitado infinitamente garante que, em cada passo da Asynchronous Value Iteration, o operador de Bellman esteja sendo aplicado a cada estado. Isso assegura que o processo iterativo ir√° convergir para o ponto fixo $v_*$, que √© a fun√ß√£o valor √≥tima. ‚ñ†

Al√©m disso, √© poss√≠vel combinar atualiza√ß√µes de **policy evaluation** e **value iteration** para produzir uma forma de **asynchronous truncated policy iteration** [^1]. Embora os detalhes desses algoritmos mais complexos estejam al√©m do escopo deste texto, fica claro que diferentes atualiza√ß√µes podem ser usadas como blocos de constru√ß√£o para uma ampla variedade de algoritmos DP sem varredura [^1].

Para complementar, podemos definir formalmente o operador de Bellman para policy evaluation. Seja $\pi$ uma pol√≠tica determin√≠stica. O operador de Bellman para policy evaluation √© dado por:

$$
v_{k+1}(s) = \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = \pi(s)]
$$
ou, equivalentemente,
$$
v_{k+1}(s) = \sum_{s', r} p(s', r | s, \pi(s)) [r + \gamma v_k(s')]
$$

> üí° **Exemplo Num√©rico:** Consideremos um MDP com dois estados, $s_1$ e $s_2$, e uma pol√≠tica $\pi$ que sempre escolhe a a√ß√£o $a_1$ em $s_1$ e a a√ß√£o $a_2$ em $s_2$. Suponha que, quando em $s_1$ e seguindo a pol√≠tica $\pi$, a transi√ß√£o para $s_2$ ocorre com probabilidade 0.8, resultando em uma recompensa de 1, e permanece em $s_1$ com probabilidade 0.2, resultando em uma recompensa de 0. Se $\gamma = 0.9$ e $v_k(s_2) = 5$, ent√£o a atualiza√ß√£o para $v_{k+1}(s_1)$ seria:
>
> $v_{k+1}(s_1) = 0.8 * (1 + 0.9 * 5) + 0.2 * (0 + 0.9 * v_k(s_1))$
>
> Se $v_k(s_1) = 2$, ent√£o:
>
> $v_{k+1}(s_1) = 0.8 * (1 + 4.5) + 0.2 * (0 + 1.8) = 0.8 * 5.5 + 0.2 * 1.8 = 4.4 + 0.36 = 4.76$

√â importante observar que evitar varreduras sistem√°ticas n√£o implica necessariamente menos computa√ß√£o total [^1]. Significa apenas que um algoritmo n√£o precisa ficar preso em uma varredura sem esperan√ßa antes de poder progredir na melhoria de uma pol√≠tica [^1].

A flexibilidade proporcionada pelo Asynchronous DP permite otimizar a taxa de progresso do algoritmo, selecionando os estados a serem atualizados de forma estrat√©gica [^1]. Por exemplo, podemos ordenar as atualiza√ß√µes para permitir que a informa√ß√£o de valor se propague de estado para estado de forma eficiente [^1]. Al√©m disso, alguns estados podem n√£o precisar ser atualizados com tanta frequ√™ncia quanto outros [^1], ou podemos at√© tentar pular completamente a atualiza√ß√£o de alguns estados se eles n√£o forem relevantes para o comportamento √≥timo [^1].

**Teorema 1.1:** (Prioritized Sweeping) Uma forma de otimizar a sele√ß√£o de estados para atualiza√ß√£o √© atrav√©s da t√©cnica de *Prioritized Sweeping*. Nessa abordagem, os estados s√£o priorizados com base em suas mudan√ßas esperadas no valor. Mais especificamente, ap√≥s uma atualiza√ß√£o do valor de um estado $s$, os estados predecessores de $s$ s√£o colocados em uma fila de prioridade, com prioridade proporcional √† magnitude da mudan√ßa no valor de $s$.

*Proof Sketch:* A prova da efici√™ncia do Prioritized Sweeping √© geralmente heur√≠stica, demonstrando que essa abordagem tende a propagar as informa√ß√µes de valor de forma mais r√°pida e eficiente do que atualiza√ß√µes aleat√≥rias ou varreduras sistem√°ticas. A prioriza√ß√£o garante que os estados que mais provavelmente ter√£o suas estimativas de valor alteradas sejam atualizados primeiro, acelerando a converg√™ncia.

A prova da afirma√ß√£o acima pode ser estruturada como segue:

Prova (Esbo√ßo Detalhado):

I.  O objetivo do Prioritized Sweeping √© acelerar a converg√™ncia da estimativa da fun√ß√£o valor, focando as atualiza√ß√µes nos estados que sofreram as maiores mudan√ßas.

II. A intui√ß√£o por tr√°s do Prioritized Sweeping √© que, ao atualizar os estados que levam a grandes mudan√ßas de valor, a informa√ß√£o se propaga mais rapidamente pelo espa√ßo de estados.

III. Seja $s$ um estado cujo valor foi atualizado. Seja $P(s)$ o conjunto de predecessores de $s$, ou seja, os estados que podem levar a $s$ em uma √∫nica transi√ß√£o.

IV. Ap√≥s a atualiza√ß√£o de $v(s)$, para cada $s' \in P(s)$, calculamos a mudan√ßa esperada em $v(s')$, que pode ser aproximada por:

    $$|\delta(s')| = | \sum_{s', r} p(s, r | s', a) [r + \gamma v(s)] - v(s')|$$
    onde $a$ √© a a√ß√£o que leva a $s$ a partir de $s'$.

V. Inserimos cada $s' \in P(s)$ em uma fila de prioridade com prioridade $|\delta(s')|$. A fila de prioridade garante que os estados com maiores valores de $|\delta(s')|$ sejam processados primeiro.

VI. O algoritmo ent√£o itera, removendo o estado de maior prioridade $s'$ da fila e atualizando seu valor. Ap√≥s a atualiza√ß√£o de $v(s')$, seus predecessores s√£o adicionados (ou re-priorizados) na fila de prioridade.

VII. Ao priorizar os estados com maiores mudan√ßas esperadas, o algoritmo concentra a computa√ß√£o nas regi√µes do espa√ßo de estados onde a informa√ß√£o de valor est√° se propagando mais rapidamente.

VIII. A an√°lise formal da converg√™ncia do Prioritized Sweeping √© complexa e depende das caracter√≠sticas espec√≠ficas do MDP. No entanto, experimentalmente, o Prioritized Sweeping geralmente demonstra uma converg√™ncia significativamente mais r√°pida do que a Value Iteration ou Policy Iteration padr√£o. ‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que, ap√≥s atualizar o valor do estado $s_3$ para $v(s_3) = 7$, descobrimos que $s_1$ e $s_2$ s√£o predecessores de $s_3$. Se a atualiza√ß√£o de $v(s_3)$ causou uma mudan√ßa esperada de $|\delta(s_1)| = 2$ e $|\delta(s_2)| = 5$, ent√£o $s_2$ seria priorizado sobre $s_1$ na fila de prioridade. Isso significa que $s_2$ seria atualizado antes de $s_1$, pois sua mudan√ßa esperada no valor √© maior, potencialmente propagando a informa√ß√£o de forma mais eficiente.

Adicionalmente, os algoritmos ass√≠ncronos facilitam a combina√ß√£o de computa√ß√£o com intera√ß√£o em tempo real [^1]. Para resolver um determinado MDP, podemos executar um algoritmo DP iterativo ao mesmo tempo em que um agente est√° realmente experimentando o MDP [^1]. A experi√™ncia do agente pode ser usada para determinar os estados aos quais o algoritmo DP aplica suas atualiza√ß√µes [^1]. Ao mesmo tempo, as informa√ß√µes mais recentes de valor e pol√≠tica do algoritmo DP podem orientar a tomada de decis√£o do agente [^1]. Por exemplo, podemos aplicar atualiza√ß√µes aos estados conforme o agente os visita [^1]. Isso torna poss√≠vel focar as atualiza√ß√µes do algoritmo DP em partes do conjunto de estados que s√£o mais relevantes para o agente [^1], um tema recorrente no aprendizado por refor√ßo.

### Conclus√£o

Asynchronous DP oferece uma alternativa valiosa aos m√©todos DP tradicionais, especialmente em problemas com grandes espa√ßos de estados [^1]. A flexibilidade na sele√ß√£o de estados para atualiza√ß√£o e a capacidade de integrar computa√ß√£o com intera√ß√£o em tempo real tornam essa t√©cnica uma ferramenta poderosa para resolver uma variedade de problemas complexos de tomada de decis√£o.

### Refer√™ncias
[^1]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement Learning: An Introduction*. MIT press, 2018.
<!-- END -->