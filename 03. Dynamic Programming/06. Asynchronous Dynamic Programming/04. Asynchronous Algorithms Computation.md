## 4.5.1 Intera√ß√£o em Tempo Real e Algoritmos Ass√≠ncronos

### Introdu√ß√£o

Como vimos na Se√ß√£o 4.5, os algoritmos de **Dynamic Programming (DP) Ass√≠ncronos** oferecem flexibilidade ao n√£o exigirem *sweeps* completos do espa√ßo de estados [^85]. Essa caracter√≠stica torna esses algoritmos particularmente adequados para cen√°rios onde a computa√ß√£o pode ser combinada com a intera√ß√£o em tempo real de um agente com o ambiente. Nesta se√ß√£o, exploraremos como a experi√™ncia do agente pode ser utilizada para guiar as atualiza√ß√µes do algoritmo DP, concentrando a computa√ß√£o nas partes mais relevantes do espa√ßo de estados [^85].

### Conceitos Fundamentais

A caracter√≠stica principal que torna os algoritmos DP ass√≠ncronos adequados para intera√ß√£o em tempo real √© a sua capacidade de atualizar os estados de forma *in-place* e sem uma ordem predefinida [^85]. Isso significa que, enquanto um agente interage com o ambiente e coleta experi√™ncia, o algoritmo DP pode focar em atualizar os estados que o agente est√° visitando ou que s√£o relevantes para as decis√µes que o agente precisa tomar.

Um exemplo pr√°tico dessa abordagem √© aplicar atualiza√ß√µes aos estados visitados pelo agente [^85]. √Ä medida que o agente explora o ambiente, o algoritmo DP pode utilizar as transi√ß√µes observadas (estado, a√ß√£o, recompensa, pr√≥ximo estado) para atualizar iterativamente as estimativas de valor desses estados. Isso pode ser implementado usando a seguinte varia√ß√£o do *update* de value iteration [^85]:

$$V(S_t) \leftarrow \max_a \mathbb{E}[R_{t+1} + \gamma V(S_{t+1}) | S_t = s, A_t = a]$$

Nessa atualiza√ß√£o, $S_t$ representa o estado visitado pelo agente no tempo $t$, $A_t$ √© a a√ß√£o tomada, $R_{t+1}$ √© a recompensa recebida e $S_{t+1}$ √© o pr√≥ximo estado. A expectativa √© calculada sobre as poss√≠veis transi√ß√µes a partir de $S_t$ dado $A_t$. Essa abordagem permite que o algoritmo DP aprenda e refine sua pol√≠tica com base na experi√™ncia real do agente, em vez de depender apenas de um modelo do ambiente [^85].

> üí° **Exemplo Num√©rico:** Imagine um agente aprendendo a navegar em um grid 3x3. O agente est√° no estado (1,1) e tem duas a√ß√µes poss√≠veis: 'Direita' e 'Cima'. Suponha que ao tomar a a√ß√£o 'Direita', ele recebe uma recompensa de -0.1 e vai para o estado (1,2). Ao tomar a a√ß√£o 'Cima', ele recebe uma recompensa de -0.2 e permanece no estado (1,1) (colis√£o com a borda). Assumindo um fator de desconto $\gamma = 0.9$, e valores iniciais $V(1,1) = 0$ e $V(1,2) = 0$, a atualiza√ß√£o do valor de $V(1,1)$ seria:
>
> $V(1,1) \leftarrow \max \begin{cases} \mathbb{E}[-0.1 + 0.9 \cdot V(1,2)] \\ \mathbb{E}[-0.2 + 0.9 \cdot V(1,1)] \end{cases} = \max \begin{cases} -0.1 + 0.9 \cdot 0 \\ -0.2 + 0.9 \cdot 0 \end{cases} = \max \begin{cases} -0.1 \\ -0.2 \end{cases} = -0.1$
>
> O agente atualizaria o valor de $V(1,1)$ para -0.1, indicando que a a√ß√£o 'Direita' √© ligeiramente melhor neste momento. √Ä medida que o agente continua explorando, esses valores ser√£o iterativamente refinados.

**Proposi√ß√£o 1** Essa atualiza√ß√£o converge para $V^*$, o valor √≥timo, sob as mesmas condi√ß√µes que o Value Iteration s√≠ncrono, nomeadamente, se o ambiente for um MDP descontado ($\gamma < 1$) e cada estado for visitado infinitas vezes.

*Prova:* A prova segue diretamente do teorema de converg√™ncia do Value Iteration ass√≠ncrono (ver Bertsekas & Tsitsiklis, 1989), que garante converg√™ncia se as atualiza√ß√µes forem aplicadas repetidamente a todos os estados. No contexto da intera√ß√£o em tempo real, podemos garantir que todos os estados relevantes (ou pelo menos um subconjunto que cont√©m a solu√ß√£o √≥tima) ser√£o visitados infinitas vezes ao longo do tempo, dada uma pol√≠tica de explora√ß√£o adequada.

A utiliza√ß√£o da experi√™ncia do agente para guiar as atualiza√ß√µes do algoritmo DP oferece diversas vantagens:

1.  **Efici√™ncia Computacional:** Ao concentrar a computa√ß√£o nos estados mais relevantes, o algoritmo DP pode convergir mais rapidamente para uma pol√≠tica √≥tima ou sub√≥tima, especialmente em ambientes grandes e complexos.
2.  **Adaptabilidade:** A capacidade de aprender com a experi√™ncia permite que o algoritmo DP se adapte a mudan√ßas no ambiente ou no comportamento do agente.
3.  **Explora√ß√£o Direcionada:** A experi√™ncia do agente pode ser usada para direcionar a explora√ß√£o para as √°reas do espa√ßo de estados que s√£o mais promissoras ou que precisam de mais informa√ß√£o.

Al√©m do *value iteration*, tamb√©m podemos adaptar a *policy iteration* para funcionar de forma ass√≠ncrona e orientada pela experi√™ncia. A seguir, apresentamos uma abordagem para isso.

**Teorema 2** (Policy Iteration Ass√≠ncrona com Amostragem)
√â poss√≠vel realizar a Policy Iteration de forma ass√≠ncrona, utilizando amostras coletadas durante a intera√ß√£o com o ambiente para estimar os valores de estado e refinar a pol√≠tica.

*Prova*: A Policy Iteration consiste em duas etapas principais: avalia√ß√£o da pol√≠tica e melhoria da pol√≠tica. Na vers√£o ass√≠ncrona com amostragem, podemos estimar $Q^\pi(s, a)$ para a pol√≠tica atual $\pi$ utilizando amostras da seguinte forma:

$$Q^\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma Q^\pi(S_{t+1}, \pi(S_{t+1})) | S_t = s, A_t = a]$$

Essa expectativa pode ser aproximada usando m√©dias amostrais das recompensas e dos pr√≥ximos estados observados durante a intera√ß√£o. Ap√≥s um n√∫mero suficiente de amostras, podemos refinar a pol√≠tica $\pi$ de forma gulosa em rela√ß√£o √† estimativa de $Q^\pi(s, a)$:

$$\pi'(s) = \arg\max_a Q^\pi(s, a)$$

Essa abordagem permite que a Policy Iteration convirja para a pol√≠tica √≥tima sem exigir um modelo completo do ambiente, utilizando apenas a experi√™ncia coletada pelo agente. A converg√™ncia √© garantida sob as mesmas condi√ß√µes da Policy Iteration tradicional, desde que a estimativa de $Q^\pi(s, a)$ convirja para o valor real.

> üí° **Exemplo Num√©rico:** Continuando o exemplo do grid 3x3, suponha que a pol√≠tica atual $\pi$ seja sempre mover para a 'Direita'. O agente est√° no estado (1,1) e executa a a√ß√£o 'Direita', recebendo uma recompensa de -0.1 e indo para o estado (1,2).  Nesse estado (1,2), a pol√≠tica $\pi$ dita mover para a 'Direita' novamente, mas como est√° na borda, o agente permanece em (1,2) e recebe uma recompensa de -0.2. Ent√£o, $S_t = (1,1)$, $A_t = \text{'Direita'}$, $R_{t+1} = -0.1$, $S_{t+1} = (1,2)$. Para estimar $Q^\pi((1,1), \text{'Direita'})$ com uma √∫nica amostra e $\gamma = 0.9$:
>
> $Q^\pi((1,1), \text{'Direita'}) \approx R_{t+1} + \gamma \cdot Q^\pi((1,2), \pi((1,2))) = -0.1 + 0.9 \cdot Q^\pi((1,2), \text{'Direita'})$.
>
> Para estimar $Q^\pi((1,2), \text{'Direita'})$: $Q^\pi((1,2), \text{'Direita'}) \approx -0.2 + 0.9 \cdot Q^\pi((1,2), \text{'Direita'})$.  Resolvendo para $Q^\pi((1,2), \text{'Direita'})$, obtemos $Q^\pi((1,2), \text{'Direita'}) \approx -2$.
>
> Substituindo de volta, $Q^\pi((1,1), \text{'Direita'}) \approx -0.1 + 0.9 \cdot (-2) = -1.9$.
>
> Com mais amostras, essa estimativa se tornar√° mais precisa. Se $Q^\pi((1,1), \text{'Cima'})$ for estimado como -1.5 (com base em outras amostras), a pol√≠tica seria atualizada para $\pi'((1,1)) = \text{'Direita'}$ porque -1.9 < -1.5, o que significa que 'Cima' parece ser a melhor a√ß√£o.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Simula√ß√£o da converg√™ncia de Q-values com amostragem
> np.random.seed(42)
>
> def q_learning_sample(q_table, state, action, reward, next_state, learning_rate=0.1, discount_factor=0.9):
>     """Atualiza o Q-valor usando uma √∫nica amostra."""
>     best_next_action = np.argmax(q_table[next_state])
>     td_target = reward + discount_factor * q_table[next_state, best_next_action]
>     td_error = td_target - q_table[state, action]
>     q_table[state, action] += learning_rate * td_error
>     return q_table
>
> # Inicializa√ß√£o do Q-table (3x3 grid, 4 a√ß√µes: Cima, Baixo, Esquerda, Direita)
> q_table = np.zeros((9, 4)) # 9 estados, 4 a√ß√µes
>
> # Simula√ß√£o de epis√≥dios de aprendizado
> num_episodes = 1000
> rewards_per_episode = []
>
> for episode in range(num_episodes):
>     state = 0  # Come√ßa no estado 0 (canto superior esquerdo)
>     total_reward = 0
>     done = False
>
>     while not done:
>         # Escolhe uma a√ß√£o (explora√ß√£o simples: 50% aleat√≥ria, 50% gulosa)
>         if np.random.rand() < 0.5:
>             action = np.random.choice(4)
>         else:
>             action = np.argmax(q_table[state])
>
>         # Simula a transi√ß√£o (simplificado, sem modelo de ambiente completo)
>         if action == 0:  # Cima
>             next_state = max(0, state - 3) # Evita sair do grid
>             reward = -0.1
>         elif action == 1:  # Baixo
>             next_state = min(8, state + 3)
>             reward = -0.1
>         elif action == 2:  # Esquerda
>             next_state = max(0, state - 1) if state % 3 != 0 else state # Evita sair do grid
>             reward = -0.1
>         else:  # Direita
>             next_state = min(8, state + 1) if (state + 1) % 3 != 0 else state
>             reward = -0.1
>
>         # Atualiza o Q-table
>         q_table = q_learning_sample(q_table, state, action, reward, next_state)
>
>         total_reward += reward
>         state = next_state
>         if total_reward < -1:
>           done = True
>
>     rewards_per_episode.append(total_reward)
>
> # Plota a curva de aprendizado
> plt.figure(figsize=(10, 6))
> plt.plot(rewards_per_episode)
> plt.xlabel("Epis√≥dio")
> plt.ylabel("Recompensa Total por Epis√≥dio")
> plt.title("Curva de Aprendizado Q-learning Ass√≠ncrono")
> plt.grid(True)
> plt.show()
>
> # Exibe o Q-table aprendido
> print("Q-table aprendido:")
> print(q_table)
> ```
> Este c√≥digo simula um agente aprendendo em um grid world 3x3 usando Q-learning com amostragem. A curva de aprendizado e o Q-table resultante s√£o exibidos, mostrando como os Q-valores convergem ao longo dos epis√≥dios.

### Aplica√ß√µes e Exemplos

Um exemplo concreto de como os algoritmos DP ass√≠ncronos podem ser usados em conjunto com a intera√ß√£o em tempo real √© no contexto de um rob√¥ navegando em um ambiente desconhecido. O rob√¥ pode utilizar um algoritmo de *value iteration* ass√≠ncrono para aprender a pol√≠tica √≥tima de navega√ß√£o, enquanto explora o ambiente.

Nesse cen√°rio, o rob√¥ aplicaria atualiza√ß√µes de valor aos estados (locais) que visita, usando as recompensas obtidas (por exemplo, alcan√ßar um objetivo, evitar obst√°culos) para refinar sua pol√≠tica de navega√ß√£o. √Ä medida que o rob√¥ explora o ambiente, o algoritmo DP concentra a computa√ß√£o nas √°reas que o rob√¥ est√° visitando, permitindo que ele aprenda a navegar de forma eficiente mesmo em ambientes complexos e desconhecidos.

Para refinar ainda mais a explora√ß√£o, podemos usar t√©cnicas de *exploration bonus*.

**Teorema 2.1** (Explora√ß√£o com B√¥nus de Contagem)
Adicionar um b√¥nus de explora√ß√£o baseado na contagem de visitas a cada estado pode melhorar a taxa de aprendizado em ambientes com explora√ß√£o esparsa.

*Prova*: A ideia central √© incentivar o agente a explorar estados menos visitados, adicionando uma recompensa intr√≠nseca a esses estados. Uma forma comum de implementar isso √© definir um b√¥nus $b(s)$ para cada estado $s$ que √© inversamente proporcional ao n√∫mero de vezes que o estado foi visitado, $N(s)$:

$$b(s) = \frac{K}{\sqrt{N(s)}}$$

onde $K$ √© uma constante que controla a magnitude do b√¥nus. A atualiza√ß√£o do valor de estado torna-se ent√£o:

$$V(S_t) \leftarrow \max_a \mathbb{E}[R_{t+1} + b(S_{t+1}) + \gamma V(S_{t+1}) | S_t = s, A_t = a]$$

I. Seja $V_{k+1}(s)$ o valor estimado do estado $s$ na itera√ß√£o $k+1$, atualizado com o b√¥nus de explora√ß√£o. Queremos mostrar que a inclus√£o do b√¥nus incentiva a explora√ß√£o de estados menos visitados.

II. Considere dois estados, $s_1$ e $s_2$, onde $N(s_1) > N(s_2)$, ou seja, $s_1$ foi visitado mais vezes que $s_2$. Os b√¥nus associados a esses estados s√£o:
    $$b(s_1) = \frac{K}{\sqrt{N(s_1)}}$$
    $$b(s_2) = \frac{K}{\sqrt{N(s_2)}}$$

III. Como $N(s_1) > N(s_2)$, temos que $\sqrt{N(s_1)} > \sqrt{N(s_2)}$, e portanto:
    $$\frac{1}{\sqrt{N(s_1)}} < \frac{1}{\sqrt{N(s_2)}}$$
    $$b(s_1) < b(s_2)$$
    Isso mostra que o estado menos visitado ($s_2$) tem um b√¥nus maior do que o estado mais visitado ($s_1$).

IV.  A atualiza√ß√£o do valor de estado √© dada por:
    $$V_{k+1}(S_t) \leftarrow \max_a \mathbb{E}[R_{t+1} + b(S_{t+1}) + \gamma V_k(S_{t+1}) | S_t = s, A_t = a]$$
    Dado que $b(s_2) > b(s_1)$, a atualiza√ß√£o do valor para estados que levam a $s_2$ ser√° favorecida em rela√ß√£o aos estados que levam a $s_1$, incentivando o agente a explorar $s_2$.

V. Portanto, ao adicionar este b√¥nus, o agente √© incentivado a visitar estados menos explorados, o que pode levar √† descoberta de novas √°reas do espa√ßo de estados e, potencialmente, a uma pol√≠tica melhor. A prova formal da converg√™ncia com b√¥nus de explora√ß√£o pode ser encontrada na literatura de *Reinforcement Learning* (por exemplo, Sutton & Barto, 2018), e geralmente envolve mostrar que, mesmo com o b√¥nus, a pol√≠tica converge para uma solu√ß√£o √≥tima no limite, desde que todos os estados sejam visitados infinitas vezes. ‚ñ†

> üí° **Exemplo Num√©rico:** Vamos adicionar um b√¥nus de explora√ß√£o ao exemplo anterior. Seja $K = 0.1$. Inicialmente, todos os estados t√™m $N(s) = 0$, ent√£o $b(s) = \frac{0.1}{\sqrt{1}} = 0.1$ (adicionamos 1 para evitar divis√£o por zero na primeira itera√ß√£o). Suponha que o agente visite o estado (1,1) pela primeira vez.  A atualiza√ß√£o do valor de $V(1,1)$ com o b√¥nus seria:
>
> $V(1,1) \leftarrow \max \begin{cases} \mathbb{E}[-0.1 + 0.1 + 0.9 \cdot V(1,2)] \\ \mathbb{E}[-0.2 + 0.1 + 0.9 \cdot V(1,1)] \end{cases}$.
>
> Se $V(1,2) = 0$ e $V(1,1) = 0$ inicialmente, $V(1,1) \leftarrow \max \begin{cases} 0 \\ -0.1 \end{cases} = 0$.
>
> Ap√≥s visitar (1,1), $N(1,1)$ se torna 1, ent√£o $b(1,1) = \frac{0.1}{\sqrt{2}} \approx 0.07$. Isso significa que a primeira visita a cada estado recebe um b√¥nus significativo, incentivando a explora√ß√£o. √Ä medida que o agente visita um estado repetidamente, o b√¥nus diminui, direcionando a explora√ß√£o para outros estados.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Simula√ß√£o de Q-learning com b√¥nus de explora√ß√£o
> np.random.seed(42)
>
> def q_learning_exploration_bonus(q_table, state, action, reward, next_state, visit_counts,
>                                 learning_rate=0.1, discount_factor=0.9, exploration_constant=0.1):
>     """Atualiza o Q-valor com b√¥nus de explora√ß√£o."""
>     # B√¥nus de explora√ß√£o inversamente proporcional √† raiz quadrada da contagem de visitas
>     bonus = exploration_constant / np.sqrt(visit_counts[next_state] + 1) # +1 para evitar divis√£o por zero
>
>     best_next_action = np.argmax(q_table[next_state])
>     td_target = reward + bonus + discount_factor * q_table[next_state, best_next_action]
>     td_error = td_target - q_table[state, action]
>     q_table[state, action] += learning_rate * td_error
>
>     visit_counts[next_state] += 1 # Incrementa a contagem de visitas
>     return q_table, visit_counts
>
> # Inicializa√ß√£o do Q-table e contagens de visitas
> q_table = np.zeros((9, 4))
> visit_counts = np.zeros(9)
>
> # Simula√ß√£o de epis√≥dios de aprendizado
> num_episodes = 1000
> rewards_per_episode = []
>
> for episode in range(num_episodes):
>     state = 0
>     total_reward = 0
>     done = False
>
>     while not done:
>         # Escolhe uma a√ß√£o (explora√ß√£o: epsilon-greedy)
>         if np.random.rand() < 0.5:
>             action = np.random.choice(4)
>         else:
>             action = np.argmax(q_table[state])
>
>         # Simula a transi√ß√£o (simplificado)
>         if action == 0:  # Cima
>             next_state = max(0, state - 3)
>             reward = -0.1
>         elif action == 1:  # Baixo
>             next_state = min(8, state + 3)
>             reward = -0.1
>         elif action == 2:  # Esquerda
>             next_state = max(0, state - 1) if state % 3 != 0 else state
>             reward = -0.1
>         else:  # Direita
>             next_state = min(8, state + 1) if (state + 1) % 3 != 0 else state
>             reward = -0.1
>
>         # Atualiza o Q-table com b√¥nus de explora√ß√£o
>         q_table, visit_counts = q_learning_exploration_bonus(q_table, state, action, reward, next_state, visit_counts)
>
>         total_reward += reward
>         state = next_state
>         if total_reward < -1:
>           done = True
>
>     rewards_per_episode.append(total_reward)
>
> # Plota a curva de aprendizado
> plt.figure(figsize=(10, 6))
> plt.plot(rewards_per_episode)
> plt.xlabel("Epis√≥dio")
> plt.ylabel("Recompensa Total por Epis√≥dio")
> plt.title("Curva de Aprendizado Q-learning com B√¥nus de Explora√ß√£o")
> plt.grid(True)
> plt.show()
>
> # Exibe o Q-table e as contagens de visita aprendidas
> print("Q-table aprendido:")
> print(q_table)
> print("\nContagens de visita por estado:")
> print(visit_counts)
> ```
> Este c√≥digo demonstra como a adi√ß√£o de um b√¥nus de explora√ß√£o baseado na contagem de visitas afeta o aprendizado. O b√¥nus incentiva o agente a explorar estados menos visitados, resultando em uma pol√≠tica melhor e aprendizado mais r√°pido, especialmente em ambientes esparsos. A curva de aprendizado e as contagens de visita s√£o plotadas.

### Conclus√£o

A combina√ß√£o de algoritmos DP ass√≠ncronos com a intera√ß√£o em tempo real oferece uma abordagem poderosa para resolver problemas de controle √≥timos em ambientes complexos e din√¢micos. Ao utilizar a experi√™ncia do agente para guiar as atualiza√ß√µes do algoritmo DP, √© poss√≠vel concentrar a computa√ß√£o nas partes mais relevantes do espa√ßo de estados, resultando em aprendizado mais eficiente e adapt√°vel. Essa abordagem √© particularmente √∫til em cen√°rios onde a computa√ß√£o √© limitada ou onde o ambiente est√° mudando com o tempo [^85].
$\blacksquare$

### Refer√™ncias
[^85]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
<!-- END -->