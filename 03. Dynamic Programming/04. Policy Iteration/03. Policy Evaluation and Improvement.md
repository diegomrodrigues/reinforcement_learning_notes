### Introdu√ß√£o
O m√©todo de **Policy Iteration** √© um algoritmo cl√°ssico em Dynamic Programming (DP) para encontrar uma pol√≠tica √≥tima em um Markov Decision Process (MDP) finito. Ele alterna entre duas fases principais: **Policy Evaluation** e **Policy Improvement**. O objetivo √©, iterativamente, melhorar a pol√≠tica atual at√© que ela converja para a pol√≠tica √≥tima. Este cap√≠tulo se aprofundar√° no algoritmo de Policy Iteration, com √™nfase em como a inicializa√ß√£o da Policy Evaluation com a fun√ß√£o valor da pol√≠tica anterior acelera a converg√™ncia [^80].

### Conceitos Fundamentais
O algoritmo de Policy Iteration envolve a inicializa√ß√£o arbitr√°ria da fun√ß√£o valor $V(s)$ e da pol√≠tica $\pi(s)$ para todos os estados $s \in S$ [^80]. A fun√ß√£o valor para o estado terminal √© definida como zero, ou seja, $V(terminal) = 0$ [^80]. O algoritmo, ent√£o, itera atrav√©s das seguintes etapas:

![Policy Iteration algorithm: iterative process of policy evaluation and improvement for optimal policy estimation.](./../images/image3.png)

1.  **Policy Evaluation:** Dada uma pol√≠tica $\pi$, avalia-se iterativamente a fun√ß√£o valor $V(s)$ at√© que ela converja para $v_{\pi}(s)$. A fun√ß√£o valor √© atualizada utilizando a equa√ß√£o de Bellman para avalia√ß√£o de pol√≠tica [^74]:

    $$
    V(s) \leftarrow \mathbb{E}_{s',r \sim p}[r + \gamma V(s') | s, \pi(s)] \quad \forall s \in S
    $$

    Essa etapa envolve realizar *expected updates* at√© que a mudan√ßa m√°xima no valor dos estados seja menor que um limiar $\theta$ [^75, 80]. Mais formalmente, o loop de Policy Evaluation continua at√© que $\Delta < \theta$, onde:

    $$
    \Delta = \max_{s \in S} |v - V(s)|
    $$

    e $v$ √© o valor anterior do estado $s$ [^80].

    Para garantir a converg√™ncia da Policy Evaluation, podemos expressar a atualiza√ß√£o de forma iterativa como:

    $$
    V_{k+1}(s) = \mathbb{E}_{s',r \sim p}[r + \gamma V_k(s') | s, \pi(s)] \quad \forall s \in S
    $$

    onde $V_k(s)$ √© a estimativa da fun√ß√£o valor no passo *k*.

    **Proposi√ß√£o 2:** A Policy Evaluation converge para $v_{\pi}$ como $k \rightarrow \infty$.

    *Prova*.

    I. A equa√ß√£o de atualiza√ß√£o da Policy Evaluation √© uma aplica√ß√£o do operador de Bellman para avalia√ß√£o da pol√≠tica:
    $$V_{k+1}(s) = \mathbb{E}_{s',r \sim p}[r + \gamma V_k(s') | s, \pi(s)] = (T^{\pi}V_k)(s)$$
    Onde $T^{\pi}$ √© o operador de Bellman para a pol√≠tica $\pi$.

    II. O operador de Bellman $T^{\pi}$ √© uma contra√ß√£o com fator $\gamma \in [0, 1)$. Para quaisquer fun√ß√µes valor $V$ e $V'$, temos:
    $$||T^{\pi}V - T^{\pi}V'||_{\infty} \leq \gamma ||V - V'||_{\infty}$$
    Onde $||V||_{\infty} = \max_{s} |V(s)|$ √© a norma do supremo.

    III. Pelo Teorema da Contra√ß√£o de Banach, se um operador √© uma contra√ß√£o em um espa√ßo m√©trico completo, ent√£o ele tem um ponto fixo √∫nico, e a aplica√ß√£o iterativa do operador converge para esse ponto fixo.

    IV. Portanto, a sequ√™ncia de fun√ß√µes valor $V_k$ converge para o ponto fixo √∫nico de $T^{\pi}$, que √© $v_{\pi}$:
    $$\lim_{k \to \infty} V_k = v_{\pi}$$ ‚ñ†

    > üí° **Exemplo Num√©rico:**
    >
    > Considere um MDP com tr√™s estados $S = \{s_1, s_2, s_3\}$ e uma pol√≠tica $\pi$ que sempre escolhe a primeira a√ß√£o dispon√≠vel em cada estado. Suponha que a taxa de desconto $\gamma = 0.9$ e o limiar de converg√™ncia $\theta = 0.01$. As recompensas imediatas e as probabilidades de transi√ß√£o s√£o dadas pelas seguintes equa√ß√µes (simplificadas para fins ilustrativos):
    >
    > -   $s_1$: Recompensa = 1, transi√ß√£o para $s_2$ com probabilidade 1.
    > -   $s_2$: Recompensa = -1, transi√ß√£o para $s_3$ com probabilidade 1.
    > -   $s_3$: Recompensa = 0, transi√ß√£o para $s_1$ com probabilidade 1.
    >
    > Inicializamos $V_0(s)$ arbitrariamente como $V_0(s_1) = 0$, $V_0(s_2) = 0$, $V_0(s_3) = 0$.
    >
    > **Itera√ß√£o 1:**
    >
    > -   $V_1(s_1) = 1 + 0.9 * V_0(s_2) = 1 + 0.9 * 0 = 1$
    > -   $V_1(s_2) = -1 + 0.9 * V_0(s_3) = -1 + 0.9 * 0 = -1$
    > -   $V_1(s_3) = 0 + 0.9 * V_0(s_1) = 0 + 0.9 * 0 = 0$
    >
    > $\Delta = \max(|0-1|, |0-(-1)|, |0-0|) = 1$
    >
    > **Itera√ß√£o 2:**
    >
    > -   $V_2(s_1) = 1 + 0.9 * V_1(s_2) = 1 + 0.9 * (-1) = 0.1$
    > -   $V_2(s_2) = -1 + 0.9 * V_1(s_3) = -1 + 0.9 * 0 = -1$
    > -   $V_2(s_3) = 0 + 0.9 * V_1(s_1) = 0 + 0.9 * 1 = 0.9$
    >
    > $\Delta = \max(|1-0.1|, |-1-(-1)|, |0-0.9|) = 0.9$
    >
    > **Itera√ß√£o 3:**
    >
    > -   $V_3(s_1) = 1 + 0.9 * V_2(s_2) = 1 + 0.9 * (-1) = 0.1$
    > -   $V_3(s_2) = -1 + 0.9 * V_2(s_3) = -1 + 0.9 * 0.9 = -0.19$
    > -   $V_3(s_3) = 0 + 0.9 * V_2(s_1) = 0 + 0.9 * 0.1 = 0.09$
    >
    > $\Delta = \max(|0.1-0.1|, |-1-(-0.19)|, |0.9-0.09|) = 0.81$
    >
    > ... e assim por diante. As itera√ß√µes continuam at√© que $\Delta < 0.01$. Este exemplo demonstra como os valores dos estados s√£o atualizados iterativamente utilizando a equa√ß√£o de Bellman e como o crit√©rio de parada $\Delta$ √© utilizado para determinar a converg√™ncia.
    >
    > ```python
    > import numpy as np
    >
    > # Definindo os par√¢metros
    > gamma = 0.9
    > theta = 0.01
    >
    > # Inicializando os valores dos estados
    > V = np.array([0.0, 0.0, 0.0])
    >
    > def policy_evaluation(V, gamma, theta):
    >     delta = float('inf')
    >     while delta > theta:
    >         delta = 0
    >         V_old = np.copy(V)
    >         V[0] = 1 + gamma * V_old[1]
    >         V[1] = -1 + gamma * V_old[2]
    >         V[2] = 0 + gamma * V_old[0]
    >         delta = np.max(np.abs(V - V_old))
    >     return V
    >
    > # Executando a policy evaluation
    > V_final = policy_evaluation(V, gamma, theta)
    > print("Valores dos estados ap√≥s a policy evaluation:", V_final)
    > ```



![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

2.  **Policy Improvement:** Ap√≥s a Policy Evaluation, a pol√≠tica √© aprimorada de forma *greedy* em rela√ß√£o √† fun√ß√£o valor $V(s)$ obtida na etapa anterior. Para cada estado $s$, a a√ß√£o que maximiza o valor esperado do pr√≥ximo estado √© selecionada [^79, 80]:

    $$
    \pi(s) \leftarrow \arg\max_{a} \mathbb{E}_{s',r \sim p}[r + \gamma V(s') | s, a] \quad \forall s \in S
    $$

    Se a pol√≠tica n√£o mudar em nenhum estado durante esta etapa, ou seja, se $oldAction = \pi(s)$ para todos os estados, ent√£o a pol√≠tica √© √≥tima e o algoritmo termina [^80]. Caso contr√°rio, retorna-se ao passo 1 (Policy Evaluation) com a nova pol√≠tica.

    **Proposi√ß√£o 1:** A etapa de Policy Improvement sempre resulta em uma pol√≠tica igual ou melhor que a pol√≠tica anterior.

    *Prova.* Seja $\pi$ a pol√≠tica anterior e $\pi'$ a pol√≠tica ap√≥s a melhoria.  Para cada estado *s*, temos:

    $$q_{\pi}(s, \pi'(s)) = \max_a q_{\pi}(s, a) \geq q_{\pi}(s, \pi(s)) = v_{\pi}(s)$$

    Aplicando o Teorema da Melhoria de Pol√≠tica, $\pi'$ √© melhor ou igual a $\pi$.

    *Prova expandida*.

    I. Definimos $q_{\pi}(s,a)$ como a fun√ß√£o de valor de a√ß√£o sob a pol√≠tica $\pi$, que representa o valor esperado de iniciar no estado *s*, tomar a a√ß√£o *a*, e ent√£o seguir a pol√≠tica $\pi$:
    $$q_{\pi}(s,a) = \mathbb{E}_{s', r \sim p} [r + \gamma v_{\pi}(s') | s, a]$$

    II. Durante a etapa de Policy Improvement, definimos uma nova pol√≠tica $\pi'$ que age greedy com respeito a $v_{\pi}$:
    $$\pi'(s) = \arg\max_{a} q_{\pi}(s, a)$$

    III. Por defini√ß√£o, para cada estado $s$, a a√ß√£o escolhida por $\pi'(s)$ maximiza $q_{\pi}(s, a)$:
    $$q_{\pi}(s, \pi'(s)) = \max_{a} q_{\pi}(s, a)$$

    IV. Isso significa que o valor de tomar a a√ß√£o $\pi'(s)$ no estado $s$ sob a pol√≠tica $\pi$ √© maior ou igual ao valor de tomar a a√ß√£o especificada pela pol√≠tica $\pi$:
    $$q_{\pi}(s, \pi'(s)) \geq q_{\pi}(s, \pi(s))$$

    V. Como $v_{\pi}(s) = q_{\pi}(s, \pi(s))$, podemos reescrever a desigualdade acima como:
    $$q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$$

    VI. Agora, consideramos o valor $v_{\pi'}(s)$. Usando o Teorema da Melhoria de Pol√≠tica, sabemos que se $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todo $s$, ent√£o $\pi' \geq \pi$. Em outras palavras, a pol√≠tica $\pi'$ √© garantida como sendo igual ou melhor que a pol√≠tica $\pi$. ‚ñ†

    > üí° **Exemplo Num√©rico:**
    >
    > Continuando o exemplo anterior, suponha que em cada estado temos duas a√ß√µes dispon√≠veis, $a_1$ e $a_2$. A pol√≠tica anterior $\pi$ sempre escolhia $a_1$. Agora, vamos avaliar se mudar a a√ß√£o para $a_2$ melhoraria o valor.
    >
    > Suponha as seguintes recompensas e transi√ß√µes para a a√ß√£o $a_2$:
    >
    > -   $s_1$: Recompensa = 0, transi√ß√£o para $s_3$ com probabilidade 1.
    > -   $s_2$: Recompensa = 1, transi√ß√£o para $s_1$ com probabilidade 1.
    > -   $s_3$: Recompensa = -1, transi√ß√£o para $s_2$ com probabilidade 1.
    >
    > Utilizando os valores $V(s)$ obtidos ap√≥s a Policy Evaluation (do exemplo anterior, vamos supor que j√° convergiu para $V(s_1) = 0.45, V(s_2) = -0.65, V(s_3) = 0.40$):
    >
    > Calculamos $q_{\pi}(s, a)$ para cada estado e cada a√ß√£o:
    >
    > -   $q_{\pi}(s_1, a_1) = 1 + 0.9 * V(s_2) = 1 + 0.9 * (-0.65) = 0.415$
    > -   $q_{\pi}(s_1, a_2) = 0 + 0.9 * V(s_3) = 0 + 0.9 * (0.40) = 0.36$
    > -   $q_{\pi}(s_2, a_1) = -1 + 0.9 * V(s_3) = -1 + 0.9 * (0.40) = -0.64$
    > -   $q_{\pi}(s_2, a_2) = 1 + 0.9 * V(s_1) = 1 + 0.9 * (0.45) = 1.405$
    > -   $q_{\pi}(s_3, a_1) = 0 + 0.9 * V(s_1) = 0 + 0.9 * (0.45) = 0.405$
    > -   $q_{\pi}(s_3, a_2) = -1 + 0.9 * V(s_2) = -1 + 0.9 * (-0.65) = -1.585$
    >
    > Agora, fazemos a Policy Improvement:
    >
    > -   $\pi'(s_1) = \arg\max_{a} q_{\pi}(s_1, a) = a_1$ (j√° que $0.415 > 0.36$)
    > -   $\pi'(s_2) = \arg\max_{a} q_{\pi}(s_2, a) = a_2$ (j√° que $1.405 > -0.64$)
    > -   $\pi'(s_3) = \arg\max_{a} q_{\pi}(s_3, a) = a_1$ (j√° que $0.405 > -1.585$)
    >
    > A nova pol√≠tica $\pi'$ √© diferente da pol√≠tica anterior $\pi$ no estado $s_2$. Portanto, a pol√≠tica foi melhorada. Retornamos √† etapa de Policy Evaluation com essa nova pol√≠tica.
    >
    > ```python
    > import numpy as np
    >
    > # Definindo os par√¢metros
    > gamma = 0.9
    >
    > # Valores dos estados (supostamente convergidos da Policy Evaluation)
    > V = np.array([0.45, -0.65, 0.40])
    >
    > # Funcao para calcular o valor da acao
    > def calculate_q(s, a, V, gamma):
    >     if s == 0 and a == 0:  # s1, a1
    >         return 1 + gamma * V[1]
    >     elif s == 0 and a == 1:  # s1, a2
    >         return 0 + gamma * V[2]
    >     elif s == 1 and a == 0:  # s2, a1
    >         return -1 + gamma * V[2]
    >     elif s == 1 and a == 1:  # s2, a2
    >         return 1 + gamma * V[0]
    >     elif s == 2 and a == 0:  # s3, a1
    >         return 0 + gamma * V[0]
    >     elif s == 2 and a == 1:  # s3, a2
    >         return -1 + gamma * V[1]
    >     else:
    >         return 0  # Caso nao definido
    >
    > # Policy Improvement
    > policy = np.array([0, 0, 0])  # Inicialmente, sempre a1
    > policy_stable = True
    >
    > for s in range(3):
    >     q_a1 = calculate_q(s, 0, V, gamma)
    >     q_a2 = calculate_q(s, 1, V, gamma)
    >     if q_a2 > q_a1:
    >         policy[s] = 1  # Mudando para a2
    >         policy_stable = False
    >
    > print("Nova pol√≠tica:", policy)
    > print("Pol√≠tica est√°vel:", policy_stable)
    > ```

Um detalhe importante do algoritmo √© que a Policy Evaluation √© frequentemente inicializada com a fun√ß√£o valor $V(s)$ da pol√≠tica anterior [^80]. Isso resulta em uma converg√™ncia significativamente mais r√°pida da Policy Evaluation, pois a fun√ß√£o valor j√° √© uma aproxima√ß√£o razo√°vel da verdadeira fun√ß√£o valor da nova pol√≠tica. A intui√ß√£o √© que a nova pol√≠tica, ap√≥s a etapa de Policy Improvement, √© geralmente uma pequena varia√ß√£o da pol√≠tica anterior, e, portanto, suas fun√ß√µes valor tamb√©m s√£o similares [^80].

**Teorema da Melhoria de Pol√≠tica:** Se $\pi'(s) = \arg\max_{a} q_{\pi}(s, a)$ para todo $s \in S$, ent√£o a nova pol√≠tica $\pi'$ √© melhor ou igual √† pol√≠tica anterior $\pi$ [^79].

**Lema:** A pol√≠tica $\pi$ √© √≥tima se e somente se a equa√ß√£o de Bellman de otimalidade for satisfeita para $v_{\pi}$ [^79].

**Teorema 1.1:** Se a pol√≠tica $\pi$ n√£o muda durante a etapa de Policy Improvement, ent√£o $\pi$ √© uma pol√≠tica √≥tima.

*Prova.* Se a pol√≠tica $\pi$ n√£o muda durante a etapa de Policy Improvement, ent√£o para todo $s \in S$:

$$
\pi(s) = \arg\max_{a} \mathbb{E}_{s',r \sim p}[r + \gamma V_{\pi}(s') | s, a]
$$

Isso significa que a equa√ß√£o de Bellman de otimalidade est√° satisfeita para $v_{\pi}$, e, portanto, $\pi$ √© uma pol√≠tica √≥tima, conforme o **Lema** anterior.

*Prova expandida.*

I. Se a pol√≠tica $\pi$ n√£o muda durante o Policy Improvement, isso significa que, para cada estado $s$, a a√ß√£o selecionada pela pol√≠tica $\pi$ j√° √© a a√ß√£o que maximiza o valor esperado, dado a fun√ß√£o valor $V_{\pi}$:
$$\pi(s) = \arg\max_{a} \mathbb{E}[r + \gamma V_{\pi}(s') | s, a]$$

II. Esta condi√ß√£o implica que, para todo estado $s$ e para toda a√ß√£o $a$:
$$V_{\pi}(s) = \max_{a} \mathbb{E}[r + \gamma V_{\pi}(s') | s, a]$$

III. A equa√ß√£o acima √© a Equa√ß√£o de Bellman de Otimalidade. Ela afirma que o valor de um estado sob a pol√≠tica $\pi$ √© igual ao valor m√°ximo esperado que pode ser obtido a partir desse estado, considerando todas as poss√≠veis a√ß√µes.

IV. Pelo **Lema**, se a Equa√ß√£o de Bellman de Otimalidade √© satisfeita para $v_{\pi}$, ent√£o $\pi$ √© uma pol√≠tica √≥tima.

V. Portanto, se a pol√≠tica $\pi$ n√£o muda durante a etapa de Policy Improvement, ent√£o ela satisfaz a Equa√ß√£o de Bellman de Otimalidade, e $\pi$ √© uma pol√≠tica √≥tima. ‚ñ†

### Exemplo de Converg√™ncia Acelerada

Considere o gridworld exemplificado na Figura 4.1 [^77]. A pol√≠tica inicial √© aleat√≥ria, com todas as a√ß√µes igualmente prov√°veis. Na primeira itera√ß√£o, a Policy Evaluation converge para $v_{\pi}$ ap√≥s v√°rias itera√ß√µes. Na segunda itera√ß√£o, a Policy Improvement torna a pol√≠tica *greedy* em rela√ß√£o a $v_{\pi}$. A inicializa√ß√£o da nova Policy Evaluation com a $v_{\pi}$ da itera√ß√£o anterior acelera significativamente a converg√™ncia [^80]. Em alguns casos, conforme mencionado no texto, apenas tr√™s itera√ß√µes de Policy Evaluation s√£o suficientes para obter uma pol√≠tica √≥tima [^77].

![Illustration of a 4x4 gridworld environment with rewards and actions for dynamic programming example.](./../images/image9.png)

![Convergence of iterative policy evaluation on a gridworld, showing improvement from random to optimal policy.](./../images/image7.png)

![Diagrama da itera√ß√£o da pol√≠tica generalizada (GPI) mostrando o ciclo entre avalia√ß√£o e melhoria da pol√≠tica.](./../images/image2.png)

![Diagrama representando a intera√ß√£o entre avalia√ß√£o e melhoria de pol√≠ticas na itera√ß√£o da pol√≠tica generalizada (GPI).](./../images/image6.png)

### Conclus√£o
Policy Iteration √© um m√©todo eficaz para encontrar pol√≠ticas √≥timas em MDPs finitos. A inicializa√ß√£o da Policy Evaluation com a fun√ß√£o valor da pol√≠tica anterior √© uma t√©cnica crucial para acelerar a converg√™ncia do algoritmo. A cada itera√ß√£o, a pol√≠tica √© garantidamente aprimorada (ou permanece a mesma se j√° for √≥tima), garantindo que o algoritmo convirja para uma pol√≠tica √≥tima em um n√∫mero finito de itera√ß√µes [^80]. Embora possa ser computacionalmente intensivo para problemas muito grandes, Policy Iteration fornece uma base s√≥lida para entender outros algoritmos de Reinforcement Learning [^73].
### Refer√™ncias
[^73]: Dynamic Programming
[^74]: Policy Evaluation (Prediction)
[^75]: Policy Evaluation (Prediction)
[^77]: Policy Improvement
[^79]: Policy Improvement
[^80]: Policy Iteration
<!-- END -->