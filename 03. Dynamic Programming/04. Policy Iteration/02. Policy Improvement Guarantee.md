## Converg√™ncia e Otimiza√ß√£o na Policy Iteration

### Introdu√ß√£o
O processo de **Policy Iteration** emerge como um m√©todo iterativo fundamental para encontrar pol√≠ticas √≥timas em **Markov Decision Processes (MDPs)** finitos. Construindo sobre a **Policy Evaluation**, que calcula a fun√ß√£o valor $v_{\pi}$ para uma dada pol√≠tica $\pi$, a Policy Iteration introduz um passo de **Policy Improvement** que gera uma nova pol√≠tica $\pi'$ que √© garantidamente melhor ou igual √† pol√≠tica anterior. Este cap√≠tulo aprofunda a garantia de melhoria estrita em cada itera√ß√£o e a converg√™ncia assegurada em um n√∫mero finito de itera√ß√µes para MDPs finitos [^80].

### Conceitos Fundamentais

O cerne da Policy Iteration reside na altern√¢ncia entre dois passos cruciais:
1.  **Policy Evaluation**: Dado uma pol√≠tica $\pi$, calcula a fun√ß√£o valor $v_{\pi}(s)$ para todos os estados $s \in S$. Este passo, como vimos anteriormente, pode ser realizado iterativamente utilizando a equa√ß√£o de Bellman para $v_{\pi}$ [^74].
2.  **Policy Improvement**: Utilizando a fun√ß√£o valor $v_{\pi}$, gera uma nova pol√≠tica $\pi'$ que √© *greedy* com rela√ß√£o a $v_{\pi}$. Formalmente, $\pi'(s) = \arg \max_{a \in A(s)} q_{\pi}(s, a)$, onde $q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a]$ [^79].

> üí° **Exemplo Num√©rico:**
>
> Imagine um MDP com 3 estados ($S = \{s_1, s_2, s_3\}$) e 2 a√ß√µes em cada estado ($A(s) = \{a_1, a_2\}$ para todo $s$).  Suponha que temos uma pol√≠tica $\pi$ que define $\pi(s_1) = a_1$, $\pi(s_2) = a_2$ e $\pi(s_3) = a_1$.  A Policy Evaluation calcularia $v_{\pi}(s_1)$, $v_{\pi}(s_2)$ e $v_{\pi}(s_3)$.  Ent√£o, o Policy Improvement analisaria se, para cada estado, mudar a a√ß√£o para a outra ($a_2$ em $s_1$, $a_1$ em $s_2$, $a_2$ em $s_3$) resultaria em um $q_{\pi}(s, a)$ maior que $v_{\pi}(s)$. Se sim, a pol√≠tica seria atualizada.

A **garantia de melhoria estrita** significa que, a menos que a pol√≠tica $\pi$ seja j√° √≥tima, a nova pol√≠tica $\pi'$ gerada pelo Policy Improvement √© estritamente melhor que $\pi$, ou seja, $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s \in S$, com uma desigualdade estrita para pelo menos um estado [^78]. Este resultado √© formalizado no *Policy Improvement Theorem*.

**Lema 1:** Se $v_{\pi'}(s) = v_{\pi}(s)$ para todo $s \in S$, ent√£o $\pi$ √© uma pol√≠tica √≥tima.

*Demonstra√ß√£o:* Se $v_{\pi'}(s) = v_{\pi}(s)$ para todo $s$, ent√£o a pol√≠tica $\pi$ j√° est√° selecionando as a√ß√µes que maximizam o valor esperado, ou seja, ela √© greedy com rela√ß√£o a sua pr√≥pria fun√ß√£o valor. Portanto, n√£o h√° melhoria poss√≠vel, e $\pi$ √© √≥tima. $\blacksquare$

#### Demonstra√ß√£o da Melhoria Estrita

Para demonstrar a melhoria estrita, considere que $\pi$ n√£o √© √≥tima. Ent√£o existe um estado $s$ e uma a√ß√£o $a$ tal que $q_{\pi}(s, a) > v_{\pi}(s)$. A nova pol√≠tica $\pi'$ √© definida tal que $\pi'(s) = a$ [^78, 79]. Utilizando a demonstra√ß√£o do Policy Improvement Theorem [^78]:

$$v_{\pi}(s) \leq q_{\pi}(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s, A_t = \pi'(s)] = \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s] \leq v_{\pi'}(s)$$

A desigualdade estrita $q_{\pi}(s, \pi'(s)) > v_{\pi}(s)$ implica que $v_{\pi'}(s) > v_{\pi}(s)$ para pelo menos um estado $s$.

> üí° **Exemplo Num√©rico:**
>
> Seja $s = s_1$ e suponha que sob a pol√≠tica $\pi$, temos $v_{\pi}(s_1) = 5$. Agora, suponha que calcularmos $q_{\pi}(s_1, a_2) = 7$. Isso significa que tomar a a√ß√£o $a_2$ em $s_1$ resulta em um valor esperado maior do que seguir a pol√≠tica $\pi$ em $s_1$. A nova pol√≠tica $\pi'$ ent√£o define $\pi'(s_1) = a_2$. Pelo Policy Improvement Theorem, $v_{\pi'}(s_1) > v_{\pi}(s_1)$, portanto $v_{\pi'}(s_1)$ ser√° maior que 5.

**Teorema 1:** (Policy Improvement Theorem - Reformulado) Se $\pi'$ √© uma pol√≠tica tal que $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todo $s$, ent√£o $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s$. Al√©m disso, se $q_{\pi}(s, \pi'(s)) > v_{\pi}(s)$ para algum $s$, ent√£o $v_{\pi'}(s) > v_{\pi}(s)$ para esse $s$.

**Corol√°rio 1:** Se $\pi'$ √© obtida a partir de $\pi$ atrav√©s de um √∫nico passo de Policy Improvement, ent√£o $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s$, e se $\pi$ n√£o √© √≥tima, ent√£o $v_{\pi'}(s) > v_{\pi}(s)$ para pelo menos um $s$.

Para fornecer uma prova mais completa do Policy Improvement Theorem, podemos expandir a l√≥gica passo a passo:

**Prova do Teorema 1 (Policy Improvement Theorem):**

Queremos provar que se $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todo $s$, ent√£o $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s$, e que se $q_{\pi}(s, \pi'(s)) > v_{\pi}(s)$ para algum $s$, ent√£o $v_{\pi'}(s) > v_{\pi}(s)$ para esse $s$.

I. Come√ßamos expressando $v_{\pi}(s)$ usando a equa√ß√£o de Bellman para a pol√≠tica $\pi$:
   $$v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]$$

II. Dado que $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todo $s$, podemos substituir $v_{\pi}(s)$ por $q_{\pi}(s, \pi'(s))$:
    $$v_{\pi}(s) \leq q_{\pi}(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = \pi'(s)]$$

III. Agora, iteramos a substitui√ß√£o da fun√ß√£o valor $v_{\pi}$ dentro da expectativa do lado direito:
     $$v_{\pi}(s) \leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi}[R_{t+2} + \gamma v_{\pi}(S_{t+2}) | S_{t+1}] | S_t = s]$$

IV. Continuando a expandir a expectativa iterativamente $k$ vezes:
    $$v_{\pi}(s) \leq \mathbb{E}_{\pi'}[\sum_{i=0}^{k-1} \gamma^i R_{t+i+1} + \gamma^k v_{\pi}(S_{t+k}) | S_t = s]$$

V. √Ä medida que $k$ se aproxima do infinito, o termo $\gamma^k v_{\pi}(S_{t+k})$ tende a zero (assumindo que as recompensas s√£o limitadas e $\gamma < 1$). Portanto:
   $$v_{\pi}(s) \leq \mathbb{E}_{\pi'}[\sum_{i=0}^{\infty} \gamma^i R_{t+i+1} | S_t = s]$$

VI. O lado direito da desigualdade √© exatamente a defini√ß√£o de $v_{\pi'}(s)$:
    $$v_{\pi}(s) \leq v_{\pi'}(s)$$

VII. Se existe um estado $s$ tal que $q_{\pi}(s, \pi'(s)) > v_{\pi}(s)$, ent√£o a desigualdade se mant√©m estrita ao longo da deriva√ß√£o, e portanto $v_{\pi'}(s) > v_{\pi}(s)$ para esse $s$.

Portanto, demonstramos que se $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todo $s$, ent√£o $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s$, e se $q_{\pi}(s, \pi'(s)) > v_{\pi}(s)$ para algum $s$, ent√£o $v_{\pi'}(s) > v_{\pi}(s)$ para esse $s$. $\blacksquare$

#### Converg√™ncia em um N√∫mero Finito de Itera√ß√µes

A converg√™ncia da Policy Iteration √© garantida para MDPs finitos porque:

1.  O conjunto de pol√≠ticas determin√≠sticas √© finito. Dado um MDP com $|S|$ estados e $|A(s)|$ a√ß√µes para cada estado $s$, o n√∫mero m√°ximo de pol√≠ticas determin√≠sticas distintas √© $\prod_{s \in S} |A(s)|$, que √© um n√∫mero finito [^80].
2.  Cada itera√ß√£o da Policy Iteration produz uma pol√≠tica estritamente melhor (a menos que a pol√≠tica atual seja √≥tima). Isto significa que a Policy Iteration nunca revisita a mesma pol√≠tica [^78, 79].
3.  Como o n√∫mero de pol√≠ticas √© finito e cada itera√ß√£o produz uma pol√≠tica √∫nica e melhor, a Policy Iteration deve convergir para uma pol√≠tica √≥tima em um n√∫mero finito de itera√ß√µes [^80].

> üí° **Exemplo Num√©rico:**
>
> No exemplo anterior com 3 estados e 2 a√ß√µes, o n√∫mero m√°ximo de pol√≠ticas determin√≠sticas √© $2 \times 2 \times 2 = 8$. A Policy Iteration exploraria essas pol√≠ticas, garantindo que cada nova pol√≠tica seja melhor que a anterior, at√© convergir para a pol√≠tica √≥tima.

Para refor√ßar a prova da converg√™ncia finita, podemos detalhar os argumentos:

**Prova da Converg√™ncia em um N√∫mero Finito de Itera√ß√µes:**

O objetivo √© mostrar que, em um MDP finito, a Policy Iteration converge para uma pol√≠tica √≥tima em um n√∫mero finito de passos.

I. **Espa√ßo de Pol√≠ticas Finito:** Em um MDP com um n√∫mero finito de estados ($|S|$) e um n√∫mero finito de a√ß√µes por estado ($|A(s)|$), o n√∫mero total de pol√≠ticas determin√≠sticas poss√≠veis √© o produto do n√∫mero de a√ß√µes dispon√≠veis em cada estado, ou seja, $\prod_{s \in S} |A(s)|$. Este n√∫mero √© finito, o que significa que existe um n√∫mero limitado de pol√≠ticas distintas que o algoritmo pode explorar.

II. **Melhoria Monot√¥nica:** O Policy Improvement Theorem garante que cada itera√ß√£o da Policy Iteration resulta em uma pol√≠tica que √© pelo menos t√£o boa quanto a pol√≠tica anterior, e estritamente melhor se a pol√≠tica anterior n√£o for √≥tima. Matematicamente, $v_{\pi_{i+1}}(s) \geq v_{\pi_i}(s)$ para todo $s \in S$, onde $\pi_i$ √© a pol√≠tica na itera√ß√£o $i$. Al√©m disso, se $\pi_i$ n√£o √© √≥tima, ent√£o $v_{\pi_{i+1}}(s) > v_{\pi_i}(s)$ para pelo menos um estado $s$.

III. **Aus√™ncia de Ciclos:** Como cada itera√ß√£o produz uma pol√≠tica estritamente melhor (a menos que a pol√≠tica atual seja √≥tima), a Policy Iteration nunca pode revisitar uma pol√≠tica previamente avaliada. Se o algoritmo retornasse a uma pol√≠tica anterior, isso implicaria que a pol√≠tica anterior era tanto melhor quanto pior que a pol√≠tica atual, o que √© uma contradi√ß√£o.

IV. **Converg√™ncia:** Dado que o n√∫mero de pol√≠ticas √© finito (I) e o algoritmo melhora monotonicamente a pol√≠tica a cada itera√ß√£o sem revisitar pol√≠ticas anteriores (II e III), a Policy Iteration deve convergir para uma pol√≠tica √≥tima em um n√∫mero finito de itera√ß√µes. O algoritmo para quando n√£o consegue mais encontrar uma pol√≠tica melhor, o que significa que atingiu uma pol√≠tica √≥tima.

Em resumo, a converg√™ncia finita da Policy Iteration √© garantida pelo espa√ßo de pol√≠ticas finito e pela melhoria monot√¥nica assegurada pelo Policy Improvement Theorem. $\blacksquare$

#### Algoritmo da Policy Iteration

O algoritmo completo da Policy Iteration pode ser expresso como [^80]:

1.  **Inicializa√ß√£o**: Escolha uma pol√≠tica arbitr√°ria $\pi_0$ e uma fun√ß√£o valor inicial $V_0(s)$ para todo $s \in S$.

2.  **Itera√ß√£o**:
    *   **Policy Evaluation**: Dado $\pi_i$, calcule $V_i(s) = v_{\pi_i}(s)$ para todo $s \in S$.
    *   **Policy Improvement**: Crie uma nova pol√≠tica $\pi_{i+1}$ tal que $\pi_{i+1}(s) = \arg \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma V_i(s')]$.

3.  **Teste de Converg√™ncia**: Se $\pi_{i+1} = \pi_i$, ent√£o $\pi_i$ √© uma pol√≠tica √≥tima e $V_i$ √© a fun√ß√£o valor √≥tima $v_{*}$. Caso contr√°rio, repita o passo 2.

![Policy Iteration algorithm: iterative process of policy evaluation and improvement for optimal policy estimation.](./../images/image3.png)

**Observa√ß√£o:** O passo de Policy Evaluation pode ser realizado at√© a converg√™ncia exata de $V_i(s)$, ou pode ser interrompido ap√≥s um n√∫mero fixo de itera√ß√µes. A converg√™ncia exata garante a melhoria estrita, mas pode ser computacionalmente cara. Interromper a avalia√ß√£o antecipadamente ainda garante melhoria, mas pode requerer mais itera√ß√µes da Policy Iteration para convergir para a pol√≠tica √≥tima.

> üí° **Exemplo Num√©rico:**
>
> Vamos simplificar ainda mais. Digamos que temos um MDP com dois estados, $s_1$ e $s_2$, e duas a√ß√µes, $a_1$ e $a_2$. As recompensas s√£o determin√≠sticas:
>
> *   Em $s_1$:
>     *   Tomar $a_1$ leva a $s_2$ com recompensa 1.
>     *   Tomar $a_2$ leva a $s_1$ com recompensa 0.
> *   Em $s_2$:
>     *   Tomar $a_1$ leva a $s_1$ com recompensa 1.
>     *   Tomar $a_2$ leva a $s_2$ com recompensa 0.
>
> Seja $\gamma = 0.9$.
>
> **Inicializa√ß√£o:** $\pi_0(s_1) = a_1$, $\pi_0(s_2) = a_1$. Inicializamos tamb√©m $V_0(s_1) = 0$, $V_0(s_2) = 0$.
>
> **Itera√ß√£o 1:**
>
> *   **Policy Evaluation:** Calculamos $V_1$ usando a equa√ß√£o de Bellman iterativamente. Ap√≥s algumas itera√ß√µes (ou converg√™ncia exata), suponha que obtemos $V_1(s_1) \approx 8.13$ e $V_1(s_2) \approx 9.04$.
> *   **Policy Improvement:**
>     *   Para $s_1$, calculamos $q_{\pi_0}(s_1, a_1)$ e $q_{\pi_0}(s_1, a_2)$.
>         *   $q_{\pi_0}(s_1, a_1) = 1 + 0.9 \times V_1(s_2) = 1 + 0.9 \times 9.04 \approx 9.14$.
>         *   $q_{\pi_0}(s_1, a_2) = 0 + 0.9 \times V_1(s_1) = 0 + 0.9 \times 8.13 \approx 7.32$.
>         *   Como $q_{\pi_0}(s_1, a_1) > q_{\pi_0}(s_1, a_2)$,  $\pi_1(s_1) = a_1$.
>     *   Para $s_2$, calculamos $q_{\pi_0}(s_2, a_1)$ e $q_{\pi_0}(s_2, a_2)$.
>         *   $q_{\pi_0}(s_2, a_1) = 1 + 0.9 \times V_1(s_1) = 1 + 0.9 \times 8.13 \approx 8.32$.
>         *   $q_{\pi_0}(s_2, a_2) = 0 + 0.9 \times V_1(s_2) = 0 + 0.9 \times 9.04 \approx 8.14$.
>         *   Como $q_{\pi_0}(s_2, a_1) > q_{\pi_0}(s_2, a_2)$,  $\pi_1(s_2) = a_1$.
>
> Portanto, $\pi_1(s_1) = a_1$ e $\pi_1(s_2) = a_1$. Neste caso, a pol√≠tica n√£o mudou ( $\pi_1 = \pi_0$), indicando que possivelmente j√° atingimos a pol√≠tica √≥tima.
>
> **Teste de Converg√™ncia:** $\pi_1 = \pi_0$, ent√£o $\pi_0$ (ou $\pi_1$) √© uma pol√≠tica √≥tima.

#### Exemplo Pr√°tico: Gridworld

Considere o exemplo do Gridworld [^76]. A Policy Iteration come√ßa com uma pol√≠tica aleat√≥ria equiprov√°vel. A cada itera√ß√£o, a fun√ß√£o valor da pol√≠tica atual √© avaliada e uma nova pol√≠tica *greedy* √© criada. Este processo converge rapidamente para a pol√≠tica √≥tima, onde o agente sempre toma o caminho mais curto para o estado terminal, maximizando assim a recompensa acumulada.

![Illustration of a 4x4 gridworld environment with rewards and actions for dynamic programming example.](./../images/image9.png)

![Convergence of iterative policy evaluation on a gridworld, showing improvement from random to optimal policy.](./../images/image7.png)

**Exemplo:** Em um Gridworld simples com recompensas -1 para cada passo e um estado terminal com recompensa 0, a Policy Iteration rapidamente converge para a pol√≠tica √≥tima, mesmo come√ßando com uma pol√≠tica aleat√≥ria. A velocidade de converg√™ncia depende do fator de desconto $\gamma$. Quanto menor o $\gamma$, mais r√°pida a converg√™ncia, pois o agente se preocupa menos com as recompensas futuras.

> üí° **Exemplo Num√©rico:**
>
> Imagine um 4x4 Gridworld. Cada c√©lula representa um estado. As a√ß√µes poss√≠veis s√£o Norte, Sul, Leste e Oeste. Mover-se para fora do grid resulta em permanecer no mesmo estado e receber uma recompensa de -1. O estado terminal est√° no canto inferior direito (3,3) com recompensa 0.  Todas as outras transi√ß√µes t√™m recompensa -1.  Se come√ßarmos com uma pol√≠tica aleat√≥ria e $\gamma = 0.9$, a Policy Iteration ir√° iterativamente melhorar a pol√≠tica. Ap√≥s a primeira Policy Evaluation, estados pr√≥ximos ao terminal ter√£o valores mais altos.  Na Policy Improvement, as a√ß√µes que direcionam o agente para o terminal ser√£o favorecidas. Ap√≥s algumas itera√ß√µes, todos os estados ter√£o uma pol√≠tica que leva o agente diretamente para o estado terminal. Para $\gamma=0.1$, a converg√™ncia seria ainda mais r√°pida, pois o agente priorizaria obter recompensas mais imediatas, logo, o n√∫mero de itera√ß√µes para alcan√ßar a pol√≠tica √≥tima diminuiria.

### Conclus√£o

A garantia de melhoria estrita e a converg√™ncia finita s√£o caracter√≠sticas cruciais da Policy Iteration, tornando-a um m√©todo confi√°vel para encontrar pol√≠ticas √≥timas em MDPs finitos. Apesar da sua natureza computacionalmente intensiva, a Policy Iteration fornece uma base te√≥rica s√≥lida para o desenvolvimento de outros algoritmos de Reinforcement Learning, incluindo aqueles que lidam com problemas maiores e mais complexos. Os m√©todos que exploraremos posteriormente visam alcan√ßar o mesmo efeito da Policy Iteration, mas com menor custo computacional e sem a necessidade de um modelo perfeito do ambiente [^76].

### Refer√™ncias
[^74]: Chapter 4: Dynamic Programming, p. 74
[^76]: Chapter 4: Dynamic Programming, p. 76
[^78]: Chapter 4: Dynamic Programming, p. 78
[^79]: Chapter 4: Dynamic Programming, p. 79
[^80]: Chapter 4: Dynamic Programming, p. 80
<!-- END -->