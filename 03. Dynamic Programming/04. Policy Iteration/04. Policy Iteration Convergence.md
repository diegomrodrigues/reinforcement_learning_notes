## 4.3.1 Convergence of Policy Iteration and Handling Policy Cycling

### Introdu√ß√£o
O algoritmo de **Policy Iteration** [^80] √© uma abordagem cl√°ssica para encontrar a pol√≠tica √≥tima em um processo de decis√£o de Markov (MDP) finito. Ele alterna entre duas fases principais: **Policy Evaluation**, onde a fun√ß√£o de valor da pol√≠tica atual √© calculada iterativamente, e **Policy Improvement**, onde a pol√≠tica √© aprimorada de forma gulosa em rela√ß√£o √† fun√ß√£o de valor. Conforme explorado na se√ß√£o anterior, esse processo leva a uma sequ√™ncia de pol√≠ticas monotonicamente melhores, convergindo para uma pol√≠tica √≥tima em um n√∫mero finito de itera√ß√µes, dado um MDP finito determin√≠stico. Esta se√ß√£o ir√° focar nas condi√ß√µes de converg√™ncia e em uma potencial "falha" de design que impede o algoritmo de atingir a converg√™ncia, e como corrigir tal problema.

### Converg√™ncia e Estabilidade da Pol√≠tica
Como visto anteriormente, a **Policy Iteration** [^80] √© garantida para convergir para uma pol√≠tica e fun√ß√£o de valor √≥timas em um n√∫mero finito de itera√ß√µes. Isso ocorre porque:
1.  Cada pol√≠tica √© garantidamente uma melhoria estrita sobre a anterior (a menos que j√° seja √≥tima).
2.  Um MDP finito tem apenas um n√∫mero finito de pol√≠ticas determin√≠sticas [^80].

Portanto, o processo de **Policy Iteration** [^80] deve convergir para uma pol√≠tica e fun√ß√£o de valor √≥timas em um n√∫mero finito de itera√ß√µes. A converg√™ncia √© atingida quando a pol√≠tica se torna *est√°vel*, ou seja, quando a fase de **Policy Improvement** [^80] n√£o resulta em nenhuma mudan√ßa na pol√≠tica [^80].

![Policy Iteration algorithm: iterative process of policy evaluation and improvement for optimal policy estimation.](./../images/image3.png)

Para formalizar essa no√ß√£o de estabilidade, podemos introduzir a seguinte defini√ß√£o:

**Defini√ß√£o 1 (Pol√≠tica Est√°vel):** Uma pol√≠tica œÄ √© considerada est√°vel se, para todo estado s ‚àà S, a a√ß√£o selecionada pela pol√≠tica œÄ(s) √© uma a√ß√£o √≥tima em rela√ß√£o √† fun√ß√£o de valor VœÄ. Formalmente, œÄ √© est√°vel se e somente se:

`œÄ(s) = argmax‚Çê  ‚àë‚Çõ',·µ£ p(s', r | s, a) [r + Œ≥VœÄ(s')]  ‚àÄ s ‚àà S`

Al√©m disso, a estabilidade da pol√≠tica est√° diretamente relacionada √† otimalidade da fun√ß√£o de valor.

**Lema 1:** Se uma pol√≠tica œÄ √© est√°vel, ent√£o sua fun√ß√£o de valor associada VœÄ √© a fun√ß√£o de valor √≥tima V*.

*Prova:* Se œÄ √© est√°vel, ent√£o satisfaz a equa√ß√£o de Bellman para a pol√≠tica √≥tima. Portanto, VœÄ = V*.

Para provar o Lema 1, podemos detalhar os passos:

I. Assumimos que a pol√≠tica $\pi$ √© est√°vel, o que significa que para todo estado $s \in S$:
   $$\pi(s) = \underset{a}{\operatorname{argmax}} \sum_{s', r} p(s', r | s, a) [r + \gamma V^{\pi}(s')]$$

II. Isso implica que para qualquer estado $s$ e qualquer a√ß√£o $a$:
    $$V^{\pi}(s) = \sum_{s', r} p(s', r | s, \pi(s)) [r + \gamma V^{\pi}(s')] \geq \sum_{s', r} p(s', r | s, a) [r + \gamma V^{\pi}(s')]$$

III. A desigualdade acima mostra que $V^{\pi}(s)$ satisfaz a equa√ß√£o de Bellman otimizada para todos os estados. A fun√ß√£o de valor √≥tima $V^{*}(s)$ tamb√©m satisfaz a equa√ß√£o de Bellman otimizada:
     $$V^{*}(s) = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma V^{*}(s')] $$

IV. Como $V^{\pi}(s)$ satisfaz a equa√ß√£o de Bellman otimizada, ent√£o $V^{\pi}(s)$ deve ser igual a $V^{*}(s)$ para todos os estados $s$. Isso ocorre porque a equa√ß√£o de Bellman otimizada tem uma solu√ß√£o √∫nica, que √© a fun√ß√£o de valor √≥tima.

V. Portanto, se a pol√≠tica $\pi$ √© est√°vel, ent√£o sua fun√ß√£o de valor associada $V^{\pi}$ √© a fun√ß√£o de valor √≥tima $V^{*}$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP com dois estados, $S = \{s_1, s_2\}$, e duas a√ß√µes em cada estado, $A = \{a_1, a_2\}$. A fun√ß√£o de recompensa e as probabilidades de transi√ß√£o s√£o as seguintes (com $\gamma = 0.9$):
>
> *   **Estado** $s_1$:
>     *   A√ß√£o $a_1$: $p(s_1, r=1 | s_1, a_1) = 0.8$, $p(s_2, r=1 | s_1, a_1) = 0.2$
>     *   A√ß√£o $a_2$: $p(s_1, r=0 | s_1, a_2) = 0.5$, $p(s_2, r=0 | s_1, a_2) = 0.5$
> *   **Estado** $s_2$:
>     *   A√ß√£o $a_1$: $p(s_1, r=2 | s_2, a_1) = 0.6$, $p(s_2, r=2 | s_2, a_1) = 0.4$
>     *   A√ß√£o $a_2$: $p(s_1, r=0 | s_2, a_2) = 0.1$, $p(s_2, r=0 | s_2, a_2) = 0.9$
>
> Inicializamos a pol√≠tica $\pi$ de forma arbitr√°ria: $\pi(s_1) = a_1$, $\pi(s_2) = a_2$.
>
> **Policy Evaluation (primeira itera√ß√£o):**
>
> Resolvemos o sistema de equa√ß√µes de Bellman para encontrar $V^{\pi}$:
>
> $V^{\pi}(s_1) = \sum_{s', r} p(s', r | s_1, a_1) [r + \gamma V^{\pi}(s')] = 0.8(1 + 0.9V^{\pi}(s_1)) + 0.2(1 + 0.9V^{\pi}(s_2))$
> $V^{\pi}(s_2) = \sum_{s', r} p(s', r | s_2, a_2) [r + \gamma V^{\pi}(s')] = 0.1(0 + 0.9V^{\pi}(s_1)) + 0.9(0 + 0.9V^{\pi}(s_2))$
>
> Resolvendo, encontramos (aproximadamente) $V^{\pi}(s_1) = 4.21$ e $V^{\pi}(s_2) = 0.53$.
>

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

> **Policy Improvement (primeira itera√ß√£o):**
>
> Para o estado $s_1$:
> *   A√ß√£o $a_1$: $Q(s_1, a_1) = 0.8(1 + 0.9 \cdot 4.21) + 0.2(1 + 0.9 \cdot 0.53) = 4.00$
> *   A√ß√£o $a_2$: $Q(s_1, a_2) = 0.5(0 + 0.9 \cdot 4.21) + 0.5(0 + 0.9 \cdot 0.53) = 2.12$
>
> Para o estado $s_2$:
> *   A√ß√£o $a_1$: $Q(s_2, a_1) = 0.6(2 + 0.9 \cdot 4.21) + 0.4(2 + 0.9 \cdot 0.53) = 5.86$
> *   A√ß√£o $a_2$: $Q(s_2, a_2) = 0.1(0 + 0.9 \cdot 4.21) + 0.9(0 + 0.9 \cdot 0.53) = 0.89$
>
> Nova pol√≠tica: $\pi(s_1) = a_1$, $\pi(s_2) = a_1$. Note que a a√ß√£o em $s_2$ mudou.
>
> Repetimos **Policy Evaluation** e **Policy Improvement** at√© a pol√≠tica convergir (estabilizar).

### Potencial Bug: Ciclo de Pol√≠ticas e Solu√ß√µes
A converg√™ncia garantida de **Policy Iteration** [^80] depende crucialmente da garantia de que cada itera√ß√£o produz uma melhoria estrita na pol√≠tica. No entanto, pode haver casos onde v√°rias a√ß√µes em um estado resultem no mesmo valor esperado, levando a empates na fase de **Policy Improvement** [^79].

Em tais cen√°rios, o algoritmo pode come√ßar a alternar entre duas ou mais pol√≠ticas que s√£o igualmente boas [^82]. Isso significa que, em vez de convergir para uma √∫nica pol√≠tica √≥tima, o algoritmo entra em um ciclo, alternando entre pol√≠ticas sub√≥timas e igualmente boas [^82]. Esta situa√ß√£o impede a termina√ß√£o do algoritmo, pois a condi√ß√£o de estabilidade da pol√≠tica nunca √© satisfeita.

*√â importante observar que o ciclo de pol√≠ticas n√£o contradiz a garantia de melhoria da pol√≠tica*. As pol√≠ticas no ciclo s√£o igualmente boas, de forma que n√£o h√° melhoria estrita. No entanto, impede que o algoritmo converja para uma √∫nica pol√≠tica √≥tima e termine.

Para ilustrar o ciclo de pol√≠ticas, considere um cen√°rio onde duas a√ß√µes, a1 e a2, levam ao mesmo valor esperado em um determinado estado s. O algoritmo pode alternar entre escolher a1 e a2 indefinidamente, sem nunca convergir para uma pol√≠tica est√°vel.

> üí° **Exemplo Num√©rico:**
>
> Imagine um estado $s$ com duas a√ß√µes $a_1$ e $a_2$. Ap√≥s a avalia√ß√£o da pol√≠tica, descobre-se que $Q(s, a_1) = 5.0$ e $Q(s, a_2) = 5.0$. Sem um crit√©rio de desempate, o algoritmo pode escolher $a_1$ em uma itera√ß√£o e $a_2$ na pr√≥xima, se a implementa√ß√£o escolher aleatoriamente entre a√ß√µes com o mesmo valor. Se isso se repetir, o algoritmo ciclar√°.

### Garantindo a Converg√™ncia: Modificando o Pseudoc√≥digo
Para garantir a converg√™ncia e evitar o ciclo de pol√≠ticas, o pseudoc√≥digo do algoritmo de **Policy Iteration** [^80] precisa ser modificado. Uma abordagem comum √© introduzir um crit√©rio de desempate que favore√ßa a pol√≠tica anterior quando v√°rios estados-a√ß√£o s√£o encontrados com o mesmo valor. Isso pode ser implementado da seguinte maneira:

Modifique a etapa 3 do algoritmo **Policy Iteration** [^80] (Policy Improvement) para:

1. `policy-stable ‚Üê true`
2. `For each s ‚àà S:`
    *  `old-action ‚Üê œÄ(s)`
    *  `œÄ(s) ‚Üê argmax‚Çê  ‚àë‚Çõ',·µ£ p(s', r | s, a) [r + Œ≥V(s')]`
    *  `If  ‚àë‚Çõ',·µ£ p(s', r | s, œÄ(s)) [r + Œ≥V(s')] == ‚àë‚Çõ',·µ£ p(s', r | s, old-action) [r + Œ≥V(s')]`
        *   `œÄ(s) ‚Üê old-action`
    *  `If old-action ‚â† œÄ(s), then policy-stable ‚Üê false`
3.  `If policy-stable, then stop and return V ‚âà v* and œÄ ‚âà œÄ*; else go to 2`

Neste pseudoc√≥digo modificado, antes de atualizar a pol√≠tica, o somat√≥rio de valores para o estado e a√ß√£o s√£o comparados com os valores da itera√ß√£o anterior. Se os valores forem iguais, a pol√≠tica √© atualizada com a a√ß√£o anterior.

Outra poss√≠vel modifica√ß√£o seria criar uma ordena√ß√£o fixa e arbitr√°ria das a√ß√µes e usar essa ordena√ß√£o para desempate [^79]. Essa abordagem garante uma escolha determin√≠stica em caso de empate, evitando a oscila√ß√£o entre pol√≠ticas igualmente boas. Podemos formalizar essa alternativa da seguinte forma:

**Proposi√ß√£o 1 (Desempate por Ordena√ß√£o Fixa):** Dada uma ordena√ß√£o fixa e arbitr√°ria das a√ß√µes para cada estado, o algoritmo de Policy Iteration modificado para usar essa ordena√ß√£o para desempate converge para uma pol√≠tica √≥tima em um n√∫mero finito de itera√ß√µes.

*Prova (Esbo√ßo):* A introdu√ß√£o de uma ordena√ß√£o fixa garante que a escolha da a√ß√£o em cada estado seja determin√≠stica. Isso elimina a possibilidade de ciclos entre pol√≠ticas igualmente boas, assegurando que cada itera√ß√£o resulte em uma melhoria (ou, no m√≠nimo, n√£o em uma piora) na pol√≠tica. Como o n√∫mero de pol√≠ticas poss√≠veis √© finito, o algoritmo eventualmente convergir√° para uma pol√≠tica √≥tima.

Para detalhar a prova da Proposi√ß√£o 1:

I. Seja $A(s)$ o conjunto de a√ß√µes dispon√≠veis no estado $s$, e seja $<$ uma ordena√ß√£o arbitr√°ria e fixa dessas a√ß√µes.

II. No passo de melhoria da pol√≠tica, modificamos o algoritmo para selecionar a a√ß√£o $a \in A(s)$ que maximiza o valor esperado, usando a ordena√ß√£o $<$ para quebrar empates:
   $$\pi(s) = \underset{a \in A(s)}{\operatorname{argmax}} \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')] $$
   onde, se houver empate (isto √©, m√∫ltiplas a√ß√µes maximizando a express√£o), selecionamos a a√ß√£o menor de acordo com a ordena√ß√£o $<$.

III. Com essa modifica√ß√£o, a atualiza√ß√£o da pol√≠tica torna-se determin√≠stica. Dado $V(s)$, existe uma √∫nica pol√≠tica $\pi$ que ser√° selecionada.

IV. O algoritmo de Policy Iteration gera uma sequ√™ncia de pol√≠ticas $\{\pi_k\}_{k=0}^{\infty}$ e fun√ß√µes de valor $\{V_k\}_{k=0}^{\infty}$. Como o espa√ßo de pol√≠ticas √© finito, e cada itera√ß√£o melhora ou mant√©m a mesma pol√≠tica, eventualmente o algoritmo deve convergir para uma pol√≠tica √≥tima.

V. Para provar que cada itera√ß√£o melhora ou mant√©m a mesma pol√≠tica, considere duas pol√≠ticas consecutivas $\pi_k$ e $\pi_{k+1}$. Pela constru√ß√£o do algoritmo, para cada estado $s$:
   $$\sum_{s', r} p(s', r | s, \pi_{k+1}(s)) [r + \gamma V_k(s')] \geq \sum_{s', r} p(s', r | s, \pi_{k}(s)) [r + \gamma V_k(s')] = V_k(s)$$
   A desigualdade decorre da etapa de melhoria da pol√≠tica, e a igualdade da defini√ß√£o de $V_k(s)$.

VI. Usando o teorema de melhoria da pol√≠tica, sabemos que $V_{k+1}(s) \geq V_k(s)$ para todos os estados $s$. Portanto, cada itera√ß√£o garante uma melhoria ou manuten√ß√£o da pol√≠tica.

VII. Uma vez que o n√∫mero de pol√≠ticas √© finito e cada itera√ß√£o melhora ou mant√©m a mesma pol√≠tica, o algoritmo deve convergir em um n√∫mero finito de itera√ß√µes para uma pol√≠tica √≥tima. ‚ñ†

Ao modificar o algoritmo desta forma, garantimos que ele ir√° parar e encontrar a pol√≠tica ideal.

> üí° **Exemplo Num√©rico:**
>
> Suponha que tenhamos tr√™s a√ß√µes $a_1, a_2, a_3$ dispon√≠veis em um estado $s$, e definimos a ordena√ß√£o fixa como $a_1 < a_2 < a_3$. Se $Q(s, a_1) = 6.0$, $Q(s, a_2) = 6.0$, e $Q(s, a_3) = 5.5$, o algoritmo sempre escolher√° $a_1$ devido √† ordena√ß√£o fixa, quebrando o empate de forma consistente.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Par√¢metros do MDP
> num_states = 2
> num_actions = 2
> gamma = 0.9
>
> # Inicializa√ß√£o aleat√≥ria da fun√ß√£o de valor e da pol√≠tica
> value_function = np.zeros(num_states)
> policy = np.random.randint(0, num_actions, num_states)
>
> # Matriz de transi√ß√£o e recompensas (exemplo)
> transition_probs = np.random.rand(num_states, num_actions, num_states)
> transition_probs /= transition_probs.sum(axis=2, keepdims=True)  # Normaliza para ser uma distribui√ß√£o de probabilidade
> rewards = np.random.rand(num_states, num_actions)
>
> def policy_evaluation(policy, value_function, transition_probs, rewards, gamma, num_states, num_actions, tolerance=1e-6):
>     """Avalia√ß√£o da pol√≠tica usando itera√ß√£o."""
>     while True:
>         delta = 0
>         for s in range(num_states):
>             old_value = value_function[s]
>             action = policy[s]
>             new_value = sum(transition_probs[s, action, s_prime] * (rewards[s, action] + gamma * value_function[s_prime]) for s_prime in range(num_states))
>             value_function[s] = new_value
>             delta = max(delta, abs(new_value - old_value))
>         if delta < tolerance:
>             break
>     return value_function
>
> def policy_improvement(value_function, transition_probs, rewards, gamma, num_states, num_actions):
>     """Melhoria da pol√≠tica de forma gulosa."""
>     new_policy = np.zeros(num_states, dtype=int)
>     policy_stable = True
>     for s in range(num_states):
>         old_action = policy[s]
>         q_values = [sum(transition_probs[s, a, s_prime] * (rewards[s, a] + gamma * value_function[s_prime]) for s_prime in range(num_actions)) for a in range(num_actions)]
>         best_action = np.argmax(q_values)
>         new_policy[s] = best_action
>         if old_action != best_action:
>             policy_stable = False
>     return new_policy, policy_stable
>
> # Policy Iteration
> num_iterations = 100
> for i in range(num_iterations):
>     value_function = policy_evaluation(policy, value_function, transition_probs, rewards, gamma, num_states, num_actions)
>     new_policy, policy_stable = policy_improvement(value_function, transition_probs, rewards, gamma, num_states, num_actions)
>     if policy_stable:
>         print(f"Pol√≠tica convergeu ap√≥s {i+1} itera√ß√µes.")
>         break
>     policy = new_policy
> else:
>     print("Pol√≠tica n√£o convergeu dentro do n√∫mero m√°ximo de itera√ß√µes.")
>
> print("Pol√≠tica √ìtima:", policy)
> print("Fun√ß√£o de Valor √ìtima:", value_function)
> ```
>
> Este c√≥digo simula a Policy Iteration com dois estados e duas a√ß√µes, mostrando como a pol√≠tica converge para um estado est√°vel ap√≥s v√°rias itera√ß√µes. Os resultados podem variar devido √† inicializa√ß√£o aleat√≥ria das matrizes de transi√ß√£o e recompensas.

### Conclus√£o
A converg√™ncia do algoritmo de **Policy Iteration** [^80] √© uma propriedade fundamental que garante sua utilidade na solu√ß√£o de MDPs finitos [^80]. No entanto, deve-se ter cuidado para evitar o ciclo de pol√≠ticas, que pode impedir a termina√ß√£o do algoritmo. Ao modificar o pseudoc√≥digo e introduzir crit√©rios de desempate consistentes, a converg√™ncia pode ser garantida, tornando o algoritmo mais robusto e confi√°vel para aplica√ß√µes pr√°ticas.

### Refer√™ncias
[^79]: Se√ß√£o 4.2 do texto original
[^80]: Se√ß√£o 4.3 do texto original
[^82]: Exerc√≠cio 4.4 do texto original
<!-- END -->