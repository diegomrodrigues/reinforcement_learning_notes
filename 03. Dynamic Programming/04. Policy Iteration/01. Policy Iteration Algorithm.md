## Policy Iteration: Alternating Policy Evaluation and Improvement

### Introdu√ß√£o
Como discutido anteriormente, a **programa√ß√£o din√¢mica (DP)** oferece uma cole√ß√£o de algoritmos para calcular pol√≠ticas √≥timas dado um modelo perfeito do ambiente como um processo de decis√£o de Markov (MDP) [^1]. Embora os algoritmos DP cl√°ssicos tenham utilidade limitada no aprendizado por refor√ßo devido √† sua suposi√ß√£o de um modelo perfeito e ao seu grande custo computacional, eles fornecem uma base essencial para entender os m√©todos apresentados [^1]. Este cap√≠tulo explora a **itera√ß√£o de pol√≠tica**, um algoritmo fundamental de DP que alterna entre a avalia√ß√£o da pol√≠tica (computando $v_\pi$ para uma dada pol√≠tica $\pi$) e a melhoria da pol√≠tica (encontrando uma pol√≠tica melhor $\pi'$ usando $v_\pi$) para obter uma sequ√™ncia de pol√≠ticas e fun√ß√µes de valor monotonicamente crescentes [^1].

### Conceitos Fundamentais
A itera√ß√£o de pol√≠tica √© um algoritmo iterativo que encontra uma pol√≠tica √≥tima para um MDP finito. O algoritmo funciona alternando entre duas etapas principais: **avalia√ß√£o da pol√≠tica** e **melhoria da pol√≠tica**.

**1. Avalia√ß√£o da Pol√≠tica (Prediction):**
Dado uma pol√≠tica $\pi$, a avalia√ß√£o da pol√≠tica calcula a fun√ß√£o de valor de estado $v_\pi(s)$ para todos os estados $s \in \mathcal{S}$. A fun√ß√£o de valor de estado representa o retorno esperado ao seguir a pol√≠tica $\pi$ a partir do estado $s$ [^1].
Como visto anteriormente, podemos calcular $v_\pi$ iterativamente usando a equa√ß√£o de Bellman para $v_\pi$ [^1]:
$$
v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s] = \sum_{a} \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_{\pi}(s')] \quad \forall s \in \mathcal{S}
$$
onde $\pi(a|s)$ √© a probabilidade de tomar a a√ß√£o $a$ no estado $s$ sob a pol√≠tica $\pi$, $p(s', r|s, a)$ √© a probabilidade de transi√ß√£o para o estado $s'$ com recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$, e $\gamma$ √© o fator de desconto [^1].

> üí° **Exemplo Num√©rico:**
> Considere um MDP com 3 estados (S1, S2, S3) e 2 a√ß√µes (A1, A2) em cada estado. Seja $\gamma = 0.9$. Suponha que temos uma pol√≠tica $\pi$ que sempre escolhe A1 em todos os estados: $\pi(A1|S1) = \pi(A1|S2) = \pi(A1|S3) = 1$.
> As probabilidades de transi√ß√£o e recompensas s√£o dadas pelas seguintes tabelas:
>
> | s   | a   | s'  | r   | p(s', r | s, a) |
> |-----|-----|-----|-----|-----------------|
> | S1  | A1  | S2  | 5   | 0.8             |
> | S1  | A1  | S3  | 2   | 0.2             |
> | S2  | A1  | S1  | -1  | 0.6             |
> | S2  | A1  | S2  | 3   | 0.4             |
> | S3  | A1  | S3  | 10  | 1.0             |
>
> Inicializamos $v_0(S1) = 0$, $v_0(S2) = 0$, $v_0(S3) = 0$.
>
> Usando a equa√ß√£o de Bellman iterativamente:
> $v_{k+1}(s) = \sum_{a} \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_k(s')]$
>
> Itera√ß√£o 1:
> $v_1(S1) = 1 * (0.8 * (5 + 0.9 * 0) + 0.2 * (2 + 0.9 * 0)) = 1 * (0.8 * 5 + 0.2 * 2) = 4.4$
> $v_1(S2) = 1 * (0.6 * (-1 + 0.9 * 0) + 0.4 * (3 + 0.9 * 0)) = 1 * (0.6 * -1 + 0.4 * 3) = 0.6$
> $v_1(S3) = 1 * (1.0 * (10 + 0.9 * 0)) = 10$
>
> Itera√ß√£o 2:
> $v_2(S1) = 1 * (0.8 * (5 + 0.9 * 0.6) + 0.2 * (2 + 0.9 * 10)) = 0.8 * (5 + 0.54) + 0.2 * (2 + 9) = 0.8 * 5.54 + 0.2 * 11 = 4.432 + 2.2 = 6.632$
> $v_2(S2) = 1 * (0.6 * (-1 + 0.9 * 4.4) + 0.4 * (3 + 0.9 * 0.6)) = 0.6 * (-1 + 3.96) + 0.4 * (3 + 0.54) = 0.6 * 2.96 + 0.4 * 3.54 = 1.776 + 1.416 = 3.192$
> $v_2(S3) = 1 * (1.0 * (10 + 0.9 * 10)) = 19$
>
> Itera√ß√£o 3:
> $v_3(S1) = 1 * (0.8 * (5 + 0.9 * 3.192) + 0.2 * (2 + 0.9 * 19)) = 0.8 * (5 + 2.8728) + 0.2 * (2 + 17.1) = 0.8 * 7.8728 + 0.2 * 19.1 = 6.29824 + 3.82 = 10.11824$
> $v_3(S2) = 1 * (0.6 * (-1 + 0.9 * 6.632) + 0.4 * (3 + 0.9 * 3.192)) = 0.6 * (-1 + 5.9688) + 0.4 * (3 + 2.8728) = 0.6 * 4.9688 + 0.4 * 5.8728 = 2.98128 + 2.34912 = 5.3304$
> $v_3(S3) = 1 * (1.0 * (10 + 0.9 * 19)) = 10 + 17.1 = 27.1$
>
> Essas itera√ß√µes continuam at√© que a mudan√ßa nos valores de estado entre itera√ß√µes seja menor que um limiar $\theta$.

O algoritmo de **avalia√ß√£o iterativa da pol√≠tica** utiliza uma sequ√™ncia de fun√ß√µes de valor aproximadas $v_0, v_1, v_2, \ldots$, cada uma mapeando $\mathcal{S}^+$ para $\mathbb{R}$ (os n√∫meros reais). A aproxima√ß√£o inicial, $v_0$, √© escolhida arbitrariamente (exceto que o estado terminal, se houver, deve receber o valor 0), e cada aproxima√ß√£o sucessiva √© obtida usando a equa√ß√£o de Bellman para $v_\pi$ como uma regra de atualiza√ß√£o [^1]:

$$
v_{k+1}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] = \sum_{a} \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_k(s')] \quad \forall s \in \mathcal{S}
$$

A itera√ß√£o continua at√© que a fun√ß√£o de valor convirja, ou seja, at√© que a mudan√ßa m√°xima na fun√ß√£o de valor entre itera√ß√µes seja menor que um pequeno limiar $\theta$ [^1].





![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)



**Lema 1:** *A sequ√™ncia de fun√ß√µes de valor $\{v_k\}$ gerada pela avalia√ß√£o iterativa da pol√≠tica converge para $v_\pi$*.

*Proof:* A prova pode ser encontrada em [^1, se√ß√£o 4.1], e se baseia na demonstra√ß√£o de que a atualiza√ß√£o de Bellman √© uma contra√ß√£o em rela√ß√£o √† norma do supremo, garantindo converg√™ncia para a solu√ß√£o √∫nica da equa√ß√£o de Bellman.

**2. Melhoria da Pol√≠tica:**
Uma vez que a fun√ß√£o de valor de estado $v_\pi$ √© calculada, a etapa de melhoria da pol√≠tica procura uma nova pol√≠tica $\pi'$ que seja melhor que a pol√≠tica original $\pi$ [^4]. A ideia √© agir *greedy* em rela√ß√£o √† fun√ß√£o de valor $v_\pi$ [^5]. Definimos $q_\pi(s, a)$ como o valor de tomar a a√ß√£o $a$ no estado $s$ e, posteriormente, seguir a pol√≠tica $\pi$ [^6]:

$$
q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a] = \sum_{s',r} p(s', r|s, a) [r + \gamma v_{\pi}(s')]
$$

> üí° **Exemplo Num√©rico:**
> Usando os valores obtidos da itera√ß√£o 3 no exemplo anterior:
> $v_3(S1) = 10.11824$, $v_3(S2) = 5.3304$, $v_3(S3) = 27.1$ e $\gamma = 0.9$.
>
> Agora, vamos calcular $q_\pi(s, a)$ para cada estado e a√ß√£o.
>
> | s   | a   | s'  | r   | p(s', r | s, a) |
> |-----|-----|-----|-----|-----------------|
> | S1  | A1  | S2  | 5   | 0.8             |
> | S1  | A1  | S3  | 2   | 0.2             |
> | S1  | A2  | S1  | -2  | 0.5             |
> | S1  | A2  | S2  | 1   | 0.5             |
> | S2  | A1  | S1  | -1  | 0.6             |
> | S2  | A1  | S2  | 3   | 0.4             |
> | S2  | A2  | S2  | 4   | 0.7             |
> | S2  | A2  | S3  | 6   | 0.3             |
> | S3  | A1  | S3  | 10  | 1.0             |
> | S3  | A2  | S1  | 8   | 0.9             |
> | S3  | A2  | S2  | 3   | 0.1             |
>
> $q_\pi(S1, A1) = 0.8 * (5 + 0.9 * 5.3304) + 0.2 * (2 + 0.9 * 27.1) = 0.8 * 9.79736 + 0.2 * 26.39 = 7.837888 + 5.278 = 13.115888$
> $q_\pi(S1, A2) = 0.5 * (-2 + 0.9 * 10.11824) + 0.5 * (1 + 0.9 * 5.3304) = 0.5 * 7.106416 + 0.5 * 5.79736 = 3.553208 + 2.89868 = 6.451888$
> $q_\pi(S2, A1) = 0.6 * (-1 + 0.9 * 10.11824) + 0.4 * (3 + 0.9 * 5.3304) = 0.6 * 8.106416 + 0.4 * 7.79736 = 4.8638496 + 3.118944 = 7.9827936$
> $q_\pi(S2, A2) = 0.7 * (4 + 0.9 * 5.3304) + 0.3 * (6 + 0.9 * 27.1) = 0.7 * 8.79736 + 0.3 * 30.39 = 6.158152 + 9.117 = 15.275152$
> $q_\pi(S3, A1) = 1.0 * (10 + 0.9 * 27.1) = 10 + 24.39 = 34.39$
> $q_\pi(S3, A2) = 0.9 * (8 + 0.9 * 10.11824) + 0.1 * (3 + 0.9 * 5.3304) = 0.9 * 17.106416 + 0.1 * 7.79736 = 15.3957744 + 0.779736 = 16.1755104$
>
> Agora, podemos escolher as a√ß√µes que maximizam $q_\pi(s, a)$ para cada estado:
> $\pi'(S1) = \arg\max_{a} q_{\pi}(S1, a) = A1$ porque $13.115888 > 6.451888$
> $\pi'(S2) = \arg\max_{a} q_{\pi}(S2, a) = A2$ porque $15.275152 > 7.9827936$
> $\pi'(S3) = \arg\max_{a} q_{\pi}(S3, a) = A1$ porque $34.39 > 16.1755104$
>
> Portanto, a nova pol√≠tica $\pi'$ √©:
> $\pi'(S1) = A1$, $\pi'(S2) = A2$, $\pi'(S3) = A1$

A nova pol√≠tica $\pi'$ √© definida como aquela que escolhe a a√ß√£o que maximiza $q_\pi(s, a)$ em cada estado $s$ [^7]:
$$
\pi'(s) = \arg\max_{a} q_{\pi}(s, a) = \arg\max_{a} \sum_{s',r} p(s', r|s, a) [r + \gamma v_{\pi}(s')]
$$

O **teorema de melhoria da pol√≠tica** garante que a nova pol√≠tica $\pi'$ √© pelo menos t√£o boa quanto, ou melhor do que, a pol√≠tica original $\pi$ [^6, 7]. Formalmente, para todas os estados $s \in \mathcal{S}$,
$$v_{\pi'}(s) \geq v_{\pi}(s)$$

**Prova do Teorema de Melhoria da Pol√≠tica:**

I.  Come√ßamos pela defini√ß√£o de $q_\pi(s, \pi'(s))$, que representa o valor de tomar a a√ß√£o especificada por $\pi'(s)$ no estado $s$ e, em seguida, seguir a pol√≠tica $\pi$:
    $$q_{\pi}(s, \pi'(s)) = \sum_{s',r} p(s', r|s, \pi'(s)) [r + \gamma v_{\pi}(s')]$$

II.  Pela defini√ß√£o de $\pi'(s)$ como a a√ß√£o greedy em rela√ß√£o a $v_\pi(s)$, temos:
    $$q_{\pi}(s, \pi'(s)) = \max_{a} \sum_{s',r} p(s', r|s, a) [r + \gamma v_{\pi}(s')] \geq  \sum_{s',r} p(s', r|s, \pi(s)) [r + \gamma v_{\pi}(s')] =  v_{\pi}(s)$$
    Essa desigualdade estabelece que tomar a a√ß√£o ditada por $\pi'$ em $s$ e, ent√£o, seguir $\pi$ nos d√° um valor maior ou igual ao de seguir $\pi$ desde o in√≠cio.

III. Agora, definimos uma pol√≠tica auxiliar $\pi''$ que segue $\pi'$ uma vez e ent√£o segue $\pi$ para sempre.  O valor de $v_{\pi''}(s)$ pode ser escrito como:
     $$v_{\pi''}(s) = q_{\pi}(s, \pi'(s))$$

IV. Podemos expandir $v_{\pi''}(s)$ iterativamente usando a equa√ß√£o de Bellman para $\pi$, substituindo recursivamente o primeiro passo seguindo $\pi'$ seguido por seguir $\pi$:
    $$v_{\pi''}(s) =  \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = \pi'(s)] = q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$$

V. Aplicando este argumento repetidamente, constru√≠mos uma sequ√™ncia de pol√≠ticas cada vez melhores:  $\pi_0 = \pi, \pi_1 = \pi'', \pi_2, \ldots$, onde $\pi_{k+1}$ segue $\pi'$ por $k+1$ passos e ent√£o segue $\pi$ para sempre.  No limite, conforme $k \to \infty$, essa sequ√™ncia converge para seguir $\pi'$ sempre.  Portanto,
    $$v_{\pi'}(s) \geq v_{\pi}(s) \quad \forall s \in \mathcal{S}$$
    O que demonstra que a pol√≠tica $\pi'$ √© pelo menos t√£o boa quanto a pol√≠tica $\pi$. ‚ñ†

Al√©m disso, se $v_{\pi'}(s) = v_{\pi}(s)$ para todos os estados $s \in \mathcal{S}$, ent√£o $\pi$ e $\pi'$ s√£o pol√≠ticas √≥timas [^7, 9].

**Teorema 1:** *Se a pol√≠tica $\pi'$ gerada pela melhoria da pol√≠tica for tal que $v_{\pi'}(s) = v_{\pi}(s)$ para todos os estados $s \in \mathcal{S}$, ent√£o $\pi$ √© uma pol√≠tica √≥tima*.

*Proof:* A prova pode ser encontrada em [^1, se√ß√£o 4.3]. Essencialmente, se uma pol√≠tica n√£o pode ser melhorada agindo greedy em rela√ß√£o √† sua fun√ß√£o de valor, ent√£o ela j√° deve ser √≥tima.

**Corol√°rio 1.1:** *A itera√ß√£o de pol√≠tica converge para uma pol√≠tica √≥tima em um n√∫mero finito de itera√ß√µes*.

*Proof:* Como o n√∫mero de pol√≠ticas determin√≠sticas √© finito ( $|\mathcal{A}|^{|\mathcal{S}|}$ ), e cada itera√ß√£o da itera√ß√£o de pol√≠tica garante uma pol√≠tica melhor ou permanece na pol√≠tica √≥tima, a itera√ß√£o de pol√≠tica deve convergir para uma pol√≠tica √≥tima em um n√∫mero finito de itera√ß√µes.

**Algoritmo de Itera√ß√£o da Pol√≠tica:**
O algoritmo de itera√ß√£o de pol√≠tica itera repetidamente entre avalia√ß√£o da pol√≠tica e melhoria da pol√≠tica at√© que a pol√≠tica convirja para uma pol√≠tica √≥tima [^8]. O algoritmo pode ser resumido da seguinte forma:

1.  **Inicializa√ß√£o:** Inicialize $V(s) \in \mathbb{R}$ e $\pi(s) \in \mathcal{A}(s)$ arbitrariamente para todos os estados $s \in \mathcal{S}$ [^8].
2.  **Avalia√ß√£o da Pol√≠tica:** Dada a pol√≠tica $\pi$, calcule $v_\pi$ iterativamente at√© a converg√™ncia [^8]:
    *   Loop:
        *   $\Delta \leftarrow 0$ [^8]
        *   Para cada estado $s \in \mathcal{S}$: [^8]
            *   $v \leftarrow V(s)$ [^8]
            *   $V(s) \leftarrow \sum_{s',r} p(s', r|s, \pi(s))[r + \gamma V(s')]$ [^8]
            *   $\Delta \leftarrow max(\Delta, |v - V(s)|)$ [^8]
        *   at√© $\Delta < \theta$ (onde $\theta$ √© um pequeno n√∫mero positivo determinando a acur√°cia da estimativa) [^8]
3.  **Melhoria da Pol√≠tica:** Dada $v_\pi$, encontre uma nova pol√≠tica $\pi'$ greedy em rela√ß√£o a $v_\pi$ [^8]:
    *   *policy-stable* $\leftarrow$ *true* [^8]
    *   Para cada estado $s \in \mathcal{S}$: [^8]
        *   *old-action* $\leftarrow \pi(s)$ [^8]
        *   $\pi(s) \leftarrow \arg\max_{a} \sum_{s',r} p(s', r|s, a)[r + \gamma V(s')]$ [^8]
        *   Se *old-action* $\neq \pi(s)$, ent√£o *policy-stable* $\leftarrow$ *false* [^8]
    *   Se *policy-stable*, ent√£o pare e retorne $V \approx v_*$ e $\pi \approx \pi_*$; sen√£o, v√° para o passo 2 [^8]



![Policy Iteration algorithm: iterative process of policy evaluation and improvement for optimal policy estimation.](./../images/image3.png)

Onde $\mathcal{A}(s)$ representa o conjunto de a√ß√µes poss√≠veis no estado $s$.

**Observa√ß√£o:** A converg√™ncia da itera√ß√£o de pol√≠tica pode ser acelerada truncando a avalia√ß√£o da pol√≠tica ap√≥s algumas itera√ß√µes. Isso leva a variantes como a *itera√ß√£o de valor modificada*.





![Diagrama da itera√ß√£o da pol√≠tica generalizada (GPI) mostrando o ciclo entre avalia√ß√£o e melhoria da pol√≠tica.](./../images/image2.png)





![Diagrama representando a intera√ß√£o entre avalia√ß√£o e melhoria de pol√≠ticas na itera√ß√£o da pol√≠tica generalizada (GPI).](./../images/image6.png)

### Conclus√£o
A itera√ß√£o de pol√≠tica √© um algoritmo fundamental na programa√ß√£o din√¢mica para encontrar pol√≠ticas √≥timas em MDPs finitos [^8]. Ao alternar iterativamente entre a avalia√ß√£o da pol√≠tica e a melhoria da pol√≠tica, o algoritmo garante uma converg√™ncia monot√¥nica para uma pol√≠tica √≥tima [^8]. Embora a itera√ß√£o de pol√≠tica assuma um modelo perfeito do ambiente e pode ser computacionalmente cara para grandes espa√ßos de estado, ela fornece uma base te√≥rica importante para entender outros algoritmos de aprendizado por refor√ßo [^1]. Al√©m disso, o conceito de **itera√ß√£o de pol√≠tica generalizada (GPI)**, onde os processos de avalia√ß√£o e melhoria da pol√≠tica interagem independentemente da granularidade e outros detalhes, √© central para a maioria dos m√©todos de aprendizado por refor√ßo [^10].

### Refer√™ncias
[^1]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^4]: Policy Improvement, page 77
[^5]: Policy Improvement, page 78
[^6]: Policy Improvement, page 78
[^7]: Policy Improvement, page 79
[^8]: Policy Iteration, page 80
[^9]: Policy Improvement, page 79
[^10]: Generalized Policy Iteration, page 86
<!-- END -->