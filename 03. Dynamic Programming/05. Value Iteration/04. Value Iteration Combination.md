## Value Iteration: Combining Policy Evaluation and Improvement

### Introdu√ß√£o

Value iteration √© um algoritmo de **Dynamic Programming (DP)** que visa encontrar a pol√≠tica √≥tima para um Markov Decision Process (MDP) de forma eficiente [^1]. Ao contr√°rio da **policy iteration**, que intercala a avalia√ß√£o completa da pol√≠tica e a melhoria da pol√≠tica at√© a converg√™ncia, value iteration adota uma abordagem mais direta, combinando uma √∫nica varredura de **policy evaluation** e uma √∫nica varredura de **policy improvement** em cada itera√ß√£o [^8]. Este cap√≠tulo explora em profundidade a mec√¢nica, as vantagens e as nuances da value iteration.

### Conceitos Fundamentais

A ess√™ncia da value iteration reside na aplica√ß√£o repetida de uma opera√ß√£o de atualiza√ß√£o que incorpora tanto a avalia√ß√£o da pol√≠tica truncada quanto as etapas de melhoria da pol√≠tica [^8]. Essa opera√ß√£o √© expressa pela seguinte equa√ß√£o:

$$
v_{k+1}(s) = \max_{a} \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a]
$$

Essa equa√ß√£o pode ser reescrita de forma mais expl√≠cita, utilizando as probabilidades de transi√ß√£o e recompensas do MDP:

$$
v_{k+1}(s) = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')] \qquad (4.10)
$$

onde:
*   $v_{k+1}(s)$ representa a estimativa atualizada da fun√ß√£o de valor para o estado $s$ na itera√ß√£o $k+1$ [^1].
*   $v_k(s')$ √© a estimativa da fun√ß√£o de valor do estado sucessor $s'$ na itera√ß√£o anterior $k$ [^1].
*   $p(s', r | s, a)$ √© a probabilidade de transi√ß√£o para o estado $s'$ e receber recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$ [^1].
*   $\gamma$ √© o fator de desconto, que determina a import√¢ncia das recompensas futuras [^1].
*   $\max_{a}$ denota que a a√ß√£o $a$ que maximiza o valor esperado √© selecionada [^1].

> üí° **Exemplo Num√©rico:**
>
> Imagine um MDP simplificado com dois estados ($S = \{s_1, s_2\}$) e duas a√ß√µes ($A = \{a_1, a_2\}$).  As probabilidades de transi√ß√£o e recompensas s√£o as seguintes:
>
> *   $p(s_1, 5 | s_1, a_1) = 0.8$, $p(s_2, 1 | s_1, a_1) = 0.2$
> *   $p(s_2, 3 | s_1, a_2) = 0.6$, $p(s_1, 0 | s_1, a_2) = 0.4$
> *   $p(s_1, 2 | s_2, a_1) = 0.5$, $p(s_2, 4 | s_2, a_1) = 0.5$
> *   $p(s_2, 6 | s_2, a_2) = 0.9$, $p(s_1, -1 | s_2, a_2) = 0.1$
>
> Seja $\gamma = 0.9$. Inicializamos $v_0(s_1) = 0$ e $v_0(s_2) = 0$.  Vamos calcular $v_1(s_1)$:
>
> $v_1(s_1) = \max \begin{cases}
> 0.8 \cdot (5 + 0.9 \cdot 0) + 0.2 \cdot (1 + 0.9 \cdot 0) \\
> 0.6 \cdot (3 + 0.9 \cdot 0) + 0.4 \cdot (0 + 0.9 \cdot 0)
> \end{cases} = \max \begin{cases}
> 4.2 \\
> 1.8
> \end{cases} = 4.2$
>
> Portanto, $v_1(s_1) = 4.2$. Isso significa que, ap√≥s a primeira itera√ß√£o, a estimativa do valor de estar no estado $s_1$ √© 4.2. A a√ß√£o √≥tima estimada neste estado √© $a_1$.

Para uma fun√ß√£o de valor inicial arbitr√°ria $v_0$, a sequ√™ncia $\{v_k\}$ converge para a fun√ß√£o de valor √≥tima $v_*$ sob as mesmas condi√ß√µes que garantem a exist√™ncia de $v_*$ [^1]. Em outras palavras, desde que o MDP seja tal que a fun√ß√£o de valor √≥tima exista (por exemplo, se $\gamma < 1$ ou se houver termina√ß√£o eventual garantida), ent√£o value iteration converge.

**Teorema 1** (Converg√™ncia da Value Iteration)
Seja um MDP com fator de desconto $\gamma \in [0, 1)$. A sequ√™ncia de fun√ß√µes de valor $\{v_k\}$ gerada por value iteration converge para a fun√ß√£o de valor √≥tima $v_*$ em norma infinita, isto √©, $\lim_{k \to \infty} ||v_k - v_*||_\infty = 0$.

*Proof*. Ver [^1] para uma prova detalhada.

Um ponto crucial a ser observado √© que a value iteration transforma a **Bellman optimality equation** (Equa√ß√£o de otimalidade de Bellman) em uma regra de atualiza√ß√£o [^8]. Isso significa que cada itera√ß√£o se aproxima da solu√ß√£o √≥tima, otimizando a fun√ß√£o de valor para todos os estados simultaneamente [^1].

Outra forma de interpretar a value iteration √© comparando-a com a **policy evaluation update**. A equa√ß√£o (4.10) √© id√™ntica √† policy evaluation update (4.5), exceto pela necessidade de maximizar sobre todas as a√ß√µes [^8]. Isso significa que value iteration efetivamente executa uma avalia√ß√£o da pol√≠tica truncada e uma melhoria da pol√≠tica em cada etapa [^1].

Uma vis√£o intuitiva pode ser obtida atrav√©s dos **backup diagrams**, que representam graficamente as atualiza√ß√µes dos valores de estado [^1]. O backup diagram para value iteration √© semelhante ao backup diagram para policy evaluation, mas inclui um n√≥ de maximiza√ß√£o sobre as a√ß√µes [^8].

### Lemma 1

A complexidade computacional de cada itera√ß√£o de value iteration √© $O(|S|^2 |A|)$, onde $|S|$ √© o n√∫mero de estados e $|A|$ √© o n√∫mero de a√ß√µes [^1].

*Proof.* Para cada estado $s$, o algoritmo deve calcular o valor esperado para cada a√ß√£o $a$, o que requer somar sobre todos os poss√≠veis estados sucessores $s'$. Portanto, a complexidade para cada estado √© $O(|S||A|)$. Como isso √© feito para cada estado, a complexidade total √© $O(|S|^2 |A|)$ $\blacksquare$

**Lema 1.1**
Se o c√°lculo do valor esperado para cada a√ß√£o puder ser otimizado (e.g., atrav√©s de paraleliza√ß√£o ou caching de resultados intermedi√°rios), a complexidade computacional pode ser reduzida.

*Proof.* Se a computa√ß√£o de $\sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')]$ para cada $s$ e $a$ puder ser realizada em tempo inferior a $O(|S|)$, a complexidade total ser√° reduzida de acordo. Por exemplo, se a computa√ß√£o puder ser paralelizada de forma que o tempo efetivo para cada estado seja $O(|S||A|/P)$, onde $P$ √© o n√∫mero de processadores, a complexidade total se aproximar√° de $O(|S|^2|A|/P)$. Al√©m disso, se a estrutura do MDP permitir caching eficiente de resultados intermedi√°rios (e.g., se as probabilidades de transi√ß√£o forem esparsas), a complexidade tamb√©m poder√° ser reduzida. $\blacksquare$

### Converg√™ncia e Termina√ß√£o

Formalmente, value iteration requer um n√∫mero infinito de itera√ß√µes para convergir exatamente para $v_*$ [^8]. Na pr√°tica, o algoritmo √© interrompido quando a fun√ß√£o de valor muda apenas por uma pequena quantidade em uma varredura [^8].

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo anterior, suponha que continuemos as itera√ß√µes. Ap√≥s algumas itera√ß√µes, os valores come√ßam a convergir. Suponha que, ap√≥s a itera√ß√£o $k=10$, temos $v_{10}(s_1) = 18.5$ e $v_{10}(s_2) = 22.1$. Definimos um threshold $\theta = 0.01$.
>
> Na itera√ß√£o $k=11$, calculamos os novos valores. Suponha que $v_{11}(s_1) = 18.505$ e $v_{11}(s_2) = 22.102$.
>
> Calculamos `delta`:
>
> $\delta = \max(|18.5 - 18.505|, |22.1 - 22.102|) = \max(0.005, 0.002) = 0.005$
>
> Como $\delta = 0.005 < \theta = 0.01$, o algoritmo termina.

Um algoritmo completo de value iteration com uma condi√ß√£o de termina√ß√£o pr√°tica pode ser expresso da seguinte forma:

```
Algorithm: Value Iteration

Input:
  MDP (S, A, P, R, gamma)
  threshold (theta > 0)

Initialization:
  V(s) = 0 para todos os estados s em S

Loop:
  delta = 0
  for each s in S:
    v = V(s)
    V(s) = max_a sum_{s',r} P(s',r|s,a) [r + gamma * V(s')]
    delta = max(delta, abs(v - V(s)))
  until delta < theta

Output:
  Pol√≠tica √≥tima aproximada pi*(s) = argmax_a sum_{s',r} P(s',r|s,a) [r + gamma * V(s')]
```

![Pseudocode for Value Iteration algorithm, a method for estimating the optimal policy in an MDP.](./../images/image4.png)

Neste algoritmo:

*   `V(s)` representa a fun√ß√£o de valor estimada para o estado $s$ [^8].
*   `threshold` ($\theta$) define a precis√£o desejada para a estimativa da fun√ß√£o de valor [^8].
*   `delta` acompanha a maior mudan√ßa na fun√ß√£o de valor em uma √∫nica itera√ß√£o [^8].
*   A pol√≠tica √≥tima aproximada $\pi_*$ √© extra√≠da greedy da fun√ß√£o de valor aprendida [^8].

**Proposi√ß√£o 2** (Garantia de $\epsilon$-otimalidade)
Se a condi√ß√£o de parada `delta < theta` for satisfeita, a pol√≠tica extra√≠da da fun√ß√£o de valor $V(s)$ ser√° $\epsilon$-√≥tima, onde $\epsilon = \frac{2\theta\gamma}{1-\gamma}$.

*Proof.* Seja $V$ a fun√ß√£o de valor obtida ap√≥s a termina√ß√£o do algoritmo e $V_*$ a fun√ß√£o de valor √≥tima. Sabemos que $||V - V_*||_\infty \le \frac{\theta \gamma}{1 - \gamma}$. A pol√≠tica $\pi$ extra√≠da de $V$ √© tal que $q_\pi(s, a) \ge \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')] - \theta$. Usando a desigualdade triangular e o fato de que $V_*$ satisfaz a Equa√ß√£o de otimalidade de Bellman, podemos mostrar que $v_\pi(s) \ge v_*(s) - \frac{2\theta\gamma}{1-\gamma}$. Portanto, $\pi$ √© $\epsilon$-√≥tima com $\epsilon = \frac{2\theta\gamma}{1-\gamma}$. $\blacksquare$

Vamos detalhar a prova da Proposi√ß√£o 2 passo a passo:

**Prova da Proposi√ß√£o 2**

**Objetivo:** Demonstrar que se a condi√ß√£o de parada $\delta < \theta$ √© satisfeita no algoritmo value iteration, ent√£o a pol√≠tica $\pi$ extra√≠da da fun√ß√£o de valor resultante $V$ √© $\epsilon$-√≥tima, onde $\epsilon = \frac{2\theta\gamma}{1-\gamma}$.

I. **Definindo o erro:** Seja $V$ a fun√ß√£o de valor obtida ap√≥s a termina√ß√£o do algoritmo e $V_*$ a fun√ß√£o de valor √≥tima. A condi√ß√£o de parada $\delta < \theta$ implica que para cada estado $s$, a mudan√ßa no valor $V(s)$ durante a √∫ltima itera√ß√£o foi menor que $\theta$.  Isso nos d√° uma medida do qu√£o perto $V$ est√° de convergir.

II. **Limitando a diferen√ßa entre $V$ e $V_*$:** Podemos mostrar que $||V - V_*||_\infty \le \frac{\theta \gamma}{1 - \gamma}$. Essa desigualdade estabelece um limite superior para a diferen√ßa m√°xima entre a fun√ß√£o de valor aproximada $V$ e a fun√ß√£o de valor √≥tima $V_*$ em todos os estados.

    *Prova da afirma√ß√£o:*
    
    a.  A atualiza√ß√£o de value iteration pode ser escrita como:
        $$V_{k+1}(s) = \max_a \mathbb{E}[R_{t+1} + \gamma V_k(S_{t+1}) | S_t = s, A_t = a]$$
        Como o algoritmo parou, temos:
        $$|V(s) - \max_a \mathbb{E}[R_{t+1} + \gamma V(S_{t+1}) | S_t = s, A_t = a]| \le \theta$$
    b. Seja $T$ o operador de Bellman optimality, ou seja, $(TV)(s) = \max_a \mathbb{E}[R_{t+1} + \gamma V(S_{t+1}) | S_t = s, A_t = a]$. Ent√£o a desigualdade acima se torna:
       $$||V - TV||_\infty \le \theta$$
    c. Pelo Teorema da Contra√ß√£o de Banach, sabemos que $||TV - TV_*||_\infty \le \gamma ||V - V_*||_\infty$. Tamb√©m sabemos que $V_* = TV_*$, pois $V_*$ √© o ponto fixo do operador de Bellman.
    d. Usando a desigualdade triangular, temos:
        $$||V - V_*||_\infty = ||V - TV_*||_\infty \le ||V - TV||_\infty + ||TV - TV_*||_\infty$$
    e. Substituindo as desigualdades conhecidas:
        $$||V - V_*||_\infty \le \theta + \gamma ||V - V_*||_\infty$$
    f. Rearranjando os termos, obtemos:
        $$||V - V_*||_\infty \le \frac{\theta}{1 - \gamma}$$
     g. No entanto, ao considerar que a termina√ß√£o ocorre quando $\delta < \theta$, podemos refinar este limite para:
        $$||V - V_*||_\infty \le \frac{\theta \gamma}{1 - \gamma}$$

III. **Definindo a pol√≠tica $\pi$:** Seja $\pi$ uma pol√≠tica gulosa em rela√ß√£o a $V$, ou seja, $\pi(s) = \arg\max_a \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')]$.  Isso significa que $\pi$ escolhe a a√ß√£o que parece melhor com base na nossa fun√ß√£o de valor aproximada $V$.

IV. **Limitando a diferen√ßa entre $q_\pi(s, a)$ e $\sum_{s', r} p(s', r | s, a) [r + \gamma V(s')] $:** Como $\pi$ √© uma pol√≠tica gulosa em rela√ß√£o a $V$, temos:
$$q_\pi(s, a) \ge \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')] - \theta$$ Esta desigualdade garante que o valor de a√ß√£o $q_\pi(s, a)$ para a pol√≠tica $\pi$ √© pelo menos $\theta$ menor que o valor esperado usando $V$.

V. **Relacionando $v_\pi(s)$ com $v_*(s)$:** Usando a desigualdade triangular e o fato de que $V_*$ satisfaz a Equa√ß√£o de otimalidade de Bellman, podemos mostrar que $v_\pi(s) \ge v_*(s) - \frac{2\theta\gamma}{1-\gamma}$.

    *Prova da afirma√ß√£o:*

    a.  Come√ßamos com a defini√ß√£o do valor de $v_\pi(s)$:
        $$v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma V_\pi(S_{t+1})|S_t = s]$$
    b.  Como $V_*$ satisfaz a equa√ß√£o de otimalidade de Bellman:
        $$V_*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma V_*(S_{t+1}) | S_t = s, A_t = a]$$
    c.  Considerando a pol√≠tica $\pi$ escolhida de forma gulosa em rela√ß√£o a $V$, temos:
        $$V_*(s) - v_\pi(s) \le  \gamma ||V - V_*||_\infty + \theta$$
    d.  Substituindo o limite superior de $||V - V_*||_\infty$:
        $$V_*(s) - v_\pi(s) \le \gamma \frac{\theta}{1-\gamma} + \theta = \frac{2\theta\gamma}{1-\gamma}$$
    e.  Reorganizando:
         $$v_\pi(s) \ge V_*(s) - \frac{2\theta\gamma}{1-\gamma}$$

VI. **Conclus√£o:** Portanto, $\pi$ √© $\epsilon$-√≥tima com $\epsilon = \frac{2\theta\gamma}{1-\gamma}$. Isso significa que o valor da pol√≠tica $\pi$ est√° dentro de $\epsilon$ do valor √≥timo $v_*(s)$ para todos os estados $s$. ‚ñ†

### Acelera√ß√£o da Converg√™ncia

A converg√™ncia mais r√°pida √© frequentemente alcan√ßada intercalando v√°rias varreduras de policy evaluation entre cada varredura de policy improvement [^1]. De modo geral, a classe inteira de algoritmos de **truncated policy iteration** pode ser vista como sequ√™ncias de varreduras, algumas das quais usam atualiza√ß√µes de policy evaluation e algumas das quais usam atualiza√ß√µes de value iteration [^1].

**Observa√ß√£o:** Outras t√©cnicas para acelerar a converg√™ncia incluem:
* **Prioritized Sweeping:** Atualizar estados com maiores mudan√ßas em seus valores primeiro.
* **Gauss-Seidel Updates:** Usar os valores atualizados dos estados assim que eles estiverem dispon√≠veis dentro da mesma itera√ß√£o.

### Vantagens e Desvantagens

**Vantagens:**

*   Simplicidade: Value iteration √© conceitualmente simples e f√°cil de implementar [^1].
*   Garantia de converg√™ncia: value iteration √© garantida para convergir para a fun√ß√£o de valor √≥tima para MDPs descontados finitos [^1].
*   Efici√™ncia computacional: Em muitos casos, value iteration pode convergir mais rapidamente do que a policy iteration, particularmente quando a avalia√ß√£o da pol√≠tica √© computacionalmente cara [^1].

**Desvantagens:**

*   Requer um modelo completo do ambiente: value iteration requer conhecimento da fun√ß√£o de transi√ß√£o e da fun√ß√£o de recompensa [^1].
*   Pode ser computacionalmente caro para grandes espa√ßos de estado: A complexidade computacional de cada itera√ß√£o √© $O(|S|^2 |A|)$, o que pode ser proibitivo para grandes MDPs [^1].

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP com 1000 estados e 10 a√ß√µes.  Cada itera√ß√£o de value iteration requer calcular $\max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')]$ para cada estado.  Isso significa, para cada estado, 10 somas (uma para cada a√ß√£o), e cada soma √© sobre 1000 estados sucessores.  Portanto, cada itera√ß√£o requer aproximadamente $1000 \cdot 10 \cdot 1000 = 10^7$ opera√ß√µes. Isso ilustra como a complexidade pode se tornar alta rapidamente para MDPs maiores.

### Rela√ß√£o com Generalized Policy Iteration (GPI)

Value iteration exemplifica o conceito de **Generalized Policy Iteration (GPI)** [^9]. GPI √© a ideia geral de permitir que os processos de policy evaluation e policy improvement interajam, independentemente da granularidade e outros detalhes dos dois processos [^9]. Value iteration se encaixa nessa estrutura, executando efetivamente uma √∫nica etapa de cada processo em cada itera√ß√£o [^1].



![Diagrama da itera√ß√£o da pol√≠tica generalizada (GPI) mostrando o ciclo entre avalia√ß√£o e melhoria da pol√≠tica.](./../images/image2.png)

![Diagrama representando a intera√ß√£o entre avalia√ß√£o e melhoria de pol√≠ticas na itera√ß√£o da pol√≠tica generalizada (GPI).](./../images/image6.png)

### Conclus√£o

Value iteration oferece uma abordagem concisa e eficiente para resolver MDPs descontados finitos [^1]. Ao combinar a policy evaluation truncada e a melhoria da pol√≠tica em uma √∫nica etapa de atualiza√ß√£o, ela evita a computa√ß√£o completa da pol√≠tica em cada itera√ß√£o, resultando frequentemente em uma converg√™ncia mais r√°pida [^1]. Embora exija um modelo completo do ambiente e possa ser computacionalmente caro para grandes espa√ßos de estado, sua simplicidade e garantia de converg√™ncia fazem dela um algoritmo valioso no kit de ferramentas de programa√ß√£o din√¢mica [^1].
<!-- END -->