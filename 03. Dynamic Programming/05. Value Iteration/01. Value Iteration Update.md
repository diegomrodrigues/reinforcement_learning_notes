## Value Iteration: Combining Policy Improvement and Truncated Policy Evaluation

### Introdu√ß√£o
O cap√≠tulo anterior introduziu o conceito de **programa√ß√£o din√¢mica (DP)** como uma cole√ß√£o de algoritmos para calcular pol√≠ticas √≥timas, dado um modelo perfeito do ambiente como um processo de decis√£o de Markov (MDP) [^1]. Abordamos a **policy evaluation**, um processo iterativo para calcular a fun√ß√£o de valor para uma dada pol√≠tica, e o **policy improvement**, um m√©todo para construir uma pol√≠tica melhorada com base na fun√ß√£o de valor atual. Vimos tamb√©m a **policy iteration**, que alterna entre policy evaluation e policy improvement para convergir para uma pol√≠tica √≥tima. No entanto, a policy iteration pode ser computacionalmente custosa devido √† necessidade de realizar policy evaluation completa em cada itera√ß√£o. Este cap√≠tulo explora uma alternativa mais eficiente, chamada **value iteration**, que combina policy improvement e truncated policy evaluation em uma √∫nica opera√ß√£o de atualiza√ß√£o.

### Conceitos Fundamentais

A principal desvantagem da **policy iteration** reside no fato de que cada itera√ß√£o envolve a **policy evaluation**, que em si pode ser uma computa√ß√£o iterativa prolongada, exigindo m√∫ltiplas varreduras atrav√©s do conjunto de estados [^10]. Se a policy evaluation √© realizada iterativamente, a converg√™ncia exata para $v_œÄ$ ocorre apenas no limite. Surge ent√£o a quest√£o: devemos aguardar a converg√™ncia exata, ou podemos interromper o processo antes disso? O exemplo da Figura 4.1 [^5] sugere que pode ser poss√≠vel truncar a policy evaluation. Nesse exemplo, as itera√ß√µes de policy evaluation al√©m das tr√™s primeiras n√£o t√™m efeito sobre a pol√≠tica *greedy* correspondente.

![Convergence of iterative policy evaluation on a gridworld, showing improvement from random to optimal policy.](./../images/image7.png)

Na verdade, a etapa de policy evaluation da policy iteration pode ser truncada de v√°rias maneiras sem perder as garantias de converg√™ncia da policy iteration [^10]. Um caso especial importante √© quando a policy evaluation √© interrompida ap√≥s apenas uma varredura (uma atualiza√ß√£o de cada estado). Este algoritmo √© chamado de **value iteration** [^10]. Ele pode ser escrito como uma opera√ß√£o de atualiza√ß√£o particularmente simples que combina as etapas de policy improvement e policy evaluation truncada:

$$
v_{k+1}(s) = \max_{a} \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a] = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')] \quad \text{[4.10]}
$$

para todo $s \in \mathcal{S}$ [^10]. Para uma $v_0$ arbitr√°ria, a sequ√™ncia $\{v_k\}$ pode ser mostrada para convergir para $v_*$ sob as mesmas condi√ß√µes que garantem a exist√™ncia de $v_*$ [^10].

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um MDP simples com dois estados, $S = \{s_1, s_2\}$, e duas a√ß√µes, $A = \{a_1, a_2\}$. A taxa de desconto √© $\gamma = 0.9$. Inicializamos $v_0(s_1) = 0$ e $v_0(s_2) = 0$. As probabilidades de transi√ß√£o e recompensas s√£o definidas da seguinte forma:
>
> *   **Estado** $s_1$:
>     *   A√ß√£o $a_1$: Transi√ß√£o para $s_2$ com probabilidade 1.0, recompensa = 1.0.
>     *   A√ß√£o $a_2$: Transi√ß√£o para $s_1$ com probabilidade 1.0, recompensa = 0.0.
> *   **Estado** $s_2$:
>     *   A√ß√£o $a_1$: Transi√ß√£o para $s_1$ com probabilidade 1.0, recompensa = -1.0.
>     *   A√ß√£o $a_2$: Transi√ß√£o para $s_2$ com probabilidade 1.0, recompensa = 0.5.
>
> Vamos realizar uma itera√ß√£o da value iteration:
>
> *   **Para** $s_1$:
>     *   $a_1$: $Q(s_1, a_1) = 1.0 + 0.9 * v_0(s_2) = 1.0 + 0.9 * 0 = 1.0$
>     *   $a_2$: $Q(s_1, a_2) = 0.0 + 0.9 * v_0(s_1) = 0.0 + 0.9 * 0 = 0.0$
>     *   $v_1(s_1) = \max(1.0, 0.0) = 1.0$
> *   **Para** $s_2$:
>     *   $a_1$: $Q(s_2, a_1) = -1.0 + 0.9 * v_0(s_1) = -1.0 + 0.9 * 0 = -1.0$
>     *   $a_2$: $Q(s_2, a_2) = 0.5 + 0.9 * v_0(s_2) = 0.5 + 0.9 * 0 = 0.5$
>     *   $v_1(s_2) = \max(-1.0, 0.5) = 0.5$
>
> Ap√≥s uma itera√ß√£o, temos $v_1(s_1) = 1.0$ e $v_1(s_2) = 0.5$. A value iteration continua iterando at√© que a mudan√ßa nos valores dos estados seja menor que um limiar $\theta$.

**Teorema 1** (Converg√™ncia da Value Iteration) A sequ√™ncia de fun√ß√µes de valor $\{v_k\}$ gerada pela value iteration converge para a fun√ß√£o de valor √≥tima $v_*$, ou seja, $\lim_{k \to \infty} v_k(s) = v_*(s)$ para todo $s \in \mathcal{S}$.

*Proof:* A prova pode ser encontrada em [^10] e repousa sobre o fato de que a value iteration √© uma contra√ß√£o no espa√ßo de fun√ß√µes de valor sob a norma do supremo.

Para ilustrar isso, podemos esbo√ßar os principais passos da prova, assumindo familiaridade com conceitos de an√°lise funcional:

I. **Definir o Operador de Bellman:** Seja $T$ o operador de Bellman definido como:
$$(Tv)(s) = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] \quad \forall s \in \mathcal{S}$$

II. **Mostrar que $T$ √© uma Contra√ß√£o:** Precisamos mostrar que $T$ √© uma contra√ß√£o sob a norma do supremo $||v|| = \max_{s} |v(s)|$. Para quaisquer duas fun√ß√µes de valor $u$ e $v$, temos:

$$||Tu - Tv|| = \max_{s} |(Tu)(s) - (Tv)(s)|$$
$$= \max_{s} |\max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma u(s')] - \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')]|$$

Usando a desigualdade $|\max_a f(a) - \max_a g(a)| \leq \max_a |f(a) - g(a)|$, temos:

$$||Tu - Tv|| \leq \max_{s} \max_{a} |\sum_{s', r} p(s', r | s, a) [r + \gamma u(s')] - \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')]|$$
$$= \max_{s} \max_{a} |\sum_{s', r} p(s', r | s, a) \gamma [u(s') - v(s')]|$$
$$= \gamma \max_{s} \max_{a} \sum_{s', r} p(s', r | s, a) |u(s') - v(s')|$$
$$\leq \gamma \max_{s'} |u(s') - v(s')| = \gamma ||u - v||$$

Portanto, $||Tu - Tv|| \leq \gamma ||u - v||$, onde $0 \leq \gamma < 1$. Isso mostra que $T$ √© uma contra√ß√£o com fator de contra√ß√£o $\gamma$.

III. **Aplicar o Teorema do Ponto Fixo de Banach:** Pelo Teorema do Ponto Fixo de Banach, um operador de contra√ß√£o em um espa√ßo m√©trico completo tem um ponto fixo √∫nico. O espa√ßo de fun√ß√µes de valor com a norma do supremo √© um espa√ßo m√©trico completo. Portanto, $T$ tem um ponto fixo √∫nico $v_*$, tal que $Tv_* = v_*$. Este ponto fixo √© a fun√ß√£o de valor √≥tima.

IV. **Converg√™ncia:** A value iteration gera uma sequ√™ncia de fun√ß√µes de valor $\{v_k\}$ tal que $v_{k+1} = Tv_k$. Pelo Teorema do Ponto Fixo de Banach, essa sequ√™ncia converge para o ponto fixo √∫nico $v_*$, ou seja, $\lim_{k \to \infty} v_k = v_*$. Portanto, a value iteration converge para a fun√ß√£o de valor √≥tima. ‚ñ†

Outra maneira de entender a value iteration √© por refer√™ncia √† **equa√ß√£o de otimalidade de Bellman** (4.1) [^1, ^10]. Observe que a value iteration √© obtida simplesmente transformando a equa√ß√£o de otimalidade de Bellman em uma regra de atualiza√ß√£o. Observe tamb√©m como a atualiza√ß√£o da value iteration √© id√™ntica √† atualiza√ß√£o da policy evaluation (4.5) [^2], exceto que ela exige que o m√°ximo seja tomado sobre todas as a√ß√µes. Outra maneira de ver essa estreita rela√ß√£o √© comparar os diagramas de backup para esses algoritmos na p√°gina 59 [^3] (policy evaluation) e √† esquerda da Figura 3.4 [^10] (value iteration). Estes dois s√£o as opera√ß√µes de backup naturais para calcular $v_œÄ$ e $v_*$.

Finalmente, vamos considerar como a value iteration termina. Como a policy evaluation, a value iteration formalmente requer um n√∫mero infinito de itera√ß√µes para convergir exatamente para $v_*$ [^10]. Na pr√°tica, paramos quando a fun√ß√£o de valor muda apenas por uma pequena quantidade em uma varredura.

**Algoritmo Value Iteration:**

Dado um limiar $\theta > 0$ que determina a acur√°cia da estimativa, o algoritmo pode ser descrito da seguinte forma [^3]:

1.  Inicializar $V(s)$, para todo $s \in \mathcal{S}^+$, arbitrariamente exceto $V(terminal) = 0$
2.  Loop:
    *   $\Delta \leftarrow 0$
    *   Loop para cada $s \in \mathcal{S}$:
        *   $v \leftarrow V(s)$
        *   $V(s) \leftarrow \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')]$
        *   $\Delta \leftarrow \max(\Delta, |v - V(s)|)$
    *   at√© que $\Delta < \theta$
3.  Emitir uma pol√≠tica determin√≠stica $\pi \approx \pi_*$, tal que
    *   $\pi(s) = \arg\max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')]$

![Pseudocode for Value Iteration algorithm, a method for estimating the optimal policy in an MDP.](./../images/image4.png)

> üí° **Exemplo Num√©rico:**
>
> Vamos aplicar o algoritmo value iteration ao exemplo num√©rico anterior, com $\theta = 0.01$. Inicializamos $V(s_1) = 0$ e $V(s_2) = 0$.
>
> **Itera√ß√£o 1:**
>
> *   $s_1$: $v = 0$, $V(s_1) = \max(1.0 + 0.9 * 0, 0.0 + 0.9 * 0) = 1.0$, $\Delta = \max(0, |0 - 1.0|) = 1.0$
> *   $s_2$: $v = 0$, $V(s_2) = \max(-1.0 + 0.9 * 0, 0.5 + 0.9 * 0) = 0.5$, $\Delta = \max(1.0, |0 - 0.5|) = 1.0$
>
> **Itera√ß√£o 2:**
>
> *   $s_1$: $v = 1.0$, $V(s_1) = \max(1.0 + 0.9 * 0.5, 0.0 + 0.9 * 1.0) = \max(1.45, 0.9) = 1.45$, $\Delta = \max(1.0, |1.0 - 1.45|) = 1.0$
> *   $s_2$: $v = 0.5$, $V(s_2) = \max(-1.0 + 0.9 * 1.0, 0.5 + 0.9 * 0.5) = \max(-0.1, 0.95) = 0.95$, $\Delta = \max(1.0, |0.5 - 0.95|) = 1.0$
>
> **Itera√ß√£o 3:**
>
> *   $s_1$: $v = 1.45$, $V(s_1) = \max(1.0 + 0.9 * 0.95, 0.0 + 0.9 * 1.45) = \max(1.855, 1.305) = 1.855$, $\Delta = \max(1.0, |1.45 - 1.855|) = 1.0$
> *   $s_2$: $v = 0.95$, $V(s_2) = \max(-1.0 + 0.9 * 1.45, 0.5 + 0.9 * 0.95) = \max(0.305, 1.355) = 1.355$, $\Delta = \max(1.0, |0.95 - 1.355|) = 1.0$
>
> ... (Continua at√© que $\Delta < 0.01$)
>
> Eventualmente, a value iteration ir√° convergir para os valores √≥timos $V(s_1) \approx 2.47$ e $V(s_2) \approx 1.82$. A pol√≠tica √≥tima seria ent√£o definida selecionando a a√ß√£o que maximiza a soma das recompensas esperadas e os valores descontados dos estados sucessores.

A value iteration combina efetivamente, em cada uma de suas varreduras, uma varredura de policy evaluation e uma varredura de policy improvement [^11]. Uma converg√™ncia mais r√°pida √© frequentemente alcan√ßada intercalando m√∫ltiplas varreduras de policy evaluation entre cada varredura de policy improvement [^11]. Em geral, a classe inteira de algoritmos de policy iteration truncados pode ser pensada como sequ√™ncias de varreduras, algumas das quais usam atualiza√ß√µes de policy evaluation e algumas das quais usam atualiza√ß√µes de value iteration [^11]. Como a opera√ß√£o `max` em (4.10) [^11] √© a √∫nica diferen√ßa entre essas atualiza√ß√µes, isso significa apenas que a opera√ß√£o `max` √© adicionada a algumas varreduras de policy evaluation. Todos esses algoritmos convergem para uma pol√≠tica √≥tima para MDPs finitos descontados [^11].

Al√©m de interromper a value iteration com base em um limiar $\theta$, podemos tamb√©m considerar o n√∫mero m√°ximo de itera√ß√µes como crit√©rio de parada.

> üí° **Exemplo Num√©rico:**
>
> Considere um cen√°rio onde, em vez de um limiar de converg√™ncia $\theta$, definimos um n√∫mero m√°ximo de itera√ß√µes $K = 100$. Mesmo que as mudan√ßas nos valores dos estados sejam maiores que um limiar muito pequeno, interromperemos o algoritmo ap√≥s 100 itera√ß√µes. Isto garante que o algoritmo termine em um tempo razo√°vel, mesmo que a converg√™ncia para os valores √≥timos seja lenta. No exemplo anterior, poder√≠amos rodar value iteration por 100 itera√ß√µes e ent√£o usar os valores resultantes para determinar uma pol√≠tica "sub-√≥tima", que ainda pode ser muito boa na pr√°tica.
>
> ```python
> import numpy as np
>
> # Define the MDP parameters
> n_states = 2
> n_actions = 2
> gamma = 0.9
>
> # Initialize value function
> V = np.zeros(n_states)
>
> # Define transition probabilities and rewards
> # P[s, a, s'] represents the probability of transitioning from state s to s' after taking action a
> # R[s, a, s'] represents the reward received when transitioning from state s to s' after taking action a
> P = np.zeros((n_states, n_actions, n_states))
> R = np.zeros((n_states, n_actions, n_states))
>
> # State s1
> P[0, 0, 1] = 1.0  # Action a1: transitions to s2 with probability 1.0
> R[0, 0, 1] = 1.0  # Action a1: reward of 1.0
> P[0, 1, 0] = 1.0  # Action a2: transitions to s1 with probability 1.0
> R[0, 1, 0] = 0.0  # Action a2: reward of 0.0
>
> # State s2
> P[1, 0, 0] = 1.0  # Action a1: transitions to s1 with probability 1.0
> R[1, 0, 0] = -1.0 # Action a1: reward of -1.0
> P[1, 1, 1] = 1.0  # Action a2: transitions to s2 with probability 1.0
> R[1, 1, 1] = 0.5  # Action a2: reward of 0.5
>
> # Value Iteration with fixed number of iterations
> K = 100  # Maximum number of iterations
>
> for k in range(K):
>     V_old = np.copy(V)
>     for s in range(n_states):
>         Q = np.zeros(n_actions)
>         for a in range(n_actions):
>             Q[a] = np.sum(P[s, a, :] * (R[s, a, :] + gamma * V_old[:]))
>         V[s] = np.max(Q)
>
> print("Final Value Function after {} iterations:".format(K))
> print(V)
>
> # Determine the optimal policy
> policy = np.zeros(n_states, dtype=int)
> for s in range(n_states):
>     Q = np.zeros(n_actions)
>     for a in range(n_actions):
>         Q[a] = np.sum(P[s, a, :] * (R[s, a, :] + gamma * V[:]))
>     policy[s] = np.argmax(Q)
>
> print("Optimal Policy:")
> print(policy)
> ```
> Este c√≥digo realiza value iteration por 100 itera√ß√µes e imprime a fun√ß√£o de valor resultante e a pol√≠tica √≥tima.

**Teorema 1.1** (Value Iteration com Crit√©rio de Parada por N√∫mero de Itera√ß√µes) Seja $v_k$ a fun√ß√£o de valor obtida ap√≥s $k$ itera√ß√µes da value iteration. Ent√£o, para qualquer estado $s \in \mathcal{S}$, o erro entre $v_k(s)$ e $v_*(s)$ √© limitado e diminui com o aumento de $k$.

*Proof:* A prova decorre da propriedade de contra√ß√£o da value iteration. Cada itera√ß√£o aproxima a fun√ß√£o de valor da √≥tima, e o erro m√°ximo diminui a uma taxa proporcional ao fator de desconto $\gamma$.

Aqui est√° uma prova mais detalhada:

I. **Definir o Operador de Bellman:** Seja $T$ o operador de Bellman definido como:
$$(Tv)(s) = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] \quad \forall s \in \mathcal{S}$$
A value iteration √© dada por $v_{k+1} = Tv_k$.

II. **Erro ap√≥s k itera√ß√µes:** Seja $v_*$ a fun√ß√£o de valor √≥tima, que satisfaz a equa√ß√£o de Bellman $v_* = Tv_*$. Definimos o erro ap√≥s $k$ itera√ß√µes como $e_k = ||v_k - v_*||$, onde $||\cdot||$ denota a norma do supremo, ou seja, $||v|| = \max_{s \in \mathcal{S}} |v(s)|$.

III. **Propriedade de Contra√ß√£o:** Como mostrado na prova do Teorema 1, o operador de Bellman $T$ √© uma contra√ß√£o com fator de contra√ß√£o $\gamma$, ou seja, para quaisquer fun√ß√µes de valor $u$ e $v$:
$$||Tu - Tv|| \leq \gamma ||u - v||$$

IV. **Limite Superior do Erro:** Aplicando a propriedade de contra√ß√£o √† value iteration, temos:
$$||v_{k+1} - v_*|| = ||Tv_k - Tv_*|| \leq \gamma ||v_k - v_*||$$
$$e_{k+1} \leq \gamma e_k$$

Aplicando esta desigualdade recursivamente, obtemos:
$$e_k \leq \gamma^k e_0$$
Onde $e_0 = ||v_0 - v_*||$ √© o erro inicial.

V. **Conclus√£o:** Como $0 \leq \gamma < 1$, temos que $\lim_{k \to \infty} \gamma^k = 0$. Portanto,
$$\lim_{k \to \infty} e_k = \lim_{k \to \infty} ||v_k - v_*|| \leq \lim_{k \to \infty} \gamma^k e_0 = 0$$
Isso significa que o erro $||v_k - v_*||$ converge para 0 √† medida que $k$ aumenta. Portanto, para qualquer estado $s \in \mathcal{S}$, o erro entre $v_k(s)$ e $v_*(s)$ √© limitado e diminui com o aumento de $k$. Especificamente, $|v_k(s) - v_*(s)| \leq \gamma^k e_0$ para todo $s \in \mathcal{S}$. ‚ñ†

### Conclus√£o

A value iteration oferece uma abordagem eficiente para resolver MDPs, evitando a necessidade de policy evaluation completa em cada itera√ß√£o [^10]. Ao combinar policy improvement e policy evaluation truncada em uma √∫nica opera√ß√£o de atualiza√ß√£o, a value iteration converge diretamente para a fun√ß√£o de valor √≥tima, que por sua vez permite a deriva√ß√£o da pol√≠tica √≥tima. Este m√©todo, juntamente com a policy iteration, fornece as bases para muitos algoritmos de reinforcement learning, e suas propriedades de converg√™ncia e efici√™ncia tornam-no uma ferramenta valiosa no arsenal do praticante de reinforcement learning.

### Refer√™ncias
[^1]: Chapter 4: Dynamic Programming
[^2]: 4.1 Policy Evaluation (Prediction)
[^3]: 4.1. Policy Evaluation (Prediction)
[^4]: 4.2 Policy Improvement
[^5]: Figure 4.1: Convergence of iterative policy evaluation on a small gridworld.
[^10]: 4.4 Value Iteration
[^11]: 4.4 Value Iteration
<!-- END -->