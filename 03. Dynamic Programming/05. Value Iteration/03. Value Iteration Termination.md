## Valor Itera√ß√£o e Converg√™ncia

### Introdu√ß√£o
Como vimos anteriormente, **Dynamic Programming (DP)** oferece uma cole√ß√£o de algoritmos para computar *pol√≠ticas √≥timas* dado um modelo perfeito do ambiente como um **Processo de Decis√£o de Markov (MDP)** [^1]. No entanto, o custo computacional dos algoritmos DP cl√°ssicos pode ser proibitivo. Para mitigar este problema, o conceito de **Value Iteration** surge como uma alternativa, que, essencialmente, combina passos de *policy improvement* e *truncated policy evaluation* [^4]. Este cap√≠tulo explora a termina√ß√£o do algoritmo de Value Iteration e a pol√≠tica resultante.

### Conceitos Fundamentais
O Value Iteration √© um algoritmo iterativo que busca encontrar a fun√ß√£o de valor √≥tima $v_*$ para um dado MDP. Diferentemente do *Policy Iteration*, o Value Iteration n√£o requer uma avalia√ß√£o completa da pol√≠tica em cada itera√ß√£o. Em vez disso, ele aplica um √∫nico passo de atualiza√ß√£o de valor para cada estado, combinando assim *policy improvement* e *truncated policy evaluation*. A atualiza√ß√£o √© definida como [^4]:
$$
v_{k+1}(s) = \max_{a} \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t=s, A_t=a] = \max_{a} \sum_{s',r} p(s', r|s, a) [r + \gamma v_k(s')]
$$
onde $v_k(s)$ representa a estimativa da fun√ß√£o de valor no passo $k$, $\gamma$ √© o fator de desconto, $p(s', r|s, a)$ √© a probabilidade de transi√ß√£o do estado $s$ para o estado $s'$ com recompensa $r$ ao tomar a a√ß√£o $a$.

> üí° **Exemplo Num√©rico:** Imagine um MDP com 3 estados (A, B, C) e 2 a√ß√µes (0, 1). Suponha que no estado A, ao tomar a a√ß√£o 0, voc√™ tem 80% de chance de ir para o estado B com recompensa 1 e 20% de chance de ir para o estado C com recompensa 0. Ao tomar a a√ß√£o 1, voc√™ tem 50% de chance de ir para o estado B com recompensa 0 e 50% de chance de ir para o estado C com recompensa 2. Seja $\gamma = 0.9$. Se $v_k(B) = 5$ e $v_k(C) = 2$, ent√£o:
>
> Para a a√ß√£o 0: $0.8 * (1 + 0.9 * 5) + 0.2 * (0 + 0.9 * 2) = 0.8 * 5.5 + 0.2 * 1.8 = 4.4 + 0.36 = 4.76$
> Para a a√ß√£o 1: $0.5 * (0 + 0.9 * 5) + 0.5 * (2 + 0.9 * 2) = 0.5 * 4.5 + 0.5 * 3.8 = 2.25 + 1.9 = 4.15$
>
> Portanto, $v_{k+1}(A) = \max(4.76, 4.15) = 4.76$.

Uma quest√£o crucial √© quando parar este processo iterativo. Formalmente, Value Iteration requer um n√∫mero infinito de itera√ß√µes para convergir exatamente para $v_*$ [^4]. No entanto, na pr√°tica, o algoritmo √© interrompido quando a fun√ß√£o de valor muda apenas por uma pequena quantidade em uma itera√ß√£o.

![Pseudocode for Value Iteration algorithm, a method for estimating the optimal policy in an MDP.](./../images/image4.png)

**Lema 1:** *Contraction Mapping*. A atualiza√ß√£o de valor no Value Iteration √© uma contra√ß√£o em rela√ß√£o √† norma do supremo (ou norma do m√°ximo), definida como $||v|| = \max_{s} |v(s)|$. Isso significa que existe um fator $\gamma \in [0, 1)$ tal que para quaisquer duas fun√ß√µes de valor $u$ e $v$:

$$||T(u) - T(v)|| \le \gamma ||u - v||$$

onde $T$ √© o operador de Bellman para o valor √≥timo: $T(v)(s) = \max_{a} \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')]$.

*Proof Strategy:* A prova envolve expandir a defini√ß√£o do operador de Bellman, aplicar a desigualdade triangular e usar o fator de desconto $\gamma$ para mostrar a contra√ß√£o. Este resultado garante que as itera√ß√µes de Value Iteration convergem para uma √∫nica fun√ß√£o de valor √≥tima.

*Prova:*
I. Seja $T(u)$ e $T(v)$ os operadores de Bellman aplicados √†s fun√ß√µes de valor $u$ e $v$, respectivamente. Para qualquer estado $s$, temos:
$$T(u)(s) = \max_{a} \sum_{s',r} p(s', r|s, a) [r + \gamma u(s')] \\ T(v)(s) = \max_{a} \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')] $$

II. Consideremos a diferen√ßa $|T(u)(s) - T(v)(s)|$. Podemos escrever:
$$|T(u)(s) - T(v)(s)| = \left| \max_{a} \sum_{s',r} p(s', r|s, a) [r + \gamma u(s')] - \max_{a} \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')] \right|$$

III. Seja $a_1$ a a√ß√£o √≥tima para $T(u)(s)$ e $a_2$ a a√ß√£o √≥tima para $T(v)(s)$. Ent√£o, por defini√ß√£o:
$$T(u)(s) = \sum_{s',r} p(s', r|s, a_1) [r + \gamma u(s')] \\ T(v)(s) = \sum_{s',r} p(s', r|s, a_2) [r + \gamma v(s')] $$
Portanto:
$$T(u)(s) \ge \sum_{s',r} p(s', r|s, a_2) [r + \gamma u(s')] \\ T(v)(s) \ge \sum_{s',r} p(s', r|s, a_1) [r + \gamma v(s')] $$

IV. Subtraindo a segunda desigualdade da primeira, temos:
$$T(u)(s) - T(v)(s) \ge \gamma \sum_{s',r} p(s', r|s, a_2) [u(s') - v(s')] $$
E subtraindo a primeira desigualdade da segunda, temos:
$$T(v)(s) - T(u)(s) \ge \gamma \sum_{s',r} p(s', r|s, a_1) [v(s') - u(s')] $$
Combinando essas desigualdades:
$$|T(u)(s) - T(v)(s)| \le \gamma \max_{a} \left| \sum_{s',r} p(s', r|s, a) [u(s') - v(s')] \right|$$

V. Usando a desigualdade triangular:
$$|T(u)(s) - T(v)(s)| \le \gamma \max_{a} \sum_{s',r} p(s', r|s, a) |u(s') - v(s')|$$

VI. Como $\sum_{s',r} p(s', r|s, a) = 1$, podemos majorar a express√£o por:
$$|T(u)(s) - T(v)(s)| \le \gamma \max_{s'} |u(s') - v(s')| = \gamma ||u - v||$$

VII. Portanto, temos:
$$||T(u) - T(v)|| = \max_{s} |T(u)(s) - T(v)(s)| \le \gamma ||u - v||$$
Isso mostra que $T$ √© uma contra√ß√£o com fator $\gamma$. ‚ñ†

**Crit√©rio de Termina√ß√£o:** O algoritmo Value Iteration termina quando a mudan√ßa m√°xima na fun√ß√£o de valor em uma √∫nica itera√ß√£o √© menor do que um limiar $\theta$ predefinido. Ou seja [^4]:
$$
\max_{s \in S} |v_{k+1}(s) - v_k(s)| < \theta
$$
onde $S$ √© o conjunto de todos os estados. Este crit√©rio garante que a fun√ß√£o de valor est√° suficientemente pr√≥xima da fun√ß√£o de valor ideal $v_*$.

> üí° **Exemplo Num√©rico:** Suponha que temos 5 estados e, ap√≥s uma itera√ß√£o de Value Iteration, os valores dos estados mudaram da seguinte forma:
>
> $v_k = [10, 12, 15, 8, 9]$
> $v_{k+1} = [10.1, 12.05, 15.01, 8, 8.9]$
>
> As diferen√ßas absolutas s√£o: $[|10.1 - 10|, |12.05 - 12|, |15.01 - 15|, |8 - 8|, |8.9 - 9|] = [0.1, 0.05, 0.01, 0, 0.1]$.
>
> Portanto, $\max_{s \in S} |v_{k+1}(s) - v_k(s)| = 0.1$. Se nosso limiar $\theta$ for 0.05, ent√£o o crit√©rio de termina√ß√£o N√ÉO foi atingido e o algoritmo deve continuar. Se $\theta$ fosse 0.2, o algoritmo terminaria.

**Teorema 1:** *Converg√™ncia do Value Iteration*. Sob o crit√©rio de termina√ß√£o definido acima, a fun√ß√£o de valor $v_k$ converge para uma vizinhan√ßa de $v_*$. Especificamente, a dist√¢ncia entre $v_k$ e $v_*$ √© limitada por:

$$||v_k - v_*|| \le \frac{\theta}{1 - \gamma}$$

*Proof Strategy:* Usando o Lema 1 (Contraction Mapping), podemos mostrar que a sequ√™ncia de fun√ß√µes de valor gerada por Value Iteration se aproxima da fun√ß√£o de valor √≥tima $v_*$. O crit√©rio de termina√ß√£o garante que essa aproxima√ß√£o est√° dentro de um raio de $\frac{\theta}{1 - \gamma}$ de $v_*$.

*Prova:*
I. Seja $T$ o operador de Bellman para o valor √≥timo e $v_*$ a fun√ß√£o de valor √≥tima. Ent√£o, $v_* = T(v_*)$.

II. Pelo crit√©rio de termina√ß√£o, temos:
$$||v_{k+1} - v_k|| \le \theta$$

III. Como $v_{k+1} = T(v_k)$, podemos escrever:
$$||T(v_k) - v_k|| \le \theta$$

IV. Agora, consideremos a diferen√ßa entre $v_k$ e $v_*$:
$$||v_k - v_*|| = ||v_k - T(v_*)||$$

V. Adicionando e subtraindo $T(v_k)$:
$$||v_k - v_*|| = ||v_k - T(v_k) + T(v_k) - T(v_*)||$$

VI. Pela desigualdade triangular:
$$||v_k - v_*|| \le ||v_k - T(v_k)|| + ||T(v_k) - T(v_*)||$$

VII. Usando o Lema 1 (Contraction Mapping):
$$||T(v_k) - T(v_*)|| \le \gamma ||v_k - v_*||$$

VIII. Substituindo na desigualdade:
$$||v_k - v_*|| \le ||v_k - T(v_k)|| + \gamma ||v_k - v_*||$$

IX. Usando o crit√©rio de termina√ß√£o $||v_k - T(v_k)|| \le \theta$:
$$||v_k - v_*|| \le \theta + \gamma ||v_k - v_*||$$

X. Rearranjando os termos:
$$||v_k - v_*|| - \gamma ||v_k - v_*|| \le \theta$$
$$(1 - \gamma) ||v_k - v_*|| \le \theta$$

XI. Finalmente:
$$||v_k - v_*|| \le \frac{\theta}{1 - \gamma}$$
Isso mostra que a dist√¢ncia entre $v_k$ e $v_*$ √© limitada por $\frac{\theta}{1 - \gamma}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Se $\theta = 0.01$ e $\gamma = 0.9$, ent√£o $||v_k - v_*|| \le \frac{0.01}{1 - 0.9} = \frac{0.01}{0.1} = 0.1$. Isso significa que a fun√ß√£o de valor obtida pelo Value Iteration est√° a no m√°ximo 0.1 de dist√¢ncia da fun√ß√£o de valor √≥tima. Se $\gamma$ fosse 0.99, ent√£o $||v_k - v_*|| \le \frac{0.01}{1 - 0.99} = \frac{0.01}{0.01} = 1$. Observe que quanto maior o fator de desconto $\gamma$, maior a dist√¢ncia potencial da solu√ß√£o obtida da solu√ß√£o √≥tima, para um mesmo $\theta$.

**Pol√≠tica Resultante:** Ap√≥s a termina√ß√£o do algoritmo Value Iteration, uma pol√≠tica determin√≠stica $\pi \approx \pi_*$ pode ser extra√≠da [^4]. Esta pol√≠tica √© definida selecionando a a√ß√£o que maximiza o valor esperado para cada estado:
$$
\pi(s) = \arg \max_{a} \sum_{s',r} p(s', r|s, a) [r + \gamma V(s')]
$$
onde $V(s)$ √© a fun√ß√£o de valor estimada ao final do algoritmo Value Iteration.

> üí° **Exemplo Num√©rico:** Usando o primeiro exemplo num√©rico, suponha que ap√≥s a termina√ß√£o do Value Iteration, $V(B) = 5.1$ e $V(C) = 2.2$.  Recalculando os valores para o estado A:
>
> Para a a√ß√£o 0: $0.8 * (1 + 0.9 * 5.1) + 0.2 * (0 + 0.9 * 2.2) = 0.8 * 5.59 + 0.2 * 1.98 = 4.472 + 0.396 = 4.868$
> Para a a√ß√£o 1: $0.5 * (0 + 0.9 * 5.1) + 0.5 * (2 + 0.9 * 2.2) = 0.5 * 4.59 + 0.5 * 3.98 = 2.295 + 1.99 = 4.285$
>
> Portanto, $\pi(A) = \arg \max(4.868, 4.285) = 0$. A pol√≠tica no estado A seria escolher a a√ß√£o 0.

**Teorema 1.1:** *Optimalidade da Pol√≠tica Resultante*.  Se $v$ √© $\epsilon$-√≥tima, ou seja, $||v - v_*|| < \epsilon$, ent√£o a pol√≠tica gulosa $\pi$ com respeito a $v$ √© $2\epsilon \gamma / (1-\gamma)$-√≥tima.

*Proof Strategy:* Este resultado demonstra que se a fun√ß√£o de valor obtida pelo Value Iteration est√° pr√≥xima da √≥tima, ent√£o a pol√≠tica derivada dessa fun√ß√£o de valor tamb√©m estar√° pr√≥xima da pol√≠tica √≥tima. A prova envolve mostrar que a pol√≠tica gulosa com respeito a uma fun√ß√£o de valor $\epsilon$-√≥tima tem um desempenho quase t√£o bom quanto a pol√≠tica √≥tima.

*Prova:*
I. Seja $v$ uma fun√ß√£o de valor $\epsilon$-√≥tima, ou seja, $||v - v_*|| < \epsilon$. Seja $\pi$ a pol√≠tica gulosa com respeito a $v$, e $\pi_*$ a pol√≠tica √≥tima.

II. Seja $v_\pi$ a fun√ß√£o de valor da pol√≠tica $\pi$. Queremos mostrar que $||v_\pi - v_*|| \le \frac{2\epsilon\gamma}{1 - \gamma}$.

III. Para qualquer estado $s$, temos:
$$v_*(s) = \max_a \sum_{s', r} p(s', r | s, a)[r + \gamma v_*(s')] \\ v_\pi(s) = \sum_{s', r} p(s', r | s, \pi(s))[r + \gamma v_\pi(s')] $$

IV. Como $\pi$ √© a pol√≠tica gulosa com respeito a $v$, temos:
$$\sum_{s', r} p(s', r | s, \pi(s))[r + \gamma v(s')] = \max_a \sum_{s', r} p(s', r | s, a)[r + \gamma v(s')] $$

V. Defina o operador de Bellman para a pol√≠tica $\pi$ como $T_\pi$. Ent√£o, $v_\pi = T_\pi v_\pi$ e $v_* = T v_*$, onde $T$ √© o operador de Bellman para o valor √≥timo.

VI. Considere a diferen√ßa:
$$||v_\pi - v_*|| = ||T_\pi v_\pi - T v_*|| \le ||T_\pi v_\pi - T_\pi v_*|| + ||T_\pi v_* - T v_*||$$

VII. Usando o fato de que $T_\pi$ √© uma contra√ß√£o com fator $\gamma$:
$$||T_\pi v_\pi - T_\pi v_*|| \le \gamma ||v_\pi - v_*||$$

VIII. Agora, precisamos limitar $||T_\pi v_* - T v_*||$. Para qualquer estado $s$:
$$|T_\pi v_*(s) - T v_*(s)| = |\sum_{s', r} p(s', r | s, \pi(s))[r + \gamma v_*(s')] - \max_a \sum_{s', r} p(s', r | s, a)[r + \gamma v_*(s')]|$$
Como $\pi(s)$ √© gulosa com respeito a $v$, mas n√£o necessariamente com respeito a $v_*$, podemos usar o fato de que $v$ √© $\epsilon$-√≥tima:
$$\sum_{s', r} p(s', r | s, \pi(s))[r + \gamma v(s')] \ge \max_a \sum_{s', r} p(s', r | s, a)[r + \gamma v(s')] - 2\epsilon$$
Isso implica:
$$|T_\pi v_*(s) - T v_*(s)| \le \gamma \epsilon + \gamma ||v - v_*|| \le 2 \gamma \epsilon$$
Assim, $||T_\pi v_* - T v_*|| \le 2\gamma \epsilon$.

IX. Substituindo na desigualdade original:
$$||v_\pi - v_*|| \le \gamma ||v_\pi - v_*|| + 2 \gamma \epsilon$$
$$(1 - \gamma) ||v_\pi - v_*|| \le 2 \gamma \epsilon$$
$$||v_\pi - v_*|| \le \frac{2\epsilon\gamma}{1 - \gamma}$$
Portanto, a pol√≠tica gulosa $\pi$ √© $\frac{2\epsilon\gamma}{1 - \gamma}$-√≥tima. ‚ñ†

> üí° **Exemplo Num√©rico:** Se $\epsilon = 0.1$ e $\gamma = 0.9$, ent√£o a pol√≠tica resultante √© $\frac{2 * 0.1 * 0.9}{1 - 0.9} = \frac{0.18}{0.1} = 1.8$-√≥tima. Isso significa que a fun√ß√£o de valor da pol√≠tica obtida est√° a no m√°ximo 1.8 de dist√¢ncia da fun√ß√£o de valor √≥tima. Este teorema nos diz que a qualidade da pol√≠tica resultante depende da qualidade da fun√ß√£o de valor estimada ($v$).

### Conclus√£o
Value Iteration √© um algoritmo poderoso para encontrar pol√≠ticas quase √≥timas em MDPs [^1]. A capacidade de truncar o processo de *policy evaluation* e a combina√ß√£o simult√¢nea de *policy improvement* e *truncated policy evaluation* tornam-no uma alternativa eficiente para o *Policy Iteration* [^4]. A termina√ß√£o do algoritmo com base em um limiar $\theta$ garante uma solu√ß√£o aproximada, mas computacionalmente trat√°vel. A pol√≠tica resultante, $\pi \approx \pi_*$, oferece uma estrat√©gia de tomada de decis√£o que busca maximizar o retorno esperado com base na fun√ß√£o de valor aprendida.

### Refer√™ncias
[^1]: Dynamic Programming.
[^2]: Bellman optimality equation (4.1).
[^3]: policy evaluation update (4.5).
[^4]: Value Iteration Algorithm.
<!-- END -->