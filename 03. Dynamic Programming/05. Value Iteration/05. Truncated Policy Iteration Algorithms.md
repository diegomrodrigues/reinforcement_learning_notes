## Truncated Policy Iteration

### Introdu√ß√£o
Este cap√≠tulo explora os algoritmos de **truncated policy iteration**, que representam uma generaliza√ß√£o dos m√©todos de dynamic programming (DP) abordados anteriormente. Em vez de realizar uma policy evaluation completa a cada itera√ß√£o, esses algoritmos truncam o processo de avalia√ß√£o, combinando passos de **policy evaluation** com passos de **value iteration** [^83]. Essa abordagem permite uma maior flexibilidade e pode resultar em converg√™ncia mais r√°pida em compara√ß√£o com a policy iteration tradicional. O foco principal ser√° demonstrar como esses algoritmos, apesar de suas varia√ß√µes, mant√™m a garantia de converg√™ncia para uma pol√≠tica √≥tima em MDPs (Markov Decision Processes) finitos com desconto.

### Conceitos Fundamentais

Como vimos anteriormente, a **policy iteration** envolve alternar entre duas fases principais: **policy evaluation** e **policy improvement** [^86]. A **policy evaluation** calcula a value function $v_\pi$ para uma dada policy $\pi$, enquanto a **policy improvement** constr√≥i uma nova policy $\pi'$ que √© greedy com respeito a $v_\pi$ [^79].

![Diagrama da itera√ß√£o da pol√≠tica generalizada (GPI) mostrando o ciclo entre avalia√ß√£o e melhoria da pol√≠tica.](./../images/image2.png)

No entanto, a policy evaluation completa pode ser computacionalmente cara, especialmente para MDPs grandes [^81]. A **value iteration** surge como uma alternativa, combinando os passos de policy evaluation e policy improvement em uma √∫nica opera√ß√£o de atualiza√ß√£o [^83]. Em vez de calcular $v_\pi$ completamente, a value iteration atualiza diretamente a value function $v_{k+1}(s)$ usando a equa√ß√£o de Bellman otimista:

$$
v_{k+1}(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a] = \max_a \sum_{s',r} p(s', r | s, a) [r + \gamma v_k(s')] \quad \forall s \in S
$$
[^83]

> üí° **Exemplo Num√©rico:** Considere um MDP com dois estados $S = \{s_1, s_2\}$ e duas a√ß√µes $A = \{a_1, a_2\}$. Suponha que a fun√ß√£o de recompensa e a fun√ß√£o de transi√ß√£o sejam as seguintes:
>
> *   $R(s_1, a_1, s_1) = 1$, $R(s_1, a_1, s_2) = 0$, $P(s_1 | s_1, a_1) = 0.8$, $P(s_2 | s_1, a_1) = 0.2$
> *   $R(s_1, a_2, s_1) = 0$, $R(s_1, a_2, s_2) = 2$, $P(s_1 | s_1, a_2) = 0.1$, $P(s_2 | s_1, a_2) = 0.9$
> *   $R(s_2, a_1, s_1) = 0$, $R(s_2, a_1, s_2) = 1$, $P(s_1 | s_2, a_1) = 0.3$, $P(s_2 | s_2, a_1) = 0.7$
> *   $R(s_2, a_2, s_1) = 2$, $R(s_2, a_2, s_2) = 0$, $P(s_1 | s_2, a_2) = 0.6$, $P(s_2 | s_2, a_2) = 0.4$
>
> Se definirmos $\gamma = 0.9$, e iniciarmos com $v_0(s_1) = 0$ e $v_0(s_2) = 0$, a primeira itera√ß√£o da value iteration seria:
>
> $v_1(s_1) = \max \{ 0.8(1 + 0.9 \cdot 0) + 0.2(0 + 0.9 \cdot 0), 0.1(0 + 0.9 \cdot 0) + 0.9(2 + 0.9 \cdot 0) \} = \max \{0.8, 1.8\} = 1.8$
>
> $v_1(s_2) = \max \{ 0.3(0 + 0.9 \cdot 0) + 0.7(1 + 0.9 \cdot 0), 0.6(2 + 0.9 \cdot 0) + 0.4(0 + 0.9 \cdot 0) \} = \max \{0.7, 1.2\} = 1.2$
>
> Portanto, ap√≥s a primeira itera√ß√£o, $v_1(s_1) = 1.8$ e $v_1(s_2) = 1.2$. Este processo √© repetido at√© a converg√™ncia.

![Pseudocode for Value Iteration algorithm, a method for estimating the optimal policy in an MDP.](./../images/image4.png)

Os algoritmos de **truncated policy iteration** exploram um espectro entre esses dois extremos, truncando o processo de policy evaluation. Isso significa que a policy evaluation √© interrompida antes da converg√™ncia completa para $v_\pi$, e a policy √© aprimorada com base nesta value function *parcialmente avaliada*.

A classe dos truncated policy iteration algorithms pode ser vista como sequ√™ncias de *sweeps* (varreduras) [^83]. Um *sweep* consiste em atualizar o valor de cada estado no espa√ßo de estados. Alguns desses sweeps usam policy evaluation updates, enquanto outros usam value iteration updates.

√â importante notar que a opera√ß√£o de maximiza√ß√£o (max) na equa√ß√£o de value iteration (4.10) [^83] √© a √∫nica diferen√ßa entre os updates de policy evaluation (4.5) [^75] e value iteration. Assim, pode-se pensar em aplicar a opera√ß√£o de maximiza√ß√£o em alguns *sweeps* de policy evaluation.

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

**Lemma 1 (Converg√™ncia da Truncated Policy Iteration):** Para um MDP finito com desconto, qualquer algoritmo de truncated policy iteration que continue a atualizar todos os estados e intercale passos de policy evaluation e value iteration ir√° convergir para a value function √≥tima $v_*$ e uma pol√≠tica √≥tima $\pi_*$.

*Proof:*
A prova desse lema se baseia no fato de que tanto policy evaluation quanto value iteration s√£o garantidos para convergir sob as condi√ß√µes especificadas. Como a truncated policy iteration combina esses dois processos, e garante-se que todos os estados sejam atualizados continuamente, o algoritmo eventualmente ir√° convergir para uma solu√ß√£o √≥tima. A converg√™ncia para a value function √≥tima $v_*$ garante que a pol√≠tica resultante $\pi_*$ seja tamb√©m √≥tima. $\blacksquare$

Uma quest√£o importante relacionada √† converg√™ncia √© o qu√£o pr√≥ximo precisamos estar da value function √≥tima $v_*$ em cada itera√ß√£o para garantir um progresso significativo. O pr√≥ximo lema aborda essa quest√£o, fornecendo um bound para a diferen√ßa entre a value function da pol√≠tica atual e a value function √≥tima, em termos da precis√£o da avalia√ß√£o da pol√≠tica.

**Lemma 1.1:** Seja $v$ uma aproxima√ß√£o da value function $v_\pi$ de uma pol√≠tica $\pi$, tal que $||v - v_\pi||_\infty \leq \epsilon$, onde $||\cdot||_\infty$ denota a norma do supremo. Seja $\pi'$ uma pol√≠tica greedy com respeito a $v$. Ent√£o,

$$v_{\pi'}(s) \geq v_\pi(s) + \frac{2\gamma\epsilon}{1-\gamma}, \quad \forall s \in S$$

*Proof:*
Seja $q_\pi(s,a) = \sum_{s',r}p(s',r|s,a)[r + \gamma v_\pi(s')]$. Como $\pi'$ √© greedy com respeito a $v$, temos:

$$v(s) \leq q_\pi(s, \pi'(s)) =  \sum_{s',r}p(s',r|s,\pi'(s))[r + \gamma v(s')]$$

Al√©m disso, como $||v - v_\pi||_\infty \leq \epsilon$, ent√£o:

$$v_\pi(s') - \epsilon \leq v(s') \leq v_\pi(s') + \epsilon$$

Combinando as desigualdades:

$$v_\pi(s) - \epsilon \leq  \sum_{s',r}p(s',r|s,\pi'(s))[r + \gamma (v_\pi(s') + \epsilon)] = q_{\pi'}(s, \pi'(s)) + \gamma \epsilon$$

Portanto,

$$v_\pi(s) \leq q_{\pi'}(s, \pi'(s)) + \epsilon + \gamma \epsilon $$

Aplicando a defini√ß√£o da value function $v_{\pi'}(s) = \mathbb{E}[R_{t+1} + \gamma v_{\pi'}(S_{t+1}) | S_t = s, A_t = \pi'(s)]$, temos:

$$ v_{\pi'}(s) \geq v_\pi(s) - \epsilon - \gamma \epsilon $$

Iterando essa rela√ß√£o, obtemos:

$$v_{\pi'}(s) \geq v_\pi(s) + \frac{2\gamma\epsilon}{1-\gamma}, \quad \forall s \in S$$

$\blacksquare$

### Exemplo

Para ilustrar, considere um algoritmo que realiza $m$ sweeps de policy evaluation para obter uma value function aproximada $v \approx v_\pi$ [^75], seguido por um *sweep* de value iteration para melhorar a value function $v \approx v_*$ [^83], e repete este processo iterativamente. Esse algoritmo se encaixa na classe de truncated policy iteration algorithms e, portanto, converge para a pol√≠tica √≥tima.

> üí° **Exemplo Num√©rico:** Seja o mesmo MDP do exemplo anterior. Suponha que a pol√≠tica inicial $\pi$ seja escolher $a_1$ em $s_1$ e $a_1$ em $s_2$.
>
> **Policy Evaluation (2 sweeps):**
>
> *   Inicializa√ß√£o: $v_0(s_1) = 0$, $v_0(s_2) = 0$
>
> *   Sweep 1:
>
>     $v_1(s_1) = 0.8(1 + 0.9v_0(s_1)) + 0.2(0 + 0.9v_0(s_2)) = 0.8$
>
>     $v_1(s_2) = 0.3(0 + 0.9v_0(s_1)) + 0.7(1 + 0.9v_0(s_2)) = 0.7$
>
> *   Sweep 2:
>
>     $v_2(s_1) = 0.8(1 + 0.9v_1(s_1)) + 0.2(0 + 0.9v_1(s_2)) = 0.8(1 + 0.9(0.8)) + 0.2(0.9(0.7)) = 1.308$
>
>     $v_2(s_2) = 0.3(0 + 0.9v_1(s_1)) + 0.7(1 + 0.9v_1(s_2)) = 0.3(0.9(0.8)) + 0.7(1 + 0.9(0.7)) = 1.166$
>
> **Value Iteration (1 sweep):**
>
> $v(s_1) = \max \{ 0.8(1 + 0.9 \cdot 1.308) + 0.2(0 + 0.9 \cdot 1.166), 0.1(0 + 0.9 \cdot 1.308) + 0.9(2 + 0.9 \cdot 1.166) \} = \max \{1.754, 2.801\} = 2.801$
>
> $v(s_2) = \max \{ 0.3(0 + 0.9 \cdot 1.308) + 0.7(1 + 0.9 \cdot 1.166), 0.6(2 + 0.9 \cdot 1.308) + 0.4(0 + 0.9 \cdot 1.166) \} = \max \{1.477, 2.280\} = 2.280$
>
> Ap√≥s este ciclo, a pol√≠tica greedy mudaria para escolher $a_2$ em $s_1$ e $a_2$ em $s_2$, pois essas a√ß√µes maximizam o valor esperado. O processo se repete com esta nova pol√≠tica.

Para formalizar e provar essa afirma√ß√£o, considere a seguinte prova:

*Proof:*
Para demonstrar que o algoritmo converge, precisamos mostrar que cada itera√ß√£o do algoritmo melhora a pol√≠tica ou, no m√≠nimo, a mant√©m inalterada.

I. Ap√≥s $m$ sweeps de policy evaluation, temos uma aproxima√ß√£o $v$ de $v_\pi$ tal que $||v - v_\pi||_\infty \leq \epsilon$ para algum $\epsilon > 0$. O valor de $\epsilon$ depende de $m$ e das propriedades do MDP, mas o importante √© que podemos tornar $\epsilon$ arbitrariamente pequeno aumentando $m$.

II. Em seguida, realizamos um *sweep* de value iteration, que essencialmente encontra a a√ß√£o greedy $a = \text{argmax}_a \sum_{s',r} p(s',r|s,a)[r + \gamma v(s')]$ para cada estado $s$ e atualiza $v(s)$ para o valor esperado da a√ß√£o greedy. Seja $\pi'$ a pol√≠tica greedy com respeito a $v$ ap√≥s o *sweep* de value iteration.

III. Pelo Lemma 1.1, sabemos que $v_{\pi'}(s) \geq v_\pi(s) - \epsilon - \gamma \epsilon$. Isso significa que a value function da nova pol√≠tica $\pi'$ √© pelo menos t√£o boa quanto a value function da pol√≠tica anterior $\pi$, menos um termo de erro proporcional a $\epsilon$.

IV. Como o MDP √© finito com desconto, e continuamos a atualizar todos os estados, o erro $\epsilon$ se propagar√° e diminuir√° a cada itera√ß√£o do algoritmo. Eventualmente, o algoritmo convergir√° para uma pol√≠tica √≥tima $\pi_*$ e sua correspondente value function √≥tima $v_*$.

V. Se em alguma itera√ß√£o, a pol√≠tica $\pi'$ for a mesma que $\pi$, ent√£o $v_{\pi'}(s) = v_\pi(s)$ para todo $s$. Isso implica que j√° atingimos a pol√≠tica √≥tima, e o algoritmo convergiu.

Portanto, o algoritmo converge para a pol√≠tica √≥tima. $\blacksquare$

Al√©m disso, considere uma varia√ß√£o onde o n√∫mero de sweeps $m$ √© adaptativo. Ou seja, $m$ pode aumentar ou diminuir com base na magnitude da mudan√ßa na value function entre itera√ß√µes. Se a mudan√ßa for pequena, $m$ pode ser diminu√≠do, e se a mudan√ßa for grande, $m$ pode ser aumentado. Essa abordagem adaptativa pode melhorar ainda mais a efici√™ncia do algoritmo.
<!-- END -->