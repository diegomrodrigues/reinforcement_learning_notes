## Value Iteration: Deriving Optimality Through Iterative Updates

### Introdu√ß√£o
No cap√≠tulo anterior, introduzimos o conceito de **Dynamic Programming (DP)** como uma cole√ß√£o de algoritmos para calcular pol√≠ticas √≥timas em ambientes modelados como Processos de Decis√£o de Markov (MDPs) [^1]. Exploramos tamb√©m a **policy evaluation** e a **policy improvement**, que s√£o os pilares do **policy iteration** [^4.1, 4.2]. Agora, vamos nos aprofundar em **value iteration**, uma abordagem relacionada, mas distinta, para encontrar a pol√≠tica √≥tima [^4.4].

### Conceitos Fundamentais
A **value iteration** oferece uma alternativa ao **policy iteration**, evitando a necessidade de uma avalia√ß√£o completa da pol√≠tica em cada itera√ß√£o [^4.4]. Em vez disso, a **value iteration** combina os passos de **policy evaluation** e **policy improvement** em uma √∫nica etapa de atualiza√ß√£o, tornando-a computacionalmente mais eficiente em certos casos [^4.4].

Para complementar essa afirma√ß√£o, podemos considerar cen√°rios onde o espa√ßo de estados √© grande, mas o n√∫mero de a√ß√µes poss√≠veis em cada estado √© limitado. Nesses casos, a maximiza√ß√£o sobre a√ß√µes na **value iteration** pode ser mais r√°pida do que uma completa **policy evaluation**.

> üí° **Exemplo Num√©rico:**
> Imagine um rob√¥ navegando em um labirinto com 1000 posi√ß√µes poss√≠veis (estados), mas em cada posi√ß√£o ele s√≥ pode escolher entre 4 a√ß√µes: ir para cima, para baixo, para a esquerda ou para a direita. Em **policy iteration**, a **policy evaluation** precisaria varrer todos os 1000 estados para avaliar o valor de uma pol√≠tica, enquanto a **value iteration** s√≥ precisa calcular o m√°ximo sobre 4 a√ß√µes para cada estado, o que pode ser mais eficiente.

O ponto chave para entender a **value iteration** √© reconhecer como ela deriva diretamente da **Bellman optimality equation** [^4.4]. A **Bellman optimality equation** define a fun√ß√£o de valor √≥tima $v_*(s)$ como:

$$
v_*(s) = \max_{a} \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]
$$

Esta equa√ß√£o expressa que o valor √≥timo de um estado √© o m√°ximo do retorno esperado obtido ao tomar a melhor a√ß√£o poss√≠vel naquele estado e, subsequentemente, seguir a pol√≠tica √≥tima.

A **value iteration** transforma esta equa√ß√£o em uma regra de atualiza√ß√£o iterativa:

$$
v_{k+1}(s) = \max_{a} \mathbb{E} [R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a] = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')]
$$

para todo $s \in \mathcal{S}$ [^4.10]. Aqui, $v_k(s)$ representa a estimativa da fun√ß√£o de valor no $k$-√©simo passo de itera√ß√£o. Esta atualiza√ß√£o substitui a antiga estimativa do valor de um estado pela recompensa m√°xima esperada mais o valor descontado dos estados sucessores, considerando todas as a√ß√µes poss√≠veis.

> üí° **Exemplo Num√©rico:**
> Considere um estado $s$ onde um agente pode escolher entre duas a√ß√µes, $a_1$ e $a_2$. Suponha que, ao tomar a a√ß√£o $a_1$, o agente receba uma recompensa de 1 e v√° para o estado $s'$ com valor $v_k(s') = 5$. Ao tomar a a√ß√£o $a_2$, o agente receba uma recompensa de 2 e v√° para o estado $s''$ com valor $v_k(s'') = 4$. Assumindo um fator de desconto $\gamma = 0.9$, a atualiza√ß√£o de **value iteration** seria:
>
> $$
> v_{k+1}(s) = \max \begin{cases} 1 + 0.9 \cdot 5 \\ 2 + 0.9 \cdot 4 \end{cases} = \max \begin{cases} 5.5 \\ 5.6 \end{cases} = 5.6
> $$
>
> Portanto, $v_{k+1}(s) = 5.6$. A a√ß√£o √≥tima neste caso √© $a_2$.

Um aspecto importante √© que, para uma fun√ß√£o de valor inicial arbitr√°ria $v_0$, a sequ√™ncia $\{v_k\}$ converge para $v_*$ sob as mesmas condi√ß√µes que garantem a exist√™ncia de $v_*$ [^4.10]. Essas condi√ß√µes geralmente envolvem um fator de desconto $\gamma < 1$ ou a garantia de termina√ß√£o eventual do MDP.

> üí° **Exemplo Num√©rico:**
> Suponha que inicializamos todos os valores de estado para 0, ou seja, $v_0(s) = 0$ para todo $s$.  Ap√≥s a primeira itera√ß√£o, os valores de estado ser√£o atualizados com as recompensas esperadas para cada estado.  Como $\gamma < 1$, o efeito das recompensas futuras diminui a cada itera√ß√£o, garantindo que os valores de estado n√£o divirjam para infinito e, em vez disso, convergem para a fun√ß√£o de valor √≥tima.

Para garantir a converg√™ncia, podemos enunciar o seguinte teorema:

**Teorema 1** A sequ√™ncia de fun√ß√µes de valor $\{v_k\}$ gerada pela **value iteration** converge para a fun√ß√£o de valor √≥tima $v_*$ se o MDP for descontado (ou seja, $\gamma < 1$) ou se o MDP for epis√≥dico e terminar com probabilidade 1.

*Proof Sketch:* A prova desse teorema se baseia no fato de que a atualiza√ß√£o de Bellman √© uma contra√ß√£o em rela√ß√£o √† norma do supremo, garantindo a converg√™ncia para um ponto fixo √∫nico, que √© a fun√ß√£o de valor √≥tima.

Vamos detalhar a prova deste teorema:

**Prova do Teorema 1:**

I. **Definindo o operador de Bellman:**
   Seja $\mathcal{B}$ o operador de Bellman que representa a atualiza√ß√£o da value iteration:
   $$(\mathcal{B}v)(s) = \max_{a} \mathbb{E} [R_{t+1} + \gamma v(S_{t+1}) | S_t = s, A_t = a]$$

II. **Propriedade de Contra√ß√£o:**
    Vamos mostrar que $\mathcal{B}$ √© uma contra√ß√£o sob a norma do supremo (m√°ximo), definida como $\|v\|_{\infty} = \max_{s} |v(s)|$. Para quaisquer duas fun√ß√µes de valor $v$ e $v'$, temos:
    $$
    \begin{aligned}
    |(\mathcal{B}v)(s) - (\mathcal{B}v')(s)| &= \left| \max_{a} \mathbb{E} [R_{t+1} + \gamma v(S_{t+1}) | S_t = s, A_t = a] - \max_{a'} \mathbb{E} [R_{t+1} + \gamma v'(S_{t+1}) | S_t = s, A_t = a'] \right| \\
    &\leq \max_{a} \left| \mathbb{E} [R_{t+1} + \gamma v(S_{t+1}) | S_t = s, A_t = a] - \mathbb{E} [R_{t+1} + \gamma v'(S_{t+1}) | S_t = s, A_t = a] \right| \\
    &= \max_{a} \left| \gamma \mathbb{E} [v(S_{t+1}) - v'(S_{t+1}) | S_t = s, A_t = a] \right| \\
    &\leq \gamma \max_{a} \mathbb{E} [|v(S_{t+1}) - v'(S_{t+1})| | S_t = s, A_t = a] \\
    &\leq \gamma \|v - v'\|_{\infty}
    \end{aligned}
    $$
    Portanto, $\|\mathcal{B}v - \mathcal{B}v'\|_{\infty} \leq \gamma \|v - v'\|_{\infty}$.  Se $\gamma < 1$, ent√£o $\mathcal{B}$ √© uma contra√ß√£o com fator $\gamma$.

III. **Teorema do Ponto Fixo de Banach:**
     Pelo Teorema do Ponto Fixo de Banach, um operador de contra√ß√£o em um espa√ßo m√©trico completo tem um √∫nico ponto fixo. O espa√ßo de fun√ß√µes de valor com a norma do supremo √© um espa√ßo m√©trico completo. Portanto, $\mathcal{B}$ tem um √∫nico ponto fixo, que √© a fun√ß√£o de valor √≥tima $v_*$.

IV. **Converg√™ncia:**
    A value iteration aplica iterativamente o operador de Bellman: $v_{k+1} = \mathcal{B}v_k$.  Como $\mathcal{B}$ √© uma contra√ß√£o, a sequ√™ncia $\{v_k\}$ converge para o ponto fixo de $\mathcal{B}$, que √© $v_*$. Portanto, $\lim_{k \to \infty} v_k = v_*$.

V. **Caso Epis√≥dico:**
    Se o MDP √© epis√≥dico e termina com probabilidade 1, existe um passo $n$ tal que, ap√≥s $n$ passos, o retorno ser√° zero. Portanto, a converg√™ncia tamb√©m √© garantida neste caso.

Assim, a sequ√™ncia de fun√ß√µes de valor $\{v_k\}$ gerada pela **value iteration** converge para a fun√ß√£o de valor √≥tima $v_*$. ‚ñ†

A atualiza√ß√£o da **value iteration** tamb√©m pode ser vista como id√™ntica √† atualiza√ß√£o da **policy evaluation**, exceto pelo fato de que ela requer a maximiza√ß√£o sobre todas as a√ß√µes [^4.4]. Outra perspectiva √© comparar os diagramas de backup para esses algoritmos [^4.4].

Em termos pr√°ticos, a **value iteration** requer um n√∫mero infinito de itera√ß√µes para convergir exatamente para $v_*$ [^4.4]. No entanto, na pr√°tica, o algoritmo √© interrompido quando as mudan√ßas na fun√ß√£o de valor se tornam suficientemente pequenas em uma varredura completa sobre o espa√ßo de estados [^4.4]. Formalmente, a condi√ß√£o de parada pode ser expressa como:

$$
\max_{s} |v_{k+1}(s) - v_k(s)| < \theta
$$

onde $\theta > 0$ √© um limiar predefinido.

> üí° **Exemplo Num√©rico:**
> Suponha que, ap√≥s 100 itera√ß√µes da **value iteration**, a maior mudan√ßa no valor de qualquer estado seja 0.001. Se definirmos o limiar $\theta$ como 0.0001, o algoritmo ir√° continuar iterando. Se definirmos $\theta$ como 0.01, o algoritmo ir√° parar.  Escolher um $\theta$ pequeno garante maior precis√£o, mas requer mais computa√ß√£o.

Para complementar a discuss√£o sobre a condi√ß√£o de parada, podemos definir o seguinte resultado que fornece um bound no erro da fun√ß√£o de valor:

**Lema 1** Seja $v_k$ a fun√ß√£o de valor obtida ap√≥s $k$ itera√ß√µes da **value iteration**. Se $\max_{s} |v_{k+1}(s) - v_k(s)| < \theta$, ent√£o $\|v_k - v_*\|_{\infty} \leq \frac{\theta \gamma}{1 - \gamma}$.

*Proof Sketch:* A prova utiliza a propriedade de contra√ß√£o da atualiza√ß√£o de Bellman e a desigualdade triangular para limitar o erro entre $v_k$ e $v_*$.

Vamos apresentar uma prova detalhada do Lema 1:

**Prova do Lema 1:**
I. **Desigualdade Triangular:**
   Usando a desigualdade triangular, podemos escrever:
   $$\|v_k - v_*\|_{\infty} \leq \|v_k - v_{k+1}\|_{\infty} + \|v_{k+1} - v_*\|_{\infty}$$

II. **Definindo a fun√ß√£o de valor √≥tima como ponto fixo:**
    Sabemos que $v_*$ √© um ponto fixo do operador de Bellman, ou seja, $v_* = \mathcal{B}v_*$. Assim, podemos escrever:
    $$v_{k+1} - v_* = \mathcal{B}v_k - \mathcal{B}v_*$$

III. **Usando a Propriedade de Contra√ß√£o:**
     Aplicando a propriedade de contra√ß√£o do operador de Bellman, temos:
     $$\|v_{k+1} - v_*\|_{\infty} = \|\mathcal{B}v_k - \mathcal{B}v_*\|_{\infty} \leq \gamma \|v_k - v_*\|_{\infty}$$

IV. **Iterando a Desigualdade:**
    Podemos iterar a desigualdade acima repetidamente:
    $$\|v_{k+1} - v_*\|_{\infty} \leq \gamma \|v_k - v_*\|_{\infty} \leq \gamma^2 \|v_{k-1} - v_*\|_{\infty} \leq \dots \leq \gamma^{k+1} \|v_0 - v_*\|_{\infty}$$
    No entanto, essa itera√ß√£o n√£o nos ajuda diretamente a obter o bound desejado em termos de $\theta$. Em vez disso, vamos usar uma abordagem diferente.

V. **Usando a Condi√ß√£o de Parada:**
   Sabemos que $\|v_{k+1} - v_k\|_{\infty} < \theta$.  Podemos escrever:
   $$\|v_k - v_*\|_{\infty} \leq \|v_k - v_{k+1}\|_{\infty} + \|v_{k+1} - v_*\|_{\infty} < \theta + \|v_{k+1} - v_*\|_{\infty}$$
   Substituindo $\|v_{k+1} - v_*\|_{\infty} \leq \gamma \|v_k - v_*\|_{\infty}$, obtemos:
   $$\|v_k - v_*\|_{\infty} < \theta + \gamma \|v_k - v_*\|_{\infty}$$

VI. **Isolando $\|v_k - v_*\|_{\infty}$:**
    Reorganizando a desigualdade, temos:
    $$\|v_k - v_*\|_{\infty} - \gamma \|v_k - v_*\|_{\infty} < \theta$$
    $$(1 - \gamma) \|v_k - v_*\|_{\infty} < \theta$$
    $$\|v_k - v_*\|_{\infty} < \frac{\theta}{1 - \gamma}$$

VII. **Ajustando o Bound:**
    Note que existe um erro na prova original. Vamos corrigir o passo V. Temos:
    $\|v_k - v_*\|_\infty \leq \frac{\theta}{1-\gamma}$. Agora, vamos provar que $\|v_{k+1}-v_*\|_\infty \leq \frac{\theta \gamma}{1-\gamma}$. Como $v_{k+1} = \mathcal{B}v_k$, ent√£o
    $\|v_{k+1} - v_*\|_\infty = \|\mathcal{B}v_k - \mathcal{B}v_*\|_\infty \leq \gamma \|v_k - v_*\|_\infty$.
    Assim, $\|v_{k+1} - v_*\|_\infty \leq \gamma \frac{\theta}{1-\gamma} = \frac{\theta \gamma}{1 - \gamma}$.
    Como a condi√ß√£o de parada √© avaliada em $v_{k+1}$, √© correto dizer que
    $\|v_{k+1} - v_*\|_\infty \leq \frac{\theta \gamma}{1 - \gamma}$ quando a condi√ß√£o de parada √© satisfeita.

Portanto, se $\max_{s} |v_{k+1}(s) - v_k(s)| < \theta$, ent√£o $\|v_{k+1} - v_*\|_{\infty} \leq \frac{\theta \gamma}{1 - \gamma}$. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Suponha que $\theta = 0.01$ e $\gamma = 0.9$. Ent√£o, o bound no erro √©:
>
> $$
> \|v_{k+1} - v_*\|_{\infty} \leq \frac{0.01 \cdot 0.9}{1 - 0.9} = \frac{0.009}{0.1} = 0.09
> $$
>
> Isso significa que, quando a maior mudan√ßa em qualquer valor de estado √© menor que 0.01, a fun√ß√£o de valor atual est√° dentro de 0.09 da fun√ß√£o de valor √≥tima.

O pseudoc√≥digo para a **value iteration** √© dado como [^4.4]:

![Pseudocode for Value Iteration algorithm, a method for estimating the optimal policy in an MDP.](./../images/image4.png)

1.  Inicializa√ß√£o: Inicialize $V(s)$ arbitrariamente para todos $s \in \mathcal{S}^+$, exceto para o estado terminal, que deve ser $V(terminal) = 0$.
2.  Loop:
    *   $\Delta \leftarrow 0$
    *   Para cada $s \in \mathcal{S}$:
        *   $v \leftarrow V(s)$
        *   $V(s) \leftarrow \max_a \sum_{s',r} p(s', r | s, a) [r + \gamma V(s')]$
        *   $\Delta \leftarrow \max(\Delta, |v - V(s)|)$
    *   Enquanto $\Delta > \theta$ (onde $\theta$ √© um pequeno limiar positivo)
3.  Sa√≠da: Uma pol√≠tica determin√≠stica $\pi \approx \pi_*$ tal que $\pi(s) = \arg\max_a \sum_{s',r} p(s', r | s, a) [r + \gamma V(s')]$

Ap√≥s a converg√™ncia da **value iteration**, a pol√≠tica √≥tima pode ser extra√≠da da fun√ß√£o de valor √≥tima. Podemos formalizar esse processo no seguinte corol√°rio:

**Corol√°rio 1** A pol√≠tica $\pi(s) = \arg\max_a \sum_{s',r} p(s', r | s, a) [r + \gamma V(s')]$ √© uma pol√≠tica √≥tima.

*Proof Sketch:* A prova decorre do fato de que, ap√≥s a converg√™ncia, $V(s)$ satisfaz a equa√ß√£o de otimalidade de Bellman. Portanto, escolher a a√ß√£o que maximiza o lado direito da equa√ß√£o de Bellman produzir√° a pol√≠tica √≥tima.

**Prova do Corol√°rio 1:**

I. **Converg√™ncia da Value Iteration:**
   Ap√≥s a converg√™ncia da Value Iteration, temos que $V(s)$ satisfaz aproximadamente a equa√ß√£o de otimalidade de Bellman:
   $$V(s) \approx \max_a \sum_{s',r} p(s', r | s, a) [r + \gamma V(s')] \quad \forall s \in \mathcal{S}$$
   No limite da converg√™ncia, $V(s)$ se torna a fun√ß√£o de valor √≥tima $v_*(s)$.

II. **Definindo a Pol√≠tica √ìtima:**
    A pol√≠tica $\pi(s)$ √© definida como:
    $$\pi(s) = \arg\max_a \sum_{s',r} p(s', r | s, a) [r + \gamma V(s')] \quad \forall s \in \mathcal{S}$$
    Isto significa que $\pi(s)$ escolhe a a√ß√£o que maximiza o valor esperado da recompensa imediata mais o valor descontado do pr√≥ximo estado, de acordo com a fun√ß√£o de valor $V(s)$.

III. **Equa√ß√£o de Otimalidade de Bellman:**
      A equa√ß√£o de otimalidade de Bellman nos diz que a fun√ß√£o de valor √≥tima $v_*(s)$ satisfaz:
      $$v_*(s) = \max_a \sum_{s',r} p(s', r | s, a) [r + \gamma v_*(s')] \quad \forall s \in \mathcal{S}$$

IV. **Liga√ß√£o entre Pol√≠tica e Fun√ß√£o de Valor √ìtima:**
     Se substituirmos $V(s)$ por $v_*(s)$ na defini√ß√£o de $\pi(s)$, obtemos:
     $$\pi_*(s) = \arg\max_a \sum_{s',r} p(s', r | s, a) [r + \gamma v_*(s')] \quad \forall s \in \mathcal{S}$$
     Essa pol√≠tica $\pi_*(s)$ √© a pol√≠tica √≥tima, pois escolhe a a√ß√£o que maximiza a fun√ß√£o de valor √≥tima em cada estado.

V. **Conclus√£o:**
   Portanto, a pol√≠tica $\pi(s) = \arg\max_a \sum_{s',r} p(s', r | s, a) [r + \gamma V(s')]$ √© uma pol√≠tica √≥tima. ‚ñ†

### Conclus√£o
A **value iteration** oferece uma abordagem eficiente para encontrar pol√≠ticas √≥timas em MDPs, combinando os passos de **policy evaluation** e **policy improvement**. Ao transformar a **Bellman optimality equation** em uma regra de atualiza√ß√£o iterativa, a **value iteration** garante a converg√™ncia para a fun√ß√£o de valor √≥tima, fornecendo uma base s√≥lida para derivar a pol√≠tica √≥tima correspondente. Sua natureza computacionalmente eficiente e facilidade de implementa√ß√£o a tornam uma ferramenta valiosa no arsenal de algoritmos de **Dynamic Programming** para resolu√ß√£o de problemas de tomada de decis√£o sequencial.

### Refer√™ncias
[^1]: Chapter 4: Dynamic Programming
[^4.1]: Section 4.1: Policy Evaluation (Prediction)
[^4.2]: Section 4.2: Policy Improvement
[^4.4]: Section 4.4: Value Iteration
[^4.10]: Equation (4.10) in Section 4.4: Value Iteration
<!-- END -->