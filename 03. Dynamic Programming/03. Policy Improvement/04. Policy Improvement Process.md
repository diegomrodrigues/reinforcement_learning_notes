## Policy Improvement via Greedy Policy Construction

### IntroduÃ§Ã£o
O objetivo central do **policy improvement** Ã© encontrar polÃ­ticas que superem ou, no mÃ­nimo, igualem a polÃ­tica atual em termos de recompensa esperada. Este processo Ã© crucial no contexto do **dynamic programming** (DP), onde se busca iterativamente a polÃ­tica Ã³tima atravÃ©s da avaliaÃ§Ã£o e subsequente melhoria das polÃ­ticas [^1]. O presente capÃ­tulo explora detalhadamente o processo de **policy improvement**, focando na construÃ§Ã£o de uma nova polÃ­tica *greedy* em relaÃ§Ã£o Ã  funÃ§Ã£o de valor da polÃ­tica original.

### Conceitos Fundamentais

O processo de **policy improvement** comeÃ§a com uma polÃ­tica arbitrÃ¡ria $\pi$ e sua funÃ§Ã£o de valor associada $v_\pi$ [^1]. Para determinar se uma mudanÃ§a na polÃ­tica Ã© benÃ©fica, avaliamos a aÃ§Ã£o $a \ne \pi(s)$ em um estado $s$ especÃ­fico. A qualidade de selecionar $a$ em $s$ e, posteriormente, seguir $\pi$ Ã© dada por $q_\pi(s, a)$ [^7]:

$$q_\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a] = \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] \$$ [^7]

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Suponha um ambiente com 3 estados ($S = \{s_1, s_2, s_3\}$) e duas aÃ§Ãµes ($A = \{a_1, a_2\}$). Considere que estamos avaliando a polÃ­tica $\pi$ e jÃ¡ calculamos $v_\pi(s_1) = 10$, $v_\pi(s_2) = 5$, e $v_\pi(s_3) = 0$. Estamos no estado $s_1$ e $\pi(s_1) = a_1$. Agora, queremos avaliar a aÃ§Ã£o $a_2$ em $s_1$. Suponha que:
>
> *   $p(s_2, 2 | s_1, a_2) = 0.8$ (probabilidade de ir para $s_2$ com recompensa 2)
> *   $p(s_3, -1 | s_1, a_2) = 0.2$ (probabilidade de ir para $s_3$ com recompensa -1)
> *   $\gamma = 0.9$ (fator de desconto)
>
> EntÃ£o, $q_\pi(s_1, a_2)$ seria calculado como:
>
> $q_\pi(s_1, a_2) = (0.8) * [2 + (0.9) * 5] + (0.2) * [-1 + (0.9) * 0] = (0.8) * [2 + 4.5] + (0.2) * [-1] = (0.8) * 6.5 - 0.2 = 5.2 - 0.2 = 5.0$
>
> Isso significa que, se escolhermos a aÃ§Ã£o $a_2$ no estado $s_1$ e seguirmos a polÃ­tica $\pi$ a partir do prÃ³ximo estado, o valor esperado Ã© 5.0.

O critÃ©rio fundamental para melhoria Ã© comparar $q_\pi(s, a)$ com $v_\pi(s)$. Se $q_\pi(s, a) > v_\pi(s)$, sugere-se que selecionar $a$ em $s$ e seguir $\pi$ Ã© superior a seguir $\pi$ integralmente [^7]. A **policy improvement theorem** formaliza esta ideia:

*Sejam $\pi$ e $\pi'$ duas polÃ­ticas determinÃ­sticas tais que, para todo $s \in S$, $q_\pi(s, \pi'(s)) \ge v_\pi(s)$, entÃ£o a polÃ­tica $\pi'$ deve ser tÃ£o boa quanto ou melhor que a polÃ­tica $\pi$, ou seja, $v_{\pi'}(s) \ge v_\pi(s)$ para todo $s \in S$.* [^7]

**Prova da Policy Improvement Theorem:**

I. ComeÃ§amos com a definiÃ§Ã£o de $v_\pi(s)$:
   $$v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$$

II. Usando a definiÃ§Ã£o de $q_\pi(s, a)$:
    $$v_\pi(s) \le q_\pi(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = \pi'(s)]$$
    (Dado que $q_\pi(s, \pi'(s)) \ge v_\pi(s)$ para todo $s$)

III. Expandindo recursivamente $v_\pi(S_{t+1})$:
     $$v_\pi(s) \le \mathbb{E}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t = s, A_t = \pi'(s)]$$

IV. Aplicando repetidamente a condiÃ§Ã£o $q_\pi(s, \pi'(s)) \ge v_\pi(s)$:
    $$v_\pi(s) \le \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots | S_t = s] = v_{\pi'}(s)$$

V. Portanto, $v_{\pi'}(s) \ge v_\pi(s)$ para todo $s \in S$. â– 

**Lema 1:** Se existe um estado $s$ tal que $q_\pi(s, \pi'(s)) > v_\pi(s)$, entÃ£o existe uma probabilidade nÃ£o nula de que a polÃ­tica $\pi'$ tenha um desempenho estritamente melhor do que a polÃ­tica $\pi$.

*Prova:* Seja $s$ um estado tal que $q_\pi(s, \pi'(s)) > v_\pi(s)$.  Considere a trajetÃ³ria que comeÃ§a no estado $s$. A diferenÃ§a entre seguir $\pi'$ e seguir $\pi$ no estado $s$ Ã© $q_\pi(s, \pi'(s)) - v_\pi(s) > 0$.  Se a trajetÃ³ria visita este estado com probabilidade nÃ£o nula, entÃ£o o valor esperado de seguir $\pi'$ ao invÃ©s de $\pi$ serÃ¡ maior. $\blacksquare$

**ConstruÃ§Ã£o da PolÃ­tica Greedy:**
Com base na **policy improvement theorem**, construÃ­mos uma nova polÃ­tica *greedy* $\pi'$, que seleciona, em cada estado $s$, a aÃ§Ã£o que maximiza $q_\pi(s, a)$ [^7]. Essa polÃ­tica Ã© definida como:

$$\pi'(s) = \underset{a}{\text{argmax}} \, q_\pi(s, a) = \underset{a}{\text{argmax}} \, \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a] = \underset{a}{\text{argmax}} \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] \$$ [^7]

> ðŸ’¡ **Exemplo NumÃ©rico:**
>
> Continuando o exemplo anterior, onde $q_\pi(s_1, a_2) = 5.0$ e $v_\pi(s_1) = 10$, vamos supor que jÃ¡ calculamos $q_\pi(s_1, a_1)$ e descobrimos que $q_\pi(s_1, a_1) = 8.0$.
>
> A polÃ­tica *greedy* $\pi'$ entÃ£o escolheria a aÃ§Ã£o que maximiza $q_\pi(s_1, a)$. Neste caso:
>
> $\pi'(s_1) = \underset{a}{\text{argmax}} \, q_\pi(s_1, a) = \text{argmax}(q_\pi(s_1, a_1), q_\pi(s_1, a_2)) = \text{argmax}(8.0, 5.0) = a_1$
>
> Portanto, a polÃ­tica *greedy* $\pi'$ seleciona a aÃ§Ã£o $a_1$ no estado $s_1$, pois $q_\pi(s_1, a_1) > q_\pi(s_1, a_2)$. Observe que, mesmo que $q_\pi(s_1, a_1) < v_\pi(s_1)$, o que importa Ã© que $a_1$ Ã© a melhor aÃ§Ã£o *em relaÃ§Ã£o a outras aÃ§Ãµes*, dadas as estimativas atuais de $v_\pi$.

Esta nova polÃ­tica, por construÃ§Ã£o, satisfaz a condiÃ§Ã£o da **policy improvement theorem**. Portanto, espera-se que ela seja uma polÃ­tica aprimorada em relaÃ§Ã£o Ã  original [^7].

**ObservaÃ§Ã£o sobre polÃ­ticas nÃ£o determinÃ­sticas:** Embora a polÃ­tica greedy $\pi'$ seja determinÃ­stica por construÃ§Ã£o, a polÃ­tica original $\pi$ nÃ£o precisa ser. A **policy improvement theorem** tambÃ©m se aplica a polÃ­ticas estocÃ¡sticas. Para ver isso, considere a polÃ­tica $\pi'$ definida como $\pi'(a|s) = \mathbb{P}(A_t = a | S_t = s)$. EntÃ£o, podemos definir $q_\pi(s, \pi'(s)) = \sum_{a} \pi'(a|s)q_\pi(s, a)$. A condiÃ§Ã£o para melhoria da polÃ­tica torna-se $\sum_{a} \pi'(a|s)q_\pi(s, a) \ge v_\pi(s)$ para todo $s \in S$.

**O Teorema da Melhoria da PolÃ­tica**
O teorema da melhoria da polÃ­tica assegura que a nova polÃ­tica *greedy* $\pi'$ Ã© pelo menos tÃ£o boa quanto a polÃ­tica original $\pi$. Se $v_{\pi'}(s) = v_{\pi}(s)$ para todos os estados, entÃ£o ambas as polÃ­ticas sÃ£o Ã³timas [^7]. Caso contrÃ¡rio, $\pi'$ Ã© estritamente melhor do que $\pi$ [^7].

**Formalmente:**
1.  Seja $\pi$ uma polÃ­tica arbitrÃ¡ria.
2.  Calcule $v_\pi$ para $\pi$.
3.  Crie uma polÃ­tica *greedy* $\pi'$ com respeito a $v_\pi$ [^7].

![Policy Iteration algorithm: iterative process of policy evaluation and improvement for optimal policy estimation.](./../images/image3.png)

4.  Se $v_{\pi'}(s) = v_\pi(s)$ para todo $s$, entÃ£o $\pi$ e $\pi'$ sÃ£o Ã³timas.
5.  Caso contrÃ¡rio, $\pi'$ Ã© uma polÃ­tica estritamente melhor.

**Prova:**
Como $\pi'(s) = \underset{a}{\text{argmax}} \, $q_\pi(s, a)$, temos que $q_\pi(s, \pi'(s)) \geq v_\pi(s)$ para todo $s \in S$.
Aplicando o teorema da melhoria da polÃ­tica, segue-se que $v_{\pi'}(s) \geq v_\pi(s)$ para todo $s \in S$.

Se $v_{\pi'}(s) = v_\pi(s)$ para todo $s \in S$, entÃ£o $v_\pi(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] \$, que Ã© a equaÃ§Ã£o de otimalidade de Bellman. Portanto, $\pi$ (e $\pi'$) sÃ£o Ã³timas [^7].
$\blacksquare$

**Teorema 2:** Se a polÃ­tica greedy $\pi'$ construÃ­da a partir de $v_\pi$ Ã© tal que $v_{\pi'} = v_\pi$, entÃ£o $\pi$ Ã© uma polÃ­tica Ã³tima.

*Prova:* Se $v_{\pi'} = v_\pi$, entÃ£o para todo $s$, $v_\pi(s) = v_{\pi'}(s)$. Como $\pi'$ Ã© uma polÃ­tica greedy com respeito a $v_\pi$, temos que $v_{\pi'}(s) = \max_a q_\pi(s, a)$. Portanto, $v_\pi(s) = \max_a q_\pi(s, a)$ para todo $s$. Isso significa que $v_\pi$ satisfaz a equaÃ§Ã£o de Bellman para otimalidade, e portanto, $\pi$ Ã© uma polÃ­tica Ã³tima. $\blacksquare$

**CorolÃ¡rio 2.1:** Se durante a iteraÃ§Ã£o de melhoria de polÃ­tica, a polÃ­tica nÃ£o muda (isto Ã©, $\pi' = \pi$), entÃ£o $\pi$ Ã© uma polÃ­tica Ã³tima.

*Prova:* Se $\pi' = \pi$, entÃ£o $v_{\pi'} = v_\pi$. Pelo Teorema 2, $\pi$ Ã© uma polÃ­tica Ã³tima. $\blacksquare$

**ObservaÃ§Ã£o:**
A polÃ­tica *greedy* Ã© determinada pela funÃ§Ã£o $q(s,a)$, que denota o valor de se executar a aÃ§Ã£o $a$ no estado $s$ e, subsequentemente, seguir a polÃ­tica $\pi$. Esta formulaÃ§Ã£o permite que a melhoria da polÃ­tica seja expressa em termos dos valores Q [^7].

**Teorema 3:** O processo iterativo de policy evaluation e policy improvement converge para a polÃ­tica Ã³tima.

*Prova:* A cada iteraÃ§Ã£o, a polÃ­tica Ã© garantidamente melhorada (ou permanece a mesma se jÃ¡ for Ã³tima). Como o nÃºmero de polÃ­ticas determinÃ­sticas Ã© finito (para um espaÃ§o de estados e aÃ§Ãµes finito), o processo deve convergir para uma polÃ­tica Ã³tima em um nÃºmero finito de iteraÃ§Ãµes. $\blacksquare$

### ConclusÃ£o
O **policy improvement** atravÃ©s da construÃ§Ã£o de uma polÃ­tica *greedy* Ã© um passo fundamental nos algoritmos de **dynamic programming**. Ao explorar a funÃ§Ã£o de valor da polÃ­tica atual e construir uma nova polÃ­tica que age *greedy* em relaÃ§Ã£o a essa funÃ§Ã£o, garantimos uma melhoria ou, no mÃ­nimo, a manutenÃ§Ã£o do desempenho. Este processo iterativo converge para a polÃ­tica Ã³tima, demonstrando a eficÃ¡cia da combinaÃ§Ã£o entre **policy evaluation** e **policy improvement** no contexto do **reinforcement learning** [^1].

### ReferÃªncias
[^1]: Chapter 4: Dynamic Programming
[^7]: Section 4.2 Policy Improvement
<!-- END -->