## Policy Improvement com Pol√≠ticas Estoc√°sticas

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre **policy improvement** [^78], exploraremos agora como o algoritmo pode ser estendido para lidar com **pol√≠ticas estoc√°sticas**. Anteriormente, focamos no caso especial de **pol√≠ticas determin√≠sticas**, onde cada estado *$s$* √© mapeado para uma √∫nica a√ß√£o *$a$* [^79]. No entanto, em muitas situa√ß√µes, √© vantajoso considerar **pol√≠ticas estoc√°sticas** que atribuem probabilidades a diferentes a√ß√µes em cada estado.

### Pol√≠ticas Estoc√°sticas e o Teorema de Policy Improvement
No caso geral, uma **pol√≠tica estoc√°stica** $\pi$ especifica probabilidades $\pi$(*a*|*s*) para tomar cada a√ß√£o *$a$* em cada estado *$s$* [^79]. Apesar da generalidade, os princ√≠pios fundamentais do **teorema de policy improvement** se mant√™m v√°lidos. Especificamente, a condi√ß√£o crucial para a melhoria da pol√≠tica permanece inalterada:

> *Selecione, em cada estado, a a√ß√£o que parece melhor de acordo com $q\pi(s, a)$.* [^79]

O objetivo √© construir uma nova pol√≠tica gulosa, $\pi'$, que supere a pol√≠tica original, $\pi$. Para pol√≠ticas determin√≠sticas, isso envolvia simplesmente escolher a a√ß√£o *$a$* que maximizava *$q\pi$*(*s*, *$a$*) [^79]. No caso estoc√°stico, a ideia central √© aumentar as probabilidades das a√ß√µes que maximizam *$q\pi$*(*s*, *$a$*) na nova pol√≠tica $\pi'$.

O **teorema de policy improvement** garante que a nova pol√≠tica $\pi'$ seja t√£o boa quanto ou melhor que a pol√≠tica original $\pi$ [^78]. Formalmente, se para todos os estados *$s$* $\in$ *$S$*:

$$
\sum_{a} \pi'(a|s) q_{\pi}(s, a) \geq v_{\pi}(s) \qquad (4.7)
$$

Ent√£o a pol√≠tica $\pi'$ deve ser t√£o boa quanto ou melhor que $\pi$. Ou seja, para todos os estados *$s$* $\in$ *$S$*:

$$
v_{\pi'}(s) \geq v_{\pi}(s) \qquad (4.8)
$$

Para demonstrar o Teorema de Policy Improvement, provaremos que se a Equa√ß√£o (4.7) for satisfeita para todos os estados *$s$*, ent√£o a Equa√ß√£o (4.8) tamb√©m ser√° satisfeita.

*Prova:*

I. Come√ßamos expandindo $v_{\pi}(s)$ usando a defini√ß√£o de fun√ß√£o valor em termos de $q_{\pi}(s,a)$ e da pol√≠tica $\pi$:
$$v_{\pi}(s) = \sum_{a} \pi(a|s) q_{\pi}(s, a)$$

II. A condi√ß√£o dada pelo teorema (Equa√ß√£o 4.7) √©:
$$\sum_{a} \pi'(a|s) q_{\pi}(s, a) \geq v_{\pi}(s)$$

III. Agora, expandimos $v_{\pi'}(s)$ iterativamente, aplicando a defini√ß√£o da fun√ß√£o valor e usando a Equa√ß√£o (4.7):
$$v_{\pi'}(s) = \sum_{a} \pi'(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_{\pi'}(s')] $$

IV. Definimos $q_{\pi}(s, a)$ como o valor esperado de come√ßar no estado *$s$*, tomar a a√ß√£o *$a$* e, em seguida, seguir a pol√≠tica $\pi$:
$$q_{\pi}(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_{\pi}(s')]$$

V. Substituindo $q_{\pi}(s, a)$ na Equa√ß√£o (4.7), obtemos:
$$\sum_{a} \pi'(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_{\pi}(s')] \geq v_{\pi}(s)$$

VI. Expandindo recursivamente $v_{\pi}(s')$, podemos eventualmente expressar $v_{\pi}(s)$ como a recompensa esperada acumulada ao longo de um epis√≥dio inteiro ao seguir a pol√≠tica $\pi$. Similarmente, $v_{\pi'}(s)$ √© a recompensa esperada acumulada ao longo de um epis√≥dio inteiro ao seguir a pol√≠tica $\pi'$.

VII. A Equa√ß√£o (4.7) garante que cada passo dado pela pol√≠tica $\pi'$ produz um valor maior ou igual ao de um passo dado pela pol√≠tica $\pi$. Como essa rela√ß√£o se mant√©m para todos os estados, segue-se que o valor total da pol√≠tica $\pi'$ deve ser maior ou igual ao valor total da pol√≠tica $\pi$ para todos os estados iniciais *$s$*.

VIII. Portanto, $v_{\pi'}(s) \geq v_{\pi}(s)$ para todos os estados *$s$* $\in$ *$S$*. ‚ñ†

Para consolidar a compreens√£o do Teorema de Policy Improvement, podemos enunciar um lema que detalha o comportamento da fun√ß√£o valor a√ß√£o ao seguir a nova pol√≠tica $\pi'$ por um passo e, em seguida, aderir √† pol√≠tica original $\pi$.

**Lema 1** Se, para todos os estados *$s$* $\in$ *$S$*, a condi√ß√£o $\sum_{a} \pi'(a|s) q_{\pi}(s, a) \geq v_{\pi}(s)$ for satisfeita, ent√£o, para qualquer estado *$s$*, temos:

$$
\sum_{a} \pi'(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_{\pi}(s')] \geq v_{\pi}(s)
$$

*Prova:*
A prova segue diretamente da defini√ß√£o de $q_{\pi}(s, a)$ e da condi√ß√£o dada. Substituindo $q_{\pi}(s, a)$ por sua defini√ß√£o, obtemos a desigualdade desejada.

I. Partimos da defini√ß√£o de  $q_{\pi}(s, a)$:
$$q_{\pi}(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_{\pi}(s')] $$

II. Substitu√≠mos essa defini√ß√£o na condi√ß√£o dada:
$$\sum_{a} \pi'(a|s) q_{\pi}(s, a) \geq v_{\pi}(s)$$
$$\sum_{a} \pi'(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_{\pi}(s')] \geq v_{\pi}(s)$$

III. Portanto, a desigualdade desejada √© obtida diretamente pela substitui√ß√£o. ‚ñ†

### Implementa√ß√£o do Policy Improvement com Pol√≠ticas Estoc√°sticas

Quando h√° empates nos passos de **policy improvement**, ou seja, se h√° diversas a√ß√µes nas quais o m√°ximo √© atingido, ent√£o, no caso estoc√°stico, n√£o precisamos selecionar uma √∫nica a√ß√£o dentre elas [^79]. Em vez disso, cada a√ß√£o maximizadora pode receber uma por√ß√£o da probabilidade de ser selecionada na nova pol√≠tica gulosa. Qualquer esquema de aloca√ß√£o √© permitido, contanto que todas as a√ß√µes sub√≥timas recebam probabilidade zero [^79].

Matematicamente, podemos expressar a **pol√≠tica gulosa** $\pi'$ como:

$$
\pi'(a|s) =
\begin{cases}
p, & \text{se } a = \arg\max_{a'} q_{\pi}(s, a') \\
0, & \text{caso contr√°rio}
\end{cases}
$$

onde *$p$* √© uma probabilidade qualquer, desde que a soma das probabilidades para as a√ß√µes maximizadoras em cada estado seja igual a 1.

Para formalizar a ideia de aloca√ß√£o de probabilidade entre a√ß√µes maximizadoras, podemos definir o conjunto de a√ß√µes √≥timas em um estado *$s$* como:

$$
A^{*}(s) = \{ a \in A \mid q_{\pi}(s, a) = \max_{a' \in A} q_{\pi}(s, a') \}
$$

Ent√£o, a pol√≠tica gulosa $\pi'$ pode ser reescrita como:

$$
\pi'(a|s) =
\begin{cases}
p_a, & \text{se } a \in A^{*}(s) \\
0, & \text{caso contr√°rio}
\end{cases}
$$

onde $\sum_{a \in A^{*}(s)} p_a = 1$ para cada estado *$s$*. Essa formula√ß√£o explicita que a pol√≠tica $\pi'$ distribui probabilidade apenas entre as a√ß√µes consideradas √≥timas em rela√ß√£o a *$q\pi$*.

> üí° **Exemplo Num√©rico:**
>
> Suponha que em um estado *$s$*, as a√ß√µes dispon√≠veis sejam A = {'Norte', 'Sul', 'Leste', 'Oeste'}. Ap√≥s uma itera√ß√£o de *policy evaluation*, obtivemos os seguintes valores de a√ß√£o:
>
> *   $q\pi$(s, 'Norte') = 5
> *   $q\pi$(s, 'Sul') = 2
> *   $q\pi$(s, 'Leste') = 5
> *   $q\pi$(s, 'Oeste') = 3
>
> O conjunto de a√ß√µes √≥timas √© A\*(s) = {'Norte', 'Leste'}, pois ambas as a√ß√µes maximizam $q\pi$(s, a) com um valor de 5.
>
> Agora, vamos construir duas poss√≠veis pol√≠ticas melhoradas, $\pi$'1 e $\pi$'2:
>
> *   **Pol√≠tica $\pi$'1:** Atribui probabilidades iguais √†s a√ß√µes √≥timas.
>     *   $\pi$'1('Norte'|s) = 0.5
>     *   $\pi$'1('Sul'|s) = 0
>     *   $\pi$'1('Leste'|s) = 0.5
>     *   $\pi$'1('Oeste'|s) = 0
> *   **Pol√≠tica $\pi$'2:** Atribui probabilidades desiguais √†s a√ß√µes √≥timas.
>     *   $\pi$'2('Norte'|s) = 0.8
>     *   $\pi$'2('Sul'|s) = 0
>     *   $\pi$'2('Leste'|s) = 0.2
>     *   $\pi$'2('Oeste'|s) = 0
>
> Ambas as pol√≠ticas $\pi$'1 e $\pi$'2 s√£o v√°lidas e garantidas pelo teorema de policy improvement para serem pelo menos t√£o boas quanto a pol√≠tica original $\pi$. A escolha entre $\pi$'1 e $\pi$'2 pode depender de outros fatores, como a necessidade de explorar o ambiente de forma mais equilibrada.
>
> Podemos representar essas pol√≠ticas em uma tabela para melhor visualiza√ß√£o:
>
> | A√ß√£o   | $q\pi$(s, a) | $\pi$'1(a|s) | $\pi$'2(a|s) |
> | :----- | :------- | :------- | :------- |
> | Norte  | 5        | 0.5      | 0.8      |
> | Sul    | 2        | 0        | 0        |
> | Leste  | 5        | 0.5      | 0.2      |
> | Oeste  | 3        | 0        | 0        |

### Exemplo: Gridworld com Pol√≠tica Estoc√°stica
Considere o exemplo do **gridworld** 4x4 [^76]. Suponha que a pol√≠tica original, $\pi$, seja uma **pol√≠tica equiprov√°vel aleat√≥ria** (todas as a√ß√µes igualmente prov√°veis) [^76, 79]. A nova pol√≠tica, $\pi'$, √© gulosa em rela√ß√£o a *$v\pi$*. Os estados com m√∫ltiplas setas no diagrama de $\pi$' s√£o aqueles em que diversas a√ß√µes alcan√ßam o m√°ximo na Equa√ß√£o (4.9) [^79]. Uma aloca√ß√£o de probabilidade entre essas a√ß√µes √© permitida.

![Illustration of a 4x4 gridworld environment with rewards and actions for dynamic programming example.](./../images/image9.png)

Para qualquer pol√≠tica desse tipo, seus valores de estado *$v\pi‚Äô$*(*s*) podem ser determinados por inspe√ß√£o como sendo -1, -2 ou -3 para todos os estados *$s$* $\in$ *$S$*, enquanto *$v\pi$*(*s*) √© no m√°ximo -14 [^79]. Assim, *$v\pi‚Äô$*(*s*) ‚â• *$v\pi$*(*s*) para todo *$s$* $\in$ *$S$* [^79].

![Convergence of iterative policy evaluation on a gridworld, showing improvement from random to optimal policy.](./../images/image7.png)

Para ilustrar ainda mais, vamos considerar um estado espec√≠fico *$s$* no gridworld onde duas a√ß√µes, *$a_1$* e *$a_2$*, s√£o √≥timas, ou seja, *$q\pi$*(*s*, *$a_1$*) = *$q\pi$*(*s*, *$a_2$*) > *$q\pi$*(*s*, *$a$*) para todas as outras a√ß√µes *$a$*.  A pol√≠tica melhorada $\pi$' poderia atribuir probabilidade 0.5 para *$a_1$* e 0.5 para *$a_2$*, e 0 para todas as outras a√ß√µes.  Uma outra pol√≠tica melhorada poss√≠vel poderia atribuir probabilidade 0.7 para *$a_1$* e 0.3 para *$a_2$*, e 0 para todas as outras a√ß√µes.  O teorema de melhoria de pol√≠tica garante que ambas as pol√≠ticas melhoradas ter√£o valor igual ou superior √† pol√≠tica original $\pi$.

> üí° **Exemplo Num√©rico:**
>
> Considere um gridworld 3x3 simplificado com as seguintes recompensas:
>
> |      |      |      |
> | :--- | :--- | :--- |
> | +1   | 0    | -1   |
> | 0    | 0    | 0    |
> | -1   | 0    | +1   |
>
> Inicialmente, a pol√≠tica $\pi$ √© equiprov√°vel aleat√≥ria ($\pi$(a|s) = 0.25 para todas as a√ß√µes em todos os estados). Ap√≥s uma itera√ß√£o de *policy evaluation* com $\gamma$ = 0.9, obtemos os seguintes valores de a√ß√£o para o estado central (0, 0):
>
> *   $q\pi$((1, 1), 'Norte') = 0.1
> *   $q\pi$((1, 1), 'Sul') = 0.1
> *   $q\pi$((1, 1), 'Leste') = -0.05
> *   $q\pi$((1, 1), 'Oeste') = -0.05
>
> As a√ß√µes 'Norte' e 'Sul' s√£o √≥timas. Podemos criar uma nova pol√≠tica $\pi$' que atribui probabilidade 0.5 para 'Norte' e 0.5 para 'Sul', e 0 para as outras a√ß√µes.
>
> Vamos calcular o valor do estado central (1, 1) para ambas as pol√≠ticas usando a fun√ß√£o valor:
>
> $v_{\pi}(s) = \sum_{a} \pi(a|s) q_{\pi}(s, a)$
>
> $v_{\pi}((1, 1)) = 0.25 * 0.1 + 0.25 * 0.1 + 0.25 * (-0.05) + 0.25 * (-0.05) = 0.025$
>
> Agora, vamos calcular o valor do estado central (1, 1) para a nova pol√≠tica $\pi$':
>
> $v_{\pi'}(s) = \sum_{a} \pi'(a|s) q_{\pi}(s, a)$
>
> $v_{\pi'}((1, 1)) = 0.5 * 0.1 + 0.5 * 0.1 + 0 * (-0.05) + 0 * (-0.05) = 0.1$
>
> Como $v_{\pi'}((1, 1)) = 0.1 > v_{\pi}((1, 1)) = 0.025$, o teorema de policy improvement √© satisfeito para este estado.
>
> Este exemplo demonstra numericamente como a nova pol√≠tica $\pi$' pode melhorar o valor de um estado em rela√ß√£o √† pol√≠tica original $\pi$.

### Conclus√£o
A extens√£o do **policy improvement** para pol√≠ticas estoc√°sticas enriquece o conjunto de ferramentas dispon√≠veis para o **planejamento via programa√ß√£o din√¢mica**. Ao permitir a aloca√ß√£o de probabilidades entre a√ß√µes maximizadoras, o algoritmo se torna mais flex√≠vel e capaz de convergir para **pol√≠ticas √≥timas** em uma variedade maior de problemas [^79]. A garantia de melhoria da pol√≠tica, expressa no **teorema de policy improvement**, continua sendo uma base s√≥lida para o desenvolvimento de algoritmos de **refor√ßo**.

Al√©m disso, a flexibilidade introduzida pelas pol√≠ticas estoc√°sticas permite uma explora√ß√£o mais eficiente do espa√ßo de pol√≠ticas, o que pode acelerar o processo de converg√™ncia para a pol√≠tica √≥tima.

### Refer√™ncias
[^78]: Cap√≠tulo 4, Dynamic Programming, p√°gina 78
[^79]: Cap√≠tulo 4, Dynamic Programming, p√°gina 79
[^76]: Cap√≠tulo 4, Dynamic Programming, p√°gina 76
<!-- END -->