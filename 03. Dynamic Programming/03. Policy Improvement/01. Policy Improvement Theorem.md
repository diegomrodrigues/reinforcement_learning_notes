## Pol√≠tica de Melhoria em Programa√ß√£o Din√¢mica

### Introdu√ß√£o

O objetivo principal de calcular a fun√ß√£o valor de uma pol√≠tica, $v_{\pi}$, √© facilitar a busca por pol√≠ticas melhores. Dado $v_{\pi}$ para uma pol√≠tica determin√≠stica arbitr√°ria $\pi$, busca-se determinar se a altera√ß√£o da pol√≠tica para escolher deterministicamente uma a√ß√£o $a \neq \pi(s)$ √© ben√©fica. J√° se conhece a qualidade de seguir a pol√≠tica atual a partir de $s$, representada por $v_{\pi}(s)$. A quest√£o crucial √© se seria melhor ou pior optar pela nova pol√≠tica [^76]. Este cap√≠tulo explora o processo de **melhoria de pol√≠ticas**, um componente fundamental dos algoritmos de Programa√ß√£o Din√¢mica (DP).

### Conceitos Fundamentais

Para avaliar uma poss√≠vel mudan√ßa na pol√≠tica, considera-se selecionar a a√ß√£o $a$ em $s$ e, subsequentemente, seguir a pol√≠tica existente, $\pi$. O valor dessa forma de agir √© definido como [^78]:

$$
q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a] = \sum_{s', r} p(s', r | s, a) [r + \gamma v_{\pi}(s')] \quad (4.6)
$$

> üí° **Exemplo Num√©rico:** Considere um ambiente simples com dois estados ($s_1, s_2$) e duas a√ß√µes ($a_1, a_2$). Seja $\pi(s_1) = a_1$ e $\pi(s_2) = a_2$. Suponha que $v_{\pi}(s_1) = 10$ e $v_{\pi}(s_2) = 5$.  Agora, imagine que ao tomar a a√ß√£o $a_2$ em $s_1$, temos as seguintes probabilidades e recompensas:
>
> *   Com probabilidade 0.6, vamos para $s_1$ e recebemos uma recompensa de 2.
> *   Com probabilidade 0.4, vamos para $s_2$ e recebemos uma recompensa de -1.
>
> Se o fator de desconto $\gamma = 0.9$, ent√£o $q_{\pi}(s_1, a_2)$ √© calculado como:
>
> $q_{\pi}(s_1, a_2) = 0.6 * [2 + 0.9 * 10] + 0.4 * [-1 + 0.9 * 5] = 0.6 * 11 + 0.4 * 3.5 = 6.6 + 1.4 = 8$
>
> Como $q_{\pi}(s_1, a_2) = 8 < v_{\pi}(s_1) = 10$, mudar a pol√≠tica para tomar $a_2$ em $s_1$ *n√£o* seria ben√©fico neste caso.
>
> ```python
> import numpy as np
>
> # Dados do exemplo
> prob_s1 = 0.6
> reward_s1 = 2
> prob_s2 = 0.4
> reward_s2 = -1
> gamma = 0.9
> v_pi_s1 = 10
> v_pi_s2 = 5
>
> # Calcula q_pi(s1, a2)
> q_pi_s1_a2 = prob_s1 * (reward_s1 + gamma * v_pi_s1) + prob_s2 * (reward_s2 + gamma * v_pi_s2)
>
> print(f"q_pi(s1, a2) = {q_pi_s1_a2}")
> print(f"v_pi(s1) = {v_pi_s1}")
> if q_pi_s1_a2 > v_pi_s1:
>     print("Mudar a pol√≠tica seria ben√©fico.")
> else:
>     print("Mudar a pol√≠tica N√ÉO seria ben√©fico.")
> ```

O crit√©rio chave √© determinar se $q_{\pi}(s, a)$ √© maior ou menor que $v_{\pi}(s)$. Se $q_{\pi}(s, a)$ for maior, ou seja, se for vantajoso selecionar $a$ uma vez em $s$ e, posteriormente, seguir $\pi$, em vez de seguir $\pi$ o tempo todo, espera-se que selecionar $a$ sempre que $s$ for encontrado resulte em uma pol√≠tica superior.

Este princ√≠pio se manifesta como um caso especial de um resultado mais abrangente conhecido como o **teorema de melhoria de pol√≠ticas**. Sejam $\pi$ e $\pi'$ um par de pol√≠ticas determin√≠sticas tais que, para todo $s \in \mathcal{S}$,

$$
q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s) \quad (4.7)
$$

Nesse contexto, a pol√≠tica $\pi'$ deve ser t√£o boa quanto ou superior a $\pi$. Isso significa que $\pi'$ deve obter um retorno esperado maior ou igual a partir de todos os estados $s \in \mathcal{S}$:

$$
v_{\pi'}(s) \geq v_{\pi}(s) \quad (4.8)
$$

Al√©m disso, se houver uma desigualdade estrita em (4.7) em algum estado, ent√£o deve haver uma desigualdade estrita correspondente em (4.8) nesse estado.

O teorema de melhoria de pol√≠ticas se aplica √†s duas pol√≠ticas consideradas no in√≠cio desta se√ß√£o: uma pol√≠tica determin√≠stica original, $\pi$, e uma pol√≠tica modificada, $\pi'$, que √© id√™ntica a $\pi$, exceto que $\pi'(s) = a \neq \pi(s)$. Para estados diferentes de $s$, (4.7) se mant√©m porque os dois lados s√£o iguais. Assim, se $q_{\pi}(s, a) > v_{\pi}(s)$, ent√£o a pol√≠tica modificada √© realmente melhor do que $\pi$ [^78].

A l√≥gica por tr√°s da prova do teorema de melhoria de pol√≠ticas √© intuitiva. Come√ßando com (4.7), a express√£o para $q_{\pi}(s, \pi'(s))$ √© expandida utilizando (4.6), e (4.7) √© reaplicado iterativamente at√© que $v_{\pi'}(s)$ seja obtido:

$$
v_{\pi}(s) \leq q_{\pi}(s, \pi'(s)) \\
= \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = \pi'(s)] \quad \text{(por (4.6))} \\
= \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s] \\
\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, \pi'(S_{t+1})) | S_t = s] \quad \text{(por (4.7))} \\
= \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}[R_{t+2} + \gamma v_{\pi}(S_{t+2}) | S_{t+1}, A_{t+1} = \pi'(S_{t+1})] | S_t = s] \\
= \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi}(S_{t+2}) | S_t = s] \\
\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_{\pi}(S_{t+3}) | S_t = s] \\
\vdots \\
\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots | S_t = s] \\
= v_{\pi'}(s).
$$

> üí° **Exemplo Num√©rico:** Vamos continuar com o exemplo anterior e iterar para ver o efeito a longo prazo. Assumindo $\pi'(s_1) = a_2$ (a a√ß√£o que antes n√£o segu√≠amos em $s_1$), podemos simular algumas itera√ß√µes:
>
> *   $v_{\pi}(s_1) = 10$
> *   $q_{\pi}(s_1, a_2) = 8$ (calculado anteriormente)
>
> Agora, assumimos que temos novos valores $v_{\pi'}(s_1)$ e $v_{\pi'}(s_2)$ ap√≥s uma itera√ß√£o de melhoria de pol√≠tica.
>
> Vamos dizer $v_{\pi'}(s_1) = 12$ e $v_{\pi'}(s_2) = 6$. Note que $v_{\pi'}(s) \geq v_{\pi}(s)$ para ambos os estados, como esperado pelo teorema da melhoria de pol√≠tica.
>
> Se recalcularmos $q_{\pi}(s_1, a_2)$ usando esses novos valores:
>
> $q_{\pi'}(s_1, a_2) = 0.6 * [2 + 0.9 * 12] + 0.4 * [-1 + 0.9 * 6] = 0.6 * 12.8 + 0.4 * 4.4 = 7.68 + 1.76 = 9.44$
>
> Observe que $q_{\pi'}(s_1, a_2)$ aumentou em rela√ß√£o ao $q_{\pi}(s_1, a_2)$ anterior, demonstrando como a melhoria da pol√≠tica iterativamente refina as estimativas de valor e as decis√µes.
>
> ```python
> # Dados atualizados do exemplo
> v_pi_prime_s1 = 12
> v_pi_prime_s2 = 6
>
> # Recalcula q_pi(s1, a2) com os novos valores
> q_pi_prime_s1_a2 = prob_s1 * (reward_s1 + gamma * v_pi_prime_s1) + prob_s2 * (reward_s2 + gamma * v_pi_prime_s2)
>
> print(f"q_pi'(s1, a2) = {q_pi_prime_s1_a2}")
> print(f"v_pi'(s1) = {v_pi_prime_s1}")
> ```

Para melhor compreens√£o, apresentamos a prova formal do Teorema da Melhoria da Pol√≠tica:

**Prova do Teorema da Melhoria da Pol√≠tica:**

O teorema afirma que se $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todo $s \in \mathcal{S}$, ent√£o $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s \in \mathcal{S}$.

I. Come√ßamos com a defini√ß√£o de $v_{\pi}(s)$:
   $$v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s]$$

II. Assumimos que $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todo $s$.

III. Usamos a defini√ß√£o de $v_{\pi'}(s)$ e expandimos recursivamente:
    $$v_{\pi'}(s) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi'}(S_{t+1}) | S_t = s]$$
    $$v_{\pi'}(s) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi'}[R_{t+2} + \gamma v_{\pi'}(S_{t+2}) | S_{t+1}] | S_t = s]$$
    $$v_{\pi'}(s) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi'}(S_{t+2}) | S_t = s]$$
    Continuando recursivamente, chegamos a:
    $$v_{\pi'}(s) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s]$$

IV. Agora, considere a expans√£o de $q_{\pi}(s, \pi'(s))$:
$$q_{\pi}(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = \pi'(s)]$$
$$q_{\pi}(s, \pi'(s)) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]$$

V. Como $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$, temos:
$$\mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s] \geq v_{\pi}(s)$$

VI. Aplicando recursivamente a desigualdade $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$:
$$\mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi'}[R_{t+2} + \gamma v_{\pi}(S_{t+2}) | S_{t+1}] | S_t = s] \geq \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s] \geq v_{\pi}(s)$$
$$\mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi}(S_{t+2}) | S_t = s] \geq v_{\pi}(s)$$
Continuando recursivamente, chegamos a:
$$\mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s] \geq v_{\pi}(s)$$

VII. Portanto, temos:
    $$v_{\pi'}(s) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s] \geq v_{\pi}(s)$$
    $$v_{\pi'}(s) \geq v_{\pi}(s)$$

Assim, provamos que se $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todo $s \in \mathcal{S}$, ent√£o $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s \in \mathcal{S}$. ‚ñ†

√â importante notar que a converg√™ncia da pol√≠tica pode ser acelerada considerando uma atualiza√ß√£o mais agressiva, que n√£o apenas seleciona a melhor a√ß√£o *imediatamente*, mas considera a melhor a√ß√£o em cada estado, dado o conhecimento atual da fun√ß√£o valor. Este conceito leva √† pol√≠tica gulosa.

O objetivo principal √© otimizar as a√ß√µes em todos os estados, selecionando, em cada estado, a a√ß√£o que parece ser a melhor de acordo com $q(s, a)$. Em outras palavras, considere a nova **pol√≠tica gulosa** , $\pi'$, dada por [^79]:

$$
\pi'(s) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s, a) = \underset{a}{\operatorname{argmax}} \ \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a] = \underset{a}{\operatorname{argmax}} \sum_{s', r} p(s', r | s, a) [r + \gamma v_{\pi}(s')] \quad (4.9)
$$
onde argmax denota o valor de $a$ no qual a express√£o subsequente √© maximizada (com empates quebrados arbitrariamente).

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, suponha que, ap√≥s v√°rias itera√ß√µes, temos a seguinte fun√ß√£o valor $v_{\pi}(s)$: $v_{\pi}(s_1) = 15$ e $v_{\pi}(s_2) = 7$. Para encontrar a pol√≠tica gulosa, precisamos calcular $q_{\pi}(s, a)$ para cada estado e cada a√ß√£o. J√° calculamos $q_{\pi}(s_1, a_2) = 9.44$. Agora, vamos supor que $q_{\pi}(s_1, a_1) = 14$ e $q_{\pi}(s_2, a_1) = 6$, e $q_{\pi}(s_2, a_2) = 7$.
>
> *   Em $s_1$, temos $q_{\pi}(s_1, a_1) = 14$ e $q_{\pi}(s_1, a_2) = 9.44$. Portanto, $\pi'(s_1) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s_1, a) = a_1$.
> *   Em $s_2$, temos $q_{\pi}(s_2, a_1) = 6$ e $q_{\pi}(s_2, a_2) = 7$. Portanto, $\pi'(s_2) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s_2, a) = a_2$.
>
> Neste caso, a pol√≠tica gulosa $\pi'$ √©: $\pi'(s_1) = a_1$ e $\pi'(s_2) = a_2$. Observe que, neste exemplo espec√≠fico, a pol√≠tica gulosa coincide com a pol√≠tica original em $s_2$, mas difere em $s_1$. Isso significa que a pol√≠tica foi melhorada ao escolher $a_1$ em $s_1$ em vez de $a_2$.
>
> ```python
> # Dados do exemplo (incluindo os valores q)
> q_pi_s1_a1 = 14
> q_pi_s1_a2 = 9.44
> q_pi_s2_a1 = 6
> q_pi_s2_a2 = 7
>
> # Encontra a pol√≠tica gulosa
> pi_prime_s1 = "a1" if q_pi_s1_a1 > q_pi_s1_a2 else "a2"
> pi_prime_s2 = "a2" if q_pi_s2_a2 > q_pi_s2_a1 else "a1"
>
> print(f"Pol√≠tica Gulosa: pi'(s1) = {pi_prime_s1}, pi'(s2) = {pi_prime_s2}")
> ```

Por constru√ß√£o, a pol√≠tica gulosa satisfaz as condi√ß√µes do teorema de melhoria de pol√≠ticas (4.7), garantindo que ela seja pelo menos t√£o boa quanto, ou melhor do que, a pol√≠tica original. O processo de cria√ß√£o de uma nova pol√≠tica que melhora uma pol√≠tica original, tornando-a gulosa em rela√ß√£o √† fun√ß√£o valor da pol√≠tica original, √© denominado **melhoria de pol√≠tica**.

Uma propriedade importante da pol√≠tica gulosa √© que, se ela n√£o for uma melhoria estrita em rela√ß√£o √† pol√≠tica original, ent√£o a pol√≠tica original j√° √© √≥tima. Formalmente:

**Teorema 1** Se $\pi'(s) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s, a)$ e $v_{\pi'}(s) = v_{\pi}(s)$ para todo $s \in \mathcal{S}$, ent√£o $\pi$ √© uma pol√≠tica √≥tima, ou seja, $\pi = \pi_*$.

*Proof.* Se $v_{\pi'}(s) = v_{\pi}(s)$, ent√£o a desigualdade (4.7) √©, na verdade, uma igualdade para todo $s \in \mathcal{S}$. Isso implica que $v_{\pi}(s) = \max_{a} \ q_{\pi}(s, a)$ para todo $s$. Esta √© a equa√ß√£o de otimalidade de Bellman para $v_{\pi}$. Como $v_{\pi}$ satisfaz a equa√ß√£o de otimalidade de Bellman, ela deve ser igual a $v_*$, a fun√ß√£o valor √≥tima. Portanto, $\pi$ deve ser uma pol√≠tica √≥tima.

Para melhor compreens√£o, apresentamos a prova formal do Teorema 1:

I. Assumimos que $\pi'(s) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s, a)$ e $v_{\pi'}(s) = v_{\pi}(s)$ para todo $s \in \mathcal{S}$.

II. Se $v_{\pi'}(s) = v_{\pi}(s)$, ent√£o a desigualdade (4.7) torna-se uma igualdade: $q_{\pi}(s, \pi'(s)) = v_{\pi}(s)$ para todo $s \in \mathcal{S}$.

III. Como $\pi'(s)$ √© definido como $\underset{a}{\operatorname{argmax}} \ q_{\pi}(s, a)$, isso significa que $q_{\pi}(s, \pi'(s)) = \max_{a} \ q_{\pi}(s, a)$ para todo $s \in \mathcal{S}$.

IV. Combinando as etapas II e III, temos $v_{\pi}(s) = \max_{a} \ q_{\pi}(s, a)$ para todo $s \in \mathcal{S}$.

V. A equa√ß√£o $v_{\pi}(s) = \max_{a} \ q_{\pi}(s, a)$ √© a equa√ß√£o de otimalidade de Bellman para $v_{\pi}(s)$.

VI. Se uma fun√ß√£o valor $v$ satisfaz a equa√ß√£o de otimalidade de Bellman, ent√£o $v$ √© a fun√ß√£o valor √≥tima $v_*$. Portanto, $v_{\pi}(s) = v_*(s)$ para todo $s \in \mathcal{S}$.

VII. Se $v_{\pi}(s) = v_*(s)$ para todo $s \in \mathcal{S}$, ent√£o a pol√≠tica $\pi$ que gera essa fun√ß√£o valor √© uma pol√≠tica √≥tima $\pi_*$.

VIII. Portanto, $\pi = \pi_*$.

Assim, provamos que se $\pi'(s) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s, a)$ e $v_{\pi'}(s) = v_{\pi}(s)$ para todo $s \in \mathcal{S}$, ent√£o $\pi$ √© uma pol√≠tica √≥tima, ou seja, $\pi = \pi_*$. ‚ñ†

Al√©m disso, podemos expressar a rela√ß√£o entre $v_{\pi}$ e $v_*$ usando a fun√ß√£o Q √≥tima $q_*(s, a)$, que representa o valor de se iniciar no estado $s$, tomar a a√ß√£o $a$ e, posteriormente, seguir a pol√≠tica √≥tima.

**Proposi√ß√£o 1** Se $v_{\pi}(s) = v_*(s)$, ent√£o $q_{\pi}(s, a) \le q_*(s, a)$ para todo $s$ e $a$.

*Proof.* Como $v_*(s)$ √© o valor m√°ximo que pode ser obtido a partir do estado $s$, tomar qualquer a√ß√£o $a$ e, subsequentemente, seguir uma pol√≠tica diferente da √≥tima resultar√° em um valor menor ou igual a $q_*(s, a)$. Portanto, $q_{\pi}(s, a) \le q_*(s, a)$.

Para melhor compreens√£o, apresentamos a prova formal da Proposi√ß√£o 1:

I. Assumimos que $v_{\pi}(s) = v_*(s)$ para todo $s$.

II. Por defini√ß√£o, $q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a]$.

III. Tamb√©m por defini√ß√£o, $q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{*}(S_{t+1}) | S_t = s, A_t = a]$.

IV. Substituindo $v_{\pi}(s)$ por $v_*(s)$ na equa√ß√£o de $q_{\pi}(s, a)$, obtemos:
$q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{*}(S_{t+1}) | S_t = s, A_t = a]$.

V. Agora, compare $q_{\pi}(s, a)$ e $q_*(s, a)$. Se seguirmos a pol√≠tica $\pi$ ap√≥s tomar a a√ß√£o $a$ no estado $s$, obteremos o valor esperado $q_{\pi}(s, a)$. Se seguirmos a pol√≠tica √≥tima ap√≥s tomar a a√ß√£o $a$ no estado $s$, obteremos o valor esperado $q_*(s, a)$.

VI. Como $v_*(s)$ √© o valor m√°ximo que pode ser obtido a partir do estado $s$, seguir qualquer pol√≠tica diferente da √≥tima (neste caso, $\pi$) resultar√° em um valor menor ou igual ao valor de seguir a pol√≠tica √≥tima.

VII. Portanto, $\mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a] \le \mathbb{E}[R_{t+1} + \gamma v_{*}(S_{t+1}) | S_t = s, A_t = a]$.

VIII. Isso implica que $q_{\pi}(s, a) \le q_*(s, a)$ para todo $s$ e $a$.

Assim, provamos que se $v_{\pi}(s) = v_*(s)$, ent√£o $q_{\pi}(s, a) \le q_*(s, a)$ para todo $s$ e $a$. ‚ñ†

### Conclus√£o

A melhoria de pol√≠tica √© um passo fundamental nos algoritmos de Programa√ß√£o Din√¢mica, permitindo refinar iterativamente uma pol√≠tica em dire√ß√£o √† otimiza√ß√£o. Ao selecionar a√ß√µes que maximizam a fun√ß√£o de valor da pol√≠tica original, criamos uma nova pol√≠tica que garante um desempenho igual ou superior. Este processo, quando combinado com a avalia√ß√£o de pol√≠tica, leva √† converg√™ncia para a pol√≠tica √≥tima.

### Refer√™ncias
[^76]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
[^78]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
[^79]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
<!-- END -->