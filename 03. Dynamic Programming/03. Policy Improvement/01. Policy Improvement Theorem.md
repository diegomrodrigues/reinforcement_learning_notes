## PolÃ­tica de Melhoria em ProgramaÃ§Ã£o DinÃ¢mica

### IntroduÃ§Ã£o

O objetivo principal de calcular a funÃ§Ã£o valor de uma polÃ­tica, $v_{\pi}$, Ã© facilitar a busca por polÃ­ticas melhores. Dado $v_{\pi}$ para uma polÃ­tica determinÃ­stica arbitrÃ¡ria $\pi$, busca-se determinar se a alteraÃ§Ã£o da polÃ­tica para escolher deterministicamente uma aÃ§Ã£o $a \neq \pi(s)$ Ã© benÃ©fica. JÃ¡ se conhece a qualidade de seguir a polÃ­tica atual a partir de $s$, representada por $v_{\pi}(s)$. A questÃ£o crucial Ã© se seria melhor ou pior optar pela nova polÃ­tica [^76]. Este capÃ­tulo explora o processo de **melhoria de polÃ­ticas**, um componente fundamental dos algoritmos de ProgramaÃ§Ã£o DinÃ¢mica (DP).

### Conceitos Fundamentais

Para avaliar uma possÃ­vel mudanÃ§a na polÃ­tica, considera-se selecionar a aÃ§Ã£o $a$ em $s$ e, subsequentemente, seguir a polÃ­tica existente, $\pi$. O valor dessa forma de agir Ã© definido como [^78]:

$$
q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a] = \sum_{s', r} p(s', r | s, a) [r + \gamma v_{\pi}(s')] \quad (4.6)
$$

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um ambiente simples com dois estados ($s_1, s_2$) e duas aÃ§Ãµes ($a_1, a_2$). Seja $\pi(s_1) = a_1$ e $\pi(s_2) = a_2$. Suponha que $v_{\pi}(s_1) = 10$ e $v_{\pi}(s_2) = 5$.  Agora, imagine que ao tomar a aÃ§Ã£o $a_2$ em $s_1$, temos as seguintes probabilidades e recompensas:
>
> *   Com probabilidade 0.6, vamos para $s_1$ e recebemos uma recompensa de 2.
> *   Com probabilidade 0.4, vamos para $s_2$ e recebemos uma recompensa de -1.
>
> Se o fator de desconto $\gamma = 0.9$, entÃ£o $q_{\pi}(s_1, a_2)$ Ã© calculado como:
>
> $q_{\pi}(s_1, a_2) = 0.6 * [2 + 0.9 * 10] + 0.4 * [-1 + 0.9 * 5] = 0.6 * 11 + 0.4 * 3.5 = 6.6 + 1.4 = 8$
>
> Como $q_{\pi}(s_1, a_2) = 8 < v_{\pi}(s_1) = 10$, mudar a polÃ­tica para tomar $a_2$ em $s_1$ *nÃ£o* seria benÃ©fico neste caso.
>
> ```python
> import numpy as np
>
> # Dados do exemplo
> prob_s1 = 0.6
> reward_s1 = 2
> prob_s2 = 0.4
> reward_s2 = -1
> gamma = 0.9
> v_pi_s1 = 10
> v_pi_s2 = 5
>
> # Calcula q_pi(s1, a2)
> q_pi_s1_a2 = prob_s1 * (reward_s1 + gamma * v_pi_s1) + prob_s2 * (reward_s2 + gamma * v_pi_s2)
>
> print(f"q_pi(s1, a2) = {q_pi_s1_a2}")
> print(f"v_pi(s1) = {v_pi_s1}")
> if q_pi_s1_a2 > v_pi_s1:
>     print("Mudar a polÃ­tica seria benÃ©fico.")
> else:
>     print("Mudar a polÃ­tica NÃƒO seria benÃ©fico.")
> ```

O critÃ©rio chave Ã© determinar se $q_{\pi}(s, a)$ Ã© maior ou menor que $v_{\pi}(s)$. Se $q_{\pi}(s, a)$ for maior, ou seja, se for vantajoso selecionar $a$ uma vez em $s$ e, posteriormente, seguir $\pi$, em vez de seguir $\pi$ o tempo todo, espera-se que selecionar $a$ sempre que $s$ for encontrado resulte em uma polÃ­tica superior.

Este princÃ­pio se manifesta como um caso especial de um resultado mais abrangente conhecido como o **teorema de melhoria de polÃ­ticas**. Sejam $\pi$ e $\pi'$ um par de polÃ­ticas determinÃ­sticas tais que, para todo $s \in \mathcal{S}$,

$$
q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s) \quad (4.7)
$$

Nesse contexto, a polÃ­tica $\pi'$ deve ser tÃ£o boa quanto ou superior a $\pi$. Isso significa que $\pi'$ deve obter um retorno esperado maior ou igual a partir de todos os estados $s \in \mathcal{S}$:

$$
v_{\pi'}(s) \geq v_{\pi}(s) \quad (4.8)
$$

AlÃ©m disso, se houver uma desigualdade estrita em (4.7) em algum estado, entÃ£o deve haver uma desigualdade estrita correspondente em (4.8) nesse estado.

O teorema de melhoria de polÃ­ticas se aplica Ã s duas polÃ­ticas consideradas no inÃ­cio desta seÃ§Ã£o: uma polÃ­tica determinÃ­stica original, $\pi$, e uma polÃ­tica modificada, $\pi'$, que Ã© idÃªntica a $\pi$, exceto que $\pi'(s) = a \neq \pi(s)$. Para estados diferentes de $s$, (4.7) se mantÃ©m porque os dois lados sÃ£o iguais. Assim, se $q_{\pi}(s, a) > v_{\pi}(s)$, entÃ£o a polÃ­tica modificada Ã© realmente melhor do que $\pi$ [^78].

A lÃ³gica por trÃ¡s da prova do teorema de melhoria de polÃ­ticas Ã© intuitiva. ComeÃ§ando com (4.7), a expressÃ£o para $q_{\pi}(s, \pi'(s))$ Ã© expandida utilizando (4.6), e (4.7) Ã© reaplicado iterativamente atÃ© que $v_{\pi'}(s)$ seja obtido:

$$
v_{\pi}(s) \leq q_{\pi}(s, \pi'(s)) \\
= \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = \pi'(s)] \quad \text{(por (4.6))} \\
= \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s] \\
\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, \pi'(S_{t+1})) | S_t = s] \quad \text{(por (4.7))} \\
= \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}[R_{t+2} + \gamma v_{\pi}(S_{t+2}) | S_{t+1}, A_{t+1} = \pi'(S_{t+1})] | S_t = s] \\
= \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi}(S_{t+2}) | S_t = s] \\
\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 v_{\pi}(S_{t+3}) | S_t = s] \\
\vdots \\
\leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \dots | S_t = s] \\
= v_{\pi'}(s).
$$

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos continuar com o exemplo anterior e iterar para ver o efeito a longo prazo. Assumindo $\pi'(s_1) = a_2$ (a aÃ§Ã£o que antes nÃ£o seguÃ­amos em $s_1$), podemos simular algumas iteraÃ§Ãµes:
>
> *   $v_{\pi}(s_1) = 10$
> *   $q_{\pi}(s_1, a_2) = 8$ (calculado anteriormente)
>
> Agora, assumimos que temos novos valores $v_{\pi'}(s_1)$ e $v_{\pi'}(s_2)$ apÃ³s uma iteraÃ§Ã£o de melhoria de polÃ­tica.
>
> Vamos dizer $v_{\pi'}(s_1) = 12$ e $v_{\pi'}(s_2) = 6$. Note que $v_{\pi'}(s) \geq v_{\pi}(s)$ para ambos os estados, como esperado pelo teorema da melhoria de polÃ­tica.
>
> Se recalcularmos $q_{\pi}(s_1, a_2)$ usando esses novos valores:
>
> $q_{\pi'}(s_1, a_2) = 0.6 * [2 + 0.9 * 12] + 0.4 * [-1 + 0.9 * 6] = 0.6 * 12.8 + 0.4 * 4.4 = 7.68 + 1.76 = 9.44$
>
> Observe que $q_{\pi'}(s_1, a_2)$ aumentou em relaÃ§Ã£o ao $q_{\pi}(s_1, a_2)$ anterior, demonstrando como a melhoria da polÃ­tica iterativamente refina as estimativas de valor e as decisÃµes.
>
> ```python
> # Dados atualizados do exemplo
> v_pi_prime_s1 = 12
> v_pi_prime_s2 = 6
>
> # Recalcula q_pi(s1, a2) com os novos valores
> q_pi_prime_s1_a2 = prob_s1 * (reward_s1 + gamma * v_pi_prime_s1) + prob_s2 * (reward_s2 + gamma * v_pi_prime_s2)
>
> print(f"q_pi'(s1, a2) = {q_pi_prime_s1_a2}")
> print(f"v_pi'(s1) = {v_pi_prime_s1}")
> ```

Para melhor compreensÃ£o, apresentamos a prova formal do Teorema da Melhoria da PolÃ­tica:

**Prova do Teorema da Melhoria da PolÃ­tica:**

O teorema afirma que se $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todo $s \in \mathcal{S}$, entÃ£o $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s \in \mathcal{S}$.

I. ComeÃ§amos com a definiÃ§Ã£o de $v_{\pi}(s)$:
   $$v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s]$$

II. Assumimos que $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todo $s$.

III. Usamos a definiÃ§Ã£o de $v_{\pi'}(s)$ e expandimos recursivamente:
    $$v_{\pi'}(s) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi'}(S_{t+1}) | S_t = s]$$
    $$v_{\pi'}(s) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi'}[R_{t+2} + \gamma v_{\pi'}(S_{t+2}) | S_{t+1}] | S_t = s]$$
    $$v_{\pi'}(s) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi'}(S_{t+2}) | S_t = s]$$
    Continuando recursivamente, chegamos a:
    $$v_{\pi'}(s) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s]$$

IV. Agora, considere a expansÃ£o de $q_{\pi}(s, \pi'(s))$:
$$q_{\pi}(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = \pi'(s)]$$
$$q_{\pi}(s, \pi'(s)) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]$$

V. Como $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$, temos:
$$\mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s] \geq v_{\pi}(s)$$

VI. Aplicando recursivamente a desigualdade $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$:
$$\mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi'}[R_{t+2} + \gamma v_{\pi}(S_{t+2}) | S_{t+1}] | S_t = s] \geq \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s] \geq v_{\pi}(s)$$
$$\mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_{\pi}(S_{t+2}) | S_t = s] \geq v_{\pi}(s)$$
Continuando recursivamente, chegamos a:
$$\mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s] \geq v_{\pi}(s)$$

VII. Portanto, temos:
    $$v_{\pi'}(s) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s] \geq v_{\pi}(s)$$
    $$v_{\pi'}(s) \geq v_{\pi}(s)$$

Assim, provamos que se $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todo $s \in \mathcal{S}$, entÃ£o $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s \in \mathcal{S}$. â– 

Ã‰ importante notar que a convergÃªncia da polÃ­tica pode ser acelerada considerando uma atualizaÃ§Ã£o mais agressiva, que nÃ£o apenas seleciona a melhor aÃ§Ã£o *imediatamente*, mas considera a melhor aÃ§Ã£o em cada estado, dado o conhecimento atual da funÃ§Ã£o valor. Este conceito leva Ã  polÃ­tica gulosa.

O objetivo principal Ã© otimizar as aÃ§Ãµes em todos os estados, selecionando, em cada estado, a aÃ§Ã£o que parece ser a melhor de acordo com $q(s, a)$. Em outras palavras, considere a nova **polÃ­tica gulosa** , $\pi'$, dada por [^79]:

$$
\pi'(s) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s, a) = \underset{a}{\operatorname{argmax}} \ \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a] = \underset{a}{\operatorname{argmax}} \sum_{s', r} p(s', r | s, a) [r + \gamma v_{\pi}(s')] \quad (4.9)
$$
onde argmax denota o valor de $a$ no qual a expressÃ£o subsequente Ã© maximizada (com empates quebrados arbitrariamente).

> ðŸ’¡ **Exemplo NumÃ©rico:** Usando o exemplo anterior, suponha que, apÃ³s vÃ¡rias iteraÃ§Ãµes, temos a seguinte funÃ§Ã£o valor $v_{\pi}(s)$: $v_{\pi}(s_1) = 15$ e $v_{\pi}(s_2) = 7$. Para encontrar a polÃ­tica gulosa, precisamos calcular $q_{\pi}(s, a)$ para cada estado e cada aÃ§Ã£o. JÃ¡ calculamos $q_{\pi}(s_1, a_2) = 9.44$. Agora, vamos supor que $q_{\pi}(s_1, a_1) = 14$ e $q_{\pi}(s_2, a_1) = 6$, e $q_{\pi}(s_2, a_2) = 7$.
>
> *   Em $s_1$, temos $q_{\pi}(s_1, a_1) = 14$ e $q_{\pi}(s_1, a_2) = 9.44$. Portanto, $\pi'(s_1) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s_1, a) = a_1$.
> *   Em $s_2$, temos $q_{\pi}(s_2, a_1) = 6$ e $q_{\pi}(s_2, a_2) = 7$. Portanto, $\pi'(s_2) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s_2, a) = a_2$.
>
> Neste caso, a polÃ­tica gulosa $\pi'$ Ã©: $\pi'(s_1) = a_1$ e $\pi'(s_2) = a_2$. Observe que, neste exemplo especÃ­fico, a polÃ­tica gulosa coincide com a polÃ­tica original em $s_2$, mas difere em $s_1$. Isso significa que a polÃ­tica foi melhorada ao escolher $a_1$ em $s_1$ em vez de $a_2$.
>
> ```python
> # Dados do exemplo (incluindo os valores q)
> q_pi_s1_a1 = 14
> q_pi_s1_a2 = 9.44
> q_pi_s2_a1 = 6
> q_pi_s2_a2 = 7
>
> # Encontra a polÃ­tica gulosa
> pi_prime_s1 = "a1" if q_pi_s1_a1 > q_pi_s1_a2 else "a2"
> pi_prime_s2 = "a2" if q_pi_s2_a2 > q_pi_s2_a1 else "a1"
>
> print(f"PolÃ­tica Gulosa: pi'(s1) = {pi_prime_s1}, pi'(s2) = {pi_prime_s2}")
> ```

Por construÃ§Ã£o, a polÃ­tica gulosa satisfaz as condiÃ§Ãµes do teorema de melhoria de polÃ­ticas (4.7), garantindo que ela seja pelo menos tÃ£o boa quanto, ou melhor do que, a polÃ­tica original. O processo de criaÃ§Ã£o de uma nova polÃ­tica que melhora uma polÃ­tica original, tornando-a gulosa em relaÃ§Ã£o Ã  funÃ§Ã£o valor da polÃ­tica original, Ã© denominado **melhoria de polÃ­tica**.

Uma propriedade importante da polÃ­tica gulosa Ã© que, se ela nÃ£o for uma melhoria estrita em relaÃ§Ã£o Ã  polÃ­tica original, entÃ£o a polÃ­tica original jÃ¡ Ã© Ã³tima. Formalmente:

**Teorema 1** Se $\pi'(s) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s, a)$ e $v_{\pi'}(s) = v_{\pi}(s)$ para todo $s \in \mathcal{S}$, entÃ£o $\pi$ Ã© uma polÃ­tica Ã³tima, ou seja, $\pi = \pi_*$.

*Proof.* Se $v_{\pi'}(s) = v_{\pi}(s)$, entÃ£o a desigualdade (4.7) Ã©, na verdade, uma igualdade para todo $s \in \mathcal{S}$. Isso implica que $v_{\pi}(s) = \max_{a} \ q_{\pi}(s, a)$ para todo $s$. Esta Ã© a equaÃ§Ã£o de otimalidade de Bellman para $v_{\pi}$. Como $v_{\pi}$ satisfaz a equaÃ§Ã£o de otimalidade de Bellman, ela deve ser igual a $v_*$, a funÃ§Ã£o valor Ã³tima. Portanto, $\pi$ deve ser uma polÃ­tica Ã³tima.

Para melhor compreensÃ£o, apresentamos a prova formal do Teorema 1:

I. Assumimos que $\pi'(s) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s, a)$ e $v_{\pi'}(s) = v_{\pi}(s)$ para todo $s \in \mathcal{S}$.

II. Se $v_{\pi'}(s) = v_{\pi}(s)$, entÃ£o a desigualdade (4.7) torna-se uma igualdade: $q_{\pi}(s, \pi'(s)) = v_{\pi}(s)$ para todo $s \in \mathcal{S}$.

III. Como $\pi'(s)$ Ã© definido como $\underset{a}{\operatorname{argmax}} \ q_{\pi}(s, a)$, isso significa que $q_{\pi}(s, \pi'(s)) = \max_{a} \ q_{\pi}(s, a)$ para todo $s \in \mathcal{S}$.

IV. Combinando as etapas II e III, temos $v_{\pi}(s) = \max_{a} \ q_{\pi}(s, a)$ para todo $s \in \mathcal{S}$.

V. A equaÃ§Ã£o $v_{\pi}(s) = \max_{a} \ q_{\pi}(s, a)$ Ã© a equaÃ§Ã£o de otimalidade de Bellman para $v_{\pi}(s)$.

VI. Se uma funÃ§Ã£o valor $v$ satisfaz a equaÃ§Ã£o de otimalidade de Bellman, entÃ£o $v$ Ã© a funÃ§Ã£o valor Ã³tima $v_*$. Portanto, $v_{\pi}(s) = v_*(s)$ para todo $s \in \mathcal{S}$.

VII. Se $v_{\pi}(s) = v_*(s)$ para todo $s \in \mathcal{S}$, entÃ£o a polÃ­tica $\pi$ que gera essa funÃ§Ã£o valor Ã© uma polÃ­tica Ã³tima $\pi_*$.

VIII. Portanto, $\pi = \pi_*$.

Assim, provamos que se $\pi'(s) = \underset{a}{\operatorname{argmax}} \ q_{\pi}(s, a)$ e $v_{\pi'}(s) = v_{\pi}(s)$ para todo $s \in \mathcal{S}$, entÃ£o $\pi$ Ã© uma polÃ­tica Ã³tima, ou seja, $\pi = \pi_*$. â– 

AlÃ©m disso, podemos expressar a relaÃ§Ã£o entre $v_{\pi}$ e $v_*$ usando a funÃ§Ã£o Q Ã³tima $q_*(s, a)$, que representa o valor de se iniciar no estado $s$, tomar a aÃ§Ã£o $a$ e, posteriormente, seguir a polÃ­tica Ã³tima.

**ProposiÃ§Ã£o 1** Se $v_{\pi}(s) = v_*(s)$, entÃ£o $q_{\pi}(s, a) \le q_*(s, a)$ para todo $s$ e $a$.

*Proof.* Como $v_*(s)$ Ã© o valor mÃ¡ximo que pode ser obtido a partir do estado $s$, tomar qualquer aÃ§Ã£o $a$ e, subsequentemente, seguir uma polÃ­tica diferente da Ã³tima resultarÃ¡ em um valor menor ou igual a $q_*(s, a)$. Portanto, $q_{\pi}(s, a) \le q_*(s, a)$.

Para melhor compreensÃ£o, apresentamos a prova formal da ProposiÃ§Ã£o 1:

I. Assumimos que $v_{\pi}(s) = v_*(s)$ para todo $s$.

II. Por definiÃ§Ã£o, $q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a]$.

III. TambÃ©m por definiÃ§Ã£o, $q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{*}(S_{t+1}) | S_t = s, A_t = a]$.

IV. Substituindo $v_{\pi}(s)$ por $v_*(s)$ na equaÃ§Ã£o de $q_{\pi}(s, a)$, obtemos:
$q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{*}(S_{t+1}) | S_t = s, A_t = a]$.

V. Agora, compare $q_{\pi}(s, a)$ e $q_*(s, a)$. Se seguirmos a polÃ­tica $\pi$ apÃ³s tomar a aÃ§Ã£o $a$ no estado $s$, obteremos o valor esperado $q_{\pi}(s, a)$. Se seguirmos a polÃ­tica Ã³tima apÃ³s tomar a aÃ§Ã£o $a$ no estado $s$, obteremos o valor esperado $q_*(s, a)$.

VI. Como $v_*(s)$ Ã© o valor mÃ¡ximo que pode ser obtido a partir do estado $s$, seguir qualquer polÃ­tica diferente da Ã³tima (neste caso, $\pi$) resultarÃ¡ em um valor menor ou igual ao valor de seguir a polÃ­tica Ã³tima.

VII. Portanto, $\mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a] \le \mathbb{E}[R_{t+1} + \gamma v_{*}(S_{t+1}) | S_t = s, A_t = a]$.

VIII. Isso implica que $q_{\pi}(s, a) \le q_*(s, a)$ para todo $s$ e $a$.

Assim, provamos que se $v_{\pi}(s) = v_*(s)$, entÃ£o $q_{\pi}(s, a) \le q_*(s, a)$ para todo $s$ e $a$. â– 

### ConclusÃ£o

A melhoria de polÃ­tica Ã© um passo fundamental nos algoritmos de ProgramaÃ§Ã£o DinÃ¢mica, permitindo refinar iterativamente uma polÃ­tica em direÃ§Ã£o Ã  otimizaÃ§Ã£o. Ao selecionar aÃ§Ãµes que maximizam a funÃ§Ã£o de valor da polÃ­tica original, criamos uma nova polÃ­tica que garante um desempenho igual ou superior. Este processo, quando combinado com a avaliaÃ§Ã£o de polÃ­tica, leva Ã  convergÃªncia para a polÃ­tica Ã³tima.

### ReferÃªncias
[^76]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
[^78]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
[^79]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
<!-- END -->