## Policy Improvement via Greedy Policy Selection

### Introdu√ß√£o
Em continuidade ao conceito de **Policy Evaluation** introduzido na Se√ß√£o 4.1, o presente cap√≠tulo aborda o **Policy Improvement**, um processo crucial para encontrar pol√≠ticas √≥timas em ambientes de Markov Decision Processes (MDPs). Como vimos anteriormente, a Policy Evaluation nos permite determinar a fun√ß√£o de valor $v_\pi(s)$ para uma pol√≠tica arbitr√°ria $\pi$. No entanto, o objetivo final √© encontrar a pol√≠tica *√≥tima* ou pelo menos uma pol√≠tica *melhor*. O Policy Improvement utiliza a fun√ß√£o de valor obtida atrav√©s da Policy Evaluation para guiar a busca por pol√≠ticas aprimoradas. Aqui, detalharemos como selecionar uma **greedy policy**, $\pi'$, baseada em $q_\pi(s, a)$, e demonstraremos como essa nova pol√≠tica atende √†s condi√ß√µes do **Policy Improvement Theorem**, garantindo que ela seja t√£o boa quanto ou melhor que a pol√≠tica original.

### Conceitos Fundamentais
O ponto de partida para o Policy Improvement √© a fun√ß√£o de valor $v_\pi(s)$ de uma pol√≠tica $\pi$. Suponha que, para algum estado $s$, temos a oportunidade de mudar nossa pol√≠tica para uma a√ß√£o $a \ne \pi(s)$. A quest√£o central √©: seria melhor escolher deterministicamente a a√ß√£o $a$ em $s$ e ent√£o seguir a pol√≠tica $\pi$? Para responder a esta quest√£o, precisamos analisar $q_\pi(s, a)$, o valor de estado-a√ß√£o para selecionar $a$ em $s$ e seguir $\pi$ a partir da√≠. Matematicamente, $q_\pi(s, a)$ √© definido como:

$$
q_\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t = a] = \sum_{s',r} p(s', r \mid s, a) [r + \gamma v_\pi(s')]
$$

> üí° **Exemplo Num√©rico:** Imagine um MDP simplificado com dois estados, $s_1$ e $s_2$, e duas a√ß√µes, $a_1$ e $a_2$. Seja $\pi(s_1) = a_1$. Suponha que $v_\pi(s_1) = 5$ e $v_\pi(s_2) = 10$, e que $q_\pi(s_1, a_2) = 7$. Isso significa que escolher a a√ß√£o $a_2$ em $s_1$ e seguir a pol√≠tica $\pi$ a partir da√≠ nos d√° um valor esperado de 7, que √© maior que o valor atual $v_\pi(s_1) = 5$. Portanto, seria vantajoso mudar a pol√≠tica para $\pi'(s_1) = a_2$.

O **Policy Improvement Theorem** estabelece uma condi√ß√£o fundamental. Sejam $\pi$ e $\pi'$ duas pol√≠ticas determin√≠sticas quaisquer. Se para todo $s \in \mathcal{S}$:

$$
q_\pi(s, \pi'(s)) \geq v_\pi(s)
$$

ent√£o a pol√≠tica $\pi'$ √© garantidamente t√£o boa quanto ou melhor que $\pi$, ou seja:

$$
v_{\pi'}(s) \geq v_\pi(s), \forall s \in \mathcal{S}
$$
Adicionalmente, se a desigualdade for estrita em pelo menos um estado, ent√£o a melhoria na pol√≠tica √© tamb√©m estrita.

*Prova do Policy Improvement Theorem:*

I.  Assumimos que $q_\pi(s, \pi'(s)) \geq v_\pi(s)$ para todo $s \in \mathcal{S}$.

II. Expandimos $v_\pi(s)$ usando a defini√ß√£o de $q_\pi(s,a)$:

$$
v_\pi(s) \leq q_\pi(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t = \pi'(s)]
$$

III. Desenrolamos recursivamente a express√£o acima:

$$
v_\pi(s) \leq \mathbb{E}_{\pi'} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \mid S_t = s]
$$

IV.  O lado direito da desigualdade acima √© exatamente a defini√ß√£o de $v_{\pi'}(s)$. Portanto:

$$
v_\pi(s) \leq v_{\pi'}(s), \forall s \in \mathcal{S}
$$

Assim, a pol√≠tica $\pi'$ √© t√£o boa quanto ou melhor que $\pi$. ‚ñ†

A **greedy policy**, $\pi'$, √© definida como aquela que, para cada estado $s$, escolhe a a√ß√£o que maximiza $q_\pi(s, a)$. Formalmente:

$$
\pi'(s) = \underset{a}{\arg \max} \; q_\pi(s, a)
$$

Substituindo a defini√ß√£o de $q_\pi(s, a)$, obtemos:

$$
\pi'(s) = \underset{a}{\arg \max} \; \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s, A_t = a] = \underset{a}{\arg \max} \; \sum_{s', r} p(s', r \mid s, a) [r + \gamma v_\pi(s')]
$$

> üí° **Exemplo Num√©rico:** Considere um estado $s$ com duas a√ß√µes poss√≠veis, $a_1$ e $a_2$. Suponha que ap√≥s a Policy Evaluation, obtivemos os seguintes valores: $q_\pi(s, a_1) = 3$ e $q_\pi(s, a_2) = 6$. A greedy policy $\pi'$ escolheria a a√ß√£o $a_2$ neste estado, pois ela maximiza o valor de estado-a√ß√£o: $\pi'(s) = a_2$.

Por constru√ß√£o, a **greedy policy** satisfaz a condi√ß√£o do Policy Improvement Theorem. Ou seja, para cada estado $s$:
$$
q_\pi(s, \pi'(s)) = \max_{a} q_\pi(s, a) \geq q_\pi(s, \pi(s)) = v_\pi(s)
$$

A desigualdade acima segue porque $\pi'(s)$ √© definido como o valor de $a$ que maximiza $q_\pi(s, a)$.

*Prova de $q_\pi(s, \pi'(s)) = \max_{a} q_\pi(s, a) \geq q_\pi(s, \pi(s)) = v_\pi(s)$:*

I. Por defini√ß√£o, $\pi'(s) = \underset{a}{\arg \max} \; q_\pi(s, a)$. Isso significa que $\pi'(s)$ √© a a√ß√£o que maximiza $q_\pi(s, a)$ sobre todas as a√ß√µes $a$.

II. Portanto, $q_\pi(s, \pi'(s))$ deve ser igual ao m√°ximo valor de $q_\pi(s, a)$ sobre todas as a√ß√µes $a$:

$$
q_\pi(s, \pi'(s)) = \max_{a} q_\pi(s, a)
$$

III. Como $\pi(s)$ √© uma a√ß√£o poss√≠vel em $s$, o valor m√°ximo de $q_\pi(s, a)$ deve ser maior ou igual a $q_\pi(s, \pi(s))$:

$$
\max_{a} q_\pi(s, a) \geq q_\pi(s, \pi(s))
$$

IV. Pela defini√ß√£o da fun√ß√£o de valor de estado $v_\pi(s)$, temos:

$$
v_\pi(s) = q_\pi(s, \pi(s))
$$

V. Combinando as etapas acima, obtemos:

$$
q_\pi(s, \pi'(s)) = \max_{a} q_\pi(s, a) \geq q_\pi(s, \pi(s)) = v_\pi(s)
$$

Portanto, a desigualdade √© satisfeita. ‚ñ†

Portanto, ao adotarmos a greedy policy, garantimos que estamos criando uma pol√≠tica $\pi'$ que √© t√£o boa quanto ou melhor que a pol√≠tica original $\pi$. Se $\pi' = \pi$, isso indica que a pol√≠tica $\pi$ j√° √© √≥tima. Se $\pi' \neq \pi$, podemos usar $v_{\pi'}$ para encontrar uma pol√≠tica ainda melhor.

> üí° **Exemplo Num√©rico:** Continuando o exemplo anterior, se ap√≥s adotarmos $\pi'(s) = a_2$ e reavaliarmos a pol√≠tica, encontrarmos que $v_{\pi'}(s) = 8$, isso demonstra que melhoramos o valor do estado $s$ em rela√ß√£o √† pol√≠tica anterior, onde $v_\pi(s) = 5$. Podemos ent√£o usar essa nova fun√ß√£o de valor para continuar o processo de Policy Improvement.

**Lema 1**: Seja $\pi'(s) = \arg \max_a q_\pi(s, a)$ uma greedy policy com rela√ß√£o a $v_\pi(s)$. Ent√£o, $v_{\pi'}(s) \geq v_\pi(s)$.

*Prova:* Como demonstrado acima, a constru√ß√£o da greedy policy garante que a condi√ß√£o $q_\pi(s, \pi'(s)) \geq v_\pi(s)$ seja satisfeita. Pelo Policy Improvement Theorem, segue que $v_{\pi'}(s) \geq v_\pi(s)$. $\blacksquare$

Para solidificar o entendimento da rela√ß√£o entre a pol√≠tica original $\pi$ e a greedy policy $\pi'$, podemos explorar a converg√™ncia deste processo iterativo.

**Teorema 1**: A aplica√ß√£o iterativa do Policy Improvement, gerando uma sequ√™ncia de pol√≠ticas $\{\pi_k\}_{k=0}^{\infty}$ onde $\pi_{k+1}$ √© a greedy policy em rela√ß√£o a $v_{\pi_k}$, converge para uma pol√≠tica √≥tima $\pi_*$ em um n√∫mero finito de itera√ß√µes se o espa√ßo de pol√≠ticas √© finito.

*Prova:* Dado que o espa√ßo de pol√≠ticas √© finito, e cada itera√ß√£o do Policy Improvement garante que $v_{\pi_{k+1}}(s) \geq v_{\pi_k}(s)$ para todo $s$, com desigualdade estrita para pelo menos um estado a menos que $\pi_k$ seja √≥tima, o processo deve convergir para uma pol√≠tica √≥tima em um n√∫mero finito de itera√ß√µes. Se $v_{\pi_{k+1}}(s) = v_{\pi_k}(s)$ para todo $s$, ent√£o $\pi_k$ √© uma pol√≠tica √≥tima, pois nenhuma melhoria √© poss√≠vel. Caso contr√°rio, a desigualdade estrita em pelo menos um estado garante que cada itera√ß√£o resulta em uma pol√≠tica estritamente melhor. Como o n√∫mero de pol√≠ticas poss√≠veis √© finito, o algoritmo deve convergir para uma pol√≠tica √≥tima $\pi_*$ em um n√∫mero finito de passos. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um MDP com apenas 2 estados e 2 a√ß√µes por estado. Existem $2^2 = 4$ pol√≠ticas poss√≠veis. O Policy Improvement garante que, a cada itera√ß√£o, encontramos uma pol√≠tica com valor igual ou superior. Como temos um n√∫mero finito de pol√≠ticas, o algoritmo deve convergir para a pol√≠tica √≥tima em no m√°ximo 4 itera√ß√µes.

**Corol√°rio 1**: Se $\pi' = \pi$, ent√£o $\pi$ √© uma pol√≠tica √≥tima.

*Prova*: Se $\pi' = \pi$, ent√£o $q_\pi(s, \pi'(s)) = q_\pi(s, \pi(s)) = v_\pi(s)$ para todo $s$. Isso implica que nenhuma a√ß√£o $a$ pode produzir um valor $q_\pi(s, a)$ maior que $v_\pi(s)$ em qualquer estado $s$. Portanto, $\pi$ √© uma pol√≠tica √≥tima. $\blacksquare$

### Conclus√£o
A sele√ß√£o de uma **greedy policy** √© um passo fundamental no processo de Policy Improvement. Ao escolher, em cada estado, a a√ß√£o que maximiza a fun√ß√£o de valor estado-a√ß√£o $q_\pi(s, a)$, garantimos que a nova pol√≠tica seja t√£o boa quanto ou melhor que a pol√≠tica original, conforme estabelecido pelo Policy Improvement Theorem. Esse processo iterativo de Policy Evaluation e Policy Improvement forma a base dos algoritmos de Dynamic Programming para encontrar pol√≠ticas √≥timas. A transi√ß√£o para Asynchronous Dynamic Programming (Se√ß√£o 4.5) e Generalized Policy Iteration (Se√ß√£o 4.6) permite flexibilidade adicional na aplica√ß√£o desses conceitos, tornando-os mais adequados para problemas de grande escala.

### Refer√™ncias
[^1]: Dynamic Programming. Chapter 4
[^6]: Policy Improvement Theorem
[^7]: A greedy policy, œÄ', is given by œÄ'(s) = arg max‚Çê qœÄ(s, a). This new policy meets the conditions of the Policy Improvement Theorem, ensuring it's as good as or better than the original.
[^8]: Asynchronous Dynamic Programming (Section 4.5) and Generalized Policy Iteration (Section 4.6)
<!-- END -->