## A Dualidade Competitiva e Cooperativa na Itera√ß√£o de Pol√≠tica Generalizada

### Introdu√ß√£o

A **itera√ß√£o de pol√≠tica generalizada (GPI)** [^86] formaliza a intera√ß√£o entre dois processos fundamentais em algoritmos de *reinforcement learning*: a **avalia√ß√£o de pol√≠tica** e a **melhora de pol√≠tica**. Conforme introduzido anteriormente, esses processos podem ser vistos tanto como competindo quanto cooperando. Este cap√≠tulo explora essa dualidade, detalhando como a tens√£o entre esses processos leva √† converg√™ncia para uma pol√≠tica √≥tima e sua fun√ß√£o de valor correspondente.

### Conceitos Fundamentais

Na ess√™ncia da GPI reside a intera√ß√£o cont√≠nua entre a avalia√ß√£o da pol√≠tica, que busca determinar a fun√ß√£o de valor $V$ para uma dada pol√≠tica $\pi$, e a melhoria da pol√≠tica, que visa encontrar uma pol√≠tica $\pi'$ que seja melhor que $\pi$ com base na fun√ß√£o de valor atual $V$.

1.  **Avalia√ß√£o da Pol√≠tica:** O objetivo da avalia√ß√£o da pol√≠tica √© estimar a fun√ß√£o de valor $V^\pi$ para uma dada pol√≠tica $\pi$. Isto √© realizado atrav√©s de itera√ß√µes que buscam tornar a fun√ß√£o de valor consistente com a pol√≠tica atual. Formalmente, a avalia√ß√£o da pol√≠tica busca satisfazer a equa√ß√£o de Bellman para $V^\pi$:

    $$V^\pi(s) = \mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s, A_t = \pi(s)] \quad \forall s \in \mathcal{S}$$
    onde $V^\pi(s)$ representa o valor do estado $s$ sob a pol√≠tica $\pi$, $R_{t+1}$ √© a recompensa no instante $t+1$, $\gamma$ √© o fator de desconto, e $S_{t+1}$ √© o pr√≥ximo estado.

    A avalia√ß√£o da pol√≠tica pode ser feita iterativamente usando a seguinte atualiza√ß√£o:

    $$V_{k+1}(s) = \mathbb{E}[R_{t+1} + \gamma V_k(S_{t+1}) | S_t = s, A_t = \pi(s)] \quad \forall s \in \mathcal{S}$$
    Esta atualiza√ß√£o √© repetida at√© que $V_k$ converja para $V^\pi$.

    > üí° **Exemplo Num√©rico:**
    >
    > Considere um MDP simples com dois estados, $S = \{s_1, s_2\}$, e duas a√ß√µes, $A = \{a_1, a_2\}$. Seja a pol√≠tica $\pi$ que sempre escolhe a a√ß√£o $a_1$ em ambos os estados, ou seja, $\pi(s_1) = a_1$ e $\pi(s_2) = a_1$. As recompensas esperadas e as transi√ß√µes s√£o as seguintes:
    >
    > -   Em $s_1$, ao tomar a a√ß√£o $a_1$, recebemos uma recompensa de 1 e vamos para $s_2$ com probabilidade 0.8 e permanecemos em $s_1$ com probabilidade 0.2.
    > -   Em $s_2$, ao tomar a a√ß√£o $a_1$, recebemos uma recompensa de -1 e vamos para $s_1$ com probabilidade 0.6 e permanecemos em $s_2$ com probabilidade 0.4.
    >
    > Seja $\gamma = 0.9$. Vamos realizar algumas itera√ß√µes de avalia√ß√£o de pol√≠tica:
    >
    > $\text{Itera√ß√£o 0: } V_0(s_1) = 0, V_0(s_2) = 0$
    >
    > $\text{Itera√ß√£o 1: }$
    >
    > $V_1(s_1) = \mathbb{E}[R_{t+1} + \gamma V_0(S_{t+1}) | S_t = s_1, A_t = a_1] = 1 + 0.9(0.2 \cdot 0 + 0.8 \cdot 0) = 1$
    >
    > $V_1(s_2) = \mathbb{E}[R_{t+1} + \gamma V_0(S_{t+1}) | S_t = s_2, A_t = a_1] = -1 + 0.9(0.6 \cdot 0 + 0.4 \cdot 0) = -1$
    >
    > $\text{Itera√ß√£o 2: }$
    >
    > $V_2(s_1) = \mathbb{E}[R_{t+1} + \gamma V_1(S_{t+1}) | S_t = s_1, A_t = a_1] = 1 + 0.9(0.2 \cdot 1 + 0.8 \cdot (-1)) = 1 + 0.9(-0.6) = 0.46$
    >
    > $V_2(s_2) = \mathbb{E}[R_{t+1} + \gamma V_1(S_{t+1}) | S_t = s_2, A_t = a_1] = -1 + 0.9(0.6 \cdot 1 + 0.4 \cdot (-1)) = -1 + 0.9(0.2) = -0.82$
    >
    > $\text{Itera√ß√£o 3: }$
    >
    > $V_3(s_1) = \mathbb{E}[R_{t+1} + \gamma V_2(S_{t+1}) | S_t = s_1, A_t = a_1] = 1 + 0.9(0.2 \cdot 0.46 + 0.8 \cdot (-0.82)) = 1 + 0.9(-0.564) = 0.4924$
    >
    > $V_3(s_2) = \mathbb{E}[R_{t+1} + \gamma V_2(S_{t+1}) | S_t = s_2, A_t = a_1] = -1 + 0.9(0.6 \cdot 0.46 + 0.4 \cdot (-0.82)) = -1 + 0.9(-0.052) = -1.0468$
    >
    > Continuando estas itera√ß√µes, os valores convergir√£o para $V^\pi(s_1) \approx 0.51$ e $V^\pi(s_2) \approx -1.04$. Este exemplo ilustra como os valores dos estados s√£o atualizados iterativamente para refletir as recompensas esperadas sob a pol√≠tica $\pi$.

    **Lema 1:** *A itera√ß√£o da avalia√ß√£o de pol√≠tica converge para $V^\pi$ para qualquer pol√≠tica fixa $\pi$*.

    *Prova:* A prova segue da aplica√ß√£o do teorema da contra√ß√£o de Banach ao operador de Bellman para avalia√ß√£o de pol√≠tica. O operador √© uma contra√ß√£o em rela√ß√£o √† norma do supremo, garantindo a converg√™ncia para um ponto fixo √∫nico, que √© $V^\pi$.

    Para explicitar melhor, podemos apresentar a prova da seguinte forma:

    I. Defina o operador de Bellman $T^\pi$ para a avalia√ß√£o de pol√≠tica como:
       $$(T^\pi V)(s) = \mathbb{E}[R_{t+1} + \gamma V(S_{t+1}) | S_t = s, A_t = \pi(s)] \quad \forall s \in \mathcal{S}$$

    II. Precisamos mostrar que $T^\pi$ √© uma contra√ß√£o sob a norma do supremo (norma do m√°ximo), definida como:
        $$||V|| = \max_{s \in \mathcal{S}} |V(s)|$$

    III. Considere duas fun√ß√µes de valor arbitr√°rias $U$ e $V$. Ent√£o:
         $$||T^\pi U - T^\pi V|| = \max_{s \in \mathcal{S}} |(T^\pi U)(s) - (T^\pi V)(s)|$$
         $$= \max_{s \in \mathcal{S}} |\mathbb{E}[R_{t+1} + \gamma U(S_{t+1}) | S_t = s, A_t = \pi(s)] - \mathbb{E}[R_{t+1} + \gamma V(S_{t+1}) | S_t = s, A_t = \pi(s)]|$$
         $$= \max_{s \in \mathcal{S}} |\gamma \mathbb{E}[U(S_{t+1}) - V(S_{t+1}) | S_t = s, A_t = \pi(s)]|$$
         $$\leq \gamma \max_{s \in \mathcal{S}} \mathbb{E}[|U(S_{t+1}) - V(S_{t+1})| | S_t = s, A_t = \pi(s)]$$
         $$\leq \gamma \max_{s \in \mathcal{S}} |U(s) - V(s)| = \gamma ||U - V||$$

    IV. Como $\gamma \in [0, 1)$, $T^\pi$ √© uma contra√ß√£o com fator de contra√ß√£o $\gamma$.

    V. Pelo Teorema da Contra√ß√£o de Banach, um operador de contra√ß√£o em um espa√ßo m√©trico completo tem um ponto fixo √∫nico. No nosso caso, o espa√ßo das fun√ß√µes de valor √© um espa√ßo m√©trico completo. Portanto, a itera√ß√£o de avalia√ß√£o de pol√≠tica converge para um √∫nico ponto fixo, que √© a fun√ß√£o de valor $V^\pi$. ‚ñ†

2.  **Melhora da Pol√≠tica:** Dado uma fun√ß√£o de valor $V^\pi$, a melhoria da pol√≠tica tem como objetivo encontrar uma pol√≠tica $\pi'$ que seja *greedy* em rela√ß√£o a $V^\pi$. Isso significa escolher, para cada estado $s$, a a√ß√£o $a$ que maximiza o valor esperado do pr√≥ximo estado:

    $$\pi'(s) = \arg\max_a \mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s, A_t = a] \quad \forall s \in \mathcal{S}$$

    A pol√≠tica $\pi'$ √© garantida ser t√£o boa quanto ou melhor que $\pi$, conforme demonstrado pelo teorema da melhora da pol√≠tica [^78].

    Para quantificar a melhoria, podemos definir a fun√ß√£o Q como:
    $$Q^\pi(s,a) = \mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s, A_t = a] \quad \forall s \in \mathcal{S}, a \in \mathcal{A}$$
    A pol√≠tica melhorada $\pi'$ √© ent√£o dada por:
    $$\pi'(s) = \arg\max_a Q^\pi(s,a)$$

    > üí° **Exemplo Num√©rico:**
    >
    > Continuando com o exemplo anterior, suponha que, ap√≥s a avalia√ß√£o da pol√≠tica, temos $V^\pi(s_1) = 0.51$ e $V^\pi(s_2) = -1.04$. Agora, vamos melhorar a pol√≠tica. Para isso, precisamos calcular a fun√ß√£o Q para cada estado e a√ß√£o:
    >
    > -   **Estado $s_1$:**
    >
    >     -   A√ß√£o $a_1$: $Q^\pi(s_1, a_1) = \mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s_1, A_t = a_1] = 1 + 0.9(0.2 \cdot 0.51 + 0.8 \cdot (-1.04)) = 1 + 0.9(-0.72) = 0.352$
    >     -   A√ß√£o $a_2$: Suponha que, ao tomar a a√ß√£o $a_2$ em $s_1$, recebemos uma recompensa de 0 e vamos para $s_1$ com probabilidade 0.5 e para $s_2$ com probabilidade 0.5. Ent√£o, $Q^\pi(s_1, a_2) = 0 + 0.9(0.5 \cdot 0.51 + 0.5 \cdot (-1.04)) = 0.9(-0.265) = -0.2385$
    >
    > -   **Estado $s_2$:**
    >
    >     -   A√ß√£o $a_1$: $Q^\pi(s_2, a_1) = \mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s_2, A_t = a_1] = -1 + 0.9(0.6 \cdot 0.51 + 0.4 \cdot (-1.04)) = -1 + 0.9(-0.098) = -1.0882$
    >     -   A√ß√£o $a_2$: Suponha que, ao tomar a a√ß√£o $a_2$ em $s_2$, recebemos uma recompensa de 2 e vamos para $s_1$ com probabilidade 0.9 e para $s_2$ com probabilidade 0.1. Ent√£o, $Q^\pi(s_2, a_2) = 2 + 0.9(0.9 \cdot 0.51 + 0.1 \cdot (-1.04)) = 2 + 0.9(0.354) = 2.3186$
    >
    > Agora, determinamos a pol√≠tica melhorada $\pi'$:
    >
    > -   $\pi'(s_1) = \arg\max_a Q^\pi(s_1, a) = \arg\max\{0.352, -0.2385\} = a_1$
    > -   $\pi'(s_2) = \arg\max_a Q^\pi(s_2, a) = \arg\max\{-1.0882, 2.3186\} = a_2$
    >
    > Portanto, a nova pol√≠tica $\pi'$ √©: $\pi'(s_1) = a_1$ e $\pi'(s_2) = a_2$. Esta pol√≠tica √© diferente da anterior, que sempre escolhia $a_1$.

    **Teorema 1:** (Teorema da Melhoria da Pol√≠tica) *Seja $\pi'$ uma pol√≠tica greedy em rela√ß√£o a $V^\pi$. Ent√£o, $\pi'$ √© t√£o boa quanto ou melhor que $\pi$, i.e., $V^{\pi'}(s) \geq V^\pi(s)$ para todo $s \in \mathcal{S}$. Se $V^{\pi'}(s) = V^\pi(s)$ para todo $s \in \mathcal{S}$, ent√£o $\pi$ √© uma pol√≠tica √≥tima*.

    Este teorema √© crucial porque garante que cada itera√ß√£o de melhoria da pol√≠tica n√£o deteriora o desempenho.

    *Prova:*
    I. Seja $\pi'$ uma pol√≠tica greedy em rela√ß√£o a $V^\pi$, ou seja, $\pi'(s) = \arg\max_a Q^\pi(s,a)$ para todo $s \in \mathcal{S}$.
    II. Por defini√ß√£o, $Q^\pi(s, \pi'(s)) = \max_a Q^\pi(s, a) \geq Q^\pi(s, \pi(s)) = V^\pi(s)$.
    III. Usando a defini√ß√£o de $Q^\pi(s,a)$:
         $$Q^\pi(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s, A_t = \pi'(s)]$$
    IV. Portanto, temos:
        $$V^\pi(s) \leq Q^\pi(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s, A_t = \pi'(s)]$$
    V. Agora, defina uma nova pol√≠tica $\pi''$ tal que $\pi''(s) = \pi'(s)$ para um estado $s$ e $\pi''(s) = \pi(s)$ para todos os outros estados. Podemos iterar a desigualdade acima:
    $$V^\pi(s) \leq \mathbb{E}[R_{t+1} + \gamma V^\pi(S_{t+1}) | S_t = s, A_t = \pi'(s)]$$
    $$V^\pi(s) \leq \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi'}[R_{t+2} + \gamma V^\pi(S_{t+2}) | S_{t+1} = s', A_{t+1} = \pi'(s')]| S_t = s, A_t = \pi'(s)]$$
    Iterando infinitamente, obtemos:
    $$V^\pi(s) \leq V^{\pi'}(s)$$

    VI. Se $V^{\pi'}(s) = V^\pi(s)$ para todo $s \in \mathcal{S}$, ent√£o $\pi$ n√£o pode ser melhorada ainda mais, e $\pi$ √© uma pol√≠tica √≥tima. ‚ñ†

#### A Competi√ß√£o entre Avalia√ß√£o e Melhora

Os processos de avalia√ß√£o e melhoria de pol√≠tica s√£o **competitivos** [^86] porque operam em dire√ß√µes opostas.

*   **A melhora da pol√≠tica torna a fun√ß√£o de valor *incorrect* para a nova pol√≠tica:** Quando melhoramos a pol√≠tica para $\pi'$, a fun√ß√£o de valor $V^\pi$ que t√≠nhamos estimado previamente n√£o √© mais precisa, pois ela reflete os valores sob a pol√≠tica $\pi$. A pol√≠tica *greedy* em rela√ß√£o √† fun√ß√£o de valor corrente tipicamente torna a fun√ß√£o de valor incorreta para a pol√≠tica modificada.
*   **A avalia√ß√£o da pol√≠tica torna a pol√≠tica menos *greedy*:** Ao avaliar a pol√≠tica modificada, a pol√≠tica n√£o mais ser√° *greedy*. Ao tornar a fun√ß√£o de valor consistente com a nova pol√≠tica, os valores dos estados s√£o ajustados, o que pode fazer com que algumas a√ß√µes que antes pareciam √≥timas sob $V^\pi$ n√£o sejam mais t√£o atraentes sob a nova fun√ß√£o de valor $V^{\pi'}$.

#### A Coopera√ß√£o entre Avalia√ß√£o e Melhora

Apesar da natureza competitiva, a avalia√ß√£o e melhora da pol√≠tica tamb√©m s√£o profundamente **cooperativas** [^86].

*   **A avalia√ß√£o da pol√≠tica direciona a pol√≠tica para o √≥timo:** Ao refinar a fun√ß√£o de valor para refletir a pol√≠tica corrente, a avalia√ß√£o da pol√≠tica fornece um conhecimento mais preciso das consequ√™ncias de longo prazo de seguir a pol√≠tica atual. Este conhecimento √© crucial para a melhora da pol√≠tica, pois permite que o agente tome decis√µes mais informadas sobre quais a√ß√µes maximizar√£o as recompensas a longo prazo.
*   **A melhora da pol√≠tica direciona a fun√ß√£o de valor para o √≥timo:** Ao atualizar a pol√≠tica para ser *greedy* em rela√ß√£o √† fun√ß√£o de valor corrente, a melhora da pol√≠tica garante que o agente esteja sempre explorando estrat√©gias melhores. Isso, por sua vez, leva a fun√ß√£o de valor a convergir para o valor √≥timo $V^*$, pois a fun√ß√£o de valor sempre reflete o desempenho da melhor pol√≠tica dispon√≠vel.

Essa intera√ß√£o competitiva-cooperativa √© ilustrada na Figura 4.6 [^86], onde a avalia√ß√£o e a melhoria da pol√≠tica s√£o representadas como processos que se movem em dire√ß√µes diferentes no espa√ßo das pol√≠ticas e das fun√ß√µes de valor. Cada processo tenta satisfazer sua pr√≥pria restri√ß√£o (a equa√ß√£o de Bellman para a avalia√ß√£o e a pol√≠tica *greedy* para a melhoria), mas ao fazer isso, eles tamb√©m ajudam o outro processo a se aproximar de sua pr√≥pria solu√ß√£o.

![Diagrama representando a intera√ß√£o entre avalia√ß√£o e melhoria de pol√≠ticas na itera√ß√£o da pol√≠tica generalizada (GPI).](./../images/image6.png)

Para formalizar um pouco mais essa coopera√ß√£o, podemos pensar na GPI como uma forma de otimiza√ß√£o coordenada. A avalia√ß√£o da pol√≠tica fornece um gradiente (aproximado) para a melhoria da pol√≠tica, e a melhoria da pol√≠tica, por sua vez, move o sistema para uma regi√£o onde a avalia√ß√£o da pol√≠tica pode fornecer um gradiente ainda melhor.

### Converg√™ncia para o √ìtimo

A beleza da GPI reside no fato de que, apesar da competi√ß√£o inicial entre a avalia√ß√£o e a melhora da pol√≠tica, esse processo iterativo sempre converge para uma solu√ß√£o conjunta √≥tima [^86]: a fun√ß√£o de valor √≥tima $V^*$ e a pol√≠tica √≥tima $\pi^*$. Isso ocorre porque a cada itera√ß√£o, a pol√≠tica √© garantida ser t√£o boa quanto ou melhor que a anterior, e a fun√ß√£o de valor √© garantida para se tornar mais precisa.

Formalmente, a converg√™ncia da GPI pode ser descrita da seguinte forma:

$$\pi_0 \xrightarrow{\text{avalia√ß√£o}} V^{\pi_0} \xrightarrow{\text{melhora}} \pi_1 \xrightarrow{\text{avalia√ß√£o}} V^{\pi_1} \xrightarrow{\text{melhora}} \dots \xrightarrow{\text{melhora}} \pi^* \xrightarrow{\text{avalia√ß√£o}} V^*$$

onde $\pi_0$ √© uma pol√≠tica inicial arbitr√°ria, $V^{\pi_i}$ √© a fun√ß√£o de valor para a pol√≠tica $\pi_i$, e $\pi^*$ e $V^*$ s√£o a pol√≠tica e fun√ß√£o de valor √≥timas, respectivamente.

A converg√™ncia √© garantida porque um MDP finito tem apenas um n√∫mero finito de pol√≠ticas determin√≠sticas, este processo deve convergir para uma pol√≠tica √≥tima e para a fun√ß√£o de valor √≥tima num n√∫mero finito de itera√ß√µes [^80].

**Teorema 2:** *A itera√ß√£o de pol√≠tica generalizada converge para a pol√≠tica √≥tima $\pi^*$ e sua fun√ß√£o de valor correspondente $V^*$.*

*Prova (Esbo√ßo):*
1.  Pelo Teorema da Melhoria da Pol√≠tica, cada itera√ß√£o de melhoria produz uma pol√≠tica que √© t√£o boa quanto ou melhor que a pol√≠tica anterior.
2.  Como o espa√ßo de pol√≠ticas determin√≠sticas √© finito em um MDP finito, a sequ√™ncia de pol√≠ticas melhoradas deve eventualmente convergir para uma pol√≠tica que n√£o pode ser mais melhorada.
3.  Neste ponto, a pol√≠tica √© √≥tima, e a fun√ß√£o de valor correspondente √© a fun√ß√£o de valor √≥tima.

Uma prova mais detalhada pode ser constru√≠da da seguinte forma:

I. Come√ßamos com uma pol√≠tica arbitr√°ria $\pi_0$ e iteramos entre avalia√ß√£o e melhoria de pol√≠tica.

II. Pelo Teorema da Melhoria da Pol√≠tica, cada itera√ß√£o de melhoria de pol√≠tica resulta em uma pol√≠tica $\pi_{k+1}$ que √© pelo menos t√£o boa quanto $\pi_k$, ou seja, $V^{\pi_{k+1}}(s) \geq V^{\pi_k}(s)$ para todo $s \in \mathcal{S}$.

III. Se $V^{\pi_{k+1}}(s) = V^{\pi_k}(s)$ para todo $s \in \mathcal{S}$, ent√£o $\pi_k$ √© uma pol√≠tica √≥tima e o algoritmo converge.

IV. Caso contr√°rio, $V^{\pi_{k+1}}(s) > V^{\pi_k}(s)$ para pelo menos um estado $s$. Como o n√∫mero de pol√≠ticas poss√≠veis √© finito em um MDP finito, e a cada itera√ß√£o melhoramos estritamente a pol√≠tica (at√© atingir a pol√≠tica √≥tima), o algoritmo deve convergir para a pol√≠tica √≥tima em um n√∫mero finito de itera√ß√µes.

V. Quando o algoritmo converge, temos uma pol√≠tica $\pi^*$ tal que $\pi^*(s) = \arg\max_a Q^{\pi^*}(s, a)$ para todo $s \in \mathcal{S}$. A fun√ß√£o de valor correspondente √© $V^*$, que satisfaz a equa√ß√£o de Bellman da otimalidade:
$$V^*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma V^*(S_{t+1}) | S_t = s, A_t = a]$$

VI. Portanto, a itera√ß√£o de pol√≠tica generalizada converge para a pol√≠tica √≥tima $\pi^*$ e sua fun√ß√£o de valor correspondente $V^*$. ‚ñ†

### Conclus√£o

A itera√ß√£o de pol√≠tica generalizada (GPI) √© um paradigma fundamental em *reinforcement learning* que formaliza a intera√ß√£o entre a avalia√ß√£o e a melhoria da pol√≠tica. Embora esses processos possam parecer inicialmente como competindo, eles cooperam no longo prazo para alcan√ßar uma solu√ß√£o conjunta √≥tima. Compreender a din√¢mica competitiva e cooperativa da GPI √© essencial para projetar e implementar algoritmos eficazes de *reinforcement learning*.

### Refer√™ncias

[^86]: Sutton, Richard S., and Andrew G. Barto. "Reinforcement learning: An introduction." *MIT press*, 2018.

[^78]: Sutton, Richard S., and Andrew G. Barto. "Reinforcement learning: An introduction." *MIT press*, 2018.

[^80]: Sutton, Richard S., and Andrew G. Barto. "Reinforcement learning: An introduction." *MIT press*, 2018.
<!-- END -->