## Generalized Policy Iteration em Reinforcement Learning: Uma Perspectiva Abrangente

### Introdu√ß√£o

O conceito de **Generalized Policy Iteration (GPI)** √© fundamental para a compreens√£o de uma vasta gama de algoritmos de *reinforcement learning* (RL). Como vimos anteriormente [^86], a *policy iteration* cl√°ssica consiste em dois processos simult√¢neos e interativos: a *policy evaluation*, que torna a fun√ß√£o de valor consistente com a pol√≠tica corrente, e a *policy improvement*, que torna a pol√≠tica *greedy* com rela√ß√£o √† fun√ß√£o de valor corrente. O GPI generaliza essa ideia, permitindo uma intera√ß√£o mais flex√≠vel entre esses dois processos. Este cap√≠tulo explora em profundidade como quase todos os m√©todos de *reinforcement learning* podem ser descritos dentro do arcabou√ßo do GPI [^86].

### Conceitos Fundamentais

A *policy iteration* cl√°ssica alterna entre os processos de *evaluation* e *improvement*, completando cada um antes de iniciar o outro [^86]. No entanto, essa altern√¢ncia r√≠gida n√£o √© estritamente necess√°ria. Por exemplo, no algoritmo de *value iteration*, apenas uma √∫nica itera√ß√£o de *policy evaluation* √© realizada entre cada passo de *policy improvement* [^86]. M√©todos de *Dynamic Programming* (DP) ass√≠ncronos levam essa flexibilidade ainda mais longe, entrela√ßando os processos de *evaluation* e *improvement* em uma granularidade ainda mais fina [^86].

**Caracter√≠sticas Essenciais do GPI**

O conceito chave do **GPI** √© que, independentemente da granularidade ou outros detalhes espec√≠ficos dos processos de *policy evaluation* e *policy improvement*, a intera√ß√£o entre eles leva √† converg√™ncia para uma pol√≠tica e fun√ß√£o de valor √≥timas. Essencialmente, todos os m√©todos de *reinforcement learning* que podem ser descritos como GPI possuem [^86]:

1.  **Pol√≠ticas Identific√°veis:** Um mecanismo para representar e modificar pol√≠ticas.
2.  **Fun√ß√µes de Valor Identific√°veis:** Um mecanismo para estimar o valor das pol√≠ticas.
3.  **Melhoria Cont√≠nua da Pol√≠tica:** A pol√≠tica √© sempre melhorada com respeito √† fun√ß√£o de valor corrente.
4.  **Converg√™ncia da Fun√ß√£o de Valor:** A fun√ß√£o de valor √© sempre direcionada para o valor da pol√≠tica corrente.

A figura a seguir ilustra o conceito de GPI:

```
     evaluation
œÄ --------> V_œÄ
     |        ^
     |        |
     v        |
   greedy(V)  |
     |        |
     --------
   improvement
```

Neste diagrama [^86], $\pi$ representa a pol√≠tica e $V$ representa a fun√ß√£o de valor. O processo de *evaluation* (avalia√ß√£o) transforma a pol√≠tica em uma fun√ß√£o de valor que estima o qu√£o bom √© seguir essa pol√≠tica. O processo de *improvement* (melhoria) transforma a fun√ß√£o de valor em uma pol√≠tica melhor, considerando os valores estimados.

![Diagrama da itera√ß√£o da pol√≠tica generalizada (GPI) mostrando o ciclo entre avalia√ß√£o e melhoria da pol√≠tica.](./../images/image2.png)

Se ambos os processos de *evaluation* e *improvement* se estabilizarem [^86], ou seja, n√£o produzirem mais mudan√ßas, ent√£o a fun√ß√£o de valor e a pol√≠tica devem ser √≥timas. A fun√ß√£o de valor se estabiliza somente quando √© consistente com a pol√≠tica corrente, e a pol√≠tica se estabiliza somente quando √© *greedy* com respeito √† fun√ß√£o de valor corrente. Assim, ambos os processos se estabilizam somente quando uma pol√≠tica foi encontrada que √© *greedy* com respeito a sua pr√≥pria fun√ß√£o de avalia√ß√£o. Isso implica que a equa√ß√£o de otimalidade de Bellman [^41] √© v√°lida, e, portanto, que a pol√≠tica e a fun√ß√£o de valor s√£o √≥timas.

> üí° **Exemplo Num√©rico:** Considere um ambiente simples com dois estados, $S = \{s_1, s_2\}$, e duas a√ß√µes, $A = \{a_1, a_2\}$. Inicialmente, nossa pol√≠tica $\pi$ √© equiprov√°vel: $\pi(a_1|s_1) = 0.5$, $\pi(a_2|s_1) = 0.5$, $\pi(a_1|s_2) = 0.5$, $\pi(a_2|s_2) = 0.5$.  Assumimos uma taxa de desconto $\gamma = 0.9$. A recompensa √© definida como: $R(s_1, a_1) = 1$, $R(s_1, a_2) = 0$, $R(s_2, a_1) = 0$, $R(s_2, a_2) = 2$. As probabilidades de transi√ß√£o s√£o: $P(s_1|s_1, a_1) = 0.6$, $P(s_2|s_1, a_1) = 0.4$, $P(s_1|s_1, a_2) = 0.3$, $P(s_2|s_1, a_2) = 0.7$, $P(s_1|s_2, a_1) = 0.8$, $P(s_2|s_2, a_1) = 0.2$, $P(s_1|s_2, a_2) = 0.1$, $P(s_2|s_2, a_2) = 0.9$.
>
> **Policy Evaluation:**
>
> Primeiro, calculamos $V(s_1)$ e $V(s_2)$ usando a equa√ß√£o de Bellman para *policy evaluation*:
>
> $V(s) = \sum_{a} \pi(a|s) \sum_{s'} P(s'|s, a) [R(s, a) + \gamma V(s')]$
>
> $\text{Step 1: } V(s_1) = 0.5 * (0.6 * (1 + 0.9 * V(s_1)) + 0.4 * (1 + 0.9 * V(s_2))) + 0.5 * (0.3 * (0 + 0.9 * V(s_1)) + 0.7 * (0 + 0.9 * V(s_2)))$
>
> $\text{Step 2: } V(s_2) = 0.5 * (0.8 * (0 + 0.9 * V(s_1)) + 0.2 * (0 + 0.9 * V(s_2))) + 0.5 * (0.1 * (2 + 0.9 * V(s_1)) + 0.9 * (2 + 0.9 * V(s_2)))$
>
> Resolvendo este sistema de equa√ß√µes lineares (pode ser feito iterativamente ou diretamente), obtemos (aproximadamente):
>
> $V(s_1) \approx 4.65$
>
> $V(s_2) \approx 8.43$
>
> **Policy Improvement:**
>
> Agora, melhoramos a pol√≠tica tornando-a *greedy* com rela√ß√£o √† fun√ß√£o de valor:
>
> $\pi'(s) = \arg\max_{a} \sum_{s'} P(s'|s, a) [R(s, a) + \gamma V(s')]$
>
> Para $s_1$:
>
>  $Q(s_1, a_1) = 0.6 * (1 + 0.9 * 4.65) + 0.4 * (1 + 0.9 * 8.43) = 5.994$
>
>  $Q(s_1, a_2) = 0.3 * (0 + 0.9 * 4.65) + 0.7 * (0 + 0.9 * 8.43) = 6.611$
>
> Para $s_2$:
>
>  $Q(s_2, a_1) = 0.8 * (0 + 0.9 * 4.65) + 0.2 * (0 + 0.9 * 8.43) = 5.065$
>
>  $Q(s_2, a_2) = 0.1 * (2 + 0.9 * 4.65) + 0.9 * (2 + 0.9 * 8.43) = 9.7657$
>
> A nova pol√≠tica $\pi'$ √©: $\pi'(a_2|s_1) = 1$, $\pi'(a_1|s_2) = 0$, $\pi'(a_2|s_2) = 1$. Repetimos os passos de *evaluation* e *improvement* at√© a converg√™ncia. Este exemplo demonstra como a intera√ß√£o entre *evaluation* e *improvement* refina a pol√≠tica e a fun√ß√£o de valor em dire√ß√£o √† otimalidade.

Para formalizar a no√ß√£o de converg√™ncia no GPI, podemos introduzir o conceito de operador de melhoria de pol√≠tica.

**Defini√ß√£o:** Um operador de melhoria de pol√≠tica, denotado por $\mathcal{I}$, mapeia uma fun√ß√£o de valor $V$ para uma pol√≠tica $\pi'$, ou seja, $\pi' = \mathcal{I}(V)$. A pol√≠tica resultante $\pi'$ √© tal que, para todo estado $s$, $\pi'(s)$ √© uma a√ß√£o que maximiza a soma esperada de recompensa e valor descontado do pr√≥ximo estado dado que estamos em $s$ e agimos de acordo com $\pi'$:
$$
\pi'(s) = \arg\max_{a} \mathbb{E}[R_{t+1} + \gamma V(S_{t+1}) | S_t = s, A_t = a]
$$

**Teorema 1** Se a aplica√ß√£o repetida dos processos de *policy evaluation* e *policy improvement* sob o arcabou√ßo do GPI resulta em pol√≠ticas e fun√ß√µes de valor que convergem, ent√£o a pol√≠tica resultante √© √≥tima.

*Prova.* Seja $\pi^*$ a pol√≠tica resultante da converg√™ncia do GPI, e $V_{\pi^*}$ sua respectiva fun√ß√£o de valor. Por defini√ß√£o de converg√™ncia, temos que $V_{\pi^*}$ √© consistente com $\pi^*$, e $\pi^*$ √© *greedy* com respeito a $V_{\pi^*}$. Isso significa que $\pi^* = \mathcal{I}(V_{\pi^*})$. Portanto, $\pi^*$ satisfaz a equa√ß√£o de otimalidade de Bellman, e consequentemente, $\pi^*$ √© uma pol√≠tica √≥tima.

I.  Assumimos que o GPI converge para uma pol√≠tica $\pi^*$ e uma fun√ß√£o de valor $V_{\pi^*}$. Isso significa que, ap√≥s um n√∫mero suficiente de itera√ß√µes dos processos de *policy evaluation* e *policy improvement*, a pol√≠tica e a fun√ß√£o de valor param de mudar.

II. A converg√™ncia da *policy evaluation* implica que $V_{\pi^*}$ √© uma fun√ß√£o de valor consistente com a pol√≠tica $\pi^*$. Em outras palavras, $V_{\pi^*}$ satisfaz as equa√ß√µes de Bellman para $\pi^*$:
    $$V_{\pi^*}(s) = \mathbb{E}[R_{t+1} + \gamma V_{\pi^*}(S_{t+1}) | S_t = s, A_t = \pi^*(s)] \text{ para todo } s$$

III. A converg√™ncia da *policy improvement* implica que $\pi^*$ √© *greedy* com respeito a $V_{\pi^*}$. Isso significa que para todo estado $s$, $\pi^*(s)$ √© a a√ß√£o que maximiza o valor esperado de $R_{t+1} + \gamma V_{\pi^*}(S_{t+1})$:
    $$\pi^*(s) = \arg\max_{a} \mathbb{E}[R_{t+1} + \gamma V_{\pi^*}(S_{t+1}) | S_t = s, A_t = a]$$

IV. Combinando os passos II e III, obtemos:
    $$V_{\pi^*}(s) = \max_{a} \mathbb{E}[R_{t+1} + \gamma V_{\pi^*}(S_{t+1}) | S_t = s, A_t = a] \text{ para todo } s$$

V. A equa√ß√£o acima √© a equa√ß√£o de otimalidade de Bellman. Se uma pol√≠tica e uma fun√ß√£o de valor a satisfazem, ent√£o a pol√≠tica √© √≥tima. Portanto, $\pi^*$ √© uma pol√≠tica √≥tima. ‚ñ†

Al√©m disso, podemos garantir que cada passo de melhoria de pol√≠tica resulta em uma pol√≠tica que √© pelo menos t√£o boa quanto a pol√≠tica anterior.

**Lema 1** Seja $\pi$ uma pol√≠tica arbitr√°ria, e seja $V_{\pi}$ sua fun√ß√£o de valor. Seja $\pi'$ uma pol√≠tica obtida aplicando o operador de melhoria de pol√≠tica $\mathcal{I}$ a $V_{\pi}$, ou seja, $\pi' = \mathcal{I}(V_{\pi})$. Ent√£o, $\pi'$ √© uma pol√≠tica melhor ou igual a $\pi$, isto √©, $V_{\pi'}(s) \geq V_{\pi}(s)$ para todo estado $s$.

*Prova.* (Esbo√ßo) A prova se baseia na defini√ß√£o do operador de melhoria de pol√≠tica e na equa√ß√£o de Bellman para $V_{\pi}$. A ideia principal √© mostrar que, ao escolher a a√ß√£o *greedy* com respeito a $V_{\pi}$ em cada estado, garantimos que o valor esperado de estar naquele estado sob a nova pol√≠tica $\pi'$ √© pelo menos t√£o bom quanto o valor sob a pol√≠tica anterior $\pi$. Uma prova formal pode ser encontrada em [^86].

I.  Seja $\pi$ uma pol√≠tica arbitr√°ria e $V_{\pi}$ sua fun√ß√£o de valor correspondente. Por defini√ß√£o, $V_{\pi}$ satisfaz a equa√ß√£o de Bellman para $\pi$:

    $$V_{\pi}(s) = \mathbb{E}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t = s, A_t = \pi(s)]$$

II. Seja $\pi' = \mathcal{I}(V_{\pi})$ a pol√≠tica obtida aplicando o operador de melhoria de pol√≠tica a $V_{\pi}$. Ent√£o, por defini√ß√£o do operador $\mathcal{I}$:

    $$\pi'(s) = \arg\max_{a} \mathbb{E}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t = s, A_t = a]$$

III. Isso implica que, para todo estado $s$:

    $$\mathbb{E}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t = s, A_t = \pi'(s)] \geq \mathbb{E}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t = s, A_t = \pi(s)] = V_{\pi}(s)$$

IV. Agora, considere a fun√ß√£o de valor $V_{\pi'}$ para a pol√≠tica $\pi'$. Podemos escrever $V_{\pi'}(s)$ recursivamente como:

    $$V_{\pi'}(s) = \mathbb{E}[R_{t+1} + \gamma V_{\pi'}(S_{t+1}) | S_t = s, A_t = \pi'(s)]$$

V. Expandindo recursivamente a equa√ß√£o acima, obtemos:

    $$V_{\pi'}(s) = \mathbb{E}_{\pi'}[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s]$$

VI. Podemos mostrar indutivamente que $V_{\pi'}(s) \geq V_{\pi}(s)$ para todo $s$.  Para o caso base (um passo):

    $$V_{\pi'}(s) \geq \mathbb{E}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t = s, A_t = \pi'(s)] \geq V_{\pi}(s)$$

VII.  Assumindo que $V_{\pi'}(s') \geq V_{\pi}(s')$ para algum estado $s'$, ent√£o, o passo indutivo mant√©m a desigualdade.  Portanto, por indu√ß√£o, $V_{\pi'}(s) \geq V_{\pi}(s)$ para todo $s$. ‚ñ†

**Conflito e Coopera√ß√£o no GPI**

Os processos de *evaluation* e *improvement* em GPI podem ser vistos como competindo e cooperando [^86]. Eles competem porque a pol√≠tica tornar-se *greedy* em rela√ß√£o √† fun√ß√£o de valor normalmente torna a fun√ß√£o de valor incorreta para a pol√≠tica alterada. Similarmente, tornar a fun√ß√£o de valor consistente com a pol√≠tica normalmente faz com que a pol√≠tica n√£o seja mais *greedy*. No entanto, a longo prazo, esses dois processos interagem para encontrar uma solu√ß√£o conjunta: a fun√ß√£o de valor √≥tima e a pol√≠tica √≥tima [^86].

Outra forma de pensar sobre a intera√ß√£o entre os processos de *evaluation* e *improvement* no GPI √© em termos de duas restri√ß√µes ou objetivos [^86], representadas por duas linhas em um espa√ßo bidimensional. Cada processo leva a fun√ß√£o de valor ou a pol√≠tica em dire√ß√£o a uma das linhas, representando uma solu√ß√£o para um dos dois objetivos. Os objetivos interagem porque as duas linhas n√£o s√£o ortogonais.

Para visualizar essa intera√ß√£o, imagine que a *policy evaluation* tenta projetar a fun√ß√£o de valor no espa√ßo das fun√ß√µes de valor consistentes com a pol√≠tica atual. Enquanto isso, a *policy improvement* tenta projetar a pol√≠tica no espa√ßo das pol√≠ticas *greedy* com respeito √† fun√ß√£o de valor atual. A solu√ß√£o √≥tima est√° na interse√ß√£o desses dois espa√ßos.

![Diagrama representando a intera√ß√£o entre avalia√ß√£o e melhoria de pol√≠ticas na itera√ß√£o da pol√≠tica generalizada (GPI).](./../images/image6.png)

**Teorema 1.1** (Converg√™ncia do GPI sob condi√ß√µes de contra√ß√£o) Se os operadores de *policy evaluation* e *policy improvement* s√£o operadores de contra√ß√£o (em alguma norma apropriada), ent√£o a aplica√ß√£o iterativa desses operadores converge para uma pol√≠tica e fun√ß√£o de valor √≥timas.

*Prova.* (Esbo√ßo) Este teorema √© uma consequ√™ncia direta do teorema do ponto fixo de Banach. Se ambos os operadores s√£o contra√ß√µes, ent√£o sua aplica√ß√£o iterativa leva a uma sequ√™ncia de pol√≠ticas e fun√ß√µes de valor que convergem para um ponto fixo √∫nico. Este ponto fixo corresponde √† pol√≠tica √≥tima e sua fun√ß√£o de valor correspondente.

I.  Seja $\mathcal{E}$ o operador de *policy evaluation* e $\mathcal{I}$ o operador de *policy improvement*.  Assumimos que ambos s√£o operadores de contra√ß√£o em alguma norma $||\cdot||$.  Isso significa que existem constantes $\alpha, \beta \in [0, 1)$ tais que para quaisquer fun√ß√µes de valor $V, V'$ e pol√≠ticas $\pi, \pi'$:

    $$||\mathcal{E}(\pi, V) - \mathcal{E}(\pi, V')|| \leq \alpha ||V - V'||$$
    $$||\mathcal{I}(V, \pi) - \mathcal{I}(V', \pi)|| \leq \beta ||V - V'||$$
    Note que aqui $\mathcal{E}$ recebe como entrada uma pol√≠tica e uma fun√ß√£o valor e retorna uma fun√ß√£o valor atualizada, enquanto $\mathcal{I}$ recebe uma fun√ß√£o valor e uma pol√≠tica e retorna uma pol√≠tica atualizada.

II. Considere a sequ√™ncia de fun√ß√µes de valor e pol√≠ticas geradas pela aplica√ß√£o iterativa dos operadores $\mathcal{E}$ e $\mathcal{I}$:

    $$V_{k+1} = \mathcal{E}(\pi_k, V_k)$$
    $$\pi_{k+1} = \mathcal{I}(V_{k+1}, \pi_k)$$

III. Queremos mostrar que as sequ√™ncias $\{V_k\}$ e $\{\pi_k\}$ convergem para um ponto fixo $(V^*, \pi^*)$, onde $V^* = \mathcal{E}(\pi^*, V^*)$ e $\pi^* = \mathcal{I}(V^*, \pi^*)$.

IV.  Aplicando a desigualdade triangular e as propriedades de contra√ß√£o, podemos derivar:

    $$||V_{k+1} - V_k|| = ||\mathcal{E}(\pi_k, V_k) - \mathcal{E}(\pi_{k-1}, V_{k-1})|| \leq ||\mathcal{E}(\pi_k, V_k) - \mathcal{E}(\pi_k, V_{k-1})|| + ||\mathcal{E}(\pi_k, V_{k-1}) - \mathcal{E}(\pi_{k-1}, V_{k-1})||$$

    $$ \leq \alpha ||V_k - V_{k-1}|| +  ||\mathcal{E}(\mathcal{I}(V_k, \pi_{k-1}), V_{k-1}) - \mathcal{E}(\pi_{k-1}, V_{k-1})||$$
    
V. Como $\mathcal{I}$ √© um operador de contra√ß√£o, e assumindo que a mudan√ßa na pol√≠tica induzida pela mudan√ßa na fun√ß√£o valor √© limitada, podemos mostrar que a sequ√™ncia $\{V_k\}$ √© uma sequ√™ncia de Cauchy. Similarmente, podemos mostrar que a sequ√™ncia $\{\pi_k\}$ √© uma sequ√™ncia de Cauchy.

VI.  Pelo teorema do ponto fixo de Banach, uma sequ√™ncia de Cauchy em um espa√ßo completo converge para um limite. Portanto, as sequ√™ncias $\{V_k\}$ e $\{\pi_k\}$ convergem para um ponto fixo $(V^*, \pi^*)$.

VII.  Finalmente, como $V^* = \mathcal{E}(\pi^*, V^*)$ e $\pi^* = \mathcal{I}(V^*, \pi^*)$, isso significa que $V^*$ √© a fun√ß√£o de valor consistente com a pol√≠tica $\pi^*$, e $\pi^*$ √© a pol√≠tica *greedy* com respeito a $V^*$.  Portanto, $(V^*, \pi^*)$ satisfaz as equa√ß√µes de otimalidade de Bellman e representa a solu√ß√£o √≥tima. ‚ñ†

### Conclus√£o

O GPI oferece uma perspectiva unificada sobre uma ampla variedade de algoritmos de *reinforcement learning* [^86]. Ao reconhecer que todos esses algoritmos compartilham a estrutura fundamental de *policy evaluation* e *policy improvement*, podemos entender melhor suas propriedades e rela√ß√µes. A flexibilidade do GPI permite a cria√ß√£o de algoritmos h√≠bridos que combinam diferentes t√©cnicas de *evaluation* e *improvement* para se adequar a diferentes tipos de problemas. Compreender o GPI √©, portanto, essencial para qualquer pessoa que deseje dominar o campo do *reinforcement learning*.

### Refer√™ncias

[^41]: Cap√≠tulo 4, Dynamic Programming
[^86]: Cap√≠tulo 4, Se√ß√£o 4.6, Generalized Policy Iteration
<!-- END -->