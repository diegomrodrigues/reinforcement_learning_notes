## 4.6 Generalized Policy Iteration

### Introdu√ß√£o
O conceito de **Generalized Policy Iteration (GPI)** √© fundamental para entender como os algoritmos de *reinforcement learning* (RL) convergem para uma pol√≠tica √≥tima. Conforme mencionado no contexto [^86], a itera√ß√£o de pol√≠tica consiste em dois processos simult√¢neos e interativos: a avalia√ß√£o da pol√≠tica e a melhoria da pol√≠tica. Enquanto na itera√ß√£o de pol√≠tica tradicional, esses processos alternam de forma completa, no GPI essa altern√¢ncia pode ocorrer de maneira mais flex√≠vel, inclusive de forma ass√≠ncrona [^86].

### Conceitos Fundamentais

**Generalized Policy Iteration (GPI)** √© a ideia geral de permitir que os processos de **avalia√ß√£o de pol√≠tica** e **melhoria de pol√≠tica** interajam, independentemente da granularidade e de outros detalhes dos dois processos [^86]. Essencialmente, quase todos os m√©todos de *reinforcement learning* podem ser descritos como GPI [^86]. Todos esses m√©todos possuem pol√≠ticas e fun√ß√µes de valor identific√°veis. A pol√≠tica √© sempre aprimorada em rela√ß√£o √† fun√ß√£o de valor, e a fun√ß√£o de valor √© sempre direcionada ao valor real da pol√≠tica.

![Diagrama da itera√ß√£o da pol√≠tica generalizada (GPI) mostrando o ciclo entre avalia√ß√£o e melhoria da pol√≠tica.](./../images/image2.png)

![Diagrama representando a intera√ß√£o entre avalia√ß√£o e melhoria de pol√≠ticas na itera√ß√£o da pol√≠tica generalizada (GPI).](./../images/image6.png)

> üí° **Exemplo Num√©rico:** Considere um agente em um grid world simples com 4 estados (1, 2, 3, 4) e duas a√ß√µes (Esquerda, Direita). A recompensa √© -1 em cada passo at√© atingir o estado 4, que √© o estado terminal com recompensa 0. Inicialmente, a pol√≠tica $\pi$ √© aleat√≥ria, com 50% de chance de ir para a Esquerda ou Direita em cada estado.
>
> $\text{Pol√≠tica Inicial } \pi(a|s) = 0.5, \forall s, a$
>
> $\text{Fun√ß√£o de Valor Inicial } v(s) = 0, \forall s$
>
> Ap√≥s uma itera√ß√£o de avalia√ß√£o de pol√≠tica com $\gamma = 0.9$, podemos obter uma fun√ß√£o de valor mais precisa. Por exemplo, para o estado 1:
>
> $v(1) = \mathbb{E}[R + \gamma v(S')] = 0.5 * (-1 + 0.9 * v(0)) + 0.5 * (-1 + 0.9 * v(2))$
>
> Assumindo $v(0) = 0$ (fora do grid) e $v(2) = -5$,
>
> $v(1) = 0.5 * (-1 + 0) + 0.5 * (-1 - 4.5) = -0.5 - 2.75 = -3.25$
>
> Ap√≥s algumas itera√ß√µes, a pol√≠tica e a fun√ß√£o de valor convergem. A pol√≠tica √≥tima direciona o agente para o estado 4 o mais r√°pido poss√≠vel.

A intera√ß√£o entre avalia√ß√£o e melhoria de pol√≠tica pode ser vista como uma competi√ß√£o e uma coopera√ß√£o simult√¢neas. Os dois processos competem porque se movem em dire√ß√µes opostas. Ao tornar uma pol√≠tica *greedy* com rela√ß√£o a uma fun√ß√£o de valor, a fun√ß√£o de valor se torna imprecisa para a nova pol√≠tica. Similarmente, tornar a fun√ß√£o de valor consistente com a pol√≠tica tende a eliminar o comportamento *greedy* da mesma [^86].

> üí° **Exemplo Num√©rico:** Suponha que temos uma pol√≠tica $\pi$ que sempre move o agente para a direita. A fun√ß√£o de valor $v_{\pi}$ reflete essa pol√≠tica. Agora, fazemos uma melhoria de pol√≠tica e encontramos uma nova pol√≠tica $\pi'$ que move o agente para cima em um estado espec√≠fico $s$ porque $q_{\pi}(s, \text{Cima}) > v_{\pi}(s)$. A nova pol√≠tica $\pi'$ agora √© *greedy* em $s$. No entanto, a fun√ß√£o de valor $v_{\pi}$ n√£o reflete mais a realidade da nova pol√≠tica $\pi'$, pois ela foi calculada com base em $\pi$.

No entanto, esses processos tamb√©m cooperam porque, ao longo do tempo, eles trabalham juntos para encontrar uma solu√ß√£o conjunta: a **fun√ß√£o de valor √≥tima** e a **pol√≠tica √≥tima** [^86]. Se ambos os processos de avalia√ß√£o e melhoria se estabilizarem, ou seja, n√£o produzirem mais mudan√ßas, ent√£o a fun√ß√£o de valor e a pol√≠tica devem ser √≥timas [^86].

A estabiliza√ß√£o da fun√ß√£o de valor ocorre somente quando ela √© consistente com a pol√≠tica atual, e a pol√≠tica se estabiliza somente quando √© *greedy* com rela√ß√£o √† fun√ß√£o de valor atual [^86]. Portanto, ambos os processos se estabilizam somente quando uma pol√≠tica *greedy* com rela√ß√£o a sua pr√≥pria fun√ß√£o de valor de avalia√ß√£o √© encontrada [^86]. Isso implica que a equa√ß√£o de otimalidade de Bellman (4.1) se mant√©m, e, portanto, a pol√≠tica e a fun√ß√£o de valor s√£o √≥timas [^86].

Para formalizar um pouco mais essa no√ß√£o de estabilidade e otimalidade, podemos expressar a seguinte proposi√ß√£o:

**Proposi√ß√£o 1** Seja $\pi$ uma pol√≠tica, e $v_{\pi}$ sua fun√ß√£o de valor correspondente. Se $\pi$ √© uma pol√≠tica *greedy* em rela√ß√£o a $v_{\pi}$, ent√£o $\pi$ √© uma pol√≠tica √≥tima.

*Prova*:
Se $\pi$ √© *greedy* em rela√ß√£o a $v_{\pi}$, ent√£o, por defini√ß√£o:
$$q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a] \leq v_{\pi}(s), \forall s, a$$
com igualdade para $a = \pi(s)$. Se $\pi'$ √© qualquer outra pol√≠tica, ent√£o:
$$v_{\pi'}(s) \leq q_{\pi'}(s, \pi'(s)) \leq v_{\pi}(s)$$
onde a primeira desigualdade segue da defini√ß√£o de $q_{\pi'}$, e a segunda segue do fato de $\pi$ ser *greedy* em rela√ß√£o a $v_{\pi}$. Portanto, $\pi$ √© uma pol√≠tica √≥tima.

Para tornar a prova mais expl√≠cita, podemos expandi-la passo a passo:

I. Assumimos que $\pi$ √© *greedy* em rela√ß√£o a $v_{\pi}$. Isso significa que para qualquer estado $s$ e a√ß√£o $a$:
   $$q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a] \leq v_{\pi}(s)$$
   e a igualdade se mant√©m quando $a = \pi(s)$, ou seja, quando tomamos a a√ß√£o ditada pela pol√≠tica $\pi$ no estado $s$.

II. Agora, considere qualquer outra pol√≠tica $\pi'$. O valor de um estado $s$ sob a pol√≠tica $\pi'$ √© dado por $v_{\pi'}(s)$. Pela defini√ß√£o da fun√ß√£o de valor $v_{\pi'}$, temos:
    $$v_{\pi'}(s) = \mathbb{E}_{\pi'} [R_{t+1} + \gamma v_{\pi'}(S_{t+1}) | S_t = s]$$

III. Podemos relacionar $v_{\pi'}(s)$ com a fun√ß√£o $q$ da pol√≠tica $\pi'$:
     $$v_{\pi'}(s) = q_{\pi'}(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma v_{\pi'}(S_{t+1}) | S_t = s, A_t = \pi'(s)]$$
     Isso significa que o valor de estar no estado $s$ e seguir a pol√≠tica $\pi'$ √© igual a tomar a a√ß√£o $\pi'(s)$ e, em seguida, seguir $\pi'$ a partir do pr√≥ximo estado.

IV. Como $\pi$ √© *greedy* em rela√ß√£o a $v_{\pi}$, sabemos que para qualquer a√ß√£o $a$, $q_{\pi}(s, a) \leq v_{\pi}(s)$. Em particular, isso se aplica √† a√ß√£o $\pi'(s)$:
    $$q_{\pi}(s, \pi'(s)) \leq v_{\pi}(s)$$

V. Agora, precisamos mostrar que $v_{\pi'}(s) \leq v_{\pi}(s)$ para todo $s$. Para isso, vamos usar a defini√ß√£o de $q_{\pi'}(s, \pi'(s))$ e a propriedade *greedy* de $\pi$:
   Como $\pi'$ √© uma pol√≠tica arbitr√°ria, temos que o valor de se iniciar em $s$ e seguir $\pi'$ √© menor ou igual ao valor de seguir $\pi$ (que √© *greedy*):
   $$v_{\pi'}(s) = q_{\pi'}(s, \pi'(s)) \leq v_{\pi}(s)$$

VI. Portanto, $v_{\pi'}(s) \leq v_{\pi}(s)$ para qualquer pol√≠tica $\pi'$ e qualquer estado $s$, o que significa que $\pi$ √© a pol√≠tica √≥tima. ‚ñ†

Ademais, podemos derivar um resultado que explicita a rela√ß√£o entre melhoria de pol√≠tica e a garantia de uma pol√≠tica estritamente melhor (ou igual).

**Teorema 1** (Pol√≠tica Melhorada) Seja $\pi$ e $\pi'$ duas pol√≠ticas arbitr√°rias, tal que para todo estado $s \in \mathcal{S}$:
$$q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$$
Ent√£o, a pol√≠tica $\pi'$ √© t√£o boa quanto ou melhor que a pol√≠tica $\pi$, i.e., $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s \in \mathcal{S}$.

> üí° **Exemplo Num√©rico:** Imagine que $\pi$ √© uma pol√≠tica que evita um estado perigoso $s_d$, e $v_{\pi}(s)$ √© o valor associado a seguir essa pol√≠tica a partir do estado $s$. Agora, $\pi'$ √© uma pol√≠tica que explora mais e, √†s vezes, vai para $s_d$. Se $q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$ para todos os estados $s$, isso significa que a a√ß√£o inicial de $\pi'$ (antes de possivelmente ir para $s_d$) √© boa o suficiente para compensar o risco potencial de se aproximar de $s_d$. Por exemplo, se $\pi'(s)$ leva a uma recompensa imediata muito alta, superando a poss√≠vel penalidade de $s_d$, ent√£o $\pi'$ √© uma pol√≠tica melhor.
>
> Matematicamente, vamos supor que $v_{\pi}(s) = 10$ e $q_{\pi}(s, \pi'(s)) = 12$ para todo $s$. De acordo com o Teorema 1, $v_{\pi'}(s) \geq v_{\pi}(s)$. Ent√£o, mesmo que $\pi'$ possa ter alguns caminhos piores, em m√©dia, ela se espera que tenha um desempenho melhor ou igual que $\pi$.

*Prova*:
A prova pode ser encontrada em [^86], se√ß√£o 4.3. Essencialmente, ela se baseia na expans√£o recursiva de $q_{\pi}(s, \pi'(s))$ e no uso repetido da desigualdade dada para mostrar que $v_{\pi'}(s) \geq v_{\pi}(s)$.

Podemos construir uma prova passo a passo:

I. **Hip√≥tese:** Assumimos que para duas pol√≠ticas arbitr√°rias $\pi$ e $\pi'$, a seguinte condi√ß√£o √© v√°lida para todos os estados $s \in \mathcal{S}$:
   $$q_{\pi}(s, \pi'(s)) \geq v_{\pi}(s)$$
   Isto significa que o valor de tomar a a√ß√£o ditada por $\pi'$ no estado $s$ e, em seguida, seguir a pol√≠tica $\pi$ √© maior ou igual ao valor de seguir a pol√≠tica $\pi$ desde o in√≠cio.

II. **Expans√£o Recursiva de $q_{\pi}(s, \pi'(s))$:**
   Podemos expandir $q_{\pi}(s, \pi'(s))$ usando a defini√ß√£o de $q$ e $v$:
   $$q_{\pi}(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = \pi'(s)]$$

III. **Expans√£o Adicional de $v_{\pi}(S_{t+1})$:**
   Podemos expandir $v_{\pi}(S_{t+1})$ em termos de $q_{\pi}$ no pr√≥ximo estado:
   $$v_{\pi}(S_{t+1}) = q_{\pi}(S_{t+1}, \pi(S_{t+1}))$$
   Substituindo na equa√ß√£o anterior:
   $$q_{\pi}(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, \pi(S_{t+1})) | S_t = s, A_t = \pi'(s)]$$

IV. **Aplica√ß√£o Repetida da Hip√≥tese:**
   Agora, aplicamos a hip√≥tese recursivamente. No pr√≥ximo estado $S_{t+1}$, temos:
   $$q_{\pi}(S_{t+1}, \pi'(S_{t+1})) \geq v_{\pi}(S_{t+1})$$
   Se continuarmos a expandir recursivamente e aplicar a hip√≥tese, obtemos uma sequ√™ncia de desigualdades.

V. **Rela√ß√£o entre $v_{\pi'}(s)$ e $v_{\pi}(s)$:**
   Ap√≥s expandir $q_{\pi}(s, \pi'(s))$ recursivamente $k$ vezes, podemos relacionar o valor de $v_{\pi'}(s)$ com $v_{\pi}(s)$. No limite, quando $k$ tende ao infinito, a pol√≠tica $\pi$ se aproxima da pol√≠tica $\pi'$. Assim, mostramos que:
   $$v_{\pi'}(s) \geq v_{\pi}(s)$$
   para todo $s \in \mathcal{S}$. Isso significa que a pol√≠tica $\pi'$ √© t√£o boa quanto ou melhor que a pol√≠tica $\pi$. ‚ñ†

### Conclus√£o

Em suma, **Generalized Policy Iteration** oferece uma estrutura flex√≠vel para projetar algoritmos de *reinforcement learning* [^86]. Ao permitir a intera√ß√£o entre avalia√ß√£o e melhoria de pol√≠tica em diferentes n√≠veis de granularidade, o GPI garante que os algoritmos de *reinforcement learning* convergem para uma pol√≠tica √≥tima [^86].

### Refer√™ncias
[^86]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, 2018.
<!-- END -->