## Generalized Policy Iteration: Interacting Processes of Evaluation and Improvement

### Introdu√ß√£o
O conceito de **Generalized Policy Iteration (GPI)**, apresentado no Cap√≠tulo 4 [^86], emerge como uma abstra√ß√£o fundamental para entender uma vasta gama de algoritmos de *reinforcement learning*. Ele encapsula a ideia de que a busca por uma pol√≠tica √≥tima pode ser decomposta em dois processos interativos: **policy evaluation** e **policy improvement**. Em vez de serem etapas distintas e sequenciais, como na Policy Iteration cl√°ssica [^86], o GPI permite que esses processos interajam de maneira mais flex√≠vel, com diferentes n√≠veis de granularidade. Este cap√≠tulo aprofunda a an√°lise desses processos simult√¢neos e interativos.

### Conceitos Fundamentais

A ess√™ncia do GPI reside na intera√ß√£o entre dois processos cruciais [^86]:

1.  **Policy Evaluation:** Este processo visa tornar a *fun√ß√£o de valor* consistente com a pol√≠tica corrente. Em outras palavras, ele busca estimar precisamente o valor de seguir a pol√≠tica atual em cada estado. Isso √© alcan√ßado atrav√©s de m√©todos iterativos, como a Iterative Policy Evaluation, que aplicam repetidamente a Equa√ß√£o de Bellman para a pol√≠tica corrente [^74]. O processo de policy evaluation pode ser entendido como a resolu√ß√£o da equa√ß√£o de Bellman para $v_\pi$ ou $q_\pi$, dadas as din√¢micas do ambiente e a pol√≠tica $\pi$. A Equa√ß√£o de Bellman para $v_\pi$ √© dada por:

    $$v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]$$

    E a Equa√ß√£o de Bellman para $q_\pi$ √© dada por:

    $$q_\pi(s, a) = \mathbb{E}_\pi[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]$$

    A solu√ß√£o iterativa para estas equa√ß√µes converge para a verdadeira fun√ß√£o de valor da pol√≠tica $\pi$ [^74].

    > üí° **Exemplo Num√©rico:** Considere um ambiente simples com dois estados, $S = \{s_1, s_2\}$, e duas a√ß√µes, $A = \{a_1, a_2\}$. Suponha que a pol√≠tica atual $\pi$ sempre escolha a a√ß√£o $a_1$ em ambos os estados. As recompensas s√£o determin√≠sticas: $R(s_1, a_1) = 1$ e $R(s_2, a_1) = 0$. O fator de desconto $\gamma = 0.9$. As transi√ß√µes s√£o: $P(s_1'|s_1, a_1) = 1$ se $s_1' = s_2$, e $P(s_2'|s_1, a_1) = 1$ se $s_2' = s_1$.
    >
    > Inicializamos $v(s_1) = 0$ e $v(s_2) = 0$. Aplicando a Equa√ß√£o de Bellman iterativamente:
    >
    > *   Itera√ß√£o 1:
    >
    >     *   $v(s_1) = R(s_1, a_1) + \gamma v(s_2) = 1 + 0.9 * 0 = 1$
    >     *   $v(s_2) = R(s_2, a_1) + \gamma v(s_1) = 0 + 0.9 * 0 = 0$
    > *   Itera√ß√£o 2:
    >
    >     *   $v(s_1) = 1 + 0.9 * 0 = 1$
    >     *   $v(s_2) = 0 + 0.9 * 1 = 0.9$
    > *   Itera√ß√£o 3:
    >
    >     *   $v(s_1) = 1 + 0.9 * 0.9 = 1.81$
    >     *   $v(s_2) = 0 + 0.9 * 1 = 0.9$
    > *   Itera√ß√£o 4:
    >
    >     *   $v(s_1) = 1 + 0.9 * 0.9 = 1.81$
    >     *   $v(s_2) = 0 + 0.9 * 1.81 = 1.629$
    >
    > ... e assim por diante. Ap√≥s algumas itera√ß√µes, os valores convergir√£o para $v_\pi(s_1)$ e $v_\pi(s_2)$. Este exemplo demonstra como a fun√ß√£o de valor √© atualizada iterativamente para se tornar consistente com a pol√≠tica atual.
    >
    > ```python
    > import numpy as np
    >
    > # Par√¢metros do ambiente
    > gamma = 0.9
    >
    > # Inicializa√ß√£o da fun√ß√£o de valor
    > v = np.array([0.0, 0.0])
    >
    > # Recompensas
    > rewards = np.array([1.0, 0.0])
    >
    > # Transi√ß√µes (pr√≥ximo estado sempre troca)
    >
    > # Iterative Policy Evaluation
    > for i in range(10):
    >     v_new = np.zeros_like(v)
    >     v_new[0] = rewards[0] + gamma * v[1]
    >     v_new[1] = rewards[1] + gamma * v[0]
    >     v = v_new
    >     print(f"Itera√ß√£o {i+1}: v = {v}")
    >
    > print(f"Fun√ß√£o de valor final: {v}")
    > ```

    **Lema 1:** *A Iterative Policy Evaluation converge para a $v_\pi$ desde que a norma m√°xima da diferen√ßa entre itera√ß√µes sucessivas tenda a zero.*

    *Proof Strategy:* A prova desse lema segue diretamente da aplica√ß√£o do Teorema da Contra√ß√£o, mostrando que a Equa√ß√£o de Bellman √© um operador de contra√ß√£o no espa√ßo das fun√ß√µes de valor, quando equipado com a norma m√°xima.

    **Prova do Lema 1:**
    I.  Definimos o operador de Bellman $T_\pi$ para uma pol√≠tica fixa $\pi$ como:
        $$(T_\pi v)(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]$$
    II. Demonstraremos que $T_\pi$ √© uma contra√ß√£o sob a norma m√°xima $||v|| = \max_s |v(s)|$. Para quaisquer duas fun√ß√µes de valor $u$ e $v$, temos:
        \begin{align*}
        ||T_\pi u - T_\pi v|| &= \max_s |(T_\pi u)(s) - (T_\pi v)(s)| \\
        &= \max_s |\mathbb{E}_\pi[R_{t+1} + \gamma u(S_{t+1}) | S_t = s] - \mathbb{E}_\pi[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]| \\
        &= \max_s |\gamma \mathbb{E}_\pi[u(S_{t+1}) - v(S_{t+1}) | S_t = s]| \\
        &\leq \gamma \max_s \mathbb{E}_\pi[|u(S_{t+1}) - v(S_{t+1})| | S_t = s] \\
        &\leq \gamma \max_s ||u - v|| = \gamma ||u - v||
        \end{align*}
    III. Portanto, $||T_\pi u - T_\pi v|| \leq \gamma ||u - v||$, onde $0 \leq \gamma < 1$ √© o fator de desconto. Isso mostra que $T_\pi$ √© uma contra√ß√£o com fator $\gamma$.
    IV. Pelo Teorema da Contra√ß√£o (ou Teorema do Ponto Fixo de Banach), um operador de contra√ß√£o em um espa√ßo m√©trico completo tem um √∫nico ponto fixo. O espa√ßo das fun√ß√µes de valor com a norma m√°xima √© um espa√ßo de Banach (espa√ßo m√©trico completo).
    V.  Portanto, a aplica√ß√£o iterativa de $T_\pi$ converge para o √∫nico ponto fixo $v_\pi$, que √© a solu√ß√£o para a equa√ß√£o de Bellman $v_\pi = T_\pi v_\pi$. ‚ñ†

    ![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

2.  **Policy Improvement:** Este processo, por sua vez, visa tornar a pol√≠tica *greedy* com rela√ß√£o √† fun√ß√£o de valor corrente. Isso significa que, em cada estado, a pol√≠tica √© atualizada para escolher a a√ß√£o que maximiza o valor esperado a partir daquele estado, dado o conhecimento da fun√ß√£o de valor. Formalmente, a pol√≠tica greedy $\pi'$ em rela√ß√£o a $v_\pi$ √© dada por:

    $$\pi'(s) = \underset{a}{\operatorname{argmax}} \ q_\pi(s, a) = \underset{a}{\operatorname{argmax}} \ \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a]$$

    O Policy Improvement Theorem [^78] garante que a pol√≠tica resultante $\pi'$ √© t√£o boa quanto ou melhor que a pol√≠tica original $\pi$.

    > üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, suponha que ap√≥s a Policy Evaluation, temos $v(s_1) = 1.81$ e $v(s_2) = 1.629$. Agora, precisamos calcular $q(s, a)$ para cada estado e a√ß√£o para melhorar a pol√≠tica. Suponha que temos as seguintes recompensas e transi√ß√µes para a a√ß√£o $a_2$: $R(s_1, a_2) = 0.5$, $R(s_2, a_2) = 0.2$, $P(s_1'|s_1, a_2) = 0.8$ se $s_1' = s_1$ e $0.2$ se $s_1' = s_2$, e $P(s_2'|s_2, a_2) = 0.5$ se $s_2' = s_1$ e $0.5$ se $s_2' = s_2$.
    >
    > Calculamos $q(s, a)$ para cada estado e a√ß√£o:
    >
    > *   $q(s_1, a_1) = R(s_1, a_1) + \gamma v(s_2) = 1 + 0.9 * 1.629 = 2.4661$
    > *   $q(s_1, a_2) = R(s_1, a_2) + \gamma [P(s_1|s_1, a_2)v(s_1) + P(s_2|s_1, a_2)v(s_2)] = 0.5 + 0.9 * (0.8 * 1.81 + 0.2 * 1.629) = 0.5 + 0.9 * (1.448 + 0.3258) = 0.5 + 0.9 * 1.7738 = 0.5 + 1.59642 = 2.09642$
    > *   $q(s_2, a_1) = R(s_2, a_1) + \gamma v(s_1) = 0 + 0.9 * 1.81 = 1.629$
    > *   $q(s_2, a_2) = R(s_2, a_2) + \gamma [P(s_1|s_2, a_2)v(s_1) + P(s_2|s_2, a_2)v(s_2)] = 0.2 + 0.9 * (0.5 * 1.81 + 0.5 * 1.629) = 0.2 + 0.9 * (0.905 + 0.8145) = 0.2 + 0.9 * 1.7195 = 0.2 + 1.54755 = 1.74755$
    >
    > A nova pol√≠tica $\pi'$ seria:
    >
    > *   $\pi'(s_1) = \underset{a}{\operatorname{argmax}} \ q(s_1, a) = a_1$ (pois $q(s_1, a_1) = 2.4661 > q(s_1, a_2) = 2.09642$)
    > *   $\pi'(s_2) = \underset{a}{\operatorname{argmax}} \ q(s_2, a) = a_2$ (pois $q(s_2, a_2) = 1.74755 > q(s_2, a_1) = 1.629$)
    >
    > Neste caso, a pol√≠tica foi alterada para escolher $a_2$ no estado $s_2$, o que potencialmente pode levar a uma pol√≠tica melhor.

    **Teorema 2:** *Se $\pi'(s) = \underset{a}{\operatorname{argmax}} \ q_\pi(s, a)$ para todo $s \in \mathcal{S}$, ent√£o $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s \in \mathcal{S}$.*

    **Prova do Teorema 2:**

    I.  Come√ßamos expandindo $v_\pi(s)$ usando a Equa√ß√£o de Bellman para a pol√≠tica $\pi$:
        $$v_\pi(s) = q_\pi(s, \pi(s)) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = \pi(s)]$$
    II. Como $\pi'(s)$ √© a a√ß√£o greedy em rela√ß√£o a $q_\pi(s, a)$, temos:
        $$q_\pi(s, \pi'(s)) = \underset{a}{\operatorname{max}} \ q_\pi(s, a) \geq q_\pi(s, \pi(s)) = v_\pi(s)$$
    III. Agora, consideramos $v_{\pi'}(s)$ e a expandimos iterativamente:
        \begin{align*}
        v_{\pi'}(s) &= \mathbb{E}[R_{t+1} + \gamma v_{\pi'}(S_{t+1}) | S_t = s, A_t = \pi'(s)] \\
                    &= q_{\pi'}(s, \pi'(s)) \\
                    &\geq q_\pi(s, \pi'(s))
        \end{align*}
    IV. Expanda $q_\pi(s, \pi'(s))$ recursivamente:
        $$q_\pi(s, \pi'(s)) = \mathbb{E}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t = s, A_t = \pi'(s)]$$
    V.  Continuando a substituir recursivamente e usando o fato de que $q_\pi(s, \pi'(s)) \geq v_\pi(s)$, podemos mostrar que:
        $$v_{\pi'}(s) \geq v_\pi(s)$$
    VI. Portanto, se $\pi'(s) = \underset{a}{\operatorname{argmax}} \ q_\pi(s, a)$ para todo $s \in \mathcal{S}$, ent√£o $v_{\pi'}(s) \geq v_{\pi}(s)$ para todo $s \in \mathcal{S}$. ‚ñ†

    **Teorema 2.1:** *Se $v_{\pi'}(s) = v_{\pi}(s)$ para todo $s \in \mathcal{S}$, ent√£o $\pi$ √© uma pol√≠tica √≥tima.*

    *Proof Strategy:* Se a pol√≠tica greedy $\pi'$ tem o mesmo valor que $\pi$, ent√£o nenhuma a√ß√£o pode melhorar o valor esperado, o que implica que $\pi$ √© √≥tima.

    **Prova do Teorema 2.1:**
    I.  Assumimos que $v_{\pi'}(s) = v_{\pi}(s)$ para todo $s \in \mathcal{S}$, onde $\pi'$ √© a pol√≠tica greedy em rela√ß√£o a $v_\pi$.
    II. Por defini√ß√£o, $v_{\pi}(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a]$.
    III. Isso significa que para cada estado $s$, o valor de seguir $\pi$ √© igual ao valor m√°ximo que poder√≠amos obter escolhendo qualquer outra a√ß√£o $a$.
    IV. Portanto, n√£o existe outra pol√≠tica $\pi''$ tal que $v_{\pi''}(s) > v_{\pi}(s)$ para algum estado $s$.
    V.  Isso implica que $\pi$ √© uma pol√≠tica √≥tima. ‚ñ†

No contexto do GPI, esses dois processos n√£o precisam ser executados em altern√¢ncia r√≠gida. Pelo contr√°rio, eles podem ser *intercalados* em diferentes n√≠veis de granularidade. Por exemplo [^86], na Value Iteration, apenas uma √∫nica itera√ß√£o de Policy Evaluation √© realizada entre cada Policy Improvement. Em m√©todos ass√≠ncronos de Programa√ß√£o Din√¢mica (DP), os processos de Evaluation e Improvement s√£o intercalados de forma ainda mais granular [^85], chegando ao ponto de um √∫nico estado ser atualizado em um processo antes de retornar ao outro [^86].

![Pseudocode for Value Iteration algorithm, a method for estimating the optimal policy in an MDP.](./../images/image4.png)

Al√©m disso, podemos considerar uma forma *generalizada* de Improvement, onde a pol√≠tica n√£o precisa ser estritamente greedy, mas sim $\epsilon$-greedy, explorando outras a√ß√µes com uma probabilidade $\epsilon > 0$. Isso leva a uma explora√ß√£o mais robusta do espa√ßo de pol√≠ticas.

    > üí° **Exemplo Num√©rico:** Suponha que implementamos uma pol√≠tica $\epsilon$-greedy com $\epsilon = 0.1$. Isso significa que em 90% das vezes, o agente escolher√° a a√ß√£o greedy (a melhor a√ß√£o de acordo com a fun√ß√£o de valor atual), e nos outros 10% das vezes, ele escolher√° uma a√ß√£o aleat√≥ria.  Em um ambiente com 4 a√ß√µes poss√≠veis, a probabilidade de escolher cada a√ß√£o √©: $P(\text{a√ß√£o √≥tima}) = 1 - \epsilon = 0.9$ e $P(\text{outra a√ß√£o}) = \epsilon / (n - 1) = 0.1 / 3 \approx 0.0333$. Isso garante que todas as a√ß√µes tenham uma chance de serem exploradas, mesmo que n√£o pare√ßam √≥timas no momento.

**Proposi√ß√£o 3:** *Uma pol√≠tica $\epsilon$-greedy garante explora√ß√£o cont√≠nua do espa√ßo de estados-a√ß√µes, permitindo escapar de √≥timos locais.*

**Prova da Proposi√ß√£o 3:**
I.  Uma pol√≠tica $\epsilon$-greedy seleciona a a√ß√£o √≥tima com probabilidade $1 - \epsilon$ e seleciona uma a√ß√£o aleat√≥ria com probabilidade $\epsilon$.
II. Isso significa que, mesmo que o agente tenha convergido para uma pol√≠tica aparentemente √≥tima, existe sempre uma probabilidade $\epsilon$ de explorar outras a√ß√µes.
III. A explora√ß√£o cont√≠nua impede que o agente fique preso em √≥timos locais, pois existe a possibilidade de descobrir a√ß√µes melhores que n√£o foram consideradas durante a fase de otimiza√ß√£o.
IV. Portanto, uma pol√≠tica $\epsilon$-greedy garante explora√ß√£o cont√≠nua do espa√ßo de estados-a√ß√µes, permitindo escapar de √≥timos locais. ‚ñ†

A chave para o sucesso do GPI reside na *continuidade* de ambos os processos na atualiza√ß√£o de todos os estados [^86]. Enquanto ambos os processos continuarem a ser aplicados a todos os estados, o resultado final √© tipicamente o mesmo: a converg√™ncia para a fun√ß√£o de valor √≥tima e uma pol√≠tica √≥tima [^86].

**Competi√ß√£o e Coopera√ß√£o:**

Os processos de Evaluation e Improvement em GPI podem ser vistos como competindo e cooperando simultaneamente [^86]. Eles competem no sentido de que cada processo tende a desfazer o trabalho do outro: tornar a pol√≠tica *greedy* em rela√ß√£o √† fun√ß√£o de valor geralmente invalida a fun√ß√£o de valor para a pol√≠tica modificada, e tornar a fun√ß√£o de valor consistente com a pol√≠tica geralmente faz com que essa pol√≠tica deixe de ser *greedy* [^86]. No entanto, a longo prazo, esses dois processos interagem para encontrar uma solu√ß√£o conjunta: a fun√ß√£o de valor √≥tima e uma pol√≠tica √≥tima [^86].

**O Diagrama GPI:**

O diagrama apresentado na p√°gina 86 [^86] ilustra a intera√ß√£o entre os processos de evaluation e improvement. O processo de evaluation atualiza a fun√ß√£o de valor $V$ para se aproximar do valor verdadeiro $v_\pi$ da pol√≠tica atual $\pi$. O processo de improvement, por sua vez, utiliza a fun√ß√£o de valor $V$ para gerar uma pol√≠tica melhorada $\pi'$, tornando-a greedy com rela√ß√£o a $V$.

![Diagrama da itera√ß√£o da pol√≠tica generalizada (GPI) mostrando o ciclo entre avalia√ß√£o e melhoria da pol√≠tica.](./../images/image2.png)

![Diagrama representando a intera√ß√£o entre avalia√ß√£o e melhoria de pol√≠ticas na itera√ß√£o da pol√≠tica generalizada (GPI).](./../images/image6.png)

Podemos formalizar essa intera√ß√£o atrav√©s de operadores. Seja $E$ o operador de Evaluation que mapeia uma pol√≠tica $\pi$ para sua fun√ß√£o de valor $v_\pi$, e $I$ o operador de Improvement que mapeia uma fun√ß√£o de valor $V$ para uma pol√≠tica $\pi'$ greedy em rela√ß√£o a $V$. Ent√£o, o GPI pode ser visto como a aplica√ß√£o iterativa desses operadores:

$$\pi_{k+1} = I(V_k)$$
$$V_{k+1} = E(\pi_{k+1})$$

### Conclus√£o

O GPI oferece um framework flex√≠vel e poderoso para o design e an√°lise de algoritmos de Reinforcement Learning [^86]. Ao permitir diferentes n√≠veis de intera√ß√£o entre os processos de Policy Evaluation e Policy Improvement, ele engloba uma ampla gama de m√©todos, desde a Policy Iteration cl√°ssica at√© abordagens ass√≠ncronas e truncadas [^86]. A compreens√£o do GPI fornece insights valiosos sobre a din√¢mica desses algoritmos e ajuda a orientar o desenvolvimento de novas abordagens para resolver problemas complexos de tomada de decis√£o.

### Refer√™ncias
[^74]: Cap√≠tulo 4, Dynamic Programming, p√°gina 74
[^78]: Cap√≠tulo 4, Dynamic Programming, p√°gina 78
[^85]: Cap√≠tulo 4, Dynamic Programming, p√°gina 85
[^86]: Cap√≠tulo 4, Dynamic Programming, p√°gina 86
<!-- END -->