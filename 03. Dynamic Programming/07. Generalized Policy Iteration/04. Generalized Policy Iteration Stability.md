## Estabiliza√ß√£o em GPI e Optimalidade

### Introdu√ß√£o
O conceito de **Generalized Policy Iteration (GPI)**, introduzido na Se√ß√£o 4.6 [^77], descreve uma estrutura geral para algoritmos de reinforcement learning, onde processos de avalia√ß√£o de pol√≠tica e melhoria de pol√≠tica interagem para encontrar uma pol√≠tica √≥tima. Esta se√ß√£o aprofunda-se no ponto crucial em que o GPI se estabiliza, demonstrando como essa estabiliza√ß√£o implica a satisfa√ß√£o da equa√ß√£o de Bellman otimizada e, consequentemente, a otimalidade da pol√≠tica e da fun√ß√£o de valor.

### Conceitos Fundamentais

Como vimos anteriormente, o GPI engloba a ideia de permitir que os processos de **avalia√ß√£o de pol√≠tica** e **melhoria de pol√≠tica** interajam, independentemente da granularidade ou de outros detalhes dos dois processos [^77]. A avalia√ß√£o de pol√≠tica visa tornar a fun√ß√£o de valor consistente com a pol√≠tica atual, enquanto a melhoria de pol√≠tica torna a pol√≠tica *greedy* em rela√ß√£o √† fun√ß√£o de valor atual [^77].

![Diagrama da itera√ß√£o da pol√≠tica generalizada (GPI) mostrando o ciclo entre avalia√ß√£o e melhoria da pol√≠tica.](./../images/image2.png)

> üí° **Exemplo Num√©rico:** Considere um ambiente simples com dois estados (S1, S2) e duas a√ß√µes (A1, A2). Inicialmente, temos uma pol√≠tica aleat√≥ria $\pi$ onde $\pi(S1) = A1$ e $\pi(S2) = A2$. Durante a avalia√ß√£o da pol√≠tica, calculamos $v_\pi(S1)$ e $v_\pi(S2)$. Se ap√≥s v√°rias itera√ß√µes de avalia√ß√£o, $v_\pi(S1)$ e $v_\pi(S2)$ convergem para valores est√°veis (por exemplo, 10 e 5, respectivamente), ent√£o passamos para a fase de melhoria da pol√≠tica. Na melhoria da pol√≠tica, para o estado S1, comparamos o valor esperado de tomar A1 (que √© a a√ß√£o atual sob $\pi$) com o valor esperado de tomar A2. Se tomar A2 resulta em um valor esperado maior (por exemplo, 12), atualizamos $\pi(S1)$ para A2. Este processo continua at√© que nenhuma mudan√ßa adicional na pol√≠tica ocorra.

O ponto chave √© que, quando ambos os processos de avalia√ß√£o e melhoria de pol√≠tica se estabilizam ‚Äì isto √©, quando n√£o produzem mais mudan√ßas [^77] ‚Äì a fun√ß√£o de valor e a pol√≠tica devem ser √≥timas.

**Teorema da Estabiliza√ß√£o em GPI:** O GPI se estabiliza se, e somente se, uma pol√≠tica for encontrada que seja *greedy* em rela√ß√£o √† sua pr√≥pria fun√ß√£o de avalia√ß√£o. Essa condi√ß√£o implica que a equa√ß√£o de Bellman otimizada √© satisfeita e, portanto, a pol√≠tica e a fun√ß√£o de valor s√£o √≥timas.

*Prova:*

Suponha que o GPI se estabilizou. Isso significa que temos uma pol√≠tica $\pi$ e uma fun√ß√£o de valor $V$ tal que:

1.  $V$ √© a fun√ß√£o de valor para $\pi$, ou seja, $V = v_\pi$.
2.  $\pi$ √© uma pol√≠tica *greedy* em rela√ß√£o a $V$, ou seja,
    $$\pi(s) = \underset{a}{\operatorname{argmax}} \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')].$$ [^78]

Agora, podemos mostrar que a equa√ß√£o de Bellman otimizada √© satisfeita. A equa√ß√£o de Bellman para $v_\pi$ √© dada por [^74]:

$$v_\pi(s) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = \pi(s)].$$

Como $\pi$ √© *greedy* em rela√ß√£o a $V = v_\pi$, podemos substituir $\pi(s)$ pelo *argmax* sobre todas as a√ß√µes $a$:

$$v_\pi(s) = \max_{a} \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a]$$
$$ = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')].$$ [^74]

Essa √© precisamente a equa√ß√£o de Bellman otimizada [^79]. Portanto, $v_\pi$ √© a fun√ß√£o de valor √≥tima $v_*$, e $\pi$ √© uma pol√≠tica √≥tima $\pi_*$.

Reciprocamente, suponha que temos uma pol√≠tica √≥tima $\pi_*$ e sua fun√ß√£o de valor √≥tima correspondente $v_*$. Ent√£o, por defini√ß√£o, a pol√≠tica $\pi_*$ √© *greedy* em rela√ß√£o a $v_*$, e $v_*$ satisfaz a equa√ß√£o de Bellman otimizada [^79]. Portanto, o GPI se estabilizar√° com $\pi_*$ e $v_*$. $\blacksquare$

Para complementar a prova do Teorema da Estabiliza√ß√£o em GPI, podemos explicitar a rela√ß√£o entre a fun√ß√£o $q_\pi(s,a)$ e a fun√ß√£o $v_\pi(s)$ no ponto de estabiliza√ß√£o.

**Lema 1:** No ponto de estabiliza√ß√£o do GPI, a fun√ß√£o de valor da pol√≠tica $v_\pi(s)$ √© igual ao valor da a√ß√£o √≥tima para cada estado $s$, ou seja, $v_\pi(s) = \max_{a} q_\pi(s, a)$.

*Prova:*

No ponto de estabiliza√ß√£o, $\pi(s) = \underset{a}{\operatorname{argmax}} \sum_{s', r} p(s', r | s, a) [r + \gamma V(s')] = \underset{a}{\operatorname{argmax}} q_\pi(s, a)$. Como $V = v_\pi$, temos que $v_\pi(s) =  \sum_{s', r} p(s', r | s, \pi(s)) [r + \gamma v_\pi(s')] = q_\pi(s, \pi(s))$. Portanto, $v_\pi(s) = q_\pi(s, \pi(s)) = \max_{a} q_\pi(s, a)$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um estado $s$ e duas a√ß√µes poss√≠veis $a_1$ e $a_2$. Suponha que $q_\pi(s, a_1) = 5$ e $q_\pi(s, a_2) = 8$. De acordo com o Lema 1, no ponto de estabiliza√ß√£o, $v_\pi(s) = \max(5, 8) = 8$. Isso significa que o valor do estado $s$ √© igual ao valor da a√ß√£o √≥tima nesse estado.

**Corol√°rio:** Se a avalia√ß√£o da pol√≠tica resultar em $V \approx v_{\pi}$ e a melhoria da pol√≠tica resultar em $\pi'$ que √© uma melhoria significativa em rela√ß√£o a $\pi$, iterando esses dois processos, eventualmente convergir√° para a pol√≠tica √≥tima $\pi_{*}$ e a fun√ß√£o de valor ideal $v_*$.

A seguir, podemos estabelecer um resultado que relaciona a monotonicidade do processo de GPI com a converg√™ncia para a pol√≠tica √≥tima.

**Teorema 1.1:** Se cada itera√ß√£o de melhoria de pol√≠tica no GPI resulta em uma pol√≠tica $\pi'$ que √© estritamente melhor que a pol√≠tica anterior $\pi$ (i.e., $v_{\pi'}(s) > v_{\pi}(s)$ para todo $s$), ent√£o o GPI converge para a pol√≠tica √≥tima $\pi_*$ em um n√∫mero finito de itera√ß√µes.

*Prova:*
Provaremos que se cada itera√ß√£o de melhoria de pol√≠tica no GPI resulta em uma pol√≠tica $\pi'$ que √© estritamente melhor que a pol√≠tica anterior $\pi$, ent√£o o GPI converge para a pol√≠tica √≥tima $\pi_*$ em um n√∫mero finito de itera√ß√µes.

I.  Definimos o conjunto de todas as pol√≠ticas poss√≠veis como $\Pi$. Como o espa√ßo de estados $S$ e o espa√ßo de a√ß√µes $A$ s√£o finitos, o conjunto $\Pi$ tamb√©m √© finito.

II. Assumimos que cada itera√ß√£o de melhoria de pol√≠tica resulta em uma pol√≠tica $\pi'$ tal que $v_{\pi'}(s) > v_{\pi}(s)$ para todo $s \in S$. Isso significa que cada nova pol√≠tica √© estritamente melhor que a anterior.

III. Considere a sequ√™ncia de pol√≠ticas geradas pelo GPI: $\pi_0, \pi_1, \pi_2, \ldots$. Pela nossa suposi√ß√£o, temos $v_{\pi_{i+1}}(s) > v_{\pi_i}(s)$ para todo $s$ e para todo $i$.

IV. Como cada pol√≠tica na sequ√™ncia √© estritamente melhor que a anterior, nenhuma pol√≠tica pode se repetir na sequ√™ncia. Se uma pol√≠tica se repetisse, digamos $\pi_i = \pi_j$ com $i < j$, ent√£o ter√≠amos $v_{\pi_j}(s) = v_{\pi_i}(s)$ para todo $s$, o que contradiz nossa suposi√ß√£o de melhoria estrita.

V. Portanto, a sequ√™ncia $\pi_0, \pi_1, \pi_2, \ldots$ consiste em pol√≠ticas distintas. Como o conjunto de todas as pol√≠ticas poss√≠veis $\Pi$ √© finito, a sequ√™ncia n√£o pode ser infinita.

VI. Isso implica que o GPI deve convergir para uma pol√≠tica $\pi_*$ ap√≥s um n√∫mero finito de itera√ß√µes. Essa pol√≠tica $\pi_*$ deve ser a pol√≠tica √≥tima, pois n√£o h√° pol√≠ticas melhores no conjunto $\Pi$.

VII. Portanto, se cada itera√ß√£o de melhoria de pol√≠tica no GPI resulta em uma pol√≠tica $\pi'$ que √© estritamente melhor que a pol√≠tica anterior $\pi$, ent√£o o GPI converge para a pol√≠tica √≥tima $\pi_*$ em um n√∫mero finito de itera√ß√µes.  $\blacksquare$

> üí° **Exemplo Num√©rico:** Suponha que temos um ambiente com tr√™s estados e duas a√ß√µes. As pol√≠ticas poss√≠veis s√£o $\pi_1, \pi_2, \pi_3, ...$. Suponha que ap√≥s a avalia√ß√£o da pol√≠tica, determinamos que $v_{\pi_1}(s) = [1, 2, 3]$, $v_{\pi_2}(s) = [2, 3, 4]$, e $v_{\pi_3}(s) = [3, 4, 5]$ para os tr√™s estados. Como $v_{\pi_2}(s) > v_{\pi_1}(s)$ e $v_{\pi_3}(s) > v_{\pi_2}(s)$ para todos os estados, o Teorema 1.1 garante que eventualmente encontraremos a pol√≠tica √≥tima em um n√∫mero finito de itera√ß√µes, pois o n√∫mero de pol√≠ticas poss√≠veis √© finito.

### Implica√ß√µes

Este teorema tem implica√ß√µes profundas. Ele nos diz que podemos projetar algoritmos de reinforcement learning que se baseiam na itera√ß√£o entre avalia√ß√£o e melhoria, e que podemos ter certeza de que esses algoritmos convergir√£o para uma solu√ß√£o √≥tima, desde que ambos os processos continuem a atualizar todos os estados [^85].

Em outras palavras, se a fun√ß√£o de valor se estabilizar, ser√° consistente com a pol√≠tica atual e se a pol√≠tica se estabilizar, ser√° *greedy* em rela√ß√£o √† fun√ß√£o de valor atual [^77]. Isso implica que a equa√ß√£o de Bellman otimizada (4.1) [^73] √© v√°lida, e, portanto, a pol√≠tica e a fun√ß√£o de valor s√£o √≥timas.

### Conclus√£o

A estabiliza√ß√£o no GPI √© um resultado poderoso que garante a otimalidade. Ao entender as condi√ß√µes sob as quais o GPI se estabiliza, podemos projetar algoritmos de reinforcement learning eficazes que convergem para solu√ß√µes √≥timas. Os conceitos discutidos aqui formam a base para muitos algoritmos de reinforcement learning avan√ßados, incluindo os que exploraremos nos cap√≠tulos subsequentes.

### Refer√™ncias
[^73]: Equa√ß√£o de Bellman Otimizada (4.1)
[^74]: Valor de uma pol√≠tica (4.4)
[^77]: Generalized Policy Iteration (GPI)
[^78]: Nova pol√≠tica gulosa (4.9)
[^79]: A Equa√ß√£o de Bellman (4.1)
<!-- END -->