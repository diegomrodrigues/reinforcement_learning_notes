## Iterative Policy Evaluation: Approximando a FunÃ§Ã£o de Valor

### IntroduÃ§Ã£o

Em continuidade ao estudo de **Dynamic Programming (DP)** [^1] e **Policy Evaluation** [^2], este capÃ­tulo aprofunda-se no mÃ©todo iterativo para aproximar a funÃ§Ã£o de valor $v_\pi$ de uma polÃ­tica arbitrÃ¡ria $\pi$. Como vimos anteriormente, a *policy evaluation*, tambÃ©m conhecida como *problema de prediÃ§Ã£o* [^2], busca determinar a funÃ§Ã£o de valor de estado $v_\pi(s)$ para cada estado $s \in \mathcal{S}$, dado que o agente segue a polÃ­tica $\pi$. Este capÃ­tulo focarÃ¡ na implementaÃ§Ã£o iterativa deste processo.

### Conceitos Fundamentais

A **iterative policy evaluation** utiliza uma sequÃªncia de funÃ§Ãµes de valor aproximadas, denotadas por $v_0, v_1, v_2, \dots$ [^2]. Cada $v_k$ mapeia o conjunto de estados $\mathcal{S}^+$ para os nÃºmeros reais $\mathbb{R}$, ou seja, $v_k: \mathcal{S}^+ \rightarrow \mathbb{R}$. A aproximaÃ§Ã£o inicial, $v_0$, Ã© escolhida arbitrariamente, com a restriÃ§Ã£o de que o estado terminal (se existir) deve ter valor 0 [^2].

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um ambiente com 3 estados nÃ£o terminais: $S = \{s_1, s_2, s_3\}$. Inicializamos $v_0(s_1) = 0$, $v_0(s_2) = 0$, $v_0(s_3) = 0$. O estado terminal $s_t$ tem $v_0(s_t) = 0$ por definiÃ§Ã£o.

A atualizaÃ§Ã£o iterativa Ã© realizada usando a equaÃ§Ã£o de Bellman para $v_\pi$ [^2]:
$$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_k(s')] $$
onde:

*   $v_{k+1}(s)$ Ã© a funÃ§Ã£o de valor aproximada no estado $s$ na iteraÃ§Ã£o $k+1$.
*   $\pi(a|s)$ Ã© a probabilidade de selecionar a aÃ§Ã£o $a$ no estado $s$ seguindo a polÃ­tica $\pi$ [^2].
*   $p(s', r|s, a)$ Ã© a probabilidade de transiÃ§Ã£o para o estado $s'$ com recompensa $r$ ao tomar a aÃ§Ã£o $a$ no estado $s$ [^1].
*   $\gamma$ Ã© o fator de desconto, com $0 \leq \gamma < 1$ [^2].
*   $v_k(s')$ Ã© a funÃ§Ã£o de valor aproximada no estado $s'$ na iteraÃ§Ã£o $k$ [^2].

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que no estado $s_1$, temos duas aÃ§Ãµes possÃ­veis: $a_1$ e $a_2$. A polÃ­tica $\pi$ define $\pi(a_1|s_1) = 0.6$ e $\pi(a_2|s_1) = 0.4$. As probabilidades de transiÃ§Ã£o sÃ£o:
>
> *   $p(s_2, -1 | s_1, a_1) = 0.8$ (transiÃ§Ã£o para $s_2$ com recompensa -1)
> *   $p(s_3, -1 | s_1, a_1) = 0.2$ (transiÃ§Ã£o para $s_3$ com recompensa -1)
> *   $p(s_3, -2 | s_1, a_2) = 0.7$ (transiÃ§Ã£o para $s_3$ com recompensa -2)
> *   $p(s_t, 0 | s_1, a_2) = 0.3$ (transiÃ§Ã£o para o estado terminal $s_t$ com recompensa 0)
>
> Se $\gamma = 0.9$, entÃ£o a atualizaÃ§Ã£o de $v_1(s_1)$ na primeira iteraÃ§Ã£o Ã©:
>
> $v_1(s_1) = (0.6) \cdot [(0.8)(-1 + 0.9 \cdot v_0(s_2)) + (0.2)(-1 + 0.9 \cdot v_0(s_3))] + (0.4) \cdot [(0.7)(-2 + 0.9 \cdot v_0(s_3)) + (0.3)(0 + 0.9 \cdot v_0(s_t))]$
>
> Como $v_0(s_2) = v_0(s_3) = v_0(s_t) = 0$, temos:
>
> $v_1(s_1) = (0.6) \cdot [(0.8)(-1) + (0.2)(-1)] + (0.4) \cdot [(0.7)(-2) + (0.3)(0)] = (0.6)(-1) + (0.4)(-1.4) = -0.6 - 0.56 = -1.16$

Essa equaÃ§Ã£o representa um **expected update** [^2], pois calcula a esperanÃ§a sobre todos os estados sucessores possÃ­veis, ponderados pelas probabilidades de transiÃ§Ã£o e pela polÃ­tica.

A convergÃªncia da sequÃªncia $\{v_k\}$ para $v_\pi$ Ã© garantida sob as mesmas condiÃ§Ãµes que garantem a existÃªncia de $v_\pi$, ou seja, se $\gamma < 1$ ou se a terminaÃ§Ã£o eventual for garantida a partir de todos os estados sob a polÃ­tica $\pi$ [^2]. Para formalizar essa convergÃªncia, podemos enunciar o seguinte teorema:

**Teorema 1** [ConvergÃªncia da Iterative Policy Evaluation]
Seja $\mathcal{S}$ o conjunto de estados, $\pi$ uma polÃ­tica fixa, e $\gamma \in [0, 1)$ o fator de desconto. A sequÃªncia de funÃ§Ãµes de valor $\{v_k\}$, gerada pela iterative policy evaluation, converge para a funÃ§Ã£o de valor verdadeira $v_\pi$ uniformemente sobre $\mathcal{S}$ quando $k \rightarrow \infty$.

*Prova (EsboÃ§o)*: A prova pode ser construÃ­da mostrando que a iterative policy evaluation Ã© uma aplicaÃ§Ã£o iterativa do operador de Bellman $T^\pi$, definido como $(T^\pi v)(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]$. Demonstra-se que $T^\pi$ Ã© uma contraÃ§Ã£o de Banach sob a norma do supremo $\|v\|_\infty = \max_{s \in \mathcal{S}} |v(s)|$. Pelo Teorema do Ponto Fixo de Banach, a aplicaÃ§Ã£o iterativa de $T^\pi$ converge para um Ãºnico ponto fixo, que Ã© a soluÃ§Ã£o Ãºnica da equaÃ§Ã£o de Bellman $v_\pi$.

*Prova (Detalhada)*:
Para provar a convergÃªncia da Iterative Policy Evaluation, precisamos mostrar que a sequÃªncia de funÃ§Ãµes de valor $\{v_k\}$ converge para a funÃ§Ã£o de valor Ã³tima $v_\pi$. Isso pode ser feito demonstrando que o operador de Bellman $T^\pi$ Ã© uma contraÃ§Ã£o de Banach sob a norma do supremo.

I. **DefiniÃ§Ã£o do Operador de Bellman:**
   O operador de Bellman $T^\pi$ Ã© definido como:
   $$(T^\pi v)(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')] $$

II. **DemonstraÃ§Ã£o de que $T^\pi$ Ã© uma ContraÃ§Ã£o:**
   Para mostrar que $T^\pi$ Ã© uma contraÃ§Ã£o, devemos provar que existe uma constante $0 \leq \beta < 1$ tal que para quaisquer duas funÃ§Ãµes de valor $u$ e $v$:
   $$\|T^\pi u - T^\pi v\|_\infty \leq \beta \|u - v\|_\infty$$
   onde $\| \cdot \|_\infty$ Ã© a norma do supremo definida como $\|v\|_\infty = \max_{s \in \mathcal{S}} |v(s)|$.

III. **Calculando a diferenÃ§a entre $T^\pi u$ e $T^\pi v$:**
    $$\begin{aligned}
    |(T^\pi u)(s) - (T^\pi v)(s)| &= \left| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma u(s')] - \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')] \right| \\
    &= \left| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \gamma [u(s') - v(s')] \right| \\
    &\leq \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \gamma |u(s') - v(s')| \\
    &\leq \gamma \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \|u - v\|_\infty \\
    &= \gamma \|u - v\|_\infty \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \\
    &= \gamma \|u - v\|_\infty
    \end{aligned}$$
    Como $\sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) = 1$ (soma das probabilidades sobre todas as aÃ§Ãµes e estados sucessores).

IV. **Aplicando a Norma do Supremo:**
    $$\|T^\pi u - T^\pi v\|_\infty = \max_{s \in \mathcal{S}} |(T^\pi u)(s) - (T^\pi v)(s)| \leq \gamma \|u - v\|_\infty$$
    Portanto, $T^\pi$ Ã© uma contraÃ§Ã£o com fator de contraÃ§Ã£o $\gamma$, onde $0 \leq \gamma < 1$.

V. **ConvergÃªncia pelo Teorema do Ponto Fixo de Banach:**
   O Teorema do Ponto Fixo de Banach afirma que se $T$ Ã© uma contraÃ§Ã£o em um espaÃ§o mÃ©trico completo, entÃ£o $T$ tem um Ãºnico ponto fixo, e a sequÃªncia $\{T^k x\}$ converge para esse ponto fixo para qualquer ponto inicial $x$.  No nosso caso, o espaÃ§o das funÃ§Ãµes de valor com a norma do supremo Ã© um espaÃ§o de Banach (espaÃ§o mÃ©trico completo).  Assim, a aplicaÃ§Ã£o iterativa de $T^\pi$ a partir de qualquer funÃ§Ã£o de valor inicial $v_0$ converge para o Ãºnico ponto fixo $v_\pi$, que Ã© a soluÃ§Ã£o Ãºnica da equaÃ§Ã£o de Bellman $v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]$.

VI. **ConclusÃ£o:**
    Demonstramos que a Iterative Policy Evaluation, que itera sobre o operador de Bellman $T^\pi$, converge para a funÃ§Ã£o de valor verdadeira $v_\pi$ uniformemente sobre $\mathcal{S}$ quando $k \rightarrow \infty$. â– 

**Iterative Policy Evaluation - PseudocÃ³digo (In-Place)** [^3]
```
Input Ï€, the policy to be evaluated
Algorithm parameter: a small threshold Î¸ > 0 determining accuracy of estimation
Initialize V(s) arbitrarily, for s âˆˆ S, and V(terminal) to 0
Loop:
    Î” â† 0
    Loop for each s âˆˆ S:
        v â† V(s)
        V(s) â† Î£â‚ Ï€(a|s) Î£â‚›',áµ£ p(s',r|s,a) [r + Î³V(s')]
        Î” â† max(Î”, |v â€“ V(s)|)
    until Î” < Î¸
```

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

O pseudocÃ³digo acima descreve uma versÃ£o *in-place* da iterative policy evaluation [^3]. Nesta versÃ£o, um Ãºnico array Ã© utilizado para armazenar os valores dos estados, e os valores sÃ£o atualizados diretamente, sobrescrevendo os valores antigos. A variÃ¡vel $\Delta$ rastreia a maior mudanÃ§a no valor de qualquer estado durante uma iteraÃ§Ã£o, permitindo que o algoritmo termine quando as mudanÃ§as se tornarem suficientemente pequenas. A ordem na qual os estados sÃ£o atualizados durante a *sweep* [^3] tem um impacto significativo na taxa de convergÃªncia. Uma alternativa Ã  versÃ£o *in-place* Ã© a versÃ£o *out-of-place*, onde dois arrays sÃ£o utilizados: um para os valores da iteraÃ§Ã£o anterior ($v_k$) e outro para os valores da iteraÃ§Ã£o atual ($v_{k+1}$).

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere $\theta = 0.001$. O algoritmo para quando a maior variaÃ§Ã£o em qualquer estado entre iteraÃ§Ãµes for menor que 0.001. Se apÃ³s 50 iteraÃ§Ãµes, a maior variaÃ§Ã£o for 0.0009, o algoritmo para.

**Iterative Policy Evaluation - PseudocÃ³digo (Out-of-Place)**
```
Input Ï€, the policy to be evaluated
Algorithm parameter: a small threshold Î¸ > 0 determining accuracy of estimation
Initialize V_old(s) and V_new(s) arbitrarily, for s âˆˆ S, and V_old(terminal) and V_new(terminal) to 0
Loop:
    Î” â† 0
    Loop for each s âˆˆ S:
        v â† V_old(s)
        V_new(s) â† Î£â‚ Ï€(a|s) Î£â‚›',áµ£ p(s',r|s,a) [r + Î³V_old(s')]
        Î” â† max(Î”, |v â€“ V_new(s)|)
    V_old â† V_new  // Update V_old with the new values
    until Î” < Î¸
```

A escolha entre *in-place* e *out-of-place* depende de consideraÃ§Ãµes de memÃ³ria e velocidade. *In-place* Ã© mais eficiente em termos de memÃ³ria, mas *out-of-place* pode convergir mais rapidamente em algumas situaÃ§Ãµes devido Ã  utilizaÃ§Ã£o de valores mais "antigos" durante a atualizaÃ§Ã£o.

AlÃ©m disso, a ordem de atualizaÃ§Ã£o dos estados influencia a velocidade de convergÃªncia. MÃ©todos como *Gauss-Seidel* (onde os estados sÃ£o atualizados em uma ordem fixa) ou *varredura aleatÃ³ria* (onde a ordem Ã© aleatÃ³ria) podem ser empregados.

### Exemplo: Gridworld

O exemplo 4.1 [^4] ilustra a aplicaÃ§Ã£o de *iterative policy evaluation* em um gridworld 4x4.  Assumindo uma polÃ­tica equiprovÃ¡vel (todas as aÃ§Ãµes igualmente provÃ¡veis) e recompensas de -1 em todas as transiÃ§Ãµes atÃ© alcanÃ§ar um estado terminal, o algoritmo calcula uma sequÃªncia de funÃ§Ãµes de valor $\{v_k\}$ que convergem para $v_\pi$. O valor final $v_\pi(s)$ representa a negaÃ§Ã£o do nÃºmero esperado de passos atÃ© a terminaÃ§Ã£o a partir do estado $s$ [^4].

![Illustration of a 4x4 gridworld environment with rewards and actions for dynamic programming example.](./../images/image9.png)

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um gridworld 4x4 com estados numerados de 1 a 16, onde 1 e 16 sÃ£o estados terminais. Inicialmente, $v_0(s) = 0$ para todos os estados. A polÃ­tica Ã© equiprovÃ¡vel, ou seja, $\pi(a|s) = 0.25$ para cada uma das quatro aÃ§Ãµes (cima, baixo, esquerda, direita), exceto nos estados terminais. A recompensa Ã© -1 para todas as transiÃ§Ãµes nÃ£o terminais e 0 para transiÃ§Ãµes para o estado terminal. O fator de desconto $\gamma = 1$.
>
> ApÃ³s uma iteraÃ§Ã£o (k=1), os valores dos estados adjacentes aos estados terminais serÃ£o atualizados:
>
> *   $v_1(2) = 0.25(-1 + 1 \cdot 0) + 0.25(-1 + 1 \cdot 0) + 0.25(-1 + 1 \cdot 0) + 0.25(-1 + 1 \cdot 0) = -1$
> *   $v_1(5) = -1$
> *   $v_1(12) = -1$
> *   $v_1(15) = -1$
>
> ApÃ³s vÃ¡rias iteraÃ§Ãµes, os valores se propagarÃ£o para os outros estados atÃ© convergirem para $v_\pi$.
>
> ```python
> import numpy as np
>
> # Gridworld size
> n = 4
>
> # Discount factor
> gamma = 1.0
>
> # Reward
> reward = -1
>
> # Threshold for convergence
> theta = 1e-6
>
> # Initialize value function
> V = np.zeros((n, n))
>
> def is_terminal(state):
>     return state == (0, 0) or state == (n - 1, n - 1)
>
> def get_reward(state):
>     return 0 if is_terminal(state) else reward
>
> def step(state, action):
>     i, j = state
>     if action == 0:  # Up
>         next_state = (max(i - 1, 0), j)
>     elif action == 1:  # Down
>         next_state = (min(i + 1, n - 1), j)
>     elif action == 2:  # Left
>         next_state = (i, max(j - 1, 0))
>     elif action == 3:  # Right
>         next_state = (i, min(j + 1, n - 1))
>     else:
>         raise ValueError("Invalid action")
>
>     return next_state, get_reward(next_state)
>
> # Iterative policy evaluation
> def policy_evaluation(V, gamma, reward, theta):
>     delta = float('inf')
>     while delta > theta:
>         delta = 0
>         for i in range(n):
>             for j in range(n):
>                 if is_terminal((i, j)):
>                     continue
>                 v = V[i, j]
>                 new_v = 0
>                 for action in range(4):
>                     next_state, r = step((i, j), action)
>                     new_v += 0.25 * (r + gamma * V[next_state])
>                 V[i, j] = new_v
>                 delta = max(delta, abs(v - new_v))
>     return V
>
> V = policy_evaluation(V, gamma, reward, theta)
> print("Final Value Function:\n", V)
> ```

Para visualizar a convergÃªncia, podemos definir uma medida de erro, como o erro quadrÃ¡tico mÃ©dio (MSE) entre $v_k$ e $v_\pi$:
$$MSE(k) = \frac{1}{|\mathcal{S}|} \sum_{s \in \mathcal{S}} (v_k(s) - v_\pi(s))^2$$
Acompanhar a evoluÃ§Ã£o do MSE ao longo das iteraÃ§Ãµes fornece uma indicaÃ§Ã£o clara da taxa de convergÃªncia do algoritmo.

> ğŸ’¡ **Exemplo NumÃ©rico:** ApÃ³s cada iteraÃ§Ã£o $k$, calcule o MSE. Plote o MSE em funÃ§Ã£o de $k$ para visualizar a convergÃªncia. Se o MSE diminui rapidamente no inÃ­cio e depois se estabiliza, isso indica uma convergÃªncia rÃ¡pida.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Gridworld size
> n = 4
>
> # Discount factor
> gamma = 1.0
>
> # Reward
> reward = -1
>
> # Threshold for convergence
> theta = 1e-6
>
> # Initialize value function
> V = np.zeros((n, n))
>
> def is_terminal(state):
>     return state == (0, 0) or state == (n - 1, n - 1)
>
> def get_reward(state):
>     return 0 if is_terminal(state) else reward
>
> def step(state, action):
>     i, j = state
>     if action == 0:  # Up
>         next_state = (max(i - 1, 0), j)
>     elif action == 1:  # Down
>         next_state = (min(i + 1, n - 1), j)
>     elif action == 2:  # Left
>         next_state = (i, max(j - 1, 0))
>     elif action == 3:  # Right
>         next_state = (i, min(j + 1, n - 1))
>     else:
>         raise ValueError("Invalid action")
>
>     return next_state, get_reward(next_state)
>
> # Iterative policy evaluation with MSE tracking
> def policy_evaluation_with_mse(V, gamma, reward, theta, true_V):
>     mse_values = []
>     delta = float('inf')
>     k = 0
>     while delta > theta:
>         delta = 0
>         old_V = np.copy(V) # Para calcular o MSE corretamente
>         for i in range(n):
>             for j in range(n):
>                 if is_terminal((i, j)):
>                     continue
>                 v = V[i, j]
>                 new_v = 0
>                 for action in range(4):
>                     next_state, r = step((i, j), action)
>                     new_v += 0.25 * (r + gamma * V[next_state])
>                 V[i, j] = new_v
>                 delta = max(delta, abs(v - new_v))
>         mse = np.mean((V - true_V)**2) # Calcula o MSE usando a funÃ§Ã£o de valor verdadeira
>         mse_values.append(mse)
>         k += 1
>     return V, mse_values
>
> # FunÃ§Ã£o de valor "verdadeira" (aproximada) - obtida apÃ³s muitas iteraÃ§Ãµes
> true_V = np.array([
>     [0.0, -14.0, -20.0, -22.0],
>     [-14.0, -18.0, -20.0, -20.0],
>     [-20.0, -20.0, -18.0, -14.0],
>     [-22.0, -20.0, -14.0, 0.0]
> ])
>
> # Executa a policy evaluation e obtÃ©m os valores de MSE
> V = np.zeros((n, n))
> V_final, mse_values = policy_evaluation_with_mse(V, gamma, reward, theta, true_V)
>
> # Cria o grÃ¡fico do MSE
> plt.figure(figsize=(10, 6))
> plt.plot(mse_values)
> plt.title("MSE vs. Iteration")
> plt.xlabel("Iteration")
> plt.ylabel("Mean Squared Error (MSE)")
> plt.grid(True)
> plt.show()
> ```

![Convergence of iterative policy evaluation on a gridworld, showing improvement from random to optimal policy.](./../images/image7.png)

### ConclusÃ£o

A **iterative policy evaluation** Ã© um mÃ©todo fundamental em DP para aproximar a funÃ§Ã£o de valor de uma polÃ­tica dada. A utilizaÃ§Ã£o da equaÃ§Ã£o de Bellman como uma regra de atualizaÃ§Ã£o iterativa garante a convergÃªncia para a funÃ§Ã£o de valor verdadeira, sob certas condiÃ§Ãµes. A compreensÃ£o deste mÃ©todo Ã© crucial para a construÃ§Ã£o de algoritmos mais avanÃ§ados de reinforcement learning, como **policy iteration** e **value iteration** [^1]. Ademais, a escolha da versÃ£o *in-place* ou *out-of-place*, juntamente com a ordem de atualizaÃ§Ã£o dos estados, pode impactar significativamente a eficiÃªncia do algoritmo.

### ReferÃªncias

[^1]: Chapter 4: Dynamic Programming.
[^2]: Section 4.1: Policy Evaluation (Prediction).
[^3]: Section 4.1: Iterative Policy Evaluation (In-Place).
[^4]: Example 4.1: Gridworld.
<!-- END -->