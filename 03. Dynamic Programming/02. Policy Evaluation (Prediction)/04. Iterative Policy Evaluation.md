## Iterative Policy Evaluation: Approximando a Função de Valor

### Introdução

Em continuidade ao estudo de **Dynamic Programming (DP)** [^1] e **Policy Evaluation** [^2], este capítulo aprofunda-se no método iterativo para aproximar a função de valor $v_\pi$ de uma política arbitrária $\pi$. Como vimos anteriormente, a *policy evaluation*, também conhecida como *problema de predição* [^2], busca determinar a função de valor de estado $v_\pi(s)$ para cada estado $s \in \mathcal{S}$, dado que o agente segue a política $\pi$. Este capítulo focará na implementação iterativa deste processo.

### Conceitos Fundamentais

A **iterative policy evaluation** utiliza uma sequência de funções de valor aproximadas, denotadas por $v_0, v_1, v_2, \dots$ [^2]. Cada $v_k$ mapeia o conjunto de estados $\mathcal{S}^+$ para os números reais $\mathbb{R}$, ou seja, $v_k: \mathcal{S}^+ \rightarrow \mathbb{R}$. A aproximação inicial, $v_0$, é escolhida arbitrariamente, com a restrição de que o estado terminal (se existir) deve ter valor 0 [^2].

> 💡 **Exemplo Numérico:** Considere um ambiente com 3 estados não terminais: $S = \{s_1, s_2, s_3\}$. Inicializamos $v_0(s_1) = 0$, $v_0(s_2) = 0$, $v_0(s_3) = 0$. O estado terminal $s_t$ tem $v_0(s_t) = 0$ por definição.

A atualização iterativa é realizada usando a equação de Bellman para $v_\pi$ [^2]:
$$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_k(s')] $$
onde:

*   $v_{k+1}(s)$ é a função de valor aproximada no estado $s$ na iteração $k+1$.
*   $\pi(a|s)$ é a probabilidade de selecionar a ação $a$ no estado $s$ seguindo a política $\pi$ [^2].
*   $p(s', r|s, a)$ é a probabilidade de transição para o estado $s'$ com recompensa $r$ ao tomar a ação $a$ no estado $s$ [^1].
*   $\gamma$ é o fator de desconto, com $0 \leq \gamma < 1$ [^2].
*   $v_k(s')$ é a função de valor aproximada no estado $s'$ na iteração $k$ [^2].

> 💡 **Exemplo Numérico:** Suponha que no estado $s_1$, temos duas ações possíveis: $a_1$ e $a_2$. A política $\pi$ define $\pi(a_1|s_1) = 0.6$ e $\pi(a_2|s_1) = 0.4$. As probabilidades de transição são:
>
> *   $p(s_2, -1 | s_1, a_1) = 0.8$ (transição para $s_2$ com recompensa -1)
> *   $p(s_3, -1 | s_1, a_1) = 0.2$ (transição para $s_3$ com recompensa -1)
> *   $p(s_3, -2 | s_1, a_2) = 0.7$ (transição para $s_3$ com recompensa -2)
> *   $p(s_t, 0 | s_1, a_2) = 0.3$ (transição para o estado terminal $s_t$ com recompensa 0)
>
> Se $\gamma = 0.9$, então a atualização de $v_1(s_1)$ na primeira iteração é:
>
> $v_1(s_1) = (0.6) \cdot [(0.8)(-1 + 0.9 \cdot v_0(s_2)) + (0.2)(-1 + 0.9 \cdot v_0(s_3))] + (0.4) \cdot [(0.7)(-2 + 0.9 \cdot v_0(s_3)) + (0.3)(0 + 0.9 \cdot v_0(s_t))]$
>
> Como $v_0(s_2) = v_0(s_3) = v_0(s_t) = 0$, temos:
>
> $v_1(s_1) = (0.6) \cdot [(0.8)(-1) + (0.2)(-1)] + (0.4) \cdot [(0.7)(-2) + (0.3)(0)] = (0.6)(-1) + (0.4)(-1.4) = -0.6 - 0.56 = -1.16$

Essa equação representa um **expected update** [^2], pois calcula a esperança sobre todos os estados sucessores possíveis, ponderados pelas probabilidades de transição e pela política.

A convergência da sequência $\{v_k\}$ para $v_\pi$ é garantida sob as mesmas condições que garantem a existência de $v_\pi$, ou seja, se $\gamma < 1$ ou se a terminação eventual for garantida a partir de todos os estados sob a política $\pi$ [^2]. Para formalizar essa convergência, podemos enunciar o seguinte teorema:

**Teorema 1** [Convergência da Iterative Policy Evaluation]
Seja $\mathcal{S}$ o conjunto de estados, $\pi$ uma política fixa, e $\gamma \in [0, 1)$ o fator de desconto. A sequência de funções de valor $\{v_k\}$, gerada pela iterative policy evaluation, converge para a função de valor verdadeira $v_\pi$ uniformemente sobre $\mathcal{S}$ quando $k \rightarrow \infty$.

*Prova (Esboço)*: A prova pode ser construída mostrando que a iterative policy evaluation é uma aplicação iterativa do operador de Bellman $T^\pi$, definido como $(T^\pi v)(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]$. Demonstra-se que $T^\pi$ é uma contração de Banach sob a norma do supremo $\|v\|_\infty = \max_{s \in \mathcal{S}} |v(s)|$. Pelo Teorema do Ponto Fixo de Banach, a aplicação iterativa de $T^\pi$ converge para um único ponto fixo, que é a solução única da equação de Bellman $v_\pi$.

*Prova (Detalhada)*:
Para provar a convergência da Iterative Policy Evaluation, precisamos mostrar que a sequência de funções de valor $\{v_k\}$ converge para a função de valor ótima $v_\pi$. Isso pode ser feito demonstrando que o operador de Bellman $T^\pi$ é uma contração de Banach sob a norma do supremo.

I. **Definição do Operador de Bellman:**
   O operador de Bellman $T^\pi$ é definido como:
   $$(T^\pi v)(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')] $$

II. **Demonstração de que $T^\pi$ é uma Contração:**
   Para mostrar que $T^\pi$ é uma contração, devemos provar que existe uma constante $0 \leq \beta < 1$ tal que para quaisquer duas funções de valor $u$ e $v$:
   $$\|T^\pi u - T^\pi v\|_\infty \leq \beta \|u - v\|_\infty$$
   onde $\| \cdot \|_\infty$ é a norma do supremo definida como $\|v\|_\infty = \max_{s \in \mathcal{S}} |v(s)|$.

III. **Calculando a diferença entre $T^\pi u$ e $T^\pi v$:**
    $$\begin{aligned}
    |(T^\pi u)(s) - (T^\pi v)(s)| &= \left| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma u(s')] - \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')] \right| \\
    &= \left| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \gamma [u(s') - v(s')] \right| \\
    &\leq \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \gamma |u(s') - v(s')| \\
    &\leq \gamma \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \|u - v\|_\infty \\
    &= \gamma \|u - v\|_\infty \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \\
    &= \gamma \|u - v\|_\infty
    \end{aligned}$$
    Como $\sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) = 1$ (soma das probabilidades sobre todas as ações e estados sucessores).

IV. **Aplicando a Norma do Supremo:**
    $$\|T^\pi u - T^\pi v\|_\infty = \max_{s \in \mathcal{S}} |(T^\pi u)(s) - (T^\pi v)(s)| \leq \gamma \|u - v\|_\infty$$
    Portanto, $T^\pi$ é uma contração com fator de contração $\gamma$, onde $0 \leq \gamma < 1$.

V. **Convergência pelo Teorema do Ponto Fixo de Banach:**
   O Teorema do Ponto Fixo de Banach afirma que se $T$ é uma contração em um espaço métrico completo, então $T$ tem um único ponto fixo, e a sequência $\{T^k x\}$ converge para esse ponto fixo para qualquer ponto inicial $x$.  No nosso caso, o espaço das funções de valor com a norma do supremo é um espaço de Banach (espaço métrico completo).  Assim, a aplicação iterativa de $T^\pi$ a partir de qualquer função de valor inicial $v_0$ converge para o único ponto fixo $v_\pi$, que é a solução única da equação de Bellman $v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]$.

VI. **Conclusão:**
    Demonstramos que a Iterative Policy Evaluation, que itera sobre o operador de Bellman $T^\pi$, converge para a função de valor verdadeira $v_\pi$ uniformemente sobre $\mathcal{S}$ quando $k \rightarrow \infty$. ■

**Iterative Policy Evaluation - Pseudocódigo (In-Place)** [^3]
```
Input π, the policy to be evaluated
Algorithm parameter: a small threshold θ > 0 determining accuracy of estimation
Initialize V(s) arbitrarily, for s ∈ S, and V(terminal) to 0
Loop:
    Δ ← 0
    Loop for each s ∈ S:
        v ← V(s)
        V(s) ← Σₐ π(a|s) Σₛ',ᵣ p(s',r|s,a) [r + γV(s')]
        Δ ← max(Δ, |v – V(s)|)
    until Δ < θ
```

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

O pseudocódigo acima descreve uma versão *in-place* da iterative policy evaluation [^3]. Nesta versão, um único array é utilizado para armazenar os valores dos estados, e os valores são atualizados diretamente, sobrescrevendo os valores antigos. A variável $\Delta$ rastreia a maior mudança no valor de qualquer estado durante uma iteração, permitindo que o algoritmo termine quando as mudanças se tornarem suficientemente pequenas. A ordem na qual os estados são atualizados durante a *sweep* [^3] tem um impacto significativo na taxa de convergência. Uma alternativa à versão *in-place* é a versão *out-of-place*, onde dois arrays são utilizados: um para os valores da iteração anterior ($v_k$) e outro para os valores da iteração atual ($v_{k+1}$).

> 💡 **Exemplo Numérico:** Considere $\theta = 0.001$. O algoritmo para quando a maior variação em qualquer estado entre iterações for menor que 0.001. Se após 50 iterações, a maior variação for 0.0009, o algoritmo para.

**Iterative Policy Evaluation - Pseudocódigo (Out-of-Place)**
```
Input π, the policy to be evaluated
Algorithm parameter: a small threshold θ > 0 determining accuracy of estimation
Initialize V_old(s) and V_new(s) arbitrarily, for s ∈ S, and V_old(terminal) and V_new(terminal) to 0
Loop:
    Δ ← 0
    Loop for each s ∈ S:
        v ← V_old(s)
        V_new(s) ← Σₐ π(a|s) Σₛ',ᵣ p(s',r|s,a) [r + γV_old(s')]
        Δ ← max(Δ, |v – V_new(s)|)
    V_old ← V_new  // Update V_old with the new values
    until Δ < θ
```

A escolha entre *in-place* e *out-of-place* depende de considerações de memória e velocidade. *In-place* é mais eficiente em termos de memória, mas *out-of-place* pode convergir mais rapidamente em algumas situações devido à utilização de valores mais "antigos" durante a atualização.

Além disso, a ordem de atualização dos estados influencia a velocidade de convergência. Métodos como *Gauss-Seidel* (onde os estados são atualizados em uma ordem fixa) ou *varredura aleatória* (onde a ordem é aleatória) podem ser empregados.

### Exemplo: Gridworld

O exemplo 4.1 [^4] ilustra a aplicação de *iterative policy evaluation* em um gridworld 4x4.  Assumindo uma política equiprovável (todas as ações igualmente prováveis) e recompensas de -1 em todas as transições até alcançar um estado terminal, o algoritmo calcula uma sequência de funções de valor $\{v_k\}$ que convergem para $v_\pi$. O valor final $v_\pi(s)$ representa a negação do número esperado de passos até a terminação a partir do estado $s$ [^4].

![Illustration of a 4x4 gridworld environment with rewards and actions for dynamic programming example.](./../images/image9.png)

> 💡 **Exemplo Numérico:** Considere um gridworld 4x4 com estados numerados de 1 a 16, onde 1 e 16 são estados terminais. Inicialmente, $v_0(s) = 0$ para todos os estados. A política é equiprovável, ou seja, $\pi(a|s) = 0.25$ para cada uma das quatro ações (cima, baixo, esquerda, direita), exceto nos estados terminais. A recompensa é -1 para todas as transições não terminais e 0 para transições para o estado terminal. O fator de desconto $\gamma = 1$.
>
> Após uma iteração (k=1), os valores dos estados adjacentes aos estados terminais serão atualizados:
>
> *   $v_1(2) = 0.25(-1 + 1 \cdot 0) + 0.25(-1 + 1 \cdot 0) + 0.25(-1 + 1 \cdot 0) + 0.25(-1 + 1 \cdot 0) = -1$
> *   $v_1(5) = -1$
> *   $v_1(12) = -1$
> *   $v_1(15) = -1$
>
> Após várias iterações, os valores se propagarão para os outros estados até convergirem para $v_\pi$.
>
> ```python
> import numpy as np
>
> # Gridworld size
> n = 4
>
> # Discount factor
> gamma = 1.0
>
> # Reward
> reward = -1
>
> # Threshold for convergence
> theta = 1e-6
>
> # Initialize value function
> V = np.zeros((n, n))
>
> def is_terminal(state):
>     return state == (0, 0) or state == (n - 1, n - 1)
>
> def get_reward(state):
>     return 0 if is_terminal(state) else reward
>
> def step(state, action):
>     i, j = state
>     if action == 0:  # Up
>         next_state = (max(i - 1, 0), j)
>     elif action == 1:  # Down
>         next_state = (min(i + 1, n - 1), j)
>     elif action == 2:  # Left
>         next_state = (i, max(j - 1, 0))
>     elif action == 3:  # Right
>         next_state = (i, min(j + 1, n - 1))
>     else:
>         raise ValueError("Invalid action")
>
>     return next_state, get_reward(next_state)
>
> # Iterative policy evaluation
> def policy_evaluation(V, gamma, reward, theta):
>     delta = float('inf')
>     while delta > theta:
>         delta = 0
>         for i in range(n):
>             for j in range(n):
>                 if is_terminal((i, j)):
>                     continue
>                 v = V[i, j]
>                 new_v = 0
>                 for action in range(4):
>                     next_state, r = step((i, j), action)
>                     new_v += 0.25 * (r + gamma * V[next_state])
>                 V[i, j] = new_v
>                 delta = max(delta, abs(v - new_v))
>     return V
>
> V = policy_evaluation(V, gamma, reward, theta)
> print("Final Value Function:\n", V)
> ```

Para visualizar a convergência, podemos definir uma medida de erro, como o erro quadrático médio (MSE) entre $v_k$ e $v_\pi$:
$$MSE(k) = \frac{1}{|\mathcal{S}|} \sum_{s \in \mathcal{S}} (v_k(s) - v_\pi(s))^2$$
Acompanhar a evolução do MSE ao longo das iterações fornece uma indicação clara da taxa de convergência do algoritmo.

> 💡 **Exemplo Numérico:** Após cada iteração $k$, calcule o MSE. Plote o MSE em função de $k$ para visualizar a convergência. Se o MSE diminui rapidamente no início e depois se estabiliza, isso indica uma convergência rápida.
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Gridworld size
> n = 4
>
> # Discount factor
> gamma = 1.0
>
> # Reward
> reward = -1
>
> # Threshold for convergence
> theta = 1e-6
>
> # Initialize value function
> V = np.zeros((n, n))
>
> def is_terminal(state):
>     return state == (0, 0) or state == (n - 1, n - 1)
>
> def get_reward(state):
>     return 0 if is_terminal(state) else reward
>
> def step(state, action):
>     i, j = state
>     if action == 0:  # Up
>         next_state = (max(i - 1, 0), j)
>     elif action == 1:  # Down
>         next_state = (min(i + 1, n - 1), j)
>     elif action == 2:  # Left
>         next_state = (i, max(j - 1, 0))
>     elif action == 3:  # Right
>         next_state = (i, min(j + 1, n - 1))
>     else:
>         raise ValueError("Invalid action")
>
>     return next_state, get_reward(next_state)
>
> # Iterative policy evaluation with MSE tracking
> def policy_evaluation_with_mse(V, gamma, reward, theta, true_V):
>     mse_values = []
>     delta = float('inf')
>     k = 0
>     while delta > theta:
>         delta = 0
>         old_V = np.copy(V) # Para calcular o MSE corretamente
>         for i in range(n):
>             for j in range(n):
>                 if is_terminal((i, j)):
>                     continue
>                 v = V[i, j]
>                 new_v = 0
>                 for action in range(4):
>                     next_state, r = step((i, j), action)
>                     new_v += 0.25 * (r + gamma * V[next_state])
>                 V[i, j] = new_v
>                 delta = max(delta, abs(v - new_v))
>         mse = np.mean((V - true_V)**2) # Calcula o MSE usando a função de valor verdadeira
>         mse_values.append(mse)
>         k += 1
>     return V, mse_values
>
> # Função de valor "verdadeira" (aproximada) - obtida após muitas iterações
> true_V = np.array([
>     [0.0, -14.0, -20.0, -22.0],
>     [-14.0, -18.0, -20.0, -20.0],
>     [-20.0, -20.0, -18.0, -14.0],
>     [-22.0, -20.0, -14.0, 0.0]
> ])
>
> # Executa a policy evaluation e obtém os valores de MSE
> V = np.zeros((n, n))
> V_final, mse_values = policy_evaluation_with_mse(V, gamma, reward, theta, true_V)
>
> # Cria o gráfico do MSE
> plt.figure(figsize=(10, 6))
> plt.plot(mse_values)
> plt.title("MSE vs. Iteration")
> plt.xlabel("Iteration")
> plt.ylabel("Mean Squared Error (MSE)")
> plt.grid(True)
> plt.show()
> ```

![Convergence of iterative policy evaluation on a gridworld, showing improvement from random to optimal policy.](./../images/image7.png)

### Conclusão

A **iterative policy evaluation** é um método fundamental em DP para aproximar a função de valor de uma política dada. A utilização da equação de Bellman como uma regra de atualização iterativa garante a convergência para a função de valor verdadeira, sob certas condições. A compreensão deste método é crucial para a construção de algoritmos mais avançados de reinforcement learning, como **policy iteration** e **value iteration** [^1]. Ademais, a escolha da versão *in-place* ou *out-of-place*, juntamente com a ordem de atualização dos estados, pode impactar significativamente a eficiência do algoritmo.

### Referências

[^1]: Chapter 4: Dynamic Programming.
[^2]: Section 4.1: Policy Evaluation (Prediction).
[^3]: Section 4.1: Iterative Policy Evaluation (In-Place).
[^4]: Example 4.1: Gridworld.
<!-- END -->