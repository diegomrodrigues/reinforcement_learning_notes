## Exist√™ncia e Unicidade da Fun√ß√£o Valor sob uma Pol√≠tica

### Introdu√ß√£o
No cap√≠tulo anterior, foi introduzido o conceito de **fun√ß√£o valor** (*value function*) $v_\pi(s)$ para avaliar a qualidade de um estado $s$ seguindo uma pol√≠tica $\pi$ [^1]. Agora, vamos nos aprofundar nas condi√ß√µes que garantem a **exist√™ncia e unicidade** dessa fun√ß√£o valor, um aspecto fundamental para a aplica√ß√£o de algoritmos de *dynamic programming* (DP). O t√≥pico abordado aqui se baseia diretamente no conceito de **Policy Evaluation (Prediction)** apresentado na se√ß√£o 4.1 [^2].

### Condi√ß√µes de Exist√™ncia e Unicidade
Conforme mencionado na se√ß√£o 4.1, a exist√™ncia e unicidade de $v_\pi$ s√£o garantidas sob duas condi√ß√µes principais [^2]:

1.  O fator de desconto $\gamma$ √© menor que 1 ($\gamma < 1$).
2.  Ocorre termina√ß√£o eventual garantida a partir de todos os estados sob a pol√≠tica $\pi$.

Vamos analisar cada uma dessas condi√ß√µes com mais detalhes.

#### Fator de Desconto $\gamma < 1$
O fator de desconto $\gamma$ √© um valor entre 0 e 1 que determina a import√¢ncia das recompensas futuras em rela√ß√£o √†s recompensas imediatas. Quando $\gamma$ √© menor que 1, as recompensas recebidas no futuro t√™m um peso menor do que as recompensas recebidas no presente. Isso impede que a soma das recompensas futuras se torne infinita, garantindo que a fun√ß√£o valor $v_\pi(s)$ seja finita e bem definida.

Para ilustrar, considere uma sequ√™ncia infinita de recompensas constantes $R$ a partir do estado $s$. A fun√ß√£o valor seria:

$v_\pi(s) = R + \gamma R + \gamma^2 R + \gamma^3 R + ... = R \sum_{k=0}^{\infty} \gamma^k$

Se $\gamma < 1$, a soma converge para $\frac{1}{1-\gamma}$, resultando em $v_\pi(s) = \frac{R}{1-\gamma}$, que √© finita. Se $\gamma = 1$, a soma diverge para o infinito, tornando a fun√ß√£o valor indefinida.

> üí° **Exemplo Num√©rico:**
>
> Suponha que um agente receba uma recompensa constante de $R = 10$ em todos os passos no estado $s$. Vamos calcular a fun√ß√£o valor para diferentes valores de $\gamma$:
>
> *   Se $\gamma = 0.9$, ent√£o $v_\pi(s) = \frac{10}{1 - 0.9} = \frac{10}{0.1} = 100$.
> *   Se $\gamma = 0.5$, ent√£o $v_\pi(s) = \frac{10}{1 - 0.5} = \frac{10}{0.5} = 20$.
> *   Se $\gamma = 0.99$, ent√£o $v_\pi(s) = \frac{10}{1 - 0.99} = \frac{10}{0.01} = 1000$.
>
> Observe que, √† medida que $\gamma$ se aproxima de 1, a fun√ß√£o valor aumenta significativamente, refletindo a maior import√¢ncia dada √†s recompensas futuras. Quando $\gamma = 1$, a soma se torna infinita, e a fun√ß√£o valor √© indefinida.

#### Termina√ß√£o Eventual Garantida
A segunda condi√ß√£o para a exist√™ncia e unicidade da fun√ß√£o valor √© que a termina√ß√£o eventual seja garantida a partir de todos os estados sob a pol√≠tica $\pi$. Isso significa que, seguindo a pol√≠tica $\pi$, o agente eventualmente alcan√ßar√° um estado terminal com probabilidade 1. Em outras palavras, n√£o pode haver ciclos infinitos de estados n√£o terminais.

Para formalizar a condi√ß√£o de termina√ß√£o eventual, podemos definir $\tau_s$ como o tempo esperado at√© a termina√ß√£o a partir do estado $s$ sob a pol√≠tica $\pi$. Formalmente, $\tau_s = \mathbb{E}_\pi[T | S_0 = s]$, onde $T$ √© o tempo de termina√ß√£o. A condi√ß√£o de termina√ß√£o eventual garantida pode ent√£o ser expressa como $\tau_s < \infty$ para todo estado $s$.

**Proposi√ß√£o 1** Se existe um $\epsilon > 0$ tal que a probabilidade de alcan√ßar um estado terminal em no m√°ximo $k$ passos √© pelo menos $\epsilon$ para algum inteiro positivo $k$ e para todo estado $s$ sob a pol√≠tica $\pi$, ent√£o a termina√ß√£o eventual √© garantida.

*Prova.* Seja $p_s$ a probabilidade de n√£o alcan√ßar um estado terminal em $k$ passos a partir de $s$. Ent√£o, $p_s \leq 1 - \epsilon$ para todo $s$. A probabilidade de n√£o terminar em $nk$ passos √© ent√£o limitada por $(1-\epsilon)^n$, que converge para 0 quando $n \rightarrow \infty$. Portanto, a termina√ß√£o eventual √© garantida com probabilidade 1.

Prova detalhada:

I.  Definimos $p_s$ como a probabilidade de *n√£o* alcan√ßar um estado terminal em $k$ passos a partir do estado $s$. Pela condi√ß√£o dada, a probabilidade de alcan√ßar um estado terminal em $k$ passos √© pelo menos $\epsilon$. Portanto, a probabilidade de *n√£o* alcan√ßar um estado terminal em $k$ passos √© limitada superiormente por $1 - \epsilon$:
    $$
    p_s \leq 1 - \epsilon, \quad \forall s
    $$

II. Agora, vamos considerar a probabilidade de n√£o terminar em $n \cdot k$ passos. Isso significa n√£o terminar nos primeiros $k$ passos, nem nos segundos $k$ passos, e assim por diante, at√© os $n$-√©simos $k$ passos. Como cada bloco de $k$ passos √© independente e limitado por $1 - \epsilon$, a probabilidade de n√£o terminar em $n \cdot k$ passos √© dada por:
    $$
    P(\text{n√£o terminar em } nk \text{ passos}) \leq (1 - \epsilon)^n
    $$

III. Analisamos o comportamento de $(1 - \epsilon)^n$ quando $n$ tende ao infinito. Como $0 < \epsilon \leq 1$, temos que $0 \leq 1 - \epsilon < 1$. Portanto, quando $n$ tende ao infinito:
    $$
    \lim_{n \to \infty} (1 - \epsilon)^n = 0
    $$

IV. Isso significa que a probabilidade de n√£o terminar em $n \cdot k$ passos converge para 0 quando $n$ tende ao infinito. Consequentemente, a probabilidade de terminar em algum momento (ou seja, a termina√ß√£o eventual) converge para 1:
    $$
    P(\text{termina√ß√£o eventual}) = 1 - \lim_{n \to \infty} (1 - \epsilon)^n = 1 - 0 = 1
    $$

V. Portanto, demonstramos que a termina√ß√£o eventual √© garantida com probabilidade 1, sob a condi√ß√£o de que existe um $\epsilon > 0$ tal que a probabilidade de alcan√ßar um estado terminal em no m√°ximo $k$ passos √© pelo menos $\epsilon$ para algum inteiro positivo $k$ e para todo estado $s$ sob a pol√≠tica $\pi$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um ambiente onde o agente pode se mover para a direita ou para a esquerda. H√° um estado terminal √† direita. Suponha que a pol√≠tica $\pi$ tenha uma probabilidade de 0.8 de se mover para a direita e 0.2 de se mover para a esquerda.
>
> Seja $k = 5$. Se $\epsilon = 0.1$, isso significa que em cada estado, h√° uma probabilidade de pelo menos 0.1 de alcan√ßar o estado terminal em no m√°ximo 5 passos. Neste caso, a termina√ß√£o eventual √© garantida porque mesmo que o agente se mova para a esquerda algumas vezes, a probabilidade de se mover para a direita √© alta o suficiente para garantir que ele eventualmente alcance o estado terminal.
>
> Se a pol√≠tica fosse sempre mover para a esquerda, ent√£o n√£o haveria termina√ß√£o eventual garantida, e a fun√ß√£o valor n√£o existiria.

### Formula√ß√£o Recursiva da Fun√ß√£o Valor
A fun√ß√£o valor $v_\pi(s)$ pode ser definida recursivamente como [^2]:

$$
v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_\pi(s')]
$$

Onde:

*   $s$ √© o estado atual.
*   $R_{t+1}$ √© a recompensa recebida ap√≥s a transi√ß√£o do estado $s$.
*   $S_{t+1}$ √© o pr√≥ximo estado.
*   $\gamma$ √© o fator de desconto.
*   $\pi(a|s)$ √© a probabilidade de selecionar a a√ß√£o $a$ no estado $s$ sob a pol√≠tica $\pi$.
*   $p(s', r|s, a)$ √© a probabilidade de transi√ß√£o para o estado $s'$ com recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$.

Essa equa√ß√£o expressa a fun√ß√£o valor de um estado como a soma ponderada das recompensas imediatas e das fun√ß√µes valor dos estados sucessores, descontadas pelo fator $\gamma$. Esta representa√ß√£o √© crucial para a compreens√£o e implementa√ß√£o dos m√©todos de *dynamic programming*.

> üí° **Exemplo Num√©rico:**
>
> Considere um ambiente com dois estados, $s_1$ e $s_2$, e uma √∫nica a√ß√£o dispon√≠vel em cada estado. Suponha que a pol√≠tica $\pi$ seja determin√≠stica e sempre escolha essa a√ß√£o. As probabilidades de transi√ß√£o e recompensas s√£o as seguintes:
>
> *   Em $s_1$, a a√ß√£o leva a $s_2$ com probabilidade 1 e recompensa 5.
> *   Em $s_2$, a a√ß√£o leva a $s_1$ com probabilidade 1 e recompensa -1.
>
> Seja $\gamma = 0.9$. Podemos escrever as equa√ß√µes de Bellman para $v_\pi(s_1)$ e $v_\pi(s_2)$:
>
> $v_\pi(s_1) = 5 + 0.9 \cdot v_\pi(s_2)$
> $v_\pi(s_2) = -1 + 0.9 \cdot v_\pi(s_1)$
>
> Substituindo a segunda equa√ß√£o na primeira:
>
> $v_\pi(s_1) = 5 + 0.9 \cdot (-1 + 0.9 \cdot v_\pi(s_1))$
> $v_\pi(s_1) = 5 - 0.9 + 0.81 \cdot v_\pi(s_1)$
> $0.19 \cdot v_\pi(s_1) = 4.1$
> $v_\pi(s_1) = \frac{4.1}{0.19} \approx 21.58$
>
> Substituindo o valor de $v_\pi(s_1)$ na segunda equa√ß√£o:
>
> $v_\pi(s_2) = -1 + 0.9 \cdot 21.58$
> $v_\pi(s_2) = -1 + 19.42 \approx 18.42$
>
> Portanto, $v_\pi(s_1) \approx 21.58$ e $v_\pi(s_2) \approx 18.42$.

A partir da formula√ß√£o recursiva da fun√ß√£o valor, podemos express√°-la na forma de operador de Bellman para policy evaluation, denotado por $T^\pi$:

$$(T^\pi v)(s) = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')]]$$

A fun√ß√£o valor $v_\pi$ √© ent√£o um ponto fixo do operador $T^\pi$, ou seja, $v_\pi = T^\pi v_\pi$.

**Teorema 1** O operador de Bellman $T^\pi$ √© uma contra√ß√£o de Banach com fator de contra√ß√£o $\gamma$ sob a norma do supremo ||.|| definida como $||v|| = \max_s |v(s)|$.

*Prova.* Para mostrar que $T^\pi$ √© uma contra√ß√£o, precisamos demonstrar que existe um $\gamma \in [0, 1)$ tal que $||T^\pi u - T^\pi v|| \leq \gamma ||u - v||$ para todas as fun√ß√µes valor $u$ e $v$.

$||T^\pi u - T^\pi v|| = \max_s |(T^\pi u)(s) - (T^\pi v)(s)|$
$= \max_s |\sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [\gamma u(s')] - \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [\gamma v(s')]|$
$= \max_s |\sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \gamma [u(s') - v(s')]|$
$\leq \max_s \gamma \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) |u(s') - v(s')|$
$\leq \gamma \max_s |u(s') - v(s')| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a)$
$= \gamma ||u - v|| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a)$

Como $\sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) = 1$, temos:
$||T^\pi u - T^\pi v|| \leq \gamma ||u - v||$.

Portanto, $T^\pi$ √© uma contra√ß√£o de Banach com fator de contra√ß√£o $\gamma$. Pelo Teorema do Ponto Fixo de Banach, $T^\pi$ possui um √∫nico ponto fixo, que √© a fun√ß√£o valor $v_\pi$.

Prova detalhada:

I.  Definimos a norma do supremo (ou norma do m√°ximo) de uma fun√ß√£o valor $v$ como:
    $$
    ||v|| = \max_s |v(s)|
    $$
    Essa norma representa o maior valor absoluto que a fun√ß√£o valor $v$ assume em qualquer estado $s$.

II. Queremos mostrar que o operador de Bellman $T^\pi$ √© uma contra√ß√£o de Banach com fator de contra√ß√£o $\gamma$. Isso significa que precisamos provar que existe um $\gamma \in [0, 1)$ tal que:
    $$
    ||T^\pi u - T^\pi v|| \leq \gamma ||u - v||
    $$
    para todas as fun√ß√µes valor $u$ e $v$.

III. Come√ßamos expandindo a express√£o $||T^\pi u - T^\pi v||$:
    $$
    ||T^\pi u - T^\pi v|| = \max_s |(T^\pi u)(s) - (T^\pi v)(s)|
    $$

IV. Substitu√≠mos a defini√ß√£o do operador de Bellman $T^\pi$:
    $$
    \max_s \left| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma u(s')] - \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')] \right|
    $$

V. Simplificamos a express√£o, notando que o termo $r$ se cancela:
    $$
    \max_s \left| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \gamma [u(s') - v(s')] \right|
    $$

VI. Usamos a desigualdade triangular para mover o valor absoluto para dentro das somas:
    $$
    \max_s \gamma \left| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [u(s') - v(s')] \right| \le \max_s \gamma \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) |u(s') - v(s')|
    $$

VII. Observamos que $|u(s') - v(s')|$ est√° limitado por $||u - v|| = \max_s |u(s) - v(s)|$:
    $$
    \max_s \gamma \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) |u(s') - v(s')| \leq \gamma ||u - v|| \max_s  \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a)
    $$

VIII. Notamos que $\sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) = 1$, pois representa a soma das probabilidades de todas as a√ß√µes poss√≠veis no estado $s$ e todas as transi√ß√µes poss√≠veis a partir dessas a√ß√µes:
    $$
     \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) = 1
    $$

IX. Portanto, a express√£o se simplifica para:
    $$
    ||T^\pi u - T^\pi v|| \leq \gamma ||u - v||
    $$

X.  Conclu√≠mos que $T^\pi$ √© uma contra√ß√£o de Banach com fator de contra√ß√£o $\gamma$. Pelo Teorema do Ponto Fixo de Banach, $T^\pi$ possui um √∫nico ponto fixo, que √© a fun√ß√£o valor $v_\pi$. ‚ñ†

### Implica√ß√µes Pr√°ticas

A garantia da exist√™ncia e unicidade da fun√ß√£o valor √© essencial para a aplica√ß√£o de algoritmos de DP. Se a fun√ß√£o valor n√£o existir ou n√£o for √∫nica, os algoritmos de DP podem n√£o convergir para uma solu√ß√£o √≥tima ou podem convergir para uma solu√ß√£o incorreta. Portanto, √© importante verificar se as condi√ß√µes de exist√™ncia e unicidade s√£o satisfeitas antes de aplicar algoritmos de DP a um problema espec√≠fico.

Adicionalmente, o fato de $T^\pi$ ser uma contra√ß√£o de Banach garante a converg√™ncia do algoritmo de *Iterative Policy Evaluation*, que calcula $v_\pi$ iterativamente aplicando $T^\pi$ repetidamente a uma fun√ß√£o valor inicial arbitr√°ria.



![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

Para ilustrar a converg√™ncia, considere o algoritmo de *Iterative Policy Evaluation*:
$v_{k+1} = T^\pi v_k$
onde $v_0$ √© uma fun√ß√£o valor inicial arbitr√°ria.

A dist√¢ncia entre $v_{k+1}$ e $v_\pi$ (o ponto fixo) √©:
$||v_{k+1} - v_\pi|| = ||T^\pi v_k - T^\pi v_\pi|| \leq \gamma ||v_k - v_\pi||$

Aplicando a desigualdade repetidamente:
$||v_{k+1} - v_\pi|| \leq \gamma^k ||v_0 - v_\pi||$

Como $\gamma < 1$, $\gamma^k$ converge para 0 quando $k$ tende ao infinito, garantindo que $v_{k+1}$ converge para $v_\pi$.

> üí° **Exemplo Num√©rico:**
>
> Vamos simular algumas itera√ß√µes do *Iterative Policy Evaluation*. Considere um ambiente simples com dois estados ($s_1$ e $s_2$) e uma pol√≠tica $\pi$ onde a a√ß√£o leva de $s_1$ para $s_2$ e de $s_2$ para $s_1$ com probabilidade 1. As recompensas s√£o $R(s_1) = 1$ e $R(s_2) = -1$, e $\gamma = 0.9$.
>
> Inicializamos $v_0(s_1) = 0$ e $v_0(s_2) = 0$.
>
> *   **Itera√ß√£o 1:**
>     *   $v_1(s_1) = 1 + 0.9 \cdot v_0(s_2) = 1 + 0.9 \cdot 0 = 1$
>     *   $v_1(s_2) = -1 + 0.9 \cdot v_0(s_1) = -1 + 0.9 \cdot 0 = -1$
> *   **Itera√ß√£o 2:**
>     *   $v_2(s_1) = 1 + 0.9 \cdot v_1(s_2) = 1 + 0.9 \cdot (-1) = 0.1$
>     *   $v_2(s_2) = -1 + 0.9 \cdot v_1(s_1) = -1 + 0.9 \cdot 1 = -0.1$
> *   **Itera√ß√£o 3:**
>     *   $v_3(s_1) = 1 + 0.9 \cdot v_2(s_2) = 1 + 0.9 \cdot (-0.1) = 0.91$
>     *   $v_3(s_2) = -1 + 0.9 \cdot v_2(s_1) = -1 + 0.9 \cdot 0.1 = -0.91$
>
> Podemos observar que as fun√ß√µes valor est√£o convergindo. Ap√≥s muitas itera√ß√µes, elas se aproximar√£o dos valores calculados no exemplo anterior ($v_\pi(s_1) \approx 21.58$ e $v_\pi(s_2) \approx 18.42$), embora neste exemplo simplificado os valores absolutos sejam menores devido √†s menores recompensas. A converg√™ncia √© garantida pelo fato de que $\gamma < 1$.

### Conclus√£o

A exist√™ncia e unicidade da fun√ß√£o valor $v_\pi(s)$ s√£o garantidas se o fator de desconto $\gamma$ for menor que 1 ou se a termina√ß√£o eventual for garantida a partir de todos os estados sob a pol√≠tica $\pi$ [^2]. A compreens√£o dessas condi√ß√µes √© crucial para a aplica√ß√£o correta e eficaz dos algoritmos de *dynamic programming*. A formula√ß√£o recursiva da fun√ß√£o valor fornece a base para o desenvolvimento de algoritmos iterativos que podem ser usados para calcular a fun√ß√£o valor para uma determinada pol√≠tica. O operador de Bellman, $T^\pi$, √© uma contra√ß√£o de Banach, o que garante a converg√™ncia do algoritmo de *Iterative Policy Evaluation*.

### Refer√™ncias
[^1]: Cap√≠tulo 3
[^2]: Cap√≠tulo 4, Se√ß√£o 4.1
<!-- END -->