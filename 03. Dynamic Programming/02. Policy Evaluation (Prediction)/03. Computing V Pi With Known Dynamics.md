## C√°lculo Iterativo da Fun√ß√£o de Valor de Estado

### Introdu√ß√£o
Este cap√≠tulo explora os algoritmos de **Programa√ß√£o Din√¢mica (DP)** para calcular pol√≠ticas √≥timas em ambientes modelados como **Processos de Decis√£o de Markov (MDPs)**. Conforme mencionado anteriormente [^1], a DP fornece uma base essencial para compreender outros m√©todos de *Reinforcement Learning*. Iremos nos concentrar na avalia√ß√£o da pol√≠tica, que √© o processo de calcular a fun√ß√£o de valor de estado $v_\pi$ para uma pol√≠tica arbitr√°ria $\pi$ [^74]. Este processo √© crucial para aprimorar pol√≠ticas iterativamente.

### Conceitos Fundamentais

Como vimos anteriormente [^74], a **avalia√ß√£o de pol√≠tica** (tamb√©m conhecida como *problema de predi√ß√£o*) busca determinar a fun√ß√£o de valor $v_\pi(s)$ para todos os estados $s \in \mathcal{S}$, dada uma pol√≠tica $\pi$. A fun√ß√£o de valor √© definida como o retorno esperado ao seguir a pol√≠tica $\pi$ a partir do estado $s$ [^74]:

$$ v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] $$.

A equa√ß√£o de Bellman para $v_\pi(s)$ pode ser expressa iterativamente como [^74]:

$$v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_\pi(s')]$$

onde $\pi(a|s)$ √© a probabilidade de tomar a a√ß√£o $a$ no estado $s$ sob a pol√≠tica $\pi$, e $p(s', r|s, a)$ √© a probabilidade de transi√ß√£o para o estado $s'$ com recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$ [^74].

> üí° **Exemplo Num√©rico:** Considere um ambiente simples com dois estados, $S = \{s_1, s_2\}$, e duas a√ß√µes, $A = \{a_1, a_2\}$. Suponha que a pol√≠tica $\pi$ sempre escolha a a√ß√£o $a_1$ em ambos os estados, ou seja, $\pi(a_1|s_1) = 1$ e $\pi(a_1|s_2) = 1$. As probabilidades de transi√ß√£o e recompensas s√£o as seguintes:
> *   $p(s_1, 0|s_1, a_1) = 0.8$
> *   $p(s_2, 1|s_1, a_1) = 0.2$
> *   $p(s_1, -1|s_2, a_1) = 0.5$
> *   $p(s_2, 2|s_2, a_1) = 0.5$
>
> Seja $\gamma = 0.9$. A equa√ß√£o de Bellman para $v_\pi(s_1)$ se torna:
>
> $v_\pi(s_1) = \pi(a_1|s_1) \sum_{s',r} p(s', r|s_1, a_1) [r + \gamma v_\pi(s')] = 1 \cdot [0.8 \cdot (0 + 0.9 v_\pi(s_1)) + 0.2 \cdot (1 + 0.9 v_\pi(s_2))]$
>
> Similarmente, para $v_\pi(s_2)$:
>
> $v_\pi(s_2) = \pi(a_1|s_2) \sum_{s',r} p(s', r|s_2, a_1) [r + \gamma v_\pi(s')] = 1 \cdot [0.5 \cdot (-1 + 0.9 v_\pi(s_1)) + 0.5 \cdot (2 + 0.9 v_\pi(s_2))]$
>
> Estas s√£o duas equa√ß√µes lineares com duas inc√≥gnitas, $v_\pi(s_1)$ e $v_\pi(s_2)$, que podem ser resolvidas para encontrar a fun√ß√£o de valor $v_\pi$.

Quando a din√¢mica do ambiente, definida por $p(s', r|s, a)$, √© completamente conhecida, o c√°lculo direto de $v_\pi$ pode ser visto como a solu√ß√£o de um sistema de $|\mathcal{S}|$ equa√ß√µes lineares simult√¢neas com $|\mathcal{S}|$ inc√≥gnitas (os valores $v_\pi(s)$ para cada $s \in \mathcal{S}$) [^74]. Embora, em princ√≠pio, isso possa ser resolvido diretamente, a computa√ß√£o pode se tornar *tediosa* e *impratic√°vel* para espa√ßos de estados grandes [^74].

Em vez de resolver o sistema linear diretamente, a **avalia√ß√£o iterativa da pol√≠tica** oferece uma abordagem pr√°tica [^74]. Este m√©todo envolve gerar uma sequ√™ncia de fun√ß√µes de valor aproximadas $v_0, v_1, v_2, \ldots$, onde cada $v_k$ mapeia estados para valores reais [^74]. A aproxima√ß√£o inicial $v_0$ √© escolhida arbitrariamente (exceto que o estado terminal, se houver, deve ser dado o valor 0). Cada aproxima√ß√£o sucessiva √© obtida usando a equa√ß√£o de Bellman para $v_\pi$ como uma regra de atualiza√ß√£o [^74]:

$$ v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_k(s')] $$

para todos os estados $s \in \mathcal{S}$ [^74].

> üí° **Exemplo Num√©rico:** Continuando com o exemplo anterior, vamos realizar algumas itera√ß√µes da avalia√ß√£o iterativa da pol√≠tica. Inicializamos $v_0(s_1) = 0$ e $v_0(s_2) = 0$. Usando a equa√ß√£o de atualiza√ß√£o:
>
> *Itera√ß√£o 1:*
>
> $v_1(s_1) = 0.8 \cdot (0 + 0.9 \cdot 0) + 0.2 \cdot (1 + 0.9 \cdot 0) = 0.2$
>
> $v_1(s_2) = 0.5 \cdot (-1 + 0.9 \cdot 0) + 0.5 \cdot (2 + 0.9 \cdot 0) = 0.5$
>
> *Itera√ß√£o 2:*
>
> $v_2(s_1) = 0.8 \cdot (0 + 0.9 \cdot 0.2) + 0.2 \cdot (1 + 0.9 \cdot 0.5) = 0.0.144 + 0.29 = 0.434$
>
> $v_2(s_2) = 0.5 \cdot (-1 + 0.9 \cdot 0.2) + 0.5 \cdot (2 + 0.9 \cdot 0.5) = -0.41 + 1.225 = 0.815$
>
> *Itera√ß√£o 3:*
>
> $v_3(s_1) = 0.8 \cdot (0 + 0.9 \cdot 0.434) + 0.2 \cdot (1 + 0.9 \cdot 0.815) = 0.312 + 0.347 = 0.659$
>
> $v_3(s_2) = 0.5 \cdot (-1 + 0.9 \cdot 0.434) + 0.5 \cdot (2 + 0.9 \cdot 0.815) = -0.230 + 1.367 = 1.137$
>
> Este processo continua iterativamente at√© que a mudan√ßa nos valores de estado entre itera√ß√µes consecutivas seja menor que um limiar predefinido.

Este processo iterativo √© um exemplo de uma **atualiza√ß√£o esperada**, pois se baseia em uma expectativa sobre todos os estados sucessores poss√≠veis [^75]. A sequ√™ncia $\{v_k\}$ pode ser mostrada para convergir para $v_\pi$ quando $k \to \infty$, sob as mesmas condi√ß√µes que garantem a exist√™ncia de $v_\pi$ [^74]. Estas condi√ß√µes s√£o $\gamma < 1$ ou garantia de termina√ß√£o a partir de todos os estados sob a pol√≠tica $\pi$ [^74].

Para complementar a garantia de converg√™ncia, podemos apresentar o seguinte resultado sobre a natureza da converg√™ncia:

**Lema 1** A sequ√™ncia de fun√ß√µes de valor $\{v_k\}$ converge monotonicamente para $v_\pi$ se a recompensa $r$ for n√£o negativa.

*Prova.* (Esbo√ßo) A prova pode ser feita por indu√ß√£o. Assumimos que $v_k(s) \le v_\pi(s)$ para todo $s$. Ent√£o,
$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] \le \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] = v_\pi(s)$.
Como $v_0$ √© inicializado arbitrariamente, mas as recompensas s√£o n√£o negativas, $v_1(s) \ge v_0(s)$. Portanto, a sequ√™ncia √© monotonicamente crescente e limitada superiormente por $v_\pi(s)$, garantindo a converg√™ncia.

Para formalizar a prova do Lema 1, podemos expandi-la da seguinte forma:

*Prova.*
I. **Caso Base:** Inicialmente, seja $v_0(s)$ a fun√ß√£o de valor inicial para todos os estados $s \in \mathcal{S}$. Como $v_0(s)$ pode ser escolhido arbitrariamente e as recompensas s√£o n√£o negativas, $v_1(s)$ ser√° maior ou igual a $v_0(s)$ para todo $s \in \mathcal{S}$. Isso ocorre porque $v_1(s)$ √© uma combina√ß√£o das recompensas esperadas e valores descontados, que ser√£o maiores ou iguais √† fun√ß√£o de valor inicial assumindo recompensas n√£o negativas.
II. **Hip√≥tese Indutiva:** Assuma que para alguma itera√ß√£o $k \ge 0$, $v_k(s) \leq v_{k+1}(s) \leq v_\pi(s)$ para todo $s \in \mathcal{S}$. Isso significa que a fun√ß√£o de valor na itera√ß√£o $k$ √© menor ou igual √† fun√ß√£o de valor na itera√ß√£o $k+1$, que por sua vez √© menor ou igual √† fun√ß√£o de valor √≥tima $v_\pi(s)$.
III. **Passo Indutivo:** Precisamos mostrar que $v_{k+1}(s) \leq v_{k+2}(s) \leq v_\pi(s)$ para todo $s \in \mathcal{S}$. Usando a equa√ß√£o de Bellman, temos:

$$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s]$$
$$v_{k+2}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_{k+1}(S_{t+1}) | S_t = s]$$

Como assumimos que $v_k(s) \leq v_{k+1}(s)$ para todo $s \in \mathcal{S}$ (pela hip√≥tese indutiva), ent√£o $\gamma v_k(S_{t+1}) \leq \gamma v_{k+1}(S_{t+1})$. Portanto,

$$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] \leq \mathbb{E}_\pi[R_{t+1} + \gamma v_{k+1}(S_{t+1}) | S_t = s] = v_{k+2}(s)$$
Isso mostra que $v_{k+1}(s) \leq v_{k+2}(s)$ para todo $s \in \mathcal{S}$.

IV. Al√©m disso, precisamos mostrar que $v_{k+2}(s) \leq v_\pi(s)$. Novamente usando a equa√ß√£o de Bellman:

$$v_{k+2}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_{k+1}(S_{t+1}) | S_t = s]$$

Como assumimos que $v_{k+1}(s) \leq v_\pi(s)$ para todo $s \in \mathcal{S}$ (pela hip√≥tese indutiva), ent√£o $\gamma v_{k+1}(S_{t+1}) \leq \gamma v_\pi(S_{t+1})$. Portanto,
$$v_{k+2}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_{k+1}(S_{t+1}) | S_t = s] \leq \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] = v_\pi(s)$$
Isso mostra que $v_{k+2}(s) \leq v_\pi(s)$ para todo $s \in \mathcal{S}$.

V. **Conclus√£o:** Por indu√ß√£o, mostramos que a sequ√™ncia $\{v_k(s)\}$ √© monotonicamente crescente e limitada superiormente por $v_\pi(s)$ para todo $s \in \mathcal{S}$. Portanto, a sequ√™ncia converge monotonicamente para $v_\pi(s)$. ‚ñ†

Al√©m disso, podemos derivar um limite superior para o erro em cada itera√ß√£o:

**Teorema 1** Seja $v_\pi$ a fun√ß√£o de valor verdadeira e $v_k$ a fun√ß√£o de valor na $k$-√©sima itera√ß√£o. Ent√£o,

$$||v_k - v_\pi||_\infty \le \frac{\gamma^k}{1 - \gamma} \max_s |v_1(s) - v_0(s)|$$

*Prova.* (Esbo√ßo)
A prova usa a desigualdade de Bellman e indu√ß√£o matem√°tica. A norma infinito √© definida como $||v||_\infty = \max_s |v(s)|$.

Para fornecer uma prova mais completa do Teorema 1:

*Prova.*
I. **Defini√ß√£o da Norma Infinito:** A norma infinito de uma fun√ß√£o de valor $v$ √© definida como $||v||_\infty = \max_s |v(s)|$, onde o m√°ximo √© tomado sobre todos os estados $s \in \mathcal{S}$. Esta norma mede a maior diferen√ßa absoluta entre os valores de estado.

II. **Desigualdade de Bellman:** Para qualquer fun√ß√£o de valor $v$, definimos o operador de Bellman $T_\pi$ como:
$$(T_\pi v)(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')] $$
A fun√ß√£o de valor verdadeira $v_\pi$ √© um ponto fixo do operador de Bellman, ou seja, $v_\pi = T_\pi v_\pi$.

III. **Passo Base:** Considere a diferen√ßa entre $v_1$ e $v_\pi$:
$$||v_1 - v_\pi||_\infty = ||T_\pi v_0 - T_\pi v_\pi||_\infty = \max_s |(T_\pi v_0)(s) - (T_\pi v_\pi)(s)|$$
Usando a defini√ß√£o do operador de Bellman:
$$||v_1 - v_\pi||_\infty = \max_s \left| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_0(s')] - \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_\pi(s')] \right|$$
$$||v_1 - v_\pi||_\infty = \max_s \left| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \gamma [v_0(s') - v_\pi(s')] \right|$$
$$||v_1 - v_\pi||_\infty \le \gamma \max_s |v_0(s) - v_\pi(s)| = \gamma ||v_0 - v_\pi||_\infty$$

IV. **Passo Indutivo:** Assuma que $||v_k - v_\pi||_\infty \le \gamma^k ||v_0 - v_\pi||_\infty$. Precisamos mostrar que $||v_{k+1} - v_\pi||_\infty \le \gamma^{k+1} ||v_0 - v_\pi||_\infty$.

Usando a mesma l√≥gica do passo base:
$$||v_{k+1} - v_\pi||_\infty = ||T_\pi v_k - T_\pi v_\pi||_\infty \le \gamma ||v_k - v_\pi||_\infty$$
Pela hip√≥tese indutiva:
$$||v_{k+1} - v_\pi||_\infty \le \gamma (\gamma^k ||v_0 - v_\pi||_\infty) = \gamma^{k+1} ||v_0 - v_\pi||_\infty$$

V. **Limite Superior:** Observe que $||v_0 - v_\pi||_\infty \le ||v_0 - v_1||_\infty + ||v_1 - v_\pi||_\infty$. Substituindo $||v_1 - v_\pi||_\infty \le \gamma ||v_0 - v_\pi||_\infty$, temos:

$$||v_0 - v_\pi||_\infty \le ||v_0 - v_1||_\infty + \gamma ||v_0 - v_\pi||_\infty$$
$$||v_0 - v_\pi||_\infty (1 - \gamma) \le ||v_0 - v_1||_\infty$$
$$||v_0 - v_\pi||_\infty \le \frac{1}{1 - \gamma} ||v_1 - v_0||_\infty = \frac{1}{1 - \gamma} \max_s |v_1(s) - v_0(s)|$$
Substituindo isso no resultado do passo indutivo:
$$||v_{k} - v_\pi||_\infty \le \gamma^k ||v_0 - v_\pi||_\infty \le \frac{\gamma^k}{1 - \gamma} ||v_1 - v_0||_\infty$$

VI. **Conclus√£o:** Portanto, $||v_k - v_\pi||_\infty \le \frac{\gamma^k}{1 - \gamma} \max_s |v_1(s) - v_0(s)|$. Este resultado nos d√° um limite superior para o erro entre a fun√ß√£o de valor na $k$-√©sima itera√ß√£o e a fun√ß√£o de valor verdadeira. Conforme $k$ aumenta, o erro diminui exponencialmente, j√° que $\gamma < 1$. ‚ñ†

### Implementa√ß√£o Computacional

A implementa√ß√£o computacional da avalia√ß√£o iterativa da pol√≠tica geralmente envolve o uso de *arrays* para armazenar os valores de estado. Duas abordagens principais s√£o comumente empregadas [^75]:

1.  **Abordagem de dois arrays:** Utiliza dois arrays, um para os valores antigos $v_k(s)$ e outro para os novos valores $v_{k+1}(s)$. Os novos valores s√£o calculados um por um a partir dos valores antigos sem modificar os valores antigos durante uma itera√ß√£o [^75].
2.  **Abordagem "in-place":** Utiliza um √∫nico array e atualiza os valores *in-place*, ou seja, cada novo valor substitui imediatamente o antigo. A converg√™ncia desta abordagem pode ser mais r√°pida, pois utiliza os dados mais recentes assim que est√£o dispon√≠veis, por√©m a ordem em que os estados s√£o atualizados tem impacto significativo na taxa de converg√™ncia [^75].

A atualiza√ß√£o *in-place* pode ser vista como uma **varredura** atrav√©s do espa√ßo de estados [^75].

Para melhor ilustrar o impacto da ordem de varredura na converg√™ncia da abordagem in-place, podemos considerar as seguintes estrat√©gias de varredura:

*   **Varredura Aleat√≥ria:** Os estados s√£o atualizados em uma ordem aleat√≥ria a cada itera√ß√£o.
*   **Varredura Prioritizada:** Os estados s√£o atualizados com base na magnitude da mudan√ßa de valor na itera√ß√£o anterior. Estados com maiores mudan√ßas s√£o priorizados.

**Proposi√ß√£o 1** A varredura prioritizada pode acelerar a converg√™ncia da avalia√ß√£o iterativa da pol√≠tica in-place em compara√ß√£o com a varredura aleat√≥ria.

*Prova.* (Esbo√ßo) A varredura prioritizada concentra a computa√ß√£o nos estados onde as mudan√ßas de valor s√£o mais significativas, propagando as informa√ß√µes de valor de forma mais eficiente. Isso reduz o n√∫mero de itera√ß√µes necess√°rias para atingir a converg√™ncia em compara√ß√£o com a varredura aleat√≥ria, onde todos os estados s√£o tratados igualmente, independentemente da magnitude de suas mudan√ßas de valor.

Para tornar a prova da Proposi√ß√£o 1 mais rigorosa, podemos detalh√°-la da seguinte forma:

*Prova.*
I. **Defini√ß√£o de Converg√™ncia:** Definimos converg√™ncia como o ponto onde a maior mudan√ßa nos valores de estado entre itera√ß√µes consecutivas cai abaixo de um limiar $\theta$, ou seja, $\max_s |v_{k+1}(s) - v_k(s)| < \theta$.

II. **Varredura Aleat√≥ria:** Na varredura aleat√≥ria, os estados s√£o atualizados em uma ordem aleat√≥ria a cada itera√ß√£o. Isso significa que, em uma √∫nica itera√ß√£o, alguns estados podem ser atualizados com informa√ß√µes mais recentes do que outros, levando a uma propaga√ß√£o de informa√ß√µes de valor inconsistente.

III. **Varredura Prioritizada:** Na varredura prioritizada, os estados s√£o atualizados com base na magnitude da mudan√ßa de valor na itera√ß√£o anterior. Formalmente, definimos uma fila de prioridade onde a prioridade de cada estado $s$ √© dada por $|v_k(s) - v_{k-1}(s)|$. Estados com maiores mudan√ßas de valor s√£o atualizados primeiro.

IV. **An√°lise da Propaga√ß√£o da Informa√ß√£o:** A chave para a efici√™ncia da varredura prioritizada reside na sua capacidade de concentrar a computa√ß√£o nos estados onde as mudan√ßas de valor s√£o mais significativas. Ao atualizar primeiro os estados com grandes mudan√ßas de valor, propagamos rapidamente informa√ß√µes importantes para estados vizinhos.

V. **Redu√ß√£o de Itera√ß√µes:** Em compara√ß√£o com a varredura aleat√≥ria, onde a propaga√ß√£o da informa√ß√£o √© inconsistente, a varredura prioritizada garante que as mudan√ßas de valor mais significativas sejam propagadas primeiro. Isso leva a uma converg√™ncia mais r√°pida, pois o algoritmo converge mais rapidamente para a fun√ß√£o de valor verdadeira.

VI. **Argumento Heur√≠stico:** Embora uma prova formal possa ser complexa e dependente do dom√≠nio, podemos argumentar heuristicamente que a varredura prioritizada reduz o n√∫mero de itera√ß√µes necess√°rias para atingir a converg√™ncia. Ao concentrar a computa√ß√£o nos estados onde as mudan√ßas de valor s√£o mais significativas, o algoritmo converge mais rapidamente para a fun√ß√£o de valor verdadeira em compara√ß√£o com a varredura aleat√≥ria, onde todos os estados s√£o tratados igualmente, independentemente da magnitude de suas mudan√ßas de valor.

VII. **Exemplo:** Considere um cen√°rio onde um √∫nico estado experimenta uma grande mudan√ßa de valor. Na varredura prioritizada, este estado ser√° atualizado primeiro, e a informa√ß√£o de valor ser√° rapidamente propagada para seus vizinhos. Na varredura aleat√≥ria, pode levar v√°rias itera√ß√µes para que este estado seja atualizado e para que a informa√ß√£o de valor se propague.

VIII. **Conclus√£o:** Portanto, a varredura prioritizada pode acelerar a converg√™ncia da avalia√ß√£o iterativa da pol√≠tica in-place em compara√ß√£o com a varredura aleat√≥ria, concentrando a computa√ß√£o nos estados onde as mudan√ßas de valor s√£o mais significativas e propagando as informa√ß√µes de valor de forma mais eficiente. ‚ñ†

> üí° **Exemplo Num√©rico:** Para ilustrar a diferen√ßa entre varredura aleat√≥ria e prioritizada, consideremos um Gridworld 4x4. Suponha que ap√≥s algumas itera√ß√µes, o estado (1,1) tenha uma grande mudan√ßa de valor.
>
> *   **Varredura Aleat√≥ria:** A pr√≥xima itera√ß√£o pode n√£o atualizar o estado (1,1) imediatamente, atrasando a propaga√ß√£o da informa√ß√£o.
> *   **Varredura Prioritizada:** O estado (1,1) √© atualizado primeiro, propagando rapidamente a informa√ß√£o para seus vizinhos (0,1), (2,1), (1,0) e (1,2).
>
> Para demonstrar computacionalmente, podemos implementar as duas abordagens e comparar o n√∫mero de itera√ß√µes necess√°rias para atingir a converg√™ncia.
>
> ```python
> import numpy as np
> import random
>
> def iterative_policy_evaluation(grid_size, policy, rewards, gamma, theta, scan_type="random"):
>     V = np.zeros((grid_size, grid_size))
>     iterations = 0
>     while True:
>         Delta = 0
>         states = [(i, j) for i in range(grid_size) for j in range(grid_size)]
>
>         if scan_type == "prioritized":
>             priority_queue = []
>             for i in range(grid_size):
>                 for j in range(grid_size):
>                     priority_queue.append(((i, j), 0)) # Initialize priority with 0
>             
>             def priority(state):
>                 return abs(V_old[state[0], state[1]] - V[state[0], state[1]]) if scan_type == "prioritized" else random.random()
>             
>             states = sorted(states, key=priority, reverse=True)
>
>         elif scan_type == "random":
>             random.shuffle(states)
>
>         V_old = np.copy(V) # Copy the current values for calculating Delta
>
>         for s_row, s_col in states:
>             v = V[s_row, s_col]
>             expected_return = 0
>
>             # Simplified Bellman equation assuming deterministic transitions based on the policy
>             action = policy[s_row, s_col] # Policy dictates the action
>             
>             # Deterministic transitions based on the grid world's actions
>             if action == "U":
>                 next_state = (max(0, s_row - 1), s_col)
>             elif action == "D":
>                 next_state = (min(grid_size - 1, s_row + 1), s_col)
>             elif action == "L":
>                 next_state = (s_row, max(0, s_col - 1))
>             elif action == "R":
>                 next_state = (s_row, min(grid_size - 1, s_col + 1))
>             
>             expected_return = rewards[s_row, s_col] + gamma * V[next_state[0], next_state[1]]
>
>             V[s_row, s_col] = expected_return
>             Delta = max(Delta, abs(v - V[s_row, s_col]))
>
>         iterations += 1
>         if Delta < theta:
>             break
>     return V, iterations
>
> # Example Usage (Simplified):
> grid_size = 4
> policy = np.array([["R"] * grid_size] * grid_size) # Simple policy: always go right.
> rewards = np.random.rand(grid_size, grid_size) - 0.5 # Random rewards between -0.5 and 0.5
> gamma = 0.9
> theta = 0.001
>
> V_random, iterations_random = iterative_policy_evaluation(grid_size, policy, rewards, gamma, theta, scan_type="random")
> V_prioritized, iterations_prioritized = iterative_policy_evaluation(grid_size, policy, rewards, gamma, theta, scan_type="prioritized")
>
> print("Iterations with random scan:", iterations_random)
> print("Iterations with prioritized scan:", iterations_prioritized)
> ```
>
> Nota: Este √© um exemplo simplificado. Em um cen√°rio real, voc√™ definiria transi√ß√µes de estado mais realistas e uma pol√≠tica mais complexa. Al√©m disso, seria necess√°rio definir as recompensas apropriadamente para observar benef√≠cios significativos da varredura prioritizada.
>
> Este exemplo ilustra como a varredura prioritizada pode reduzir o n√∫mero de itera√ß√µes necess√°rias para convergir para a fun√ß√£o de valor √≥tima.
>
> ```mermaid
> graph LR
> A[Estado (1,1) - Grande Mudan√ßa de Valor] --> B(Varredura Prioritizada: Atualiza√ß√£o Imediata);
> A --> C(Varredura Aleat√≥ria: Atraso na Atualiza√ß√£o);
> B --> D(Propaga√ß√£o R√°pida da Informa√ß√£o);
> C --> E(Propaga√ß√£o Lenta da Informa√ß√£o);
> ```

### Pseudoc√≥digo
Um pseudoc√≥digo para a avalia√ß√£o iterativa da pol√≠tica in-place √© apresentado a seguir [^75]:

```
Iterative Policy Evaluation, for estimating V ‚âà vœÄ
Input œÄ, the policy to be evaluated
Algorithm parameter: a small threshold Œ∏ > 0 determining accuracy of estimation
Initialize V(s) arbitrarily, for all s ‚àà S, and V(terminal) to 0

Loop:
    Œî ‚Üê 0
    Loop for each s ‚àà S:
        v ‚Üê V(s)
        V(s) ‚Üê Œ£a œÄ(a|s) Œ£s',r p(s',r|s,a) [r + Œ≥V(s')]
        Œî ‚Üê max(Œî, |v ‚Äì V(s)|)
    until Œî < Œ∏
```

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

O algoritmo itera at√© que a maior mudan√ßa nos valores de estado, denotada por $\Delta$, seja menor que um limiar $\theta$ [^75].

Para exemplificar o funcionamento da avalia√ß√£o iterativa da pol√≠tica em um Gridworld, vejamos a figura a seguir:

![Convergence of iterative policy evaluation on a gridworld, showing improvement from random to optimal policy.](./../images/image7.png)

Esta figura demonstra a converg√™ncia da avalia√ß√£o iterativa da pol√≠tica em um Gridworld 4x4. O processo de avalia√ß√£o iterativa da pol√≠tica converge, e a pol√≠tica correspondente melhora a cada itera√ß√£o, convergindo para a pol√≠tica √≥tima.

![Illustration of a 4x4 gridworld environment with rewards and actions for dynamic programming example.](./../images/image9.png)

A imagem acima representa o ambiente Gridworld 4x4, com os estados terminais sombreados e as a√ß√µes poss√≠veis (cima, baixo, esquerda, direita). A recompensa √© -1 para todas as transi√ß√µes.

### Conclus√£o

A avalia√ß√£o iterativa da pol√≠tica √© uma t√©cnica fundamental na programa√ß√£o din√¢mica para estimar a fun√ß√£o de valor de estado para uma pol√≠tica dada [^74]. Ao aplicar iterativamente a equa√ß√£o de Bellman como uma regra de atualiza√ß√£o, este m√©todo converge para a fun√ß√£o de valor correta, mesmo para grandes MDPs onde a solu√ß√£o direta do sistema de equa√ß√µes lineares √© computacionalmente proibitiva [^74]. Este procedimento fornece uma base s√≥lida para a melhoria da pol√≠tica, que exploraremos nas se√ß√µes seguintes [^76].

### Refer√™ncias
[^1]: Chapter 4: Dynamic Programming
[^74]: Chapter 4: Dynamic Programming
[^75]: Chapter 4: Dynamic Programming
[^76]: Chapter 4: Dynamic Programming
<!-- END -->