## CÃ¡lculo Iterativo da FunÃ§Ã£o de Valor de Estado

### IntroduÃ§Ã£o
Este capÃ­tulo explora os algoritmos de **ProgramaÃ§Ã£o DinÃ¢mica (DP)** para calcular polÃ­ticas Ã³timas em ambientes modelados como **Processos de DecisÃ£o de Markov (MDPs)**. Conforme mencionado anteriormente [^1], a DP fornece uma base essencial para compreender outros mÃ©todos de *Reinforcement Learning*. Iremos nos concentrar na avaliaÃ§Ã£o da polÃ­tica, que Ã© o processo de calcular a funÃ§Ã£o de valor de estado $v_\pi$ para uma polÃ­tica arbitrÃ¡ria $\pi$ [^74]. Este processo Ã© crucial para aprimorar polÃ­ticas iterativamente.

### Conceitos Fundamentais

Como vimos anteriormente [^74], a **avaliaÃ§Ã£o de polÃ­tica** (tambÃ©m conhecida como *problema de prediÃ§Ã£o*) busca determinar a funÃ§Ã£o de valor $v_\pi(s)$ para todos os estados $s \in \mathcal{S}$, dada uma polÃ­tica $\pi$. A funÃ§Ã£o de valor Ã© definida como o retorno esperado ao seguir a polÃ­tica $\pi$ a partir do estado $s$ [^74]:

$$ v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] $$.

A equaÃ§Ã£o de Bellman para $v_\pi(s)$ pode ser expressa iterativamente como [^74]:

$$v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_\pi(s')]$$

onde $\pi(a|s)$ Ã© a probabilidade de tomar a aÃ§Ã£o $a$ no estado $s$ sob a polÃ­tica $\pi$, e $p(s', r|s, a)$ Ã© a probabilidade de transiÃ§Ã£o para o estado $s'$ com recompensa $r$ ao tomar a aÃ§Ã£o $a$ no estado $s$ [^74].

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um ambiente simples com dois estados, $S = \{s_1, s_2\}$, e duas aÃ§Ãµes, $A = \{a_1, a_2\}$. Suponha que a polÃ­tica $\pi$ sempre escolha a aÃ§Ã£o $a_1$ em ambos os estados, ou seja, $\pi(a_1|s_1) = 1$ e $\pi(a_1|s_2) = 1$. As probabilidades de transiÃ§Ã£o e recompensas sÃ£o as seguintes:
> *   $p(s_1, 0|s_1, a_1) = 0.8$
> *   $p(s_2, 1|s_1, a_1) = 0.2$
> *   $p(s_1, -1|s_2, a_1) = 0.5$
> *   $p(s_2, 2|s_2, a_1) = 0.5$
>
> Seja $\gamma = 0.9$. A equaÃ§Ã£o de Bellman para $v_\pi(s_1)$ se torna:
>
> $v_\pi(s_1) = \pi(a_1|s_1) \sum_{s',r} p(s', r|s_1, a_1) [r + \gamma v_\pi(s')] = 1 \cdot [0.8 \cdot (0 + 0.9 v_\pi(s_1)) + 0.2 \cdot (1 + 0.9 v_\pi(s_2))]$
>
> Similarmente, para $v_\pi(s_2)$:
>
> $v_\pi(s_2) = \pi(a_1|s_2) \sum_{s',r} p(s', r|s_2, a_1) [r + \gamma v_\pi(s')] = 1 \cdot [0.5 \cdot (-1 + 0.9 v_\pi(s_1)) + 0.5 \cdot (2 + 0.9 v_\pi(s_2))]$
>
> Estas sÃ£o duas equaÃ§Ãµes lineares com duas incÃ³gnitas, $v_\pi(s_1)$ e $v_\pi(s_2)$, que podem ser resolvidas para encontrar a funÃ§Ã£o de valor $v_\pi$.

Quando a dinÃ¢mica do ambiente, definida por $p(s', r|s, a)$, Ã© completamente conhecida, o cÃ¡lculo direto de $v_\pi$ pode ser visto como a soluÃ§Ã£o de um sistema de $|\mathcal{S}|$ equaÃ§Ãµes lineares simultÃ¢neas com $|\mathcal{S}|$ incÃ³gnitas (os valores $v_\pi(s)$ para cada $s \in \mathcal{S}$) [^74]. Embora, em princÃ­pio, isso possa ser resolvido diretamente, a computaÃ§Ã£o pode se tornar *tediosa* e *impraticÃ¡vel* para espaÃ§os de estados grandes [^74].

Em vez de resolver o sistema linear diretamente, a **avaliaÃ§Ã£o iterativa da polÃ­tica** oferece uma abordagem prÃ¡tica [^74]. Este mÃ©todo envolve gerar uma sequÃªncia de funÃ§Ãµes de valor aproximadas $v_0, v_1, v_2, \ldots$, onde cada $v_k$ mapeia estados para valores reais [^74]. A aproximaÃ§Ã£o inicial $v_0$ Ã© escolhida arbitrariamente (exceto que o estado terminal, se houver, deve ser dado o valor 0). Cada aproximaÃ§Ã£o sucessiva Ã© obtida usando a equaÃ§Ã£o de Bellman para $v_\pi$ como uma regra de atualizaÃ§Ã£o [^74]:

$$ v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_k(s')] $$

para todos os estados $s \in \mathcal{S}$ [^74].

> ğŸ’¡ **Exemplo NumÃ©rico:** Continuando com o exemplo anterior, vamos realizar algumas iteraÃ§Ãµes da avaliaÃ§Ã£o iterativa da polÃ­tica. Inicializamos $v_0(s_1) = 0$ e $v_0(s_2) = 0$. Usando a equaÃ§Ã£o de atualizaÃ§Ã£o:
>
> *IteraÃ§Ã£o 1:*
>
> $v_1(s_1) = 0.8 \cdot (0 + 0.9 \cdot 0) + 0.2 \cdot (1 + 0.9 \cdot 0) = 0.2$
>
> $v_1(s_2) = 0.5 \cdot (-1 + 0.9 \cdot 0) + 0.5 \cdot (2 + 0.9 \cdot 0) = 0.5$
>
> *IteraÃ§Ã£o 2:*
>
> $v_2(s_1) = 0.8 \cdot (0 + 0.9 \cdot 0.2) + 0.2 \cdot (1 + 0.9 \cdot 0.5) = 0.0.144 + 0.29 = 0.434$
>
> $v_2(s_2) = 0.5 \cdot (-1 + 0.9 \cdot 0.2) + 0.5 \cdot (2 + 0.9 \cdot 0.5) = -0.41 + 1.225 = 0.815$
>
> *IteraÃ§Ã£o 3:*
>
> $v_3(s_1) = 0.8 \cdot (0 + 0.9 \cdot 0.434) + 0.2 \cdot (1 + 0.9 \cdot 0.815) = 0.312 + 0.347 = 0.659$
>
> $v_3(s_2) = 0.5 \cdot (-1 + 0.9 \cdot 0.434) + 0.5 \cdot (2 + 0.9 \cdot 0.815) = -0.230 + 1.367 = 1.137$
>
> Este processo continua iterativamente atÃ© que a mudanÃ§a nos valores de estado entre iteraÃ§Ãµes consecutivas seja menor que um limiar predefinido.

Este processo iterativo Ã© um exemplo de uma **atualizaÃ§Ã£o esperada**, pois se baseia em uma expectativa sobre todos os estados sucessores possÃ­veis [^75]. A sequÃªncia $\{v_k\}$ pode ser mostrada para convergir para $v_\pi$ quando $k \to \infty$, sob as mesmas condiÃ§Ãµes que garantem a existÃªncia de $v_\pi$ [^74]. Estas condiÃ§Ãµes sÃ£o $\gamma < 1$ ou garantia de terminaÃ§Ã£o a partir de todos os estados sob a polÃ­tica $\pi$ [^74].

Para complementar a garantia de convergÃªncia, podemos apresentar o seguinte resultado sobre a natureza da convergÃªncia:

**Lema 1** A sequÃªncia de funÃ§Ãµes de valor $\{v_k\}$ converge monotonicamente para $v_\pi$ se a recompensa $r$ for nÃ£o negativa.

*Prova.* (EsboÃ§o) A prova pode ser feita por induÃ§Ã£o. Assumimos que $v_k(s) \le v_\pi(s)$ para todo $s$. EntÃ£o,
$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] \le \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] = v_\pi(s)$.
Como $v_0$ Ã© inicializado arbitrariamente, mas as recompensas sÃ£o nÃ£o negativas, $v_1(s) \ge v_0(s)$. Portanto, a sequÃªncia Ã© monotonicamente crescente e limitada superiormente por $v_\pi(s)$, garantindo a convergÃªncia.

Para formalizar a prova do Lema 1, podemos expandi-la da seguinte forma:

*Prova.*
I. **Caso Base:** Inicialmente, seja $v_0(s)$ a funÃ§Ã£o de valor inicial para todos os estados $s \in \mathcal{S}$. Como $v_0(s)$ pode ser escolhido arbitrariamente e as recompensas sÃ£o nÃ£o negativas, $v_1(s)$ serÃ¡ maior ou igual a $v_0(s)$ para todo $s \in \mathcal{S}$. Isso ocorre porque $v_1(s)$ Ã© uma combinaÃ§Ã£o das recompensas esperadas e valores descontados, que serÃ£o maiores ou iguais Ã  funÃ§Ã£o de valor inicial assumindo recompensas nÃ£o negativas.
II. **HipÃ³tese Indutiva:** Assuma que para alguma iteraÃ§Ã£o $k \ge 0$, $v_k(s) \leq v_{k+1}(s) \leq v_\pi(s)$ para todo $s \in \mathcal{S}$. Isso significa que a funÃ§Ã£o de valor na iteraÃ§Ã£o $k$ Ã© menor ou igual Ã  funÃ§Ã£o de valor na iteraÃ§Ã£o $k+1$, que por sua vez Ã© menor ou igual Ã  funÃ§Ã£o de valor Ã³tima $v_\pi(s)$.
III. **Passo Indutivo:** Precisamos mostrar que $v_{k+1}(s) \leq v_{k+2}(s) \leq v_\pi(s)$ para todo $s \in \mathcal{S}$. Usando a equaÃ§Ã£o de Bellman, temos:

$$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s]$$
$$v_{k+2}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_{k+1}(S_{t+1}) | S_t = s]$$

Como assumimos que $v_k(s) \leq v_{k+1}(s)$ para todo $s \in \mathcal{S}$ (pela hipÃ³tese indutiva), entÃ£o $\gamma v_k(S_{t+1}) \leq \gamma v_{k+1}(S_{t+1})$. Portanto,

$$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] \leq \mathbb{E}_\pi[R_{t+1} + \gamma v_{k+1}(S_{t+1}) | S_t = s] = v_{k+2}(s)$$
Isso mostra que $v_{k+1}(s) \leq v_{k+2}(s)$ para todo $s \in \mathcal{S}$.

IV. AlÃ©m disso, precisamos mostrar que $v_{k+2}(s) \leq v_\pi(s)$. Novamente usando a equaÃ§Ã£o de Bellman:

$$v_{k+2}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_{k+1}(S_{t+1}) | S_t = s]$$

Como assumimos que $v_{k+1}(s) \leq v_\pi(s)$ para todo $s \in \mathcal{S}$ (pela hipÃ³tese indutiva), entÃ£o $\gamma v_{k+1}(S_{t+1}) \leq \gamma v_\pi(S_{t+1})$. Portanto,
$$v_{k+2}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_{k+1}(S_{t+1}) | S_t = s] \leq \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] = v_\pi(s)$$
Isso mostra que $v_{k+2}(s) \leq v_\pi(s)$ para todo $s \in \mathcal{S}$.

V. **ConclusÃ£o:** Por induÃ§Ã£o, mostramos que a sequÃªncia $\{v_k(s)\}$ Ã© monotonicamente crescente e limitada superiormente por $v_\pi(s)$ para todo $s \in \mathcal{S}$. Portanto, a sequÃªncia converge monotonicamente para $v_\pi(s)$. â– 

AlÃ©m disso, podemos derivar um limite superior para o erro em cada iteraÃ§Ã£o:

**Teorema 1** Seja $v_\pi$ a funÃ§Ã£o de valor verdadeira e $v_k$ a funÃ§Ã£o de valor na $k$-Ã©sima iteraÃ§Ã£o. EntÃ£o,

$$||v_k - v_\pi||_\infty \le \frac{\gamma^k}{1 - \gamma} \max_s |v_1(s) - v_0(s)|$$

*Prova.* (EsboÃ§o)
A prova usa a desigualdade de Bellman e induÃ§Ã£o matemÃ¡tica. A norma infinito Ã© definida como $||v||_\infty = \max_s |v(s)|$.

Para fornecer uma prova mais completa do Teorema 1:

*Prova.*
I. **DefiniÃ§Ã£o da Norma Infinito:** A norma infinito de uma funÃ§Ã£o de valor $v$ Ã© definida como $||v||_\infty = \max_s |v(s)|$, onde o mÃ¡ximo Ã© tomado sobre todos os estados $s \in \mathcal{S}$. Esta norma mede a maior diferenÃ§a absoluta entre os valores de estado.

II. **Desigualdade de Bellman:** Para qualquer funÃ§Ã£o de valor $v$, definimos o operador de Bellman $T_\pi$ como:
$$(T_\pi v)(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')] $$
A funÃ§Ã£o de valor verdadeira $v_\pi$ Ã© um ponto fixo do operador de Bellman, ou seja, $v_\pi = T_\pi v_\pi$.

III. **Passo Base:** Considere a diferenÃ§a entre $v_1$ e $v_\pi$:
$$||v_1 - v_\pi||_\infty = ||T_\pi v_0 - T_\pi v_\pi||_\infty = \max_s |(T_\pi v_0)(s) - (T_\pi v_\pi)(s)|$$
Usando a definiÃ§Ã£o do operador de Bellman:
$$||v_1 - v_\pi||_\infty = \max_s \left| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_0(s')] - \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_\pi(s')] \right|$$
$$||v_1 - v_\pi||_\infty = \max_s \left| \sum_a \pi(a|s) \sum_{s',r} p(s', r|s, a) \gamma [v_0(s') - v_\pi(s')] \right|$$
$$||v_1 - v_\pi||_\infty \le \gamma \max_s |v_0(s) - v_\pi(s)| = \gamma ||v_0 - v_\pi||_\infty$$

IV. **Passo Indutivo:** Assuma que $||v_k - v_\pi||_\infty \le \gamma^k ||v_0 - v_\pi||_\infty$. Precisamos mostrar que $||v_{k+1} - v_\pi||_\infty \le \gamma^{k+1} ||v_0 - v_\pi||_\infty$.

Usando a mesma lÃ³gica do passo base:
$$||v_{k+1} - v_\pi||_\infty = ||T_\pi v_k - T_\pi v_\pi||_\infty \le \gamma ||v_k - v_\pi||_\infty$$
Pela hipÃ³tese indutiva:
$$||v_{k+1} - v_\pi||_\infty \le \gamma (\gamma^k ||v_0 - v_\pi||_\infty) = \gamma^{k+1} ||v_0 - v_\pi||_\infty$$

V. **Limite Superior:** Observe que $||v_0 - v_\pi||_\infty \le ||v_0 - v_1||_\infty + ||v_1 - v_\pi||_\infty$. Substituindo $||v_1 - v_\pi||_\infty \le \gamma ||v_0 - v_\pi||_\infty$, temos:

$$||v_0 - v_\pi||_\infty \le ||v_0 - v_1||_\infty + \gamma ||v_0 - v_\pi||_\infty$$
$$||v_0 - v_\pi||_\infty (1 - \gamma) \le ||v_0 - v_1||_\infty$$
$$||v_0 - v_\pi||_\infty \le \frac{1}{1 - \gamma} ||v_1 - v_0||_\infty = \frac{1}{1 - \gamma} \max_s |v_1(s) - v_0(s)|$$
Substituindo isso no resultado do passo indutivo:
$$||v_{k} - v_\pi||_\infty \le \gamma^k ||v_0 - v_\pi||_\infty \le \frac{\gamma^k}{1 - \gamma} ||v_1 - v_0||_\infty$$

VI. **ConclusÃ£o:** Portanto, $||v_k - v_\pi||_\infty \le \frac{\gamma^k}{1 - \gamma} \max_s |v_1(s) - v_0(s)|$. Este resultado nos dÃ¡ um limite superior para o erro entre a funÃ§Ã£o de valor na $k$-Ã©sima iteraÃ§Ã£o e a funÃ§Ã£o de valor verdadeira. Conforme $k$ aumenta, o erro diminui exponencialmente, jÃ¡ que $\gamma < 1$. â– 

### ImplementaÃ§Ã£o Computacional

A implementaÃ§Ã£o computacional da avaliaÃ§Ã£o iterativa da polÃ­tica geralmente envolve o uso de *arrays* para armazenar os valores de estado. Duas abordagens principais sÃ£o comumente empregadas [^75]:

1.  **Abordagem de dois arrays:** Utiliza dois arrays, um para os valores antigos $v_k(s)$ e outro para os novos valores $v_{k+1}(s)$. Os novos valores sÃ£o calculados um por um a partir dos valores antigos sem modificar os valores antigos durante uma iteraÃ§Ã£o [^75].
2.  **Abordagem "in-place":** Utiliza um Ãºnico array e atualiza os valores *in-place*, ou seja, cada novo valor substitui imediatamente o antigo. A convergÃªncia desta abordagem pode ser mais rÃ¡pida, pois utiliza os dados mais recentes assim que estÃ£o disponÃ­veis, porÃ©m a ordem em que os estados sÃ£o atualizados tem impacto significativo na taxa de convergÃªncia [^75].

A atualizaÃ§Ã£o *in-place* pode ser vista como uma **varredura** atravÃ©s do espaÃ§o de estados [^75].

Para melhor ilustrar o impacto da ordem de varredura na convergÃªncia da abordagem in-place, podemos considerar as seguintes estratÃ©gias de varredura:

*   **Varredura AleatÃ³ria:** Os estados sÃ£o atualizados em uma ordem aleatÃ³ria a cada iteraÃ§Ã£o.
*   **Varredura Prioritizada:** Os estados sÃ£o atualizados com base na magnitude da mudanÃ§a de valor na iteraÃ§Ã£o anterior. Estados com maiores mudanÃ§as sÃ£o priorizados.

**ProposiÃ§Ã£o 1** A varredura prioritizada pode acelerar a convergÃªncia da avaliaÃ§Ã£o iterativa da polÃ­tica in-place em comparaÃ§Ã£o com a varredura aleatÃ³ria.

*Prova.* (EsboÃ§o) A varredura prioritizada concentra a computaÃ§Ã£o nos estados onde as mudanÃ§as de valor sÃ£o mais significativas, propagando as informaÃ§Ãµes de valor de forma mais eficiente. Isso reduz o nÃºmero de iteraÃ§Ãµes necessÃ¡rias para atingir a convergÃªncia em comparaÃ§Ã£o com a varredura aleatÃ³ria, onde todos os estados sÃ£o tratados igualmente, independentemente da magnitude de suas mudanÃ§as de valor.

Para tornar a prova da ProposiÃ§Ã£o 1 mais rigorosa, podemos detalhÃ¡-la da seguinte forma:

*Prova.*
I. **DefiniÃ§Ã£o de ConvergÃªncia:** Definimos convergÃªncia como o ponto onde a maior mudanÃ§a nos valores de estado entre iteraÃ§Ãµes consecutivas cai abaixo de um limiar $\theta$, ou seja, $\max_s |v_{k+1}(s) - v_k(s)| < \theta$.

II. **Varredura AleatÃ³ria:** Na varredura aleatÃ³ria, os estados sÃ£o atualizados em uma ordem aleatÃ³ria a cada iteraÃ§Ã£o. Isso significa que, em uma Ãºnica iteraÃ§Ã£o, alguns estados podem ser atualizados com informaÃ§Ãµes mais recentes do que outros, levando a uma propagaÃ§Ã£o de informaÃ§Ãµes de valor inconsistente.

III. **Varredura Prioritizada:** Na varredura prioritizada, os estados sÃ£o atualizados com base na magnitude da mudanÃ§a de valor na iteraÃ§Ã£o anterior. Formalmente, definimos uma fila de prioridade onde a prioridade de cada estado $s$ Ã© dada por $|v_k(s) - v_{k-1}(s)|$. Estados com maiores mudanÃ§as de valor sÃ£o atualizados primeiro.

IV. **AnÃ¡lise da PropagaÃ§Ã£o da InformaÃ§Ã£o:** A chave para a eficiÃªncia da varredura prioritizada reside na sua capacidade de concentrar a computaÃ§Ã£o nos estados onde as mudanÃ§as de valor sÃ£o mais significativas. Ao atualizar primeiro os estados com grandes mudanÃ§as de valor, propagamos rapidamente informaÃ§Ãµes importantes para estados vizinhos.

V. **ReduÃ§Ã£o de IteraÃ§Ãµes:** Em comparaÃ§Ã£o com a varredura aleatÃ³ria, onde a propagaÃ§Ã£o da informaÃ§Ã£o Ã© inconsistente, a varredura prioritizada garante que as mudanÃ§as de valor mais significativas sejam propagadas primeiro. Isso leva a uma convergÃªncia mais rÃ¡pida, pois o algoritmo converge mais rapidamente para a funÃ§Ã£o de valor verdadeira.

VI. **Argumento HeurÃ­stico:** Embora uma prova formal possa ser complexa e dependente do domÃ­nio, podemos argumentar heuristicamente que a varredura prioritizada reduz o nÃºmero de iteraÃ§Ãµes necessÃ¡rias para atingir a convergÃªncia. Ao concentrar a computaÃ§Ã£o nos estados onde as mudanÃ§as de valor sÃ£o mais significativas, o algoritmo converge mais rapidamente para a funÃ§Ã£o de valor verdadeira em comparaÃ§Ã£o com a varredura aleatÃ³ria, onde todos os estados sÃ£o tratados igualmente, independentemente da magnitude de suas mudanÃ§as de valor.

VII. **Exemplo:** Considere um cenÃ¡rio onde um Ãºnico estado experimenta uma grande mudanÃ§a de valor. Na varredura prioritizada, este estado serÃ¡ atualizado primeiro, e a informaÃ§Ã£o de valor serÃ¡ rapidamente propagada para seus vizinhos. Na varredura aleatÃ³ria, pode levar vÃ¡rias iteraÃ§Ãµes para que este estado seja atualizado e para que a informaÃ§Ã£o de valor se propague.

VIII. **ConclusÃ£o:** Portanto, a varredura prioritizada pode acelerar a convergÃªncia da avaliaÃ§Ã£o iterativa da polÃ­tica in-place em comparaÃ§Ã£o com a varredura aleatÃ³ria, concentrando a computaÃ§Ã£o nos estados onde as mudanÃ§as de valor sÃ£o mais significativas e propagando as informaÃ§Ãµes de valor de forma mais eficiente. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Para ilustrar a diferenÃ§a entre varredura aleatÃ³ria e prioritizada, consideremos um Gridworld 4x4. Suponha que apÃ³s algumas iteraÃ§Ãµes, o estado (1,1) tenha uma grande mudanÃ§a de valor.
>
> *   **Varredura AleatÃ³ria:** A prÃ³xima iteraÃ§Ã£o pode nÃ£o atualizar o estado (1,1) imediatamente, atrasando a propagaÃ§Ã£o da informaÃ§Ã£o.
> *   **Varredura Prioritizada:** O estado (1,1) Ã© atualizado primeiro, propagando rapidamente a informaÃ§Ã£o para seus vizinhos (0,1), (2,1), (1,0) e (1,2).
>
> Para demonstrar computacionalmente, podemos implementar as duas abordagens e comparar o nÃºmero de iteraÃ§Ãµes necessÃ¡rias para atingir a convergÃªncia.
>
> ```python
> import numpy as np
> import random
>
> def iterative_policy_evaluation(grid_size, policy, rewards, gamma, theta, scan_type="random"):
>     V = np.zeros((grid_size, grid_size))
>     iterations = 0
>     while True:
>         Delta = 0
>         states = [(i, j) for i in range(grid_size) for j in range(grid_size)]
>
>         if scan_type == "prioritized":
>             priority_queue = []
>             for i in range(grid_size):
>                 for j in range(grid_size):
>                     priority_queue.append(((i, j), 0)) # Initialize priority with 0
>             
>             def priority(state):
>                 return abs(V_old[state[0], state[1]] - V[state[0], state[1]]) if scan_type == "prioritized" else random.random()
>             
>             states = sorted(states, key=priority, reverse=True)
>
>         elif scan_type == "random":
>             random.shuffle(states)
>
>         V_old = np.copy(V) # Copy the current values for calculating Delta
>
>         for s_row, s_col in states:
>             v = V[s_row, s_col]
>             expected_return = 0
>
>             # Simplified Bellman equation assuming deterministic transitions based on the policy
>             action = policy[s_row, s_col] # Policy dictates the action
>             
>             # Deterministic transitions based on the grid world's actions
>             if action == "U":
>                 next_state = (max(0, s_row - 1), s_col)
>             elif action == "D":
>                 next_state = (min(grid_size - 1, s_row + 1), s_col)
>             elif action == "L":
>                 next_state = (s_row, max(0, s_col - 1))
>             elif action == "R":
>                 next_state = (s_row, min(grid_size - 1, s_col + 1))
>             
>             expected_return = rewards[s_row, s_col] + gamma * V[next_state[0], next_state[1]]
>
>             V[s_row, s_col] = expected_return
>             Delta = max(Delta, abs(v - V[s_row, s_col]))
>
>         iterations += 1
>         if Delta < theta:
>             break
>     return V, iterations
>
> # Example Usage (Simplified):
> grid_size = 4
> policy = np.array([["R"] * grid_size] * grid_size) # Simple policy: always go right.
> rewards = np.random.rand(grid_size, grid_size) - 0.5 # Random rewards between -0.5 and 0.5
> gamma = 0.9
> theta = 0.001
>
> V_random, iterations_random = iterative_policy_evaluation(grid_size, policy, rewards, gamma, theta, scan_type="random")
> V_prioritized, iterations_prioritized = iterative_policy_evaluation(grid_size, policy, rewards, gamma, theta, scan_type="prioritized")
>
> print("Iterations with random scan:", iterations_random)
> print("Iterations with prioritized scan:", iterations_prioritized)
> ```
>
> Nota: Este Ã© um exemplo simplificado. Em um cenÃ¡rio real, vocÃª definiria transiÃ§Ãµes de estado mais realistas e uma polÃ­tica mais complexa. AlÃ©m disso, seria necessÃ¡rio definir as recompensas apropriadamente para observar benefÃ­cios significativos da varredura prioritizada.
>
> Este exemplo ilustra como a varredura prioritizada pode reduzir o nÃºmero de iteraÃ§Ãµes necessÃ¡rias para convergir para a funÃ§Ã£o de valor Ã³tima.
>
> ```mermaid
> graph LR
> A[Estado (1,1) - Grande MudanÃ§a de Valor] --> B(Varredura Prioritizada: AtualizaÃ§Ã£o Imediata);
> A --> C(Varredura AleatÃ³ria: Atraso na AtualizaÃ§Ã£o);
> B --> D(PropagaÃ§Ã£o RÃ¡pida da InformaÃ§Ã£o);
> C --> E(PropagaÃ§Ã£o Lenta da InformaÃ§Ã£o);
> ```

### PseudocÃ³digo
Um pseudocÃ³digo para a avaliaÃ§Ã£o iterativa da polÃ­tica in-place Ã© apresentado a seguir [^75]:

```
Iterative Policy Evaluation, for estimating V â‰ˆ vÏ€
Input Ï€, the policy to be evaluated
Algorithm parameter: a small threshold Î¸ > 0 determining accuracy of estimation
Initialize V(s) arbitrarily, for all s âˆˆ S, and V(terminal) to 0

Loop:
    Î” â† 0
    Loop for each s âˆˆ S:
        v â† V(s)
        V(s) â† Î£a Ï€(a|s) Î£s',r p(s',r|s,a) [r + Î³V(s')]
        Î” â† max(Î”, |v â€“ V(s)|)
    until Î” < Î¸
```

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

O algoritmo itera atÃ© que a maior mudanÃ§a nos valores de estado, denotada por $\Delta$, seja menor que um limiar $\theta$ [^75].

Para exemplificar o funcionamento da avaliaÃ§Ã£o iterativa da polÃ­tica em um Gridworld, vejamos a figura a seguir:

![Convergence of iterative policy evaluation on a gridworld, showing improvement from random to optimal policy.](./../images/image7.png)

Esta figura demonstra a convergÃªncia da avaliaÃ§Ã£o iterativa da polÃ­tica em um Gridworld 4x4. O processo de avaliaÃ§Ã£o iterativa da polÃ­tica converge, e a polÃ­tica correspondente melhora a cada iteraÃ§Ã£o, convergindo para a polÃ­tica Ã³tima.

![Illustration of a 4x4 gridworld environment with rewards and actions for dynamic programming example.](./../images/image9.png)

A imagem acima representa o ambiente Gridworld 4x4, com os estados terminais sombreados e as aÃ§Ãµes possÃ­veis (cima, baixo, esquerda, direita). A recompensa Ã© -1 para todas as transiÃ§Ãµes.

### ConclusÃ£o

A avaliaÃ§Ã£o iterativa da polÃ­tica Ã© uma tÃ©cnica fundamental na programaÃ§Ã£o dinÃ¢mica para estimar a funÃ§Ã£o de valor de estado para uma polÃ­tica dada [^74]. Ao aplicar iterativamente a equaÃ§Ã£o de Bellman como uma regra de atualizaÃ§Ã£o, este mÃ©todo converge para a funÃ§Ã£o de valor correta, mesmo para grandes MDPs onde a soluÃ§Ã£o direta do sistema de equaÃ§Ãµes lineares Ã© computacionalmente proibitiva [^74]. Este procedimento fornece uma base sÃ³lida para a melhoria da polÃ­tica, que exploraremos nas seÃ§Ãµes seguintes [^76].

### ReferÃªncias
[^1]: Chapter 4: Dynamic Programming
[^74]: Chapter 4: Dynamic Programming
[^75]: Chapter 4: Dynamic Programming
[^76]: Chapter 4: Dynamic Programming
<!-- END -->