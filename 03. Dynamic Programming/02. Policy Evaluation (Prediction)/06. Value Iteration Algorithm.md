## Iterative Policy Evaluation: Algoritmo e Convergência

### Introdução
O objetivo da **policy evaluation** é calcular a função valor de estado $v_{\pi}$ para uma política arbitrária $\pi$ [^74]. Como mencionado anteriormente, este processo é fundamental para a resolução do problema de *prediction* [^74]. Uma vez que as funções ótimas de valor ($v_*$ ou $q_*$) são encontradas, é possível determinar facilmente as políticas ótimas que satisfazem as equações de otimalidade de Bellman [^73]. Esta seção detalha o algoritmo iterativo para policy evaluation, enfatizando a sua aplicação repetida e as condições de convergência.

### Algoritmo de Policy Evaluation Iterativa
O algoritmo de **policy evaluation iterativa** baseia-se na aplicação repetida da equação de Bellman para $v_{\pi}$ [^74]. Dado que as dinâmicas do ambiente são completamente conhecidas, a equação (4.4) [^74] representa um sistema de $|S|$ equações lineares simultâneas com $|S|$ incógnitas, sendo estas $v_{\pi}(s)$, para $s \in S$ [^74]. Embora a solução direta deste sistema seja teoricamente possível, os métodos iterativos são mais práticos [^74].

O algoritmo começa com uma sequência de funções de valor aproximadas $v_0, v_1, v_2, \dots$, em que cada $v_k$ mapeia $S^+$ para $\mathbb{R}$ [^74]. A aproximação inicial $v_0$ é escolhida arbitrariamente, exceto que o estado terminal (se existir) deve ter um valor de 0 [^74]. Cada aproximação sucessiva é obtida usando a equação de Bellman (4.4) como uma regra de atualização:
$$
v_{k+1}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] \quad \forall s \in S
$$
Esta equação (4.5) [^74] é aplicada iterativamente para atualizar as estimativas de valor até que a função valor convirja para $v_{\pi}$ [^74]. Claramente, $v_k = v_{\pi}$ é um ponto fixo para esta regra de atualização, visto que a equação de Bellman garante a igualdade neste caso [^74].

> 💡 **Exemplo Numérico:**
>
> Considere um ambiente simples com 3 estados: $S = \{s_1, s_2, s_3\}$, onde $s_3$ é o estado terminal, e uma política $\pi$ que define as probabilidades de ação para cada estado. Assume-se que $\gamma = 0.9$ e que temos as seguintes recompensas e probabilidades de transição:
>
> - $p(s_2, -1 | s_1, a) = 1$ (tomar ação 'a' em $s_1$ leva a $s_2$ com recompensa -1)
> - $p(s_3, 0 | s_2, a) = 1$ (tomar ação 'a' em $s_2$ leva a $s_3$ com recompensa 0)
> - $v_0(s_1) = 0$, $v_0(s_2) = 0$, $v_0(s_3) = 0$ (inicialização)
>
> Iteração 1:
>
> - $v_1(s_1) = -1 + 0.9 \cdot v_0(s_2) = -1 + 0.9 \cdot 0 = -1$
> - $v_1(s_2) = 0 + 0.9 \cdot v_0(s_3) = 0 + 0.9 \cdot 0 = 0$
> - $v_1(s_3) = 0$ (estado terminal)
>
> Iteração 2:
>
> - $v_2(s_1) = -1 + 0.9 \cdot v_1(s_2) = -1 + 0.9 \cdot 0 = -1$
> - $v_2(s_2) = 0 + 0.9 \cdot v_1(s_3) = 0 + 0.9 \cdot 0 = 0$
> - $v_2(s_3) = 0$
>
> Neste caso simples, o algoritmo converge após uma iteração para os valores de estado $v(s_1) = -1$, $v(s_2) = 0$ e $v(s_3) = 0$.

**Iterative Policy Evaluation**

O pseudocódigo fornecido [^75] descreve uma versão *in-place* do algoritmo, que utiliza uma única matriz para armazenar as funções de valor [^75]. Isto significa que o algoritmo atualiza os valores "in-place", substituindo o valor antigo de um estado pelo novo valor imediatamente [^75]. A ordem em que os estados são atualizados durante cada *sweep* influencia significativamente a taxa de convergência [^75].

> 💡 **Exemplo Numérico (In-Place vs. Two Matrices):**
>
> Considere um ambiente com dois estados $s_1$ e $s_2$, com $\gamma = 0.5$.
>
> - $p(s_2, 2 | s_1, a) = 1$
> - $p(s_1, 0 | s_2, a) = 1$
> - $v_0(s_1) = 0$, $v_0(s_2) = 0$
>
> *In-Place*: Atualizamos $s_1$ primeiro, depois $s_2$.
>
> - $v_1(s_1) = 2 + 0.5 * v_0(s_2) = 2$
> - $v_1(s_2) = 0 + 0.5 * v_1(s_1) = 0 + 0.5 * 2 = 1$
>
> *Two Matrices*: Calculamos todos os novos valores usando os valores da iteração anterior.
>
> - $v_1(s_1) = 2 + 0.5 * v_0(s_2) = 2$
> - $v_1(s_2) = 0 + 0.5 * v_0(s_1) = 0$
>
> Observe que a atualização *in-place* usa o valor recém-atualizado de $s_1$ para calcular o valor de $s_2$, enquanto a atualização com duas matrizes usa os valores da iteração anterior para ambos.

A versão *in-place* do algoritmo converge para $v_{\pi}$, e geralmente converge mais rapidamente do que a versão com duas matrizes, pois utiliza os dados mais recentes assim que estão disponíveis [^75]. A atualização é considerada um **expected update**, pois é baseada numa expectativa sobre todos os possíveis estados seguintes, em vez de numa única amostra [^75].

**Pseudocódigo:**

```
Input π, the policy to be evaluated
Algorithm parameter: a small threshold θ > 0 determining accuracy of estimation
Initialize V(s) arbitrarily, for all s ∈ S, and V(terminal) to 0

Loop:
    Δ ← 0
    Loop for each s ∈ S:
        v ← V(s)
        V(s) ← Σa π(a|s) Σs',r p(s',r|s,a) [r + γV(s')]
        Δ ← max(Δ, |v – V(s)|)
    until Δ < θ
```

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

Neste pseudocódigo, $\Delta$ acumula a maior diferença absoluta entre os valores antigos e novos, permitindo que o algoritmo pare quando as alterações se tornam suficientemente pequenas [^75]. Formalmente, a policy evaluation iterativa converge apenas no limite, mas, na prática, é interrompida quando a variação máxima dos valores de estado entre as iterações ($max_{s \in S} |v_{k+1}(s) - v_k(s)|$) é inferior a um limiar $\theta$ [^75].

> 💡 **Exemplo Numérico (Convergência com Threshold):**
>
> Considere um ambiente com dois estados $s_1$ e $s_2$, com $\gamma = 0.5$ e $\theta = 0.01$.
>
> - $p(s_2, 1 | s_1, a) = 1$
> - $p(s_1, 0 | s_2, a) = 1$
> - $v_0(s_1) = 0$, $v_0(s_2) = 0$
>
> Iterações:
>
> 1.  $v_1(s_1) = 1$, $v_1(s_2) = 0$  $\Delta = 1$
> 2.  $v_2(s_1) = 1$, $v_2(s_2) = 0.5$ $\Delta = 0.5$
> 3.  $v_3(s_1) = 1.25$, $v_3(s_2) = 0.5$ $\Delta = 0.25$
> 4.  $v_4(s_1) = 1.25$, $v_4(s_2) = 0.625$ $\Delta = 0.125$
> 5.  $v_5(s_1) = 1.3125$, $v_5(s_2) = 0.625$ $\Delta = 0.0625$
> 6.  $v_6(s_1) = 1.3125$, $v_6(s_2) = 0.65625$ $\Delta = 0.03125$
> 7.  $v_7(s_1) = 1.328125$, $v_7(s_2) = 0.65625$ $\Delta = 0.015625$
> 8.  $v_8(s_1) = 1.328125$, $v_8(s_2) = 0.6640625$ $\Delta = 0.0078125$
>
> O algoritmo para na iteração 8, pois $\Delta < \theta = 0.01$.

Para complementar a compreensão da atualização *in-place*, podemos formalizar a sua relação com a atualização com duas matrizes.

**Lema 1:** Seja $V_{k+1}^{in}(s)$ o valor do estado $s$ na iteração $k+1$ usando a atualização *in-place*, e $V_{k+1}^{two}(s)$ o valor do estado $s$ na iteração $k+1$ usando duas matrizes. Então, para cada $s \in S$,

$V_{k+1}^{in}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma V_k'(s')]$,

onde $V_k'(s') = V_{k+1}^{in}(s')$ se o estado $s'$ foi atualizado antes de $s$ na iteração $k+1$, e $V_k'(s') = V_k(s')$ caso contrário.

*Proof:* A atualização *in-place* utiliza os valores mais recentes disponíveis. Se um estado $s'$ já foi atualizado na iteração $k+1$, o seu novo valor $V_{k+1}^{in}(s')$ é usado. Caso contrário, o valor da iteração anterior $V_k(s')$ é utilizado. $\blacksquare$

Além disso, podemos definir uma medida de erro para cada iteração.

**Definição:** O erro máximo na iteração $k$ é definido como $e_k = \max_{s \in S} |v_k(s) - v_{\pi}(s)|$.

**Teorema 2:** O algoritmo de policy evaluation iterativa é uma contração em relação à norma do máximo. Especificamente,

$||v_{k+1} - v_{\pi}||_{\infty} \leq \gamma ||v_k - v_{\pi}||_{\infty}$,

onde $||v||_{\infty} = \max_{s \in S} |v(s)|$.

*Proof:* Provaremos que $||v_{k+1} - v_{\pi}||_{\infty} \leq \gamma ||v_k - v_{\pi}||_{\infty}$.

I. Começamos pela equação de atualização iterativa:
   $$v_{k+1}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] $$

II. Subtraímos $v_{\pi}(s)$ de ambos os lados da equação:
   $$v_{k+1}(s) - v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] - v_{\pi}(s)$$

III. Sabendo que $v_{\pi}(s)$ satisfaz a equação de Bellman:
    $$v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_{\pi}(s')] $$
    Substituímos $v_{\pi}(s)$ na equação do passo II:
    $$v_{k+1}(s) - v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] - \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_{\pi}(s')] $$

IV. Simplificando a equação, obtemos:
    $$v_{k+1}(s) - v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [\gamma (v_k(s') - v_{\pi}(s'))] $$

V. Tomando o valor absoluto de ambos os lados:
    $$|v_{k+1}(s) - v_{\pi}(s)| = |\sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [\gamma (v_k(s') - v_{\pi}(s'))]|$$
    Aplicando a desigualdade triangular:
    $$|v_{k+1}(s) - v_{\pi}(s)| \leq \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) |\gamma (v_k(s') - v_{\pi}(s'))|$$
    $$|v_{k+1}(s) - v_{\pi}(s)| \leq \gamma \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) |v_k(s') - v_{\pi}(s')|$$

VI. Como $\sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) = 1$, podemos limitar a expressão por:
     $$|v_{k+1}(s) - v_{\pi}(s)| \leq \gamma \max_{s' \in S} |v_k(s') - v_{\pi}(s')|$$
     $$|v_{k+1}(s) - v_{\pi}(s)| \leq \gamma ||v_k - v_{\pi}||_{\infty}$$

VII. Finalmente, tomando o máximo sobre todos os estados $s \in S$:
     $$\max_{s \in S} |v_{k+1}(s) - v_{\pi}(s)| \leq \gamma ||v_k - v_{\pi}||_{\infty}$$
     $$||v_{k+1} - v_{\pi}||_{\infty} \leq \gamma ||v_k - v_{\pi}||_{\infty}$$

Portanto, demonstramos que $||v_{k+1} - v_{\pi}||_{\infty} \leq \gamma ||v_k - v_{\pi}||_{\infty}$. $\blacksquare$

Este teorema garante que o erro máximo diminui a cada iteração, desde que $\gamma < 1$.

> 💡 **Exemplo Numérico (Erro Máximo e Contração):**
>
> Suponha que $v_{\pi}(s_1) = 2$ e $v_{\pi}(s_2) = 1$ e que na iteração $k$, $v_k(s_1) = 1.5$ e $v_k(s_2) = 0.8$, então:
>
> $||v_k - v_{\pi}||_{\infty} = \max(|1.5 - 2|, |0.8 - 1|) = \max(0.5, 0.2) = 0.5$.
>
> Se $\gamma = 0.9$, o teorema afirma que $||v_{k+1} - v_{\pi}||_{\infty} \leq 0.9 * 0.5 = 0.45$. Isso significa que o erro máximo na próxima iteração será menor ou igual a 0.45, demonstrando a propriedade de contração.

### Convergência
A convergência do algoritmo de policy evaluation iterativa é garantida sob certas condições. Em particular, a convergência para $v_{\pi}$ ocorre à medida que $k \rightarrow \infty$ sob as mesmas condições que garantem a existência de $v_{\pi}$ [^74]. Estas condições são satisfeitas se $\gamma < 1$ ou se a terminação eventual for garantida a partir de todos os estados sob a política $\pi$ [^74].

### Conclusão
O algoritmo de policy evaluation iterativa fornece um método fundamental para estimar a função valor de estado para uma dada política [^74]. Através da aplicação repetida da equação de Bellman, o algoritmo refina iterativamente as estimativas de valor até que a função valor convirja [^74]. A versão *in-place* otimiza o uso de memória e geralmente acelera a convergência [^75]. A terminação prática do algoritmo é alcançada quando as mudanças nos valores de estado se tornam suficientemente pequenas, garantindo uma aproximação precisa de $v_{\pi}$ num número finito de iterações [^75].

### Referências
[^73]: Chapter 4: Dynamic Programming.
[^74]: Section 4.1: Policy Evaluation (Prediction).
[^75]: Section 4.1: Iterative Policy Evaluation, for estimating $V \approx v_{\pi}$.
<!-- END -->