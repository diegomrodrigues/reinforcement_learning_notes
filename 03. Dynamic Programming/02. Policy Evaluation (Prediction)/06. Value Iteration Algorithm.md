## Iterative Policy Evaluation: Algoritmo e ConvergÃªncia

### IntroduÃ§Ã£o
O objetivo da **policy evaluation** Ã© calcular a funÃ§Ã£o valor de estado $v_{\pi}$ para uma polÃ­tica arbitrÃ¡ria $\pi$ [^74]. Como mencionado anteriormente, este processo Ã© fundamental para a resoluÃ§Ã£o do problema de *prediction* [^74]. Uma vez que as funÃ§Ãµes Ã³timas de valor ($v_*$ ou $q_*$) sÃ£o encontradas, Ã© possÃ­vel determinar facilmente as polÃ­ticas Ã³timas que satisfazem as equaÃ§Ãµes de otimalidade de Bellman [^73]. Esta seÃ§Ã£o detalha o algoritmo iterativo para policy evaluation, enfatizando a sua aplicaÃ§Ã£o repetida e as condiÃ§Ãµes de convergÃªncia.

### Algoritmo de Policy Evaluation Iterativa
O algoritmo de **policy evaluation iterativa** baseia-se na aplicaÃ§Ã£o repetida da equaÃ§Ã£o de Bellman para $v_{\pi}$ [^74]. Dado que as dinÃ¢micas do ambiente sÃ£o completamente conhecidas, a equaÃ§Ã£o (4.4) [^74] representa um sistema de $|S|$ equaÃ§Ãµes lineares simultÃ¢neas com $|S|$ incÃ³gnitas, sendo estas $v_{\pi}(s)$, para $s \in S$ [^74]. Embora a soluÃ§Ã£o direta deste sistema seja teoricamente possÃ­vel, os mÃ©todos iterativos sÃ£o mais prÃ¡ticos [^74].

O algoritmo comeÃ§a com uma sequÃªncia de funÃ§Ãµes de valor aproximadas $v_0, v_1, v_2, \dots$, em que cada $v_k$ mapeia $S^+$ para $\mathbb{R}$ [^74]. A aproximaÃ§Ã£o inicial $v_0$ Ã© escolhida arbitrariamente, exceto que o estado terminal (se existir) deve ter um valor de 0 [^74]. Cada aproximaÃ§Ã£o sucessiva Ã© obtida usando a equaÃ§Ã£o de Bellman (4.4) como uma regra de atualizaÃ§Ã£o:
$$
v_{k+1}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] \quad \forall s \in S
$$
Esta equaÃ§Ã£o (4.5) [^74] Ã© aplicada iterativamente para atualizar as estimativas de valor atÃ© que a funÃ§Ã£o valor convirja para $v_{\pi}$ [^74]. Claramente, $v_k = v_{\pi}$ Ã© um ponto fixo para esta regra de atualizaÃ§Ã£o, visto que a equaÃ§Ã£o de Bellman garante a igualdade neste caso [^74].

> ğŸ’¡ **Exemplo NumÃ©rico:**
>
> Considere um ambiente simples com 3 estados: $S = \{s_1, s_2, s_3\}$, onde $s_3$ Ã© o estado terminal, e uma polÃ­tica $\pi$ que define as probabilidades de aÃ§Ã£o para cada estado. Assume-se que $\gamma = 0.9$ e que temos as seguintes recompensas e probabilidades de transiÃ§Ã£o:
>
> - $p(s_2, -1 | s_1, a) = 1$ (tomar aÃ§Ã£o 'a' em $s_1$ leva a $s_2$ com recompensa -1)
> - $p(s_3, 0 | s_2, a) = 1$ (tomar aÃ§Ã£o 'a' em $s_2$ leva a $s_3$ com recompensa 0)
> - $v_0(s_1) = 0$, $v_0(s_2) = 0$, $v_0(s_3) = 0$ (inicializaÃ§Ã£o)
>
> IteraÃ§Ã£o 1:
>
> - $v_1(s_1) = -1 + 0.9 \cdot v_0(s_2) = -1 + 0.9 \cdot 0 = -1$
> - $v_1(s_2) = 0 + 0.9 \cdot v_0(s_3) = 0 + 0.9 \cdot 0 = 0$
> - $v_1(s_3) = 0$ (estado terminal)
>
> IteraÃ§Ã£o 2:
>
> - $v_2(s_1) = -1 + 0.9 \cdot v_1(s_2) = -1 + 0.9 \cdot 0 = -1$
> - $v_2(s_2) = 0 + 0.9 \cdot v_1(s_3) = 0 + 0.9 \cdot 0 = 0$
> - $v_2(s_3) = 0$
>
> Neste caso simples, o algoritmo converge apÃ³s uma iteraÃ§Ã£o para os valores de estado $v(s_1) = -1$, $v(s_2) = 0$ e $v(s_3) = 0$.

**Iterative Policy Evaluation**

O pseudocÃ³digo fornecido [^75] descreve uma versÃ£o *in-place* do algoritmo, que utiliza uma Ãºnica matriz para armazenar as funÃ§Ãµes de valor [^75]. Isto significa que o algoritmo atualiza os valores "in-place", substituindo o valor antigo de um estado pelo novo valor imediatamente [^75]. A ordem em que os estados sÃ£o atualizados durante cada *sweep* influencia significativamente a taxa de convergÃªncia [^75].

> ğŸ’¡ **Exemplo NumÃ©rico (In-Place vs. Two Matrices):**
>
> Considere um ambiente com dois estados $s_1$ e $s_2$, com $\gamma = 0.5$.
>
> - $p(s_2, 2 | s_1, a) = 1$
> - $p(s_1, 0 | s_2, a) = 1$
> - $v_0(s_1) = 0$, $v_0(s_2) = 0$
>
> *In-Place*: Atualizamos $s_1$ primeiro, depois $s_2$.
>
> - $v_1(s_1) = 2 + 0.5 * v_0(s_2) = 2$
> - $v_1(s_2) = 0 + 0.5 * v_1(s_1) = 0 + 0.5 * 2 = 1$
>
> *Two Matrices*: Calculamos todos os novos valores usando os valores da iteraÃ§Ã£o anterior.
>
> - $v_1(s_1) = 2 + 0.5 * v_0(s_2) = 2$
> - $v_1(s_2) = 0 + 0.5 * v_0(s_1) = 0$
>
> Observe que a atualizaÃ§Ã£o *in-place* usa o valor recÃ©m-atualizado de $s_1$ para calcular o valor de $s_2$, enquanto a atualizaÃ§Ã£o com duas matrizes usa os valores da iteraÃ§Ã£o anterior para ambos.

A versÃ£o *in-place* do algoritmo converge para $v_{\pi}$, e geralmente converge mais rapidamente do que a versÃ£o com duas matrizes, pois utiliza os dados mais recentes assim que estÃ£o disponÃ­veis [^75]. A atualizaÃ§Ã£o Ã© considerada um **expected update**, pois Ã© baseada numa expectativa sobre todos os possÃ­veis estados seguintes, em vez de numa Ãºnica amostra [^75].

**PseudocÃ³digo:**

```
Input Ï€, the policy to be evaluated
Algorithm parameter: a small threshold Î¸ > 0 determining accuracy of estimation
Initialize V(s) arbitrarily, for all s âˆˆ S, and V(terminal) to 0

Loop:
    Î” â† 0
    Loop for each s âˆˆ S:
        v â† V(s)
        V(s) â† Î£a Ï€(a|s) Î£s',r p(s',r|s,a) [r + Î³V(s')]
        Î” â† max(Î”, |v â€“ V(s)|)
    until Î” < Î¸
```

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

Neste pseudocÃ³digo, $\Delta$ acumula a maior diferenÃ§a absoluta entre os valores antigos e novos, permitindo que o algoritmo pare quando as alteraÃ§Ãµes se tornam suficientemente pequenas [^75]. Formalmente, a policy evaluation iterativa converge apenas no limite, mas, na prÃ¡tica, Ã© interrompida quando a variaÃ§Ã£o mÃ¡xima dos valores de estado entre as iteraÃ§Ãµes ($max_{s \in S} |v_{k+1}(s) - v_k(s)|$) Ã© inferior a um limiar $\theta$ [^75].

> ğŸ’¡ **Exemplo NumÃ©rico (ConvergÃªncia com Threshold):**
>
> Considere um ambiente com dois estados $s_1$ e $s_2$, com $\gamma = 0.5$ e $\theta = 0.01$.
>
> - $p(s_2, 1 | s_1, a) = 1$
> - $p(s_1, 0 | s_2, a) = 1$
> - $v_0(s_1) = 0$, $v_0(s_2) = 0$
>
> IteraÃ§Ãµes:
>
> 1.  $v_1(s_1) = 1$, $v_1(s_2) = 0$  $\Delta = 1$
> 2.  $v_2(s_1) = 1$, $v_2(s_2) = 0.5$ $\Delta = 0.5$
> 3.  $v_3(s_1) = 1.25$, $v_3(s_2) = 0.5$ $\Delta = 0.25$
> 4.  $v_4(s_1) = 1.25$, $v_4(s_2) = 0.625$ $\Delta = 0.125$
> 5.  $v_5(s_1) = 1.3125$, $v_5(s_2) = 0.625$ $\Delta = 0.0625$
> 6.  $v_6(s_1) = 1.3125$, $v_6(s_2) = 0.65625$ $\Delta = 0.03125$
> 7.  $v_7(s_1) = 1.328125$, $v_7(s_2) = 0.65625$ $\Delta = 0.015625$
> 8.  $v_8(s_1) = 1.328125$, $v_8(s_2) = 0.6640625$ $\Delta = 0.0078125$
>
> O algoritmo para na iteraÃ§Ã£o 8, pois $\Delta < \theta = 0.01$.

Para complementar a compreensÃ£o da atualizaÃ§Ã£o *in-place*, podemos formalizar a sua relaÃ§Ã£o com a atualizaÃ§Ã£o com duas matrizes.

**Lema 1:** Seja $V_{k+1}^{in}(s)$ o valor do estado $s$ na iteraÃ§Ã£o $k+1$ usando a atualizaÃ§Ã£o *in-place*, e $V_{k+1}^{two}(s)$ o valor do estado $s$ na iteraÃ§Ã£o $k+1$ usando duas matrizes. EntÃ£o, para cada $s \in S$,

$V_{k+1}^{in}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma V_k'(s')]$,

onde $V_k'(s') = V_{k+1}^{in}(s')$ se o estado $s'$ foi atualizado antes de $s$ na iteraÃ§Ã£o $k+1$, e $V_k'(s') = V_k(s')$ caso contrÃ¡rio.

*Proof:* A atualizaÃ§Ã£o *in-place* utiliza os valores mais recentes disponÃ­veis. Se um estado $s'$ jÃ¡ foi atualizado na iteraÃ§Ã£o $k+1$, o seu novo valor $V_{k+1}^{in}(s')$ Ã© usado. Caso contrÃ¡rio, o valor da iteraÃ§Ã£o anterior $V_k(s')$ Ã© utilizado. $\blacksquare$

AlÃ©m disso, podemos definir uma medida de erro para cada iteraÃ§Ã£o.

**DefiniÃ§Ã£o:** O erro mÃ¡ximo na iteraÃ§Ã£o $k$ Ã© definido como $e_k = \max_{s \in S} |v_k(s) - v_{\pi}(s)|$.

**Teorema 2:** O algoritmo de policy evaluation iterativa Ã© uma contraÃ§Ã£o em relaÃ§Ã£o Ã  norma do mÃ¡ximo. Especificamente,

$||v_{k+1} - v_{\pi}||_{\infty} \leq \gamma ||v_k - v_{\pi}||_{\infty}$,

onde $||v||_{\infty} = \max_{s \in S} |v(s)|$.

*Proof:* Provaremos que $||v_{k+1} - v_{\pi}||_{\infty} \leq \gamma ||v_k - v_{\pi}||_{\infty}$.

I. ComeÃ§amos pela equaÃ§Ã£o de atualizaÃ§Ã£o iterativa:
   $$v_{k+1}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] $$

II. SubtraÃ­mos $v_{\pi}(s)$ de ambos os lados da equaÃ§Ã£o:
   $$v_{k+1}(s) - v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] - v_{\pi}(s)$$

III. Sabendo que $v_{\pi}(s)$ satisfaz a equaÃ§Ã£o de Bellman:
    $$v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_{\pi}(s')] $$
    SubstituÃ­mos $v_{\pi}(s)$ na equaÃ§Ã£o do passo II:
    $$v_{k+1}(s) - v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] - \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_{\pi}(s')] $$

IV. Simplificando a equaÃ§Ã£o, obtemos:
    $$v_{k+1}(s) - v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [\gamma (v_k(s') - v_{\pi}(s'))] $$

V. Tomando o valor absoluto de ambos os lados:
    $$|v_{k+1}(s) - v_{\pi}(s)| = |\sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [\gamma (v_k(s') - v_{\pi}(s'))]|$$
    Aplicando a desigualdade triangular:
    $$|v_{k+1}(s) - v_{\pi}(s)| \leq \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) |\gamma (v_k(s') - v_{\pi}(s'))|$$
    $$|v_{k+1}(s) - v_{\pi}(s)| \leq \gamma \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) |v_k(s') - v_{\pi}(s')|$$

VI. Como $\sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) = 1$, podemos limitar a expressÃ£o por:
     $$|v_{k+1}(s) - v_{\pi}(s)| \leq \gamma \max_{s' \in S} |v_k(s') - v_{\pi}(s')|$$
     $$|v_{k+1}(s) - v_{\pi}(s)| \leq \gamma ||v_k - v_{\pi}||_{\infty}$$

VII. Finalmente, tomando o mÃ¡ximo sobre todos os estados $s \in S$:
     $$\max_{s \in S} |v_{k+1}(s) - v_{\pi}(s)| \leq \gamma ||v_k - v_{\pi}||_{\infty}$$
     $$||v_{k+1} - v_{\pi}||_{\infty} \leq \gamma ||v_k - v_{\pi}||_{\infty}$$

Portanto, demonstramos que $||v_{k+1} - v_{\pi}||_{\infty} \leq \gamma ||v_k - v_{\pi}||_{\infty}$. $\blacksquare$

Este teorema garante que o erro mÃ¡ximo diminui a cada iteraÃ§Ã£o, desde que $\gamma < 1$.

> ğŸ’¡ **Exemplo NumÃ©rico (Erro MÃ¡ximo e ContraÃ§Ã£o):**
>
> Suponha que $v_{\pi}(s_1) = 2$ e $v_{\pi}(s_2) = 1$ e que na iteraÃ§Ã£o $k$, $v_k(s_1) = 1.5$ e $v_k(s_2) = 0.8$, entÃ£o:
>
> $||v_k - v_{\pi}||_{\infty} = \max(|1.5 - 2|, |0.8 - 1|) = \max(0.5, 0.2) = 0.5$.
>
> Se $\gamma = 0.9$, o teorema afirma que $||v_{k+1} - v_{\pi}||_{\infty} \leq 0.9 * 0.5 = 0.45$. Isso significa que o erro mÃ¡ximo na prÃ³xima iteraÃ§Ã£o serÃ¡ menor ou igual a 0.45, demonstrando a propriedade de contraÃ§Ã£o.

### ConvergÃªncia
A convergÃªncia do algoritmo de policy evaluation iterativa Ã© garantida sob certas condiÃ§Ãµes. Em particular, a convergÃªncia para $v_{\pi}$ ocorre Ã  medida que $k \rightarrow \infty$ sob as mesmas condiÃ§Ãµes que garantem a existÃªncia de $v_{\pi}$ [^74]. Estas condiÃ§Ãµes sÃ£o satisfeitas se $\gamma < 1$ ou se a terminaÃ§Ã£o eventual for garantida a partir de todos os estados sob a polÃ­tica $\pi$ [^74].

### ConclusÃ£o
O algoritmo de policy evaluation iterativa fornece um mÃ©todo fundamental para estimar a funÃ§Ã£o valor de estado para uma dada polÃ­tica [^74]. AtravÃ©s da aplicaÃ§Ã£o repetida da equaÃ§Ã£o de Bellman, o algoritmo refina iterativamente as estimativas de valor atÃ© que a funÃ§Ã£o valor convirja [^74]. A versÃ£o *in-place* otimiza o uso de memÃ³ria e geralmente acelera a convergÃªncia [^75]. A terminaÃ§Ã£o prÃ¡tica do algoritmo Ã© alcanÃ§ada quando as mudanÃ§as nos valores de estado se tornam suficientemente pequenas, garantindo uma aproximaÃ§Ã£o precisa de $v_{\pi}$ num nÃºmero finito de iteraÃ§Ãµes [^75].

### ReferÃªncias
[^73]: Chapter 4: Dynamic Programming.
[^74]: Section 4.1: Policy Evaluation (Prediction).
[^75]: Section 4.1: Iterative Policy Evaluation, for estimating $V \approx v_{\pi}$.
<!-- END -->