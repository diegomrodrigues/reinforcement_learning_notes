## Converg√™ncia e Implementa√ß√£o da Avalia√ß√£o de Pol√≠tica Iterativa

### Introdu√ß√£o
Em continuidade √† discuss√£o sobre **avalia√ß√£o de pol√≠tica (prediction)** [^74], este cap√≠tulo aprofunda-se na an√°lise da converg√™ncia da sequ√™ncia de aproxima√ß√µes da fun√ß√£o valor, $\{v_k\}$, em dire√ß√£o √† fun√ß√£o valor real, $v_\pi$, sob uma pol√≠tica $\pi$ espec√≠fica. Exploraremos tamb√©m as diferentes abordagens de implementa√ß√£o do algoritmo de avalia√ß√£o de pol√≠tica iterativa, destacando as vantagens e desvantagens de cada uma em termos de converg√™ncia e utiliza√ß√£o de mem√≥ria.

### Converg√™ncia da Sequ√™ncia $\{v_k\}$
O algoritmo de **avalia√ß√£o de pol√≠tica iterativa** [^74] gera uma sequ√™ncia de fun√ß√µes valor aproximadas, denotadas por $\{v_k\}$, onde cada $v_k$ √© uma estimativa da fun√ß√£o valor real, $v_\pi$, para uma pol√≠tica $\pi$ fixa. Um aspecto fundamental √© garantir que esta sequ√™ncia convirja para $v_\pi$, assegurando que, ap√≥s um n√∫mero suficiente de itera√ß√µes, obtenhamos uma representa√ß√£o precisa do valor de cada estado sob a pol√≠tica em quest√£o.

O contexto [^74] afirma que *a sequ√™ncia {vk} pode ser mostrada em geral para convergir para œÖœÄ as k ‚Üí ‚àû sob as mesmas condi√ß√µes que garantem a exist√™ncia de œÖœÄ*. Isso significa que, desde que a taxa de desconto $\gamma$ seja menor que 1 ($\gamma < 1$) ou que a termina√ß√£o eventual seja garantida a partir de todos os estados sob a pol√≠tica $\pi$, a sequ√™ncia $\{v_k\}$ ir√° convergir para a fun√ß√£o valor real $v_\pi$. Matematicamente, isso pode ser expresso como:

$$
\lim_{k \to \infty} v_k(s) = v_\pi(s), \quad \forall s \in S
$$

Essa converg√™ncia √© crucial porque permite que o algoritmo de avalia√ß√£o de pol√≠tica iterativa seja uma ferramenta confi√°vel para estimar a fun√ß√£o valor de uma pol√≠tica dada.

> üí° **Exemplo Num√©rico:** Considere um ambiente simples com 3 estados (S = {s1, s2, s3}) e uma pol√≠tica œÄ que define as a√ß√µes em cada estado. Suponha que ap√≥s algumas itera√ß√µes, tenhamos as seguintes fun√ß√µes valor aproximadas:
>
> *   v‚ÇÄ(s1) = 0, v‚ÇÄ(s2) = 0, v‚ÇÄ(s3) = 0
> *   v‚ÇÅ(s1) = 0.5, v‚ÇÅ(s2) = 1.0, v‚ÇÅ(s3) = 1.5
> *   v‚ÇÇ(s1) = 0.75, v‚ÇÇ(s2) = 1.25, v‚ÇÇ(s3) = 1.75
> *   v‚ÇÉ(s1) = 0.875, v‚ÇÉ(s2) = 1.375, v‚ÇÉ(s3) = 1.875
>
> Se continuarmos as itera√ß√µes, a sequ√™ncia $\{v_k\}$ converger√° para a fun√ß√£o valor real $v_\pi$. Por exemplo, se vœÄ(s1) = 1, vœÄ(s2) = 1.5, e vœÄ(s3) = 2, vemos que a sequ√™ncia se aproxima desses valores.
>
> Este exemplo ilustra numericamente como as estimativas da fun√ß√£o valor se aproximam da fun√ß√£o valor real √† medida que o n√∫mero de itera√ß√µes aumenta.

**Teorema 1:** (Teorema da Contra√ß√£o) A atualiza√ß√£o da fun√ß√£o valor na avalia√ß√£o de pol√≠tica iterativa √© uma contra√ß√£o de Bellman com fator $\gamma$ na norma do supremo.

*Prova:* Seja $v_\pi$ a fun√ß√£o valor √≥tima e $v_k$ a fun√ß√£o valor na itera√ß√£o k. Definimos a norma do supremo como $||v|| = \max_{s \in S} |v(s)|$. Queremos mostrar que $||v_{k+1} - v_\pi|| \le \gamma ||v_k - v_\pi||$.

I. Temos:
   $$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s]$$
   $$v_{\pi}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]$$

II. Subtraindo as duas equa√ß√µes:
    $$v_{k+1}(s) - v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] - \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]$$
    $$v_{k+1}(s) - v_\pi(s) = \mathbb{E}_\pi[\gamma (v_k(S_{t+1}) - v_\pi(S_{t+1})) | S_t = s]$$

III. Tomando o valor absoluto e usando a desigualdade triangular e a propriedade de valor esperado:
     $$|v_{k+1}(s) - v_\pi(s)| = |\mathbb{E}_\pi[\gamma (v_k(S_{t+1}) - v_\pi(S_{t+1})) | S_t = s]|$$
     $$|v_{k+1}(s) - v_\pi(s)| \le \mathbb{E}_\pi[|\gamma (v_k(S_{t+1}) - v_\pi(S_{t+1}))| | S_t = s]$$
     $$|v_{k+1}(s) - v_\pi(s)| \le \gamma \mathbb{E}_\pi[|v_k(S_{t+1}) - v_\pi(S_{t+1})| | S_t = s]$$

IV. Como $||v_k - v_\pi|| = \max_{s \in S} |v_k(s) - v_\pi(s)|$, temos $|v_k(S_{t+1}) - v_\pi(S_{t+1})| \le ||v_k - v_\pi||$ para todo $S_{t+1}$.  Ent√£o,

    $$|v_{k+1}(s) - v_\pi(s)| \le \gamma \mathbb{E}_\pi[||v_k - v_\pi|| | S_t = s] = \gamma ||v_k - v_\pi||$$

V.  Portanto, $\max_s |v_{k+1}(s) - v_\pi(s)| \le \gamma ||v_k - v_\pi||$, o que implica $||v_{k+1} - v_\pi|| \le \gamma ||v_k - v_\pi||$. Isso demonstra que a atualiza√ß√£o √© uma contra√ß√£o de Bellman com fator Œ≥. Como Œ≥ < 1, a sequ√™ncia converge para um √∫nico ponto fixo, que √© a fun√ß√£o valor √≥tima $v_\pi$. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $\gamma = 0.9$ e suponha que na itera√ß√£o *k*, a diferen√ßa m√°xima entre a fun√ß√£o valor aproximada e a fun√ß√£o valor real seja $||v_k - v_\pi|| = 2$.  Ent√£o, ap√≥s uma itera√ß√£o, teremos $||v_{k+1} - v_\pi|| \le 0.9 * 2 = 1.8$. Isso demonstra que a diferen√ßa m√°xima diminui a cada itera√ß√£o, confirmando a converg√™ncia.

### Implementa√ß√µes com Dois Arrays vs. "In-Place"
O contexto [^75] descreve duas abordagens principais para implementar o algoritmo de avalia√ß√£o de pol√≠tica iterativa, que se diferenciam na forma como os valores das fun√ß√µes valor aproximadas s√£o armazenados e atualizados:

1.  **Implementa√ß√£o com Dois Arrays:** Esta abordagem utiliza dois arrays, um para armazenar os valores da fun√ß√£o valor na itera√ß√£o anterior, $v_k(s)$, e outro para armazenar os novos valores calculados na itera√ß√£o atual, $v_{k+1}(s)$. Os novos valores s√£o calculados com base nos valores antigos, e somente ap√≥s o c√°lculo de todos os novos valores, o array de valores antigos √© substitu√≠do pelo array de novos valores.
    Essa implementa√ß√£o garante que todos os estados sejam atualizados simultaneamente usando os valores da itera√ß√£o anterior. A atualiza√ß√£o √© feita usando a Equa√ß√£o 4.5 [^74]:

    $$
    v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s',r} p(s',r|s,a) [r + \gamma v_k(s')]
    $$

2.  **Implementa√ß√£o "In-Place":** Nesta abordagem, apenas um array √© utilizado para armazenar os valores da fun√ß√£o valor. Os valores s√£o atualizados "no local", ou seja, cada novo valor $v_{k+1}(s)$ sobrescreve imediatamente o valor antigo $v_k(s)$ no mesmo array.

A principal diferen√ßa entre as duas implementa√ß√µes reside no fato de que a implementa√ß√£o "in-place" utiliza os valores mais recentes dispon√≠veis durante a atualiza√ß√£o, que podem ser valores da itera√ß√£o atual (j√° atualizados) ou da itera√ß√£o anterior (ainda n√£o atualizados), dependendo da ordem em que os estados s√£o visitados. O contexto [^75] salienta que *√†s vezes novos valores s√£o usados em vez de antigos no lado direito de (4.5)*, na implementa√ß√£o "in-place".

**Proposi√ß√£o 1:** A implementa√ß√£o "in-place" pode ser vista como uma forma de atualiza√ß√£o ass√≠ncrona, onde cada estado √© atualizado individualmente com as informa√ß√µes mais recentes dispon√≠veis.

*Prova:* Na implementa√ß√£o "in-place", a atualiza√ß√£o de um estado $s$ utiliza os valores dos estados sucessores $s'$. Se $s'$ j√° foi atualizado na itera√ß√£o atual, ent√£o o novo valor $v_{k+1}(s')$ √© utilizado. Caso contr√°rio, o valor $v_k(s')$ da itera√ß√£o anterior √© utilizado. Isso significa que a atualiza√ß√£o de $s$ n√£o depende de todos os estados serem atualizados simultaneamente, caracterizando uma atualiza√ß√£o ass√≠ncrona.

I. Seja $S$ o conjunto de todos os estados e $s \in S$ um estado espec√≠fico.
II. Na atualiza√ß√£o "in-place", o valor de $s$ na itera√ß√£o $k+1$, denotado por $v_{k+1}(s)$, √© calculado usando a seguinte equa√ß√£o:

   $$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]$$

III. A diferen√ßa fundamental √© que $v(S_{t+1})$ pode ser $v_{k+1}(S_{t+1})$ se $S_{t+1}$ j√° foi atualizado na itera√ß√£o $k+1$, ou $v_k(S_{t+1})$ se $S_{t+1}$ ainda n√£o foi atualizado.

IV. Formalmente, seja $U$ o conjunto de estados j√° atualizados na itera√ß√£o $k+1$ antes da atualiza√ß√£o de $s$. Ent√£o, $v(S_{t+1})$ pode ser descrito como:

   $$v(S_{t+1}) = \begin{cases} v_{k+1}(S_{t+1}) & \text{se } S_{t+1} \in U \\ v_k(S_{t+1}) & \text{se } S_{t+1} \notin U \end{cases}$$

V. Como diferentes estados podem usar valores de diferentes itera√ß√µes (k ou k+1) durante a atualiza√ß√£o de um estado, a atualiza√ß√£o n√£o √© s√≠ncrona, pois n√£o depende de todos os valores serem da mesma itera√ß√£o. Portanto, a implementa√ß√£o "in-place" representa uma forma de atualiza√ß√£o ass√≠ncrona. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um ambiente com dois estados, s1 e s2. Suponha que a atualiza√ß√£o "in-place" ocorra na ordem s1, depois s2. Seja $\gamma = 0.9$, R(s1) = 1 e R(s2) = 2. Inicialmente, v‚ÇÄ(s1) = 0 e v‚ÇÄ(s2) = 0.
>
> *   **Itera√ß√£o 1:**
>     *   Atualizando s1: v‚ÇÅ(s1) = R(s1) + Œ≥ * v‚ÇÄ(s2) = 1 + 0.9 * 0 = 1
>     *   Atualizando s2: v‚ÇÅ(s2) = R(s2) + Œ≥ * v‚ÇÅ(s1) = 2 + 0.9 * 1 = 2.9 (Note que usamos o valor *atualizado* de v‚ÇÅ(s1))
>
> Agora, se tiv√©ssemos usado a implementa√ß√£o com dois arrays, ter√≠amos v‚ÇÅ(s2) = R(s2) + Œ≥ * v‚ÇÄ(s1) = 2 + 0.9 * 0 = 2. A diferen√ßa no valor de v‚ÇÅ(s2) demonstra como a implementa√ß√£o "in-place" incorpora informa√ß√µes mais recentes.





![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

### Vantagens e Desvantagens

*   **Implementa√ß√£o com Dois Arrays:**
    *   *Vantagem:* Garante a utiliza√ß√£o de valores consistentes da itera√ß√£o anterior para todos os estados, simplificando a an√°lise te√≥rica e a depura√ß√£o.
    *   *Desvantagem:* Requer o dobro da mem√≥ria para armazenar os dois arrays, o que pode ser limitante em problemas com um grande n√∫mero de estados.

*   **Implementa√ß√£o "In-Place":**
    *   *Vantagem:* Utiliza menos mem√≥ria, pois requer apenas um array.
    *   *Vantagem:* Geralmente converge mais rapidamente na pr√°tica, pois incorpora as informa√ß√µes mais recentes dispon√≠veis.
    *   *Desvantagem:* A ordem em que os estados s√£o atualizados pode influenciar significativamente a taxa de converg√™ncia. Uma ordem inadequada pode levar a uma converg√™ncia mais lenta ou at√© mesmo a oscila√ß√µes. A an√°lise te√≥rica da converg√™ncia pode ser mais complexa devido √† utiliza√ß√£o de valores de diferentes itera√ß√µes.

> üí° **Exemplo Num√©rico:** Considere um jogo em uma grade 10x10 (100 estados). Implementa√ß√£o com dois arrays precisaria armazenar 200 valores. "In-place" precisa de 100, economizando mem√≥ria. Mas, se a ordem de varredura "in-place" for ruim (e.g., aleat√≥ria), pode levar mais itera√ß√µes para convergir comparado a dois arrays com uma pol√≠tica √≥tima.

### Influ√™ncia da Ordem de Atualiza√ß√£o na Implementa√ß√£o "In-Place"
Como mencionado anteriormente, a ordem em que os estados s√£o atualizados na implementa√ß√£o "in-place" pode ter um impacto significativo na taxa de converg√™ncia [^75]. O contexto [^75] explica que *a ordem em que os estados t√™m seus valores atualizados durante o sweep tem uma influ√™ncia significativa na taxa de converg√™ncia*.

Em geral, uma ordem que propague rapidamente as informa√ß√µes de valor pelos estados tende a levar a uma converg√™ncia mais r√°pida. Por exemplo, em um problema em que os estados est√£o dispostos em uma grade, atualizar os estados em uma ordem que siga um padr√£o "varrendo" a grade pode ser eficiente. No entanto, a melhor ordem de atualiza√ß√£o pode depender da estrutura espec√≠fica do problema e da pol√≠tica em avalia√ß√£o.

**Lema 1:** Se a ordem de atualiza√ß√£o dos estados na implementa√ß√£o "in-place" seguir a ordem de um caminho √≥timo, a converg√™ncia pode ser acelerada.

*Prova (Esbo√ßo):*  Suponha que exista um caminho √≥timo para um estado objetivo. Se atualizarmos os estados ao longo deste caminho, come√ßando pelo estado mais pr√≥ximo do objetivo, a informa√ß√£o de valor ser√° propagada mais rapidamente para o estado inicial. Isso ocorre porque cada atualiza√ß√£o ao longo do caminho incorpora a melhor estimativa dispon√≠vel do valor do estado sucessor, acelerando a converg√™ncia para os estados anteriores no caminho.

I. Considere um problema onde existe um estado objetivo $s_G$ e um caminho √≥timo $P = (s_1, s_2, \ldots, s_n = s_G)$ do estado inicial $s_1$ para $s_G$.

II. Se a atualiza√ß√£o "in-place" seguir a ordem inversa do caminho √≥timo, ou seja, $s_G, s_{n-1}, \ldots, s_1$, ent√£o a informa√ß√£o de valor de $s_G$ ser√° propagada rapidamente para $s_1$.

III. Na primeira itera√ß√£o, $v(s_G)$ ser√° atualizado primeiro. Em seguida, $v(s_{n-1})$ ser√° atualizado usando o valor atualizado de $v(s_G)$. Este processo continua at√© que $v(s_1)$ seja atualizado usando os valores atualizados de todos os estados no caminho √≥timo.

IV. Este processo garante que a informa√ß√£o de valor (uma estimativa melhorada do valor de estar em $s_G$) seja rapidamente incorporada nas estimativas de valor dos estados anteriores no caminho.

V. Em contraste, se a ordem de atualiza√ß√£o for aleat√≥ria ou oposta √† do caminho √≥timo, a informa√ß√£o de valor pode levar mais tempo para se propagar, resultando em uma converg√™ncia mais lenta. Portanto, atualizar os estados ao longo do caminho √≥timo (na ordem inversa) acelera a converg√™ncia. ‚ñ†

> üí° **Exemplo Num√©rico:** Imagine um labirinto unidimensional com estados numerados de 1 a 10, onde o objetivo √© chegar ao estado 10. Suponha que o caminho √≥timo seja 1 -> 2 -> 3 -> ... -> 10.
>
> *   **Ordem de Atualiza√ß√£o Ideal:** 10, 9, 8, ..., 1. Atualizar nesta ordem propaga rapidamente a recompensa do estado 10 para os estados anteriores.
> *   **Ordem de Atualiza√ß√£o Ruim:** 1, 2, 3, ..., 10. A informa√ß√£o de valor do estado 10 demora muito para influenciar os estados iniciais, resultando em uma converg√™ncia lenta.
>
> Em uma simula√ß√£o, a ordem ideal pode convergir em 10 itera√ß√µes, enquanto a ordem ruim pode levar 50 ou mais itera√ß√µes.

### Conclus√£o

A converg√™ncia da sequ√™ncia $\{v_k\}$ para $v_\pi$ √© um aspecto crucial do algoritmo de avalia√ß√£o de pol√≠tica iterativa, garantindo que as estimativas da fun√ß√£o valor se aproximem da fun√ß√£o valor real sob a pol√≠tica em quest√£o. As implementa√ß√µes com dois arrays e "in-place" oferecem diferentes compromissos entre utiliza√ß√£o de mem√≥ria e taxa de converg√™ncia, com a implementa√ß√£o "in-place" geralmente convergindo mais rapidamente, mas sendo mais sens√≠vel √† ordem de atualiza√ß√£o dos estados. A escolha da implementa√ß√£o e da ordem de atualiza√ß√£o deve ser feita considerando as caracter√≠sticas espec√≠ficas do problema em quest√£o. $\blacksquare$
<!-- END -->