## Avalia√ß√£o de Pol√≠tica (Predi√ß√£o)

### Introdu√ß√£o
Este cap√≠tulo foca em **Dynamic Programming (DP)** para calcular pol√≠ticas √≥timas em ambientes modelados como Processos de Decis√£o de Markov (MDPs) [^1]. Em continuidade com o Cap√≠tulo 3, onde as fun√ß√µes de valor foram definidas, exploraremos como DP pode ser usada para computar essas fun√ß√µes de valor, especificamente $v_*$ e $q_*$, que satisfazem as equa√ß√µes de otimalidade de Bellman [^1]. A **avalia√ß√£o de pol√≠tica**, tamb√©m conhecida como o **problema de predi√ß√£o**, √© o processo de computar a fun√ß√£o de valor de estado $v_\pi$ para uma pol√≠tica arbitr√°ria $\pi$ [^2].

### Conceitos Fundamentais

A avalia√ß√£o de pol√≠tica visa determinar a fun√ß√£o de valor de estado $v_\pi$ para uma determinada pol√≠tica $\pi$. Essa fun√ß√£o representa o retorno esperado ao seguir a pol√≠tica $\pi$ a partir de cada estado $s$ [^2]. Formalmente, a fun√ß√£o de valor de estado √© definida como:

$$v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$$

onde $G_t$ √© o retorno a partir do tempo $t$, e a expectativa √© tomada seguindo a pol√≠tica $\pi$ [^2]. Essa defini√ß√£o pode ser expandida usando a equa√ß√£o de Bellman para $v_\pi$ [^2]:

$$v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s]$$
$$= \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]$$
$$= \sum_a \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')] \quad (4.4) $$

Aqui, $\pi(a|s)$ √© a probabilidade de tomar a a√ß√£o $a$ no estado $s$ sob a pol√≠tica $\pi$, e $p(s', r|s, a)$ √© a probabilidade de transi√ß√£o para o estado $s'$ com recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$ [^2]. As expectativas s√£o indexadas por $\pi$ para indicar que s√£o condicionais a seguir $\pi$ [^2].

> üí° **Exemplo Num√©rico:** Considere um MDP com tr√™s estados ($S_1, S_2, S_3$) e uma pol√≠tica $\pi$ que sempre escolhe a a√ß√£o $a_1$ em todos os estados. As probabilidades de transi√ß√£o e recompensas s√£o as seguintes:
>
> *   $p(S_2, 5 | S_1, a_1) = 1.0$
> *   $p(S_3, -2 | S_2, a_1) = 1.0$
> *   $p(S_1, 1 | S_3, a_1) = 1.0$
>
> Seja $\gamma = 0.9$. Ent√£o, podemos calcular $v_\pi(S_1)$ usando a equa√ß√£o de Bellman:
>
> $v_\pi(S_1) = \sum_{s', r} p(s', r|S_1, a_1) [r + \gamma v_\pi(s')]$
> $v_\pi(S_1) = 1.0 * [5 + 0.9 * v_\pi(S_2)]$
>
> Da mesma forma,
>
> $v_\pi(S_2) = 1.0 * [-2 + 0.9 * v_\pi(S_3)]$
> $v_\pi(S_3) = 1.0 * [1 + 0.9 * v_\pi(S_1)]$
>
> Resolvendo este sistema de equa√ß√µes lineares:
>
> $v_\pi(S_1) = 5 + 0.9 * (-2 + 0.9 * (1 + 0.9 * v_\pi(S_1)))$
> $v_\pi(S_1) = 5 - 1.8 + 0.81 + 0.6561 * v_\pi(S_1)$
> $0.3439 * v_\pi(S_1) = 4.01$
> $v_\pi(S_1) \approx 11.66$
>
> Substituindo de volta:
>
> $v_\pi(S_2) = -2 + 0.9 * (1 + 0.9 * 11.66) \approx 8.40$
> $v_\pi(S_3) = 1 + 0.9 * 11.66 \approx 11.49$
>
> Portanto, a fun√ß√£o de valor para a pol√≠tica $\pi$ √© aproximadamente $v_\pi(S_1) \approx 11.66$, $v_\pi(S_2) \approx 8.40$, e $v_\pi(S_3) \approx 11.49$.

A exist√™ncia e a unicidade de $v_\pi$ s√£o garantidas se $\gamma < 1$ ou se a termina√ß√£o eventual for garantida a partir de todos os estados sob a pol√≠tica $\pi$ [^2]. Se a din√¢mica do ambiente √© completamente conhecida, a Equa√ß√£o (4.4) forma um sistema de $|S|$ equa√ß√µes lineares simult√¢neas em $|S|$ inc√≥gnitas, onde as inc√≥gnitas s√£o os valores de $v_\pi(s)$ para cada estado $s \in S$ [^2]. Em princ√≠pio, resolver este sistema √© direto, embora computacionalmente caro [^2].

**Lema 1.** *A contra√ß√£o de Bellman:* O operador de Bellman para avalia√ß√£o de pol√≠tica, denotado por $T^\pi$, √© uma contra√ß√£o sob a norma do supremo ||.||.

*Demonstra√ß√£o.* Seja $v$ e $v'$ duas fun√ß√µes de valor arbitr√°rias. Aplicando o operador de Bellman $T^\pi$ a ambas, obtemos:

$$T^\pi v(s) = \sum_a \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v(s')] $$
$$T^\pi v'(s) = \sum_a \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v'(s')] $$

A diferen√ßa entre as duas equa√ß√µes √©:

$$|T^\pi v(s) - T^\pi v'(s)| = |\sum_a \pi(a|s) \sum_{s', r} p(s', r|s, a) \gamma [v(s') - v'(s')] |$$
$$\leq \gamma \sum_a \pi(a|s) \sum_{s', r} p(s', r|s, a) |v(s') - v'(s')|$$
$$\leq \gamma \sum_a \pi(a|s) \sum_{s', r} p(s', r|s, a) ||v - v'||$$
$$= \gamma ||v - v'|| \sum_a \pi(a|s) \sum_{s', r} p(s', r|s, a)$$

Como $\sum_a \pi(a|s) = 1$ e $\sum_{s', r} p(s', r|s, a) = 1$, temos:

$$|T^\pi v(s) - T^\pi v'(s)| \leq \gamma ||v - v'||$$

Portanto, $||T^\pi v - T^\pi v'|| \leq \gamma ||v - v'||$. Dado que $\gamma < 1$, $T^\pi$ √© uma contra√ß√£o. ‚ñ†

**M√©todos Iterativos:** M√©todos iterativos s√£o mais adequados para os nossos prop√≥sitos. Considere uma sequ√™ncia de fun√ß√µes de valor aproximadas $v_0, v_1, v_2, \ldots$, cada uma mapeando $S^+$ para $\mathbb{R}$ [^2]. A aproxima√ß√£o inicial, $v_0$, √© escolhida arbitrariamente (exceto que o estado terminal, se houver, deve receber o valor 0), e cada aproxima√ß√£o sucessiva √© obtida usando a equa√ß√£o de Bellman para $v_\pi$ como uma regra de atualiza√ß√£o [^2]:

$$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s]$$
$$= \sum_a \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] \quad (4.5) $$

para todos os $s \in S$ [^2]. Claramente, $v_k = v_\pi$ √© um ponto fixo para esta regra de atualiza√ß√£o, pois a equa√ß√£o de Bellman para $v_\pi$ garante a igualdade neste caso [^2]. De fato, a sequ√™ncia $\{v_k\}$ pode ser mostrada em geral para convergir para $v_\pi$ quando $k \rightarrow \infty$ sob as mesmas condi√ß√µes que garantem a exist√™ncia de $v_\pi$ [^2]. Este algoritmo √© chamado de **iterative policy evaluation** [^2].

> üí° **Exemplo Num√©rico:** Usando o mesmo MDP do exemplo anterior, vamos realizar duas itera√ß√µes de avalia√ß√£o iterativa de pol√≠tica. Inicialize $v_0(s) = 0$ para todos os estados.
>
> **Itera√ß√£o 1:**
>
> $v_1(S_1) = 1.0 * [5 + 0.9 * v_0(S_2)] = 5 + 0.9 * 0 = 5$
> $v_1(S_2) = 1.0 * [-2 + 0.9 * v_0(S_3)] = -2 + 0.9 * 0 = -2$
> $v_1(S_3) = 1.0 * [1 + 0.9 * v_0(S_1)] = 1 + 0.9 * 0 = 1$
>
> **Itera√ß√£o 2:**
>
> $v_2(S_1) = 1.0 * [5 + 0.9 * v_1(S_2)] = 5 + 0.9 * (-2) = 3.2$
> $v_2(S_2) = 1.0 * [-2 + 0.9 * v_1(S_3)] = -2 + 0.9 * 1 = -1.1$
> $v_2(S_3) = 1.0 * [1 + 0.9 * v_1(S_1)] = 1 + 0.9 * 5 = 5.5$
>
> Ap√≥s duas itera√ß√µes, temos $v_2(S_1) = 3.2$, $v_2(S_2) = -1.1$, e $v_2(S_3) = 5.5$.  Observe que esses valores est√£o se aproximando dos valores de $v_\pi$ calculados anteriormente.

**Teorema 2.** *Converg√™ncia da Iterative Policy Evaluation:* A sequ√™ncia de fun√ß√µes de valor $\{v_k\}$ gerada pela iterative policy evaluation converge para $v_\pi$ sob a norma do supremo, ou seja, $||v_k - v_\pi|| \rightarrow 0$ quando $k \rightarrow \infty$.

*Demonstra√ß√£o.* Podemos expressar a equa√ß√£o (4.5) usando o operador de Bellman $T^\pi$ como $v_{k+1} = T^\pi v_k$. Seja $v_\pi$ o ponto fixo do operador $T^\pi$, ent√£o $v_\pi = T^\pi v_\pi$. Temos que:

$$||v_{k+1} - v_\pi|| = ||T^\pi v_k - T^\pi v_\pi||$$

Pelo Lema 1, $T^\pi$ √© uma contra√ß√£o com fator $\gamma$, ent√£o:

$$||T^\pi v_k - T^\pi v_\pi|| \leq \gamma ||v_k - v_\pi||$$

Aplicando esta desigualdade recursivamente, obtemos:

$$||v_{k+1} - v_\pi|| \leq \gamma^{k+1} ||v_0 - v_\pi||$$

Como $\gamma < 1$, $\gamma^{k+1} \rightarrow 0$ quando $k \rightarrow \infty$. Portanto,

$$||v_{k+1} - v_\pi|| \rightarrow 0 \quad \text{quando} \quad k \rightarrow \infty$$

Isso demonstra que a sequ√™ncia $\{v_k\}$ converge para $v_\pi$. ‚ñ†

Para produzir cada aproxima√ß√£o sucessiva, $v_{k+1}$ de $v_k$, a avalia√ß√£o iterativa da pol√≠tica aplica a mesma opera√ß√£o a cada estado $s$: ela substitui o valor antigo de $s$ por um novo valor obtido dos valores antigos dos estados sucessores de $s$ e das recompensas imediatas esperadas, ao longo de todas as transi√ß√µes de um passo poss√≠veis sob a pol√≠tica que est√° sendo avaliada [^2]. Chamamos este tipo de opera√ß√£o de **expected update** [^2]. Cada itera√ß√£o de avalia√ß√£o iterativa da pol√≠tica atualiza o valor de cada estado uma vez para produzir a nova fun√ß√£o de valor aproximada $v_{k+1}$ [^2].

Existem v√°rios tipos diferentes de *expected updates*, dependendo se um estado (como aqui) ou um par estado-a√ß√£o est√° sendo atualizado, e dependendo da maneira precisa como os valores estimados dos estados sucessores s√£o combinados [^3]. Todas as atualiza√ß√µes feitas nos algoritmos de DP s√£o chamadas *expected updates* porque s√£o baseadas em uma expectativa sobre todos os estados seguintes poss√≠veis, em vez de uma amostra do pr√≥ximo estado [^3]. A natureza de uma atualiza√ß√£o pode ser expressa em uma equa√ß√£o, como acima, ou em um diagrama de *backup* como aqueles introduzidos no Cap√≠tulo 3 [^3].

Para escrever um programa de computador sequencial para implementar a avalia√ß√£o iterativa da pol√≠tica conforme dado por (4.5), seria necess√°rio usar duas matrizes, uma para os valores antigos, $v_k(s)$, e uma para os novos valores, $v_{k+1}(s)$ [^3]. Com duas matrizes, os novos valores podem ser calculados um por um a partir dos valores antigos sem que os valores antigos sejam alterados [^3]. Alternativamente, pode-se usar uma matriz e atualizar os valores "no local", ou seja, com cada novo valor sobrescrevendo imediatamente o antigo [^3]. Ent√£o, dependendo da ordem em que os estados s√£o atualizados, √†s vezes novos valores s√£o usados em vez de antigos no lado direito de (4.5) [^3]. Este algoritmo *in-place* tamb√©m converge para $v_\pi$; de fato, geralmente converge mais r√°pido do que a vers√£o de duas matrizes, como seria de esperar, porque usa novos dados assim que eles est√£o dispon√≠veis [^3]. Pensamos nas atualiza√ß√µes como sendo feitas em uma *varredura* atrav√©s do espa√ßo de estados [^3]. Para o algoritmo *in-place*, a ordem em que os estados t√™m seus valores atualizados durante a varredura tem uma influ√™ncia significativa na taxa de converg√™ncia [^3].

A vers√£o completa do algoritmo *in-place* de avalia√ß√£o iterativa da pol√≠tica √© mostrada em pseudoc√≥digo abaixo [^3]:

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

**Observa√ß√£o:** A escolha da threshold $\theta$ afeta diretamente a precis√£o da estimativa de $v_\pi$. Um valor menor de $\theta$ resulta em uma estimativa mais precisa, mas requer mais itera√ß√µes para convergir.

> üí° **Exemplo Num√©rico:** Implementa√ß√£o do algoritmo *in-place* em Python usando NumPy para o mesmo MDP.
>
> ```python
> import numpy as np
>
> # Define o ambiente MDP
> n_states = 3
> gamma = 0.9
> theta = 0.001 # Threshold
>
> # Probabilidades de transi√ß√£o e recompensas (como definido anteriormente)
> p = np.zeros((n_states, n_states))
> r = np.zeros((n_states, n_states))
>
> p[0, 1] = 1.0  # S1 -> S2
> r[0, 1] = 5.0
> p[1, 2] = 1.0  # S2 -> S3
> r[1, 2] = -2.0
> p[2, 0] = 1.0  # S3 -> S1
> r[2, 0] = 1.0
>
> # Pol√≠tica (sempre a√ß√£o a1)
> pi = np.ones((n_states, n_states)) # Simplifica√ß√£o: Probabilidade 1 de ir para o pr√≥ximo estado
>
> # Inicializa a fun√ß√£o de valor
> V = np.zeros(n_states)
>
> # Iterative Policy Evaluation
> while True:
>     delta = 0
>     for s in range(n_states):
>         v = V[s]
>         V[s] = np.sum(pi[s, :] * p[s, :] * (r[s, :] + gamma * V))
>         delta = max(delta, abs(v - V[s]))
>     if delta < theta:
>         break
>
> print("Fun√ß√£o de Valor Estimada:", V)
> ```
>
> Este c√≥digo simula o algoritmo *in-place* e imprime a fun√ß√£o de valor estimada ap√≥s a converg√™ncia.

### Conclus√£o
A avalia√ß√£o de pol√≠tica, por meio da iterative policy evaluation, fornece um m√©todo fundamental para estimar a fun√ß√£o de valor de estado $v_\pi$ para uma pol√≠tica arbitr√°ria $\pi$ [^2]. Este processo iterativo utiliza *expected updates* baseados na equa√ß√£o de Bellman, convergindo para a fun√ß√£o de valor verdadeira sob certas condi√ß√µes [^2]. A compreens√£o da avalia√ß√£o de pol√≠tica √© essencial para os m√©todos de melhoria de pol√≠tica que ser√£o explorados nos t√≥picos subsequentes [^1].

### Refer√™ncias
[^1]: Cap√≠tulo 4: Dynamic Programming.
[^2]: Se√ß√£o 4.1: Policy Evaluation (Prediction).
[^3]: Se√ß√£o 4.1: Policy Evaluation (Prediction), continua√ß√£o.

<!-- END -->