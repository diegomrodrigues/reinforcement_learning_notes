## Value Functions and Bellman Optimality Equations in Dynamic Programming

### Introdu√ß√£o
Dynamic Programming (DP) provides a foundational framework for understanding and solving Markov Decision Processes (MDPs) [^1]. A cornerstone of DP, as well as reinforcement learning (RL) in general, is the use of **value functions** to structure the search for optimal policies. This chapter delves into the crucial role of value functions and explores the Bellman optimality equations, which are central to deriving optimal policies in DP [^1].

### Conceitos Fundamentais

The core idea behind DP, as highlighted in the introductory section, is the strategic utilization of **value functions** to organize and structure the search for desirable policies [^1]. Value functions serve as estimators of the "goodness" of being in a particular state or taking a specific action in a state. More formally, a value function assigns a numerical value to each state (or state-action pair) representing the expected cumulative reward that an agent can accumulate starting from that state (or state-action pair) and following a particular policy.

There are primarily two types of value functions:

1.  **State-value function**, denoted as $v_{\pi}(s)$, represents the expected return starting from state $s$ and following policy $\pi$ thereafter.
2.  **Action-value function**, denoted as $q_{\pi}(s, a)$, represents the expected return starting from state $s$, taking action $a$, and following policy $\pi$ thereafter.

> üí° **Exemplo Num√©rico:** Imagine a simple grid world with four states (A, B, C, D) and a policy $\pi$ that always moves right if possible, otherwise stays put. Assume immediate rewards of +1 for transitioning to state D and 0 for all other transitions, and a discount factor $\gamma = 0.9$.
>
> ```mermaid
> graph LR
>     A -- Right (0) --> B
>     B -- Right (1) --> D
>     C -- Right (0) --> D
>     D -- Stay (0) --> D
> ```
>
> If $v_{\pi}(D) = 10$ (because staying in D gives a reward of 0 each time, but is defined this way for illustrative purposes, in practice, the DP algorithm will calculate this), we can calculate $v_{\pi}(B)$ as follows:
>
> $v_{\pi}(B) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=B, A_t=Right, \pi] = 1 + 0.9 * v_{\pi}(D) = 1 + 0.9 * 10 = 10$.  Similarly, $v_{\pi}(A) = 0 + 0.9 * v_{\pi}(B) = 0 + 0.9 * 10 = 9$.

The objective in many RL problems is to find an **optimal policy** $\pi_*$, which achieves the maximum expected return from every state. This optimal policy corresponds to **optimal value functions**, denoted as $v_*(s)$ and $q_*(s)$, which satisfy the Bellman optimality equations [^1]. These equations provide a recursive definition of the optimal value functions by relating the value of a state (or state-action pair) to the values of its successor states (or state-action pairs) under the optimal policy.

Before diving into the Bellman optimality equations, it's useful to define the Bellman equations for a given policy $\pi$, which are closely related and provide a foundation for understanding the optimality equations.

**Defini√ß√£o:** The **Bellman equation for $v_{\pi}(s)$** is expressed as:

$$
v_{\pi}(s) = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \,|\, S_t=s, A_t=a, \pi]
$$

This equation expresses the value of a state *s* under policy $\pi$ in terms of the immediate expected reward and the discounted expected value of the next state, also under policy $\pi$.

> üí° **Exemplo Num√©rico:** Consider a state $s$ where taking action $a$ leads to state $s'$ with a reward of $r = 2$, and $v_{\pi}(s') = 5$. If $\gamma = 0.8$, then $v_{\pi}(s) = 2 + 0.8 * 5 = 6$. This means the expected return from state $s$ following policy $\pi$ is 6.

Similarly, the **Bellman equation for $q_{\pi}(s, a)$** is expressed as:

$$
q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) \,|\, S_t=s, A_t=a, \pi]
$$

This equation defines the action-value function for a given policy $\pi$ recursively.

> üí° **Exemplo Num√©rico:** Suppose being in state $s$ and taking action $a$ yields an immediate reward of $r = -1$ and transitions to a new state $s'$. If, under policy $\pi$, the best action $a'$ in state $s'$ has $q_{\pi}(s', a') = 8$ and $\gamma = 0.9$, then $q_{\pi}(s, a) = -1 + 0.9 * 8 = 6.2$.

The **Bellman optimality equation for $v_*(s)$** is expressed as:

$$
v_*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) \,|\, S_t=s, A_t=a] \label{eq:4.1}
$$

This equation states that the value of a state $s$ under the optimal policy is equal to the maximum expected return achievable by taking the best action $a$ in that state and then following the optimal policy thereafter [^1]. This equation underscores the fact that $v_*(s)$ is the highest achievable expected return starting in state $s$.

The equation can be rewritten using the transition probabilities as:

$$
v_*(s) = \max_a \sum_{s',r} p(s', r \,|\, s, a) [r + \gamma v_*(s')]
$$

Where:
*   $p(s', r \,|\, s, a)$ is the probability of transitioning to state $s'$ with reward $r$ after taking action $a$ in state $s$ [^1].
*   $\gamma$ is the discount factor, which determines the importance of future rewards [^1].

> üí° **Exemplo Num√©rico:** Let's say in state $s$, there are two possible actions, $a_1$ and $a_2$. Action $a_1$ leads to state $s_1$ with $r_1 = 3$ and probability $0.6$, and to state $s_2$ with $r_2 = -1$ and probability $0.4$. Action $a_2$ leads to state $s_3$ with $r_3 = 1$ and probability $0.8$, and to state $s_4$ with $r_4 = 0$ and probability $0.2$.  Let $\gamma = 0.9$, $v_*(s_1) = 10$, $v_*(s_2) = 2$, $v_*(s_3) = 6$, and $v_*(s_4) = 4$.
>
> We can calculate the expected return for each action:
>
> Action $a_1$: $0.6 * (3 + 0.9 * 10) + 0.4 * (-1 + 0.9 * 2) = 0.6 * (3 + 9) + 0.4 * (-1 + 1.8) = 0.6 * 12 + 0.4 * 0.8 = 7.2 + 0.32 = 7.52$
>
> Action $a_2$: $0.8 * (1 + 0.9 * 6) + 0.2 * (0 + 0.9 * 4) = 0.8 * (1 + 5.4) + 0.2 * (0 + 3.6) = 0.8 * 6.4 + 0.2 * 3.6 = 5.12 + 0.72 = 5.84$
>
> Therefore, $v_*(s) = \max(7.52, 5.84) = 7.52$.  The optimal action in state $s$ is $a_1$.

Similarly, the **Bellman optimality equation for $q_*(s, a)$** is expressed as:

$$
q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') \,|\, S_t=s, A_t=a] \label{eq:4.2}
$$

This equation states that the value of taking action $a$ in state $s$ under the optimal policy is equal to the expected immediate reward plus the discounted value of the best action $a'$ in the next state $S_{t+1}$, assuming the optimal policy is followed thereafter [^1]. This emphasizes that $q_*(s, a)$ represents the maximum achievable expected return starting in state $s$, taking action $a$, and acting optimally thereafter.

The equation can be rewritten using transition probabilities as:

$$
q_*(s, a) = \sum_{s',r} p(s', r \,|\, s, a) [r + \gamma \max_{a'} q_*(s', a')]
$$

> üí° **Exemplo Num√©rico:** In state $s$, taking action $a$ transitions to state $s'$ with probability 1 and reward $r = 4$. In state $s'$, there are two possible actions, $a_1'$ and $a_2'$, with $q_*(s', a_1') = 7$ and $q_*(s', a_2') = 9$. Let $\gamma = 0.7$.
>
> Then $q_*(s, a) = 4 + 0.7 * \max(7, 9) = 4 + 0.7 * 9 = 4 + 6.3 = 10.3$.

The Bellman optimality equations play a critical role in DP algorithms because they provide the foundation for iteratively computing the optimal value functions and, subsequently, deriving the optimal policies. If we find $v_*$ or $q_*$, obtaining an optimal policy is straightforward. From $q_*(s, a)$, an optimal policy can be obtained by simply selecting the action that maximizes the optimal action-value function for each state:

$$
\pi_*(s) = \arg\max_a q_*(s, a)
$$

> üí° **Exemplo Num√©rico:** Suppose for state $s$, we have three actions $a_1, a_2, a_3$ with $q_*(s, a_1) = 5.2$, $q_*(s, a_2) = 6.8$, and $q_*(s, a_3) = 4.1$.
>
> Then, $\pi_*(s) = \arg\max_a q_*(s, a) = a_2$ because $q_*(s, a_2) = 6.8$ is the highest action-value for that state. Therefore, the optimal policy in state $s$ is to take action $a_2$.

The Bellman optimality equations highlight a recursive structure that optimal policies must satisfy. Any policy that doesn't satisfy these equations isn't optimal.
**Prova:** Vamos demonstrar que uma pol√≠tica que n√£o satisfaz a equa√ß√£o de otimalidade de Bellman n√£o √© √≥tima.

I. Suponha que temos uma pol√≠tica $\pi$ e uma fun√ß√£o de valor $v_{\pi}(s)$ que n√£o satisfaz a equa√ß√£o de otimalidade de Bellman para pelo menos um estado $s$. Isso significa que:

$$v_{\pi}(s) \neq \max_a \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \,|\, S_t = s, A_t = a, \pi]$$

II. Como $v_{\pi}(s)$ n√£o satisfaz a equa√ß√£o de otimalidade, existe pelo menos uma a√ß√£o $a'$ tal que:

$$\mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) \,|\, S_t = s, A_t = a', \pi] > v_{\pi}(s)$$

III. Agora, vamos construir uma nova pol√≠tica $\pi'$ que seja id√™ntica a $\pi$, exceto no estado $s$, onde $\pi'(s) = a'$.

IV. Seja $v_{\pi'}(s)$ a fun√ß√£o de valor para a pol√≠tica $\pi'$. Pela constru√ß√£o de $\pi'$, temos que:

$$v_{\pi'}(s) = \mathbb{E}[R_{t+1} + \gamma v_{\pi'}(S_{t+1}) \,|\, S_t = s, A_t = a', \pi']$$

V. Combinando os passos II e IV, podemos dizer que:
$$v_{\pi'}(s) > v_{\pi}(s)$$

VI. Isso demonstra que existe uma pol√≠tica $\pi'$ que obt√©m um retorno esperado maior a partir do estado $s$ do que a pol√≠tica $\pi$. Portanto, $\pi$ n√£o pode ser a pol√≠tica √≥tima. $\blacksquare$

Furthermore, we can define an operator that represents one step of improvement according to the Bellman optimality equations. This operator, when applied repeatedly, can converge to the optimal value function.

**Defini√ß√£o:** The **Bellman optimality operator** $T_*$ for state-value functions is defined as:

$$(T_* v)(s) = \max_a \sum_{s', r} p(s', r \,|\, s, a) [r + \gamma v(s')] $$

Applying this operator repeatedly to an arbitrary value function *v* will, under certain conditions, cause *v* to converge to $v_*$. Similarly, we can define the Bellman optimality operator for action-value functions.

**Defini√ß√£o:** The **Bellman optimality operator** $T_*$ for action-value functions is defined as:

$$(T_* q)(s, a) = \sum_{s', r} p(s', r \,|\, s, a) [r + \gamma \max_{a'} q(s', a')] $$

Applying this operator repeatedly to an arbitrary action-value function *q* will cause *q* to converge to $q_*$. The convergence properties of these operators are crucial for the theoretical justification of many DP algorithms.

**Prova:** Vamos mostrar que a aplica√ß√£o repetida do operador de otimalidade de Bellman $T_*$ converge para a fun√ß√£o de valor √≥tima $v_*$.

I. Seja $v$ uma fun√ß√£o de valor arbitr√°ria. Aplicamos o operador $T_*$ a $v$ para obter uma nova fun√ß√£o de valor $v' = T_*v$.

II. Pela defini√ß√£o do operador $T_*$, temos:

$$v'(s) = (T_*v)(s) = \max_a \sum_{s', r} p(s', r \,|\, s, a) [r + \gamma v(s')] $$

III. Queremos mostrar que se aplicarmos $T_*$ repetidamente, $v$ converge para $v_*$. Para isso, precisamos mostrar que $T_*$ √© uma contra√ß√£o. Uma contra√ß√£o √© um operador que aproxima os valores ap√≥s cada aplica√ß√£o.

IV. Considere duas fun√ß√µes de valor arbitr√°rias, $v_1$ e $v_2$. Aplicamos $T_*$ a ambas:

$$v_1' = T_*v_1$$
$$v_2' = T_*v_2$$

V. Usando a norma do supremo (m√°xima diferen√ßa) para medir a dist√¢ncia entre as fun√ß√µes de valor:

$$||v_1 - v_2|| = \max_s |v_1(s) - v_2(s)|$$

VI. Agora, vamos mostrar que $||T_*v_1 - T_*v_2|| \leq \gamma ||v_1 - v_2||$:

$$||T_*v_1 - T_*v_2|| = \max_s |(T_*v_1)(s) - (T_*v_2)(s)|$$
$$= \max_s |\max_a \sum_{s', r} p(s', r \,|\, s, a) [r + \gamma v_1(s')] - \max_a \sum_{s', r} p(s', r \,|\, s, a) [r + \gamma v_2(s')]|$$

VII. Usando a propriedade de que $|\max_a f(a) - \max_a g(a)| \leq \max_a |f(a) - g(a)|$, temos:

$$||T_*v_1 - T_*v_2|| \leq \max_s \max_a |\sum_{s', r} p(s', r \,|\, s, a) [r + \gamma v_1(s')] - \sum_{s', r} p(s', r \,|\, s, a) [r + \gamma v_2(s')]|$$
$$= \max_s \max_a |\sum_{s', r} p(s', r \,|\, s, a) \gamma [v_1(s') - v_2(s')]|$$
$$= \gamma \max_s \max_a \sum_{s', r} p(s', r \,|\, s, a) |v_1(s') - v_2(s')|$$

VIII. Como $\sum_{s', r} p(s', r \,|\, s, a) = 1$, temos:

$$||T_*v_1 - T_*v_2|| \leq \gamma \max_s |v_1(s) - v_2(s)| = \gamma ||v_1 - v_2||$$

IX. Portanto, $T_*$ √© uma contra√ß√£o com fator de contra√ß√£o $\gamma$. Pelo teorema da contra√ß√£o de Banach, a aplica√ß√£o repetida de $T_*$ converge para um ponto fixo, que √© a fun√ß√£o de valor √≥tima $v_*$. $\blacksquare$

### Conclus√£o

In conclusion, value functions are a central concept in Dynamic Programming and reinforcement learning, providing a structured approach to searching for optimal policies. The Bellman optimality equations, which define the recursive relationships between a state (or state-action pair) and its successors, are fundamental to deriving optimal value functions and, subsequently, optimal policies. These equations form the backbone of many DP algorithms, allowing for the iterative computation of optimal solutions in MDPs. Understanding these concepts is essential for grasping the theoretical underpinnings and practical applications of Dynamic Programming in reinforcement learning.

### Refer√™ncias
[^1]: Chapter 4: Dynamic Programming
<!-- END -->