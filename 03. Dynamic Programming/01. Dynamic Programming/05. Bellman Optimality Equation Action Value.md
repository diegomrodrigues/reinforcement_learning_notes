## A Equa√ß√£o de Otimalidade de Bellman para a Fun√ß√£o Valor-A√ß√£o

### Introdu√ß√£o

Em continuidade ao cap√≠tulo sobre Dynamic Programming (DP), exploramos a **equa√ß√£o de otimalidade de Bellman** para a fun√ß√£o valor-a√ß√£o, denotada como $q_*(s, a)$ [^1]. Esta equa√ß√£o √© fundamental para a determina√ß√£o de pol√≠ticas √≥timas em problemas de decis√£o de Markov (MDP) [^1]. A equa√ß√£o expressa o valor de se tomar uma a√ß√£o em um determinado estado sob uma pol√≠tica √≥tima, considerando tanto a recompensa imediata quanto o valor descontado da melhor a√ß√£o poss√≠vel no pr√≥ximo estado.

### Conceitos Fundamentais

A equa√ß√£o de otimalidade de Bellman para a fun√ß√£o valor-a√ß√£o $q_*(s, a)$ √© dada por [^1]:

$$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a]$$

Onde:

*   $q_*(s, a)$ representa o valor √≥timo de se tomar a a√ß√£o $a$ no estado $s$.
*   $\mathbb{E}$ denota o valor esperado.
*   $R_{t+1}$ √© a recompensa recebida ap√≥s tomar a a√ß√£o $a$ no estado $s$.
*   $\gamma$ √© o fator de desconto, $0 \leq \gamma \leq 1$, que determina a import√¢ncia das recompensas futuras [^1].
*   $S_{t+1}$ √© o pr√≥ximo estado alcan√ßado ap√≥s tomar a a√ß√£o $a$ no estado $s$.
*   $a'$ representa todas as poss√≠veis a√ß√µes no pr√≥ximo estado $S_{t+1}$.
*   $\max_{a'} q_*(S_{t+1}, a')$ representa o valor da melhor a√ß√£o que pode ser tomada no pr√≥ximo estado $S_{t+1}$, sob a pol√≠tica √≥tima.

**Interpreta√ß√£o:** A equa√ß√£o afirma que o valor √≥timo de um par estado-a√ß√£o $(s, a)$ √© igual √† recompensa imediata esperada, $R_{t+1}$, mais o valor descontado da melhor a√ß√£o poss√≠vel no pr√≥ximo estado, $S_{t+1}$. Essencialmente, $q_*(s, a)$ decomp√µe o problema de encontrar a pol√≠tica √≥tima em subproblemas menores, que podem ser resolvidos iterativamente.

> üí° **Exemplo Num√©rico:** Considere um ambiente simples com dois estados ($s_1, s_2$) e duas a√ß√µes ($a_1, a_2$). Suponha que, no estado $s_1$, tomar a a√ß√£o $a_1$ leva deterministicamente ao estado $s_2$ com uma recompensa de 10, enquanto tomar $a_2$ leva a $s_1$ com recompensa de 1. No estado $s_2$, ambas as a√ß√µes levam a $s_1$, com recompensas de 5 e 2 respectivamente. Seja $\gamma = 0.9$.
>
> Para calcular $q_*(s_1, a_1)$, precisamos do valor de $\max_{a'} q_*(s_2, a')$. Suponha que j√° determinamos que $q_*(s_2, a_1) = 60$ e $q_*(s_2, a_2) = 40$. Ent√£o, $\max_{a'} q_*(s_2, a') = 60$.
>
> Portanto:
>
> $q_*(s_1, a_1) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s_1, A_t = a_1] = 10 + 0.9 * 60 = 10 + 54 = 64$.
>
> Similarmente, se $q_*(s_1, a_2)$ leva a $s_1$ com recompensa 1 e $\max_{a'} q_*(s_1, a')$ √©, digamos, 64, ent√£o $q_*(s_1, a_2) = 1 + 0.9 * 64 = 58.6$.
>
> Este exemplo demonstra como o valor de uma a√ß√£o num estado depende tanto da recompensa imediata quanto do valor da melhor a√ß√£o que pode ser tomada no pr√≥ximo estado, descontado pelo fator $\gamma$.

Considerando que a din√¢mica do ambiente √© dada pelo conjunto de probabilidades $p(s', r|s, a)$ [^1], a equa√ß√£o pode ser reescrita como:

$$q_*(s, a) = \sum_{s', r} p(s', r|s, a) \left[r + \gamma \max_{a'} q_*(s', a')\right]$$

Esta forma da equa√ß√£o √© particularmente √∫til para implementa√ß√£o computacional, uma vez que explicita a depend√™ncia das probabilidades de transi√ß√£o e das recompensas.

> üí° **Exemplo Num√©rico:** Expandindo o exemplo anterior, vamos supor que as transi√ß√µes n√£o s√£o determin√≠sticas. No estado $s_1$, a a√ß√£o $a_1$ tem probabilidade 0.8 de levar a $s_2$ com recompensa 10 e probabilidade 0.2 de permanecer em $s_1$ com recompensa 2.
>
> Ent√£o:
>
> $q_*(s_1, a_1) = \sum_{s', r} p(s', r|s_1, a_1) \left[r + \gamma \max_{a'} q_*(s_2, a')\right] = 0.8 * (10 + 0.9 * \max_{a'} q_*(s_2, a')) + 0.2 * (2 + 0.9 * \max_{a'} q_*(s_1, a'))$
>
> Usando os valores anteriores, $\max_{a'} q_*(s_2, a') = 60$ e $\max_{a'} q_*(s_1, a') = 64$:
>
> $q_*(s_1, a_1) = 0.8 * (10 + 0.9 * 60) + 0.2 * (2 + 0.9 * 64) = 0.8 * 64 + 0.2 * 59.6 = 51.2 + 11.92 = 63.12$.
>
> Este valor difere do exemplo determin√≠stico porque agora existe uma probabilidade de permanecer no mesmo estado com uma recompensa diferente.

**Deriva√ß√£o da Equa√ß√£o:** A equa√ß√£o de otimalidade de Bellman deriva da ideia de que, sob uma pol√≠tica √≥tima, qualquer sub-pol√≠tica tamb√©m deve ser √≥tima. Em outras palavras, se $q_*(s, a)$ √© o valor √≥timo de se come√ßar no estado $s$ e tomar a a√ß√£o $a$, ent√£o, independentemente da primeira a√ß√£o tomada, a pol√≠tica subsequente deve ser √≥tima. Isso leva √† recurs√£o expressa na equa√ß√£o, onde o valor √≥timo de um estado √© definido em termos dos valores √≥timos dos estados sucessores.

**Observa√ß√£o:** A equa√ß√£o de otimalidade de Bellman assume que o ambiente √© Markoviano. Em outras palavras, o pr√≥ximo estado e recompensa dependem apenas do estado e a√ß√£o atual, e n√£o do hist√≥rico de estados e a√ß√µes anteriores.

**Lema 1:** A equa√ß√£o de otimalidade de Bellman para a fun√ß√£o valor-a√ß√£o possui uma solu√ß√£o √∫nica.

**Prova:** A prova da unicidade da solu√ß√£o pode ser demonstrada utilizando o argumento do mapeamento de contra√ß√£o (Banach fixed-point theorem). Definimos um operador de Bellman $\mathcal{T}$ que age sobre as fun√ß√µes valor-a√ß√£o $q$ da seguinte forma:
$$(\mathcal{T}q)(s, a) = \sum_{s', r} p(s', r|s, a) \left[r + \gamma \max_{a'} q(s', a')\right]$$
Mostra-se que $\mathcal{T}$ √© uma contra√ß√£o em rela√ß√£o √† norma do supremo, e portanto, possui um √∫nico ponto fixo, que corresponde √† solu√ß√£o da equa√ß√£o de otimalidade de Bellman.

I. **Defini√ß√£o do Operador de Bellman:**
   Definimos o operador de Bellman $\mathcal{T}$ como:
   $$(\mathcal{T}q)(s, a) = \sum_{s', r} p(s', r|s, a) \left[r + \gamma \max_{a'} q(s', a')\right]$$

II. **Norma do Supremo:**
    Definimos a norma do supremo para uma fun√ß√£o $q$ como:
    $$||q|| = \max_{s, a} |q(s, a)|$$

III. **Demonstra√ß√£o da Contra√ß√£o:**
     Precisamos mostrar que existe um $\gamma \in [0, 1)$ tal que para quaisquer duas fun√ß√µes valor-a√ß√£o $q_1$ e $q_2$:
     $$||\mathcal{T}q_1 - \mathcal{T}q_2|| \leq \gamma ||q_1 - q_2||$$

IV. **Aplicando o Operador:**
    Considere $(\mathcal{T}q_1)(s, a) - (\mathcal{T}q_2)(s, a)$:
    $$(\mathcal{T}q_1)(s, a) - (\mathcal{T}q_2)(s, a) = \sum_{s', r} p(s', r|s, a) \left[\gamma \max_{a'} q_1(s', a') - \gamma \max_{a'} q_2(s', a')\right]$$

V. **Usando a Desigualdade:**
   Sabemos que:
   $$|\max_{a'} q_1(s', a') - \max_{a'} q_2(s', a')| \leq \max_{a'} |q_1(s', a') - q_2(s', a')| \leq ||q_1 - q_2||$$

VI. **Limitando a Diferen√ßa:**
    Portanto:
    $$|(\mathcal{T}q_1)(s, a) - (\mathcal{T}q_2)(s, a)| \leq \sum_{s', r} p(s', r|s, a) \gamma ||q_1 - q_2||$$

VII. **Simplificando:**
     Como $\sum_{s', r} p(s', r|s, a) = 1$, temos:
     $$|(\mathcal{T}q_1)(s, a) - (\mathcal{T}q_2)(s, a)| \leq \gamma ||q_1 - q_2||$$

VIII. **Conclus√£o:**
      Tomando o m√°ximo sobre todos os $(s, a)$:
      $$||\mathcal{T}q_1 - \mathcal{T}q_2|| = \max_{s, a} |(\mathcal{T}q_1)(s, a) - (\mathcal{T}q_2)(s, a)| \leq \gamma ||q_1 - q_2||$$
      Isso mostra que $\mathcal{T}$ √© uma contra√ß√£o.

IX. **Ponto Fixo √önico:**
    Pelo teorema do ponto fixo de Banach, $\mathcal{T}$ possui um √∫nico ponto fixo, que √© a solu√ß√£o √∫nica da equa√ß√£o de otimalidade de Bellman. ‚ñ†

**Rela√ß√£o com a Equa√ß√£o de Otimalidade de Bellman para a Fun√ß√£o Valor-Estado:**  A equa√ß√£o para $q_*(s, a)$ est√° intrinsecamente ligada √† equa√ß√£o de otimalidade de Bellman para a fun√ß√£o valor-estado, $v_*(s)$ [^1].  A rela√ß√£o √© dada por:

$$v_*(s) = \max_a q_*(s, a)$$

Isso significa que o valor √≥timo de um estado $s$ √© o valor da melhor a√ß√£o que pode ser tomada nesse estado. A fun√ß√£o valor-estado $v_*(s)$ pode ser usada para encontrar a pol√≠tica √≥tima [^1]. Sabendo $v_*(s)$, a pol√≠tica √≥tima $\pi_*(s)$ pode ser definida como:

$$\pi_*(s) = \arg\max_a \sum_{s',r} p(s', r|s, a) \left[r + \gamma v_*(s')\right]$$

Para explicitar a rela√ß√£o entre as fun√ß√µes $q_*(s, a)$ e $v_*(s)$, podemos rescrever a equa√ß√£o de otimalidade de Bellman para $q_*(s, a)$ em termos de $v_*(s')$:

$$q_*(s, a) = \sum_{s', r} p(s', r|s, a) \left[r + \gamma v_*(s')\right]$$

Esta equa√ß√£o demonstra como o valor √≥timo de uma a√ß√£o em um estado √© diretamente influenciado pelo valor √≥timo do pr√≥ximo estado, ponderado pela probabilidade de transi√ß√£o e descontado pelo fator $\gamma$.

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, suponha que $v_*(s_1) = 64$ e $v_*(s_2) = 60$. Podemos recalcular $q_*(s_1, a_1)$ usando $v_*(s')$:
>
> $q_*(s_1, a_1) = \sum_{s', r} p(s', r|s_1, a_1) \left[r + \gamma v_*(s')\right] = 0.8 * (10 + 0.9 * v_*(s_2)) + 0.2 * (2 + 0.9 * v_*(s_1)) = 0.8 * (10 + 0.9 * 60) + 0.2 * (2 + 0.9 * 64) = 0.8 * 64 + 0.2 * 59.6 = 63.12$, o mesmo resultado obtido antes.
>
> A pol√≠tica √≥tima no estado $s_1$ seria ent√£o definida como $\pi_*(s_1) = \arg\max_a q_*(s_1, a)$. Se $q_*(s_1, a_1) = 63.12$ e $q_*(s_1, a_2) = 58.6$ (calculado anteriormente), ent√£o $\pi_*(s_1) = a_1$, pois $a_1$ tem o valor esperado mais alto.

**Teorema 1:** Uma pol√≠tica $\pi$ √© √≥tima se, e somente se, satisfaz as equa√ß√µes de Bellman para $v_*$ e $q_*$.

**Prova:** (Parte 1: Se $\pi$ √© √≥tima, ent√£o satisfaz as equa√ß√µes de Bellman). Se $\pi$ √© √≥tima, ent√£o $v_\pi = v_*$ e $q_\pi = q_*$. Como $v_\pi$ e $q_\pi$ satisfazem as equa√ß√µes de Bellman para avalia√ß√£o de pol√≠tica, e $v_*$ e $q_*$ s√£o iguais a $v_\pi$ e $q_\pi$ respectivamente, ent√£o $v_*$ e $q_*$ tamb√©m devem satisfazer as equa√ß√µes de Bellman.

(Parte 2: Se $\pi$ satisfaz as equa√ß√µes de Bellman, ent√£o √© √≥tima). Assuma que $v_\pi$ satisfaz a equa√ß√£o de Bellman para $v_*$, ou seja, $v_\pi(s) = \max_a \sum_{s',r} p(s', r|s, a) \left[r + \gamma v_\pi(s')\right]$. Isto implica que $v_\pi(s) = v_*(s)$ para todo $s$, e portanto $\pi$ √© uma pol√≠tica √≥tima. Uma prova similar pode ser feita para $q_\pi$.

I. **Defini√ß√£o de Pol√≠tica √ìtima:**
   Uma pol√≠tica $\pi$ √© √≥tima se, e somente se, $v_\pi(s) = v_*(s)$ para todo estado $s$. Equivalentemente, $\pi$ √© √≥tima se, e somente se, $q_\pi(s, a) = q_*(s, a)$ para todo estado-a√ß√£o $(s, a)$.

II. **Equa√ß√£o de Bellman para $v_*$:**
    A equa√ß√£o de Bellman para $v_*$ √© dada por:
    $$v_*(s) = \max_a \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')] \quad \forall s$$

III. **Equa√ß√£o de Bellman para $q_*$:**
     A equa√ß√£o de Bellman para $q_*$ √© dada por:
     $$q_*(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma \max_{a'} q_*(s', a')] \quad \forall s, a$$

IV. **Parte 1: Se $\pi$ √© √≥tima, ent√£o satisfaz as equa√ß√µes de Bellman:**
    Se $\pi$ √© √≥tima, ent√£o $v_\pi = v_*$ e $q_\pi = q_*$. As fun√ß√µes valor $v_\pi$ e $q_\pi$ satisfazem as equa√ß√µes de Bellman para avalia√ß√£o de pol√≠tica:
    $$v_\pi(s) = \sum_{s', r} p(s', r|s, \pi(s)) [r + \gamma v_\pi(s')] \quad \forall s$$
    $$q_\pi(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_\pi(s', a')] \quad \forall s, a$$
    Substituindo $v_\pi$ por $v_*$ e $q_\pi$ por $q_*$, vemos que $v_*$ e $q_*$ satisfazem as equa√ß√µes de avalia√ß√£o de pol√≠tica para a pol√≠tica √≥tima $\pi$. Al√©m disso, como $\pi$ √© √≥tima, ela deve escolher a a√ß√£o que maximiza o valor esperado, ent√£o as equa√ß√µes de otimalidade de Bellman s√£o satisfeitas.

V. **Parte 2: Se $\pi$ satisfaz as equa√ß√µes de Bellman, ent√£o √© √≥tima:**
   Assuma que $v_\pi$ satisfaz a equa√ß√£o de Bellman para $v_*$:
   $$v_\pi(s) = \max_a \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')] \quad \forall s$$
   Isso significa que para todo $s$, $v_\pi(s)$ √© igual ao valor m√°ximo que pode ser obtido seguindo $\pi$ a partir de $s$. Portanto, $v_\pi(s) = v_*(s)$ para todo $s$, e $\pi$ √© uma pol√≠tica √≥tima. Um argumento similar se aplica se $q_\pi$ satisfaz a equa√ß√£o de Bellman para $q_*$.

VI. **Conclus√£o:**
    Portanto, uma pol√≠tica $\pi$ √© √≥tima se, e somente se, satisfaz as equa√ß√µes de Bellman para $v_*$ e $q_*$. ‚ñ†

### Conclus√£o

A equa√ß√£o de otimalidade de Bellman para a fun√ß√£o valor-a√ß√£o $q_*(s, a)$ fornece uma ferramenta poderosa para encontrar pol√≠ticas √≥timas em MDPs [^1]. Atrav√©s da decomposi√ß√£o do problema em subproblemas menores e da itera√ß√£o sobre os valores, √© poss√≠vel convergir para a pol√≠tica √≥tima. Embora computacionalmente intensivo, o DP, e consequentemente, o uso da equa√ß√£o de Bellman, fornece uma base te√≥rica s√≥lida para o desenvolvimento de m√©todos de reinforcement learning mais eficientes. O conceito de **Generalized Policy Iteration (GPI)**, que envolve a intera√ß√£o entre processos de avalia√ß√£o e melhoria da pol√≠tica [^1], √© fundamental para a compreens√£o de como os m√©todos de DP convergem para uma solu√ß√£o √≥tima. As pr√≥ximas se√ß√µes do livro, provavelmente, explorar√£o m√©todos que tentam alcan√ßar resultados similares aos do DP, mas com menor custo computacional e sem a necessidade de um modelo perfeito do ambiente [^1].

### Refer√™ncias

[^1]: Dynamic Programming. (n.d.). Retrieved from OCR text provided.

<!-- END -->