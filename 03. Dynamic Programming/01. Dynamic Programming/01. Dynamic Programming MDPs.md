## Dynamic Programming: Foundations and Algorithms

### Introdu√ß√£o
O termo **Dynamic Programming (DP)** refere-se a um conjunto de algoritmos que podem ser utilizados para computar pol√≠ticas √≥timas, dado um modelo perfeito do ambiente como um **Markov Decision Process (MDP)** [^1]. Embora os algoritmos cl√°ssicos de DP tenham utilidade limitada em *reinforcement learning* (RL) devido √† sua grande despesa computacional e √† sua depend√™ncia de um modelo perfeito, eles permanecem teoricamente importantes [^1]. De fato, todos os m√©todos de RL podem ser vistos como tentativas de alcan√ßar o mesmo efeito que DP, mas com menos computa√ß√£o e sem assumir um modelo perfeito do ambiente [^1]. Este cap√≠tulo explora os conceitos e algoritmos fundamentais de DP, fornecendo uma base te√≥rica para o entendimento dos m√©todos de RL que ser√£o abordados posteriormente.

### Conceitos Fundamentais
**Dynamic Programming (DP)** √© uma abordagem algor√≠tmica para resolver problemas de otimiza√ß√£o complexos, decompondo-os em subproblemas mais simples e sobrepostos. Em RL, DP √© utilizada para encontrar pol√≠ticas √≥timas em MDPs quando o modelo do ambiente √© completamente conhecido.

**Markov Decision Process (MDP):** Para aplicar DP, √© crucial que o ambiente seja modelado como um MDP. Um MDP √© definido por:
*   Um conjunto de estados $S$
*   Um conjunto de a√ß√µes $A$
*   Um conjunto de recompensas $R$
*   Uma fun√ß√£o de transi√ß√£o de probabilidade $p(s', r | s, a)$, que define a probabilidade de transitar para o estado $s'$ e receber a recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$ [^1].

A partir deste cap√≠tulo, assume-se que o ambiente √© um MDP finito. Isso significa que os conjuntos de estados, a√ß√µes e recompensas, $S$, $A$, e $R$, s√£o finitos [^1].

**Value Functions:** A ideia chave de DP, e de RL em geral, √© o uso de **value functions** para organizar e estruturar a busca por boas pol√≠ticas [^1]. Existem dois tipos principais de value functions:
*   **State-value function** $v_\pi(s)$: representa o valor esperado de iniciar no estado $s$ e seguir a pol√≠tica $\pi$.
*   **Action-value function** $q_\pi(s, a)$: representa o valor esperado de iniciar no estado $s$, tomar a a√ß√£o $a$ e seguir a pol√≠tica $\pi$ a partir da√≠.

> üí° **Exemplo Num√©rico:** Considere um MDP simples com dois estados ($S = \{s_1, s_2\}$) e duas a√ß√µes ($A = \{a_1, a_2\}$). Suponha que a pol√≠tica $\pi$ √© tal que $\pi(a_1|s_1) = 0.7$ e $\pi(a_2|s_1) = 0.3$. Se soubermos que $q_\pi(s_1, a_1) = 10$ e $q_\pi(s_1, a_2) = 5$, ent√£o $v_\pi(s_1) = (0.7)(10) + (0.3)(5) = 7 + 1.5 = 8.5$. Isso significa que, em m√©dia, seguir a pol√≠tica $\pi$ a partir do estado $s_1$ resulta em um retorno de 8.5.

**Bellman Equations:** As value functions podem ser definidas recursivamente atrav√©s das Bellman equations. As Bellman optimality equations definem as value functions √≥timas $v_*(s)$ e $q_*(s, a)$ [^1]:

$$v_*(s) = \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$

$$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a]$$

onde $\gamma$ √© o fator de desconto, que determina a import√¢ncia das recompensas futuras [^1].

> üí° **Exemplo Num√©rico:** Considere um estado $s$ onde, ao tomar a a√ß√£o $a_1$, obtemos uma recompensa de 10 e transitamos para o estado $s'$ com valor √≥timo $v_*(s') = 20$. Ao tomar a a√ß√£o $a_2$, obtemos uma recompensa de 5 e transitamos para o estado $s''$ com valor √≥timo $v_*(s'') = 30$. Se $\gamma = 0.9$, ent√£o:
>
> $q_*(s, a_1) = 10 + 0.9 \cdot 20 = 28$
>
> $q_*(s, a_2) = 5 + 0.9 \cdot 30 = 32$
>
> Portanto, $v_*(s) = \max(28, 32) = 32$. Isso significa que o valor √≥timo de estar no estado $s$ √© 32, obtido ao tomar a a√ß√£o $a_2$.

As equa√ß√µes de Bellman [^2] s√£o:

$$v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]$$

$$q_\pi(s, a) = \mathbb{E}_\pi[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]$$

Essas equa√ß√µes fornecem a base para os algoritmos de DP, que iterativamente atualizam as estimativas das value functions at√© convergirem para a solu√ß√£o √≥tima.

> üí° **Exemplo Num√©rico:** Suponha que temos um estado $s$ e uma pol√≠tica $\pi$. Ao seguir $\pi$ a partir de $s$, esperamos receber uma recompensa de $R_{t+1} = 2$ e transitar para um estado $S_{t+1}$ com $v_\pi(S_{t+1}) = 5$. Se $\gamma = 0.8$, ent√£o $v_\pi(s) = 2 + 0.8 \cdot 5 = 6$.

Para consolidar a compreens√£o das equa√ß√µes de Bellman, podemos derivar uma rela√ß√£o entre $v_\pi(s)$ e $q_\pi(s, a)$ sob uma pol√≠tica $\pi$.

**Lema 1:** *Rela√ß√£o entre State-value e Action-value functions*.
Para qualquer estado $s \in S$ e pol√≠tica $\pi$, a state-value function $v_\pi(s)$ pode ser expressa em termos da action-value function $q_\pi(s, a)$ como:

$$v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) q_\pi(s, a)$$

*Prova.*
A state-value function $v_\pi(s)$ representa o valor esperado de iniciar no estado $s$ e seguir a pol√≠tica $\pi$. Este valor esperado pode ser decomposto em uma soma ponderada dos valores esperados de realizar cada a√ß√£o $a$ no estado $s$ e, em seguida, seguir a pol√≠tica $\pi$. A pondera√ß√£o para cada a√ß√£o $a$ √© a probabilidade $\pi(a|s)$ de selecionar a a√ß√£o $a$ no estado $s$ sob a pol√≠tica $\pi$. Portanto:

$$v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \sum_{a \in A(s)} \pi(a|s) \mathbb{E}_\pi[G_t | S_t = s, A_t = a] = \sum_{a \in A(s)} \pi(a|s) q_\pi(s, a)$$

Essa rela√ß√£o √© √∫til para conectar as duas value functions e pode simplificar os c√°lculos em alguns casos.

### Algoritmos de Dynamic Programming

Existem dois algoritmos principais de DP: **Policy Iteration** e **Value Iteration**.

**1. Policy Iteration:** Este algoritmo consiste em duas etapas principais que se repetem iterativamente [^8]:
    *   **Policy Evaluation (Prediction):** Calcula a value function $v_\pi(s)$ para uma pol√≠tica $\pi$ dada [^2, 8]. Isso √© feito resolvendo o sistema de equa√ß√µes de Bellman para $v_\pi$. Na pr√°tica, √© comum usar *iterative policy evaluation*, onde a value function √© atualizada iterativamente usando a seguinte regra [^2, 8]:
    $$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s]$$
    Esta etapa termina quando a value function converge, ou seja, quando as mudan√ßas em $v(s)$ se tornam suficientemente pequenas.

> üí° **Exemplo Num√©rico:** Considere um estado $s$ e uma pol√≠tica $\pi$. Suponha que $v_k(s)$ √© a estimativa atual do valor de $s$ na itera√ß√£o $k$. Se, seguindo $\pi$ a partir de $s$, esperamos receber $R_{t+1} = 3$ e transitar para $S_{t+1}$ com $v_k(S_{t+1}) = 7$, e $\gamma = 0.9$, ent√£o $v_{k+1}(s) = 3 + 0.9 \cdot 7 = 9.3$. Esta atualiza√ß√£o √© repetida at√© que a mudan√ßa em $v(s)$ seja desprez√≠vel.

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

*   **Policy Improvement:** Cria uma nova pol√≠tica $\pi'$ que √© *greedy* com rela√ß√£o √† value function $v_\pi$ [^5, 8]. Ou seja, para cada estado $s$, a nova pol√≠tica escolhe a a√ß√£o que maximiza o valor esperado da recompensa imediata mais o valor descontado do pr√≥ximo estado:
$$\pi'(s) = \arg \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a]$$
O teorema de melhoria de pol√≠tica garante que $\pi'$ √© t√£o boa quanto ou melhor que $\pi$ [^6].

> üí° **Exemplo Num√©rico:** Em um estado $s$, temos duas a√ß√µes, $a_1$ e $a_2$. Ap√≥s a policy evaluation, encontramos que $\mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a_1] = 12$ e $\mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a_2] = 15$. Ent√£o, a policy improvement escolher√° a a√ß√£o $a_2$ para o estado $s$, pois ela maximiza o valor esperado. Portanto, $\pi'(s) = a_2$.

O processo se repete at√© que a pol√≠tica n√£o mude mais, indicando que a pol√≠tica √≥tima foi encontrada [^8].

![Policy Iteration algorithm: iterative process of policy evaluation and improvement for optimal policy estimation.](./../images/image3.png)

Podemos formalizar o Teorema de Melhoria de Pol√≠tica mencionado.

**Teorema 2** (Teorema da Melhoria da Pol√≠tica): Seja $\pi$ e $\pi'$ pol√≠ticas arbitr√°rias tais que para todo $s \in S$:

$$q_\pi(s, \pi'(s)) \geq v_\pi(s)$$

Ent√£o, a pol√≠tica $\pi'$ deve ser t√£o boa ou melhor que $\pi$, isto √©, $v_{\pi'}(s) \geq v_\pi(s)$ para todo $s \in S$.

*Proof*: A prova completa pode ser encontrada em Sutton & Barto (2018). A intui√ß√£o principal √© que, ao selecionar a√ß√µes que maximizam $q_\pi(s, a)$, estamos garantindo que a nova pol√≠tica $\pi'$ ter√° um desempenho igual ou superior a $\pi$ em todos os estados.

**2. Value Iteration:** Este algoritmo combina as etapas de policy evaluation e policy improvement em uma √∫nica etapa de atualiza√ß√£o [^11]. Ele calcula a value function √≥tima $v_*(s)$ iterativamente usando a seguinte regra:
    $$v_{k+1}(s) = \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a]$$
    Ap√≥s cada itera√ß√£o, uma pol√≠tica √≥tima pode ser extra√≠da da value function resultante:
    $$\pi_*(s) = \arg \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$

> üí° **Exemplo Num√©rico:** Considere um estado $s$ com duas a√ß√µes $a_1$ e $a_2$. Suponha que na itera√ß√£o $k$, temos:
>
> $\mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a_1] = 8 + 0.9 \cdot 10 = 17$
>
> $\mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a_2] = 5 + 0.9 \cdot 12 = 15.8$
>
> Ent√£o, $v_{k+1}(s) = \max(17, 15.8) = 17$. A pol√≠tica √≥tima neste estado seria $\pi_*(s) = a_1$.

Value Iteration converge para a value function √≥tima $v_*$ e, portanto, encontra a pol√≠tica √≥tima [^11].

![Pseudocode for Value Iteration algorithm, a method for estimating the optimal policy in an MDP.](./../images/image4.png)

Para melhor ilustrar a converg√™ncia do Value Iteration, podemos estabelecer um resultado sobre a monotonicidade da sequ√™ncia de value functions geradas por este algoritmo.

**Lema 3** (Monotonicidade do Value Iteration): Seja $v_k(s)$ a sequ√™ncia de state-value functions gerada pelo Value Iteration. Ent√£o, para cada estado $s \in S$, a sequ√™ncia $v_k(s)$ √© monotonicamente n√£o-decrescente e converge para $v_*(s)$.

*Proof Strategy:* A prova pode ser constru√≠da mostrando que $v_{k+1}(s) \geq v_k(s)$ para todo $s$ e $k$, utilizando indu√ß√£o e as propriedades da equa√ß√£o de Bellman de otimalidade. A converg√™ncia para $v_*(s)$ segue do fato de que o Value Iteration √© um operador de contra√ß√£o no espa√ßo das value functions, garantindo que ele converge para um ponto fixo, que √© a value function √≥tima.

A prova detalhada da monotonicidade do Value Iteration √© a seguinte:

*Prova:*

I. **Base:** Para $k = 0$, $v_0(s)$ pode ser inicializado arbitrariamente.

II. **Hip√≥tese Indutiva:** Assuma que $v_k(s) \geq v_{k-1}(s)$ para todo $s \in S$.

III. **Passo Indutivo:** Precisamos mostrar que $v_{k+1}(s) \geq v_k(s)$ para todo $s \in S$. Usando a atualiza√ß√£o do Value Iteration:

$$v_{k+1}(s) = \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a]$$

Pela hip√≥tese indutiva, $v_k(S_{t+1}) \geq v_{k-1}(S_{t+1})$ para todo $S_{t+1}$. Portanto:

$$\mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a] \geq \mathbb{E}[R_{t+1} + \gamma v_{k-1}(S_{t+1}) | S_t = s, A_t = a]$$

Como o lado esquerdo √© maior ou igual ao lado direito para cada a√ß√£o $a$, o m√°ximo sobre todas as a√ß√µes tamb√©m preserva a desigualdade:

$$\max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a] \geq \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma v_{k-1}(S_{t+1}) | S_t = s, A_t = a]$$

Isso implica que:

$$v_{k+1}(s) \geq v_k(s)$$

IV. **Conclus√£o:** Pelo princ√≠pio da indu√ß√£o matem√°tica, $v_k(s)$ √© monotonicamente n√£o-decrescente para todo $s \in S$.

V. **Converg√™ncia:** O Value Iteration √© um operador de contra√ß√£o no espa√ßo das value functions com a norma do supremo (m√°xima diferen√ßa). Isso garante que a sequ√™ncia $v_k$ converge para um ponto fixo, que √© a value function √≥tima $v_*$.

Portanto, a sequ√™ncia $v_k(s)$ √© monotonicamente n√£o-decrescente e converge para $v_*(s)$. ‚ñ†

**Asynchronous Dynamic Programming:** Os algoritmos de DP cl√°ssicos envolvem opera√ß√µes sobre todo o conjunto de estados do MDP [^13]. Para problemas com grandes conjuntos de estados, mesmo uma √∫nica varredura pode ser proibitivamente cara [^13]. **Asynchronous DP** (ADP) s√£o algoritmos de DP iterativos *in-place* que n√£o s√£o organizados em termos de varreduras sistem√°ticas do conjunto de estados [^13]. Estes algoritmos atualizam os valores dos estados em qualquer ordem, usando quaisquer valores de outros estados que estejam dispon√≠veis [^13].

Para refinar a compreens√£o dos algoritmos Asynchronous Dynamic Programming, podemos destacar algumas de suas variantes.

**Proposi√ß√£o 4:** *Variantes de Asynchronous Dynamic Programming*.
Existem diferentes formas de implementar Asynchronous DP, incluindo:

*   *In-place Policy Iteration*: Atualiza a pol√≠tica e a value function em cada estado assim que um novo valor √© computado, utilizando a informa√ß√£o mais recente dispon√≠vel.

*   *Prioritized Sweeping*: Prioriza a atualiza√ß√£o de estados com base na magnitude da mudan√ßa em seus valores esperados (Bellman error), focando em estados que contribuem mais para a melhoria da pol√≠tica.

*   *Real-Time Dynamic Programming (RTDP)*: Aplica DP apenas aos estados que s√£o realmente encontrados durante a intera√ß√£o com o ambiente, tornando-o especialmente adequado para problemas com espa√ßos de estados muito grandes.

### Conclus√£o
Dynamic Programming fornece uma estrutura te√≥rica fundamental para resolver problemas de RL em MDPs [^1]. Embora os algoritmos cl√°ssicos de DP tenham limita√ß√µes pr√°ticas devido √† sua exig√™ncia de um modelo perfeito do ambiente e seu custo computacional, eles introduzem conceitos essenciais como value functions, Bellman equations, policy evaluation e policy improvement [^1]. Estes conceitos s√£o amplamente utilizados em muitos algoritmos de RL mais pr√°ticos [^1]. Adicionalmente, foi discutido um breve resumo dos algoritmos Asynchronous Dynamic Programming que podem ser uma solu√ß√£o aos problemas computacionais dos algoritmos de DP cl√°ssicos.

### Refer√™ncias
[^1]: Cap√≠tulo 4: Dynamic Programming.
[^2]: Se√ß√£o 4.1: Policy Evaluation (Prediction).
[^5]: Se√ß√£o 4.2: Policy Improvement.
[^6]: Se√ß√£o 4.2: Policy Improvement.
[^8]: Se√ß√£o 4.3: Policy Iteration.
[^11]: Se√ß√£o 4.4: Value Iteration.
[^13]: Se√ß√£o 4.5: Asynchronous Dynamic Programming.
<!-- END -->