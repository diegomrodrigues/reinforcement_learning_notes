## Value Functions in Dynamic Programming

### Introdu√ß√£o
As **value functions**, representadas por $v_*(s)$ e $q_*(s, a)$, s√£o conceitos fundamentais tanto em Dynamic Programming (DP) quanto em Reinforcement Learning (RL) [^1]. Elas fornecem um mecanismo para organizar e estruturar a busca por pol√≠ticas √≥timas em um ambiente de decis√£o sequencial. Em ess√™ncia, uma value function estima o *valor* de estar em um determinado estado ou de tomar uma a√ß√£o espec√≠fica em um estado, considerando o retorno esperado a longo prazo. Este cap√≠tulo aprofunda o papel central das value functions e sua rela√ß√£o com as equa√ß√µes de otimalidade de Bellman.

### Conceitos Fundamentais
**Value functions** s√£o estimativas do *qu√£o bom* √© para um agente estar em um determinado estado (state-value function) ou executar uma determinada a√ß√£o em um determinado estado (action-value function) [^1]. Mais formalmente:

*   **State-value function $v_*(s)$**: Representa o valor do estado $s$ seguindo a pol√≠tica √≥tima. Ela quantifica o retorno esperado m√°ximo que um agente pode obter se come√ßar no estado $s$ e seguir a pol√≠tica √≥tima a partir de ent√£o.
*   **Action-value function $q_*(s, a)$**: Representa o valor de executar a a√ß√£o $a$ no estado $s$ e, subsequentemente, seguir a pol√≠tica √≥tima. Ela quantifica o retorno esperado m√°ximo que um agente pode obter se come√ßar no estado $s$, executar a a√ß√£o $a$ e seguir a pol√≠tica √≥tima a partir de ent√£o.

As value functions $v_*(s)$ e $q_*(s, a)$ est√£o intrinsecamente ligadas pelas **Bellman optimality equations** [^1]. Estas equa√ß√µes fornecem uma defini√ß√£o recursiva do valor √≥timo, decompondo o problema em subproblemas menores e mais trat√°veis.

A **Bellman optimality equation** para $v_*(s)$ √© dada por:

$$v_*(s) = \max_{a} \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s, A_t=a]$$

Esta equa√ß√£o afirma que o valor √≥timo de um estado $s$ √© igual ao retorno esperado m√°ximo que se pode obter ao tomar a melhor a√ß√£o $a$ em $s$ e, em seguida, seguir a pol√≠tica √≥tima a partir do pr√≥ximo estado $S_{t+1}$. Expans√£o [^1]:

$$v_*(s) = \max_{a} \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')] \qquad (4.1)$$

Onde:
*   $p(s', r|s, a)$ √© a probabilidade de transi√ß√£o para o estado $s'$ com recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$.
*   $\gamma$ √© o fator de desconto, que determina o peso relativo das recompensas futuras em rela√ß√£o √†s recompensas imediatas.

> üí° **Exemplo Num√©rico:** Imagine um MDP com tr√™s estados: $S = \{s_1, s_2, s_3\}$ e duas a√ß√µes: $A = \{a_1, a_2\}$. Suponha que estamos no estado $s_1$. Se tomarmos a a√ß√£o $a_1$, temos uma probabilidade de 0.7 de ir para o estado $s_2$ com uma recompensa de 10 e uma probabilidade de 0.3 de ir para o estado $s_3$ com uma recompensa de 0. Se tomarmos a a√ß√£o $a_2$, temos uma probabilidade de 0.9 de ir para o estado $s_3$ com uma recompensa de 5 e uma probabilidade de 0.1 de ficar em $s_1$ com uma recompensa de 2.  O fator de desconto $\gamma$ √© 0.9.  Podemos expressar isso como:
>
> $p(s_2, 10 | s_1, a_1) = 0.7$
> $p(s_3, 0 | s_1, a_1) = 0.3$
> $p(s_3, 5 | s_1, a_2) = 0.9$
> $p(s_1, 2 | s_1, a_2) = 0.1$
>
> Para calcular $v_*(s_1)$, precisamos saber $v_*(s_2)$ e $v_*(s_3)$. Suponha que, ap√≥s algumas itera√ß√µes, determinamos que $v_*(s_2) = 100$ e $v_*(s_3) = 50$. Agora, podemos calcular o valor de $s_1$:
>
> $v_*(s_1) = \max_{a} \sum_{s', r} p(s', r|s_1, a) [r + \gamma v_*(s')]$
> $v_*(s_1) = \max \begin{cases} 0.7 \cdot [10 + 0.9 \cdot 100] + 0.3 \cdot [0 + 0.9 \cdot 50] \\ 0.9 \cdot [5 + 0.9 \cdot 50] + 0.1 \cdot [2 + 0.9 \cdot v_*(s_1)] \end{cases}$
> $v_*(s_1) = \max \begin{cases} 0.7 \cdot [10 + 90] + 0.3 \cdot [0 + 45] \\ 0.9 \cdot [5 + 45] + 0.1 \cdot [2 + 0.9 \cdot v_*(s_1)] \end{cases}$
> $v_*(s_1) = \max \begin{cases} 0.7 \cdot 100 + 0.3 \cdot 45 \\ 0.9 \cdot 50 + 0.1 \cdot [2 + 0.9 \cdot v_*(s_1)] \end{cases}$
> $v_*(s_1) = \max \begin{cases} 70 + 13.5 \\ 45 + 0.2 + 0.09 \cdot v_*(s_1) \end{cases}$
> $v_*(s_1) = \max \begin{cases} 83.5 \\ 45.2 + 0.09 \cdot v_*(s_1) \end{cases}$
>
> Para encontrar o valor de $v_*(s_1)$, podemos resolver a equa√ß√£o $v_*(s_1) = 45.2 + 0.09 \cdot v_*(s_1)$ se $45.2 + 0.09 \cdot v_*(s_1) > 83.5$.
> $v_*(s_1) - 0.09 \cdot v_*(s_1) = 45.2$
> $0.91 \cdot v_*(s_1) = 45.2$
> $v_*(s_1) = \frac{45.2}{0.91} \approx 49.67$
>
> Como $49.67 < 83.5$, ent√£o $v_*(s_1) = 83.5$, e a a√ß√£o √≥tima no estado $s_1$ √© $a_1$.

A **Bellman optimality equation** para $q_*(s, a)$ √© dada por:

$$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t=s, A_t=a]$$

Esta equa√ß√£o afirma que o valor √≥timo de tomar a a√ß√£o $a$ no estado $s$ √© igual ao retorno esperado que se pode obter ao tomar a a√ß√£o $a$ em $s$ e, em seguida, seguir a pol√≠tica √≥tima a partir do pr√≥ximo estado $S_{t+1}$. Expans√£o [^1]:

$$q_*(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma \max_{a'} q_*(s', a')] \qquad (4.2)$$

> üí° **Exemplo Num√©rico:** Usando o mesmo MDP do exemplo anterior, vamos calcular $q_*(s_1, a_1)$. Sabemos que $v_*(s_2) = 100$ e $v_*(s_3) = 50$.  Da rela√ß√£o entre $q_*$ e $v_*$, temos que $\max_{a'} q_*(s_2, a') = v_*(s_2) = 100$ e $\max_{a'} q_*(s_3, a') = v_*(s_3) = 50$.
>
> $q_*(s_1, a_1) = \sum_{s', r} p(s', r|s_1, a_1) [r + \gamma \max_{a'} q_*(s', a')]$
> $q_*(s_1, a_1) = 0.7 \cdot [10 + 0.9 \cdot v_*(s_2)] + 0.3 \cdot [0 + 0.9 \cdot v_*(s_3)]$
> $q_*(s_1, a_1) = 0.7 \cdot [10 + 0.9 \cdot 100] + 0.3 \cdot [0 + 0.9 \cdot 50]$
> $q_*(s_1, a_1) = 0.7 \cdot [10 + 90] + 0.3 \cdot [0 + 45]$
> $q_*(s_1, a_1) = 0.7 \cdot 100 + 0.3 \cdot 45$
> $q_*(s_1, a_1) = 70 + 13.5 = 83.5$
>
> Este resultado corresponde ao valor que calculamos para $v_*(s_1)$ quando escolhemos $a_1$ como a a√ß√£o √≥tima.

As equa√ß√µes de otimalidade de Bellman s√£o *n√£o-lineares* devido √† opera√ß√£o de m√°ximo ($\max$). Isso significa que n√£o podemos simplesmente resolver essas equa√ß√µes diretamente usando t√©cnicas lineares. Em vez disso, empregamos m√©todos iterativos, como policy iteration e value iteration, para encontrar solu√ß√µes aproximadas [^1].

As equa√ß√µes de Bellman s√£o fundamentais para Dynamic Programming, pois fornecem uma maneira de calcular o valor √≥timo recursivamente. Elas garantem que, ao atualizar iterativamente as estimativas de valor com base nessas equa√ß√µes, as estimativas convergir√£o para os valores √≥timos reais.

**Teorema 1** [Converg√™ncia das itera√ß√µes de valor]: Sob condi√ß√µes de contri√ß√£o (e.g., o ambiente √© um MDP descontado com fator de desconto $\gamma < 1$), a itera√ß√£o de valor converge para a value function √≥tima $v_*$.

*Estrat√©gia de prova:* A prova envolve demonstrar que a itera√ß√£o de valor √© uma aplica√ß√£o de contra√ß√£o no espa√ßo de value functions, utilizando a norma do supremo. Como uma aplica√ß√£o de contra√ß√£o tem um ponto fixo √∫nico (pelo Teorema do Ponto Fixo de Banach), e esse ponto fixo satisfaz a equa√ß√£o de otimalidade de Bellman, segue-se que ele √© a value function √≥tima.

Para formalizar a estrat√©gia de prova do Teorema 1, podemos fornecer a seguinte prova:

**Prova do Teorema 1:**

I.  Definimos o operador de Bellman $T$ para itera√ß√£o de valor como:

$$(Tv)(s) = \max_{a} \sum_{s', r} p(s', r|s, a) [r + \gamma v(s')] \qquad (1)$$

Onde $v$ √© uma value function arbitr√°ria.

II. Mostraremos que $T$ √© uma contra√ß√£o sob a norma do supremo (tamb√©m conhecida como norma do m√°ximo), definida como:

$$||v|| = \max_{s} |v(s)|$$

III. Considere duas value functions arbitr√°rias $u$ e $v$. Queremos mostrar que existe um $\gamma \in [0, 1)$ tal que:

 $$||Tu - Tv|| \le \gamma ||u - v||$$

IV.  Expandindo $||Tu - Tv||$, temos:

 $$||Tu - Tv|| = \max_{s} |(Tu)(s) - (Tv)(s)|$$

V. Usando a defini√ß√£o do operador de Bellman $T$:

$$||Tu - Tv|| = \max_{s} \left| \max_{a} \sum_{s', r} p(s', r|s, a) [r + \gamma u(s')] - \max_{a} \sum_{s', r} p(s', r|s, a) [r + \gamma v(s')] \right|$$

VI. Seja $a_1^*$ a a√ß√£o √≥tima para $(Tu)(s)$ e $a_2^*$ a a√ß√£o √≥tima para $(Tv)(s)$. Ent√£o,

$$||Tu - Tv|| = \max_{s} \left| \sum_{s', r} p(s', r|s, a_1^*) [r + \gamma u(s')] - \sum_{s', r} p(s', r|s, a_2^*) [r + \gamma v(s')] \right|$$

VII. Adicionando e subtraindo um termo similar, podemos escrever:

 $$||Tu - Tv|| = \max_{s} \left| \sum_{s', r} p(s', r|s, a_1^*) [r + \gamma u(s')] - \sum_{s', r} p(s', r|s, a_1^*) [r + \gamma v(s')] + \sum_{s', r} p(s', r|s, a_1^*) [r + \gamma v(s')] - \sum_{s', r} p(s', r|s, a_2^*) [r + \gamma v(s')] \right|$$

VIII. Usando a desigualdade triangular:

  $$||Tu - Tv|| \le \max_{s} \left| \sum_{s', r} p(s', r|s, a_1^*) [r + \gamma u(s')] - \sum_{s', r} p(s', r|s, a_1^*) [r + \gamma v(s')] \right| + \max_{s} \left| \sum_{s', r} p(s', r|s, a_1^*) [r + \gamma v(s')] - \sum_{s', r} p(s', r|s, a_2^*) [r + \gamma v(s')] \right|$$

IX. Simplificando o primeiro termo:
      $$||Tu - Tv|| \le \max_{s} \left| \sum_{s', r} p(s', r|s, a_1^*) \gamma [u(s') - v(s')] \right| + \max_{s} \left| \sum_{s', r} p(s', r|s, a_1^*) [r + \gamma v(s')] - \sum_{s', r} p(s', r|s, a_2^*) [r + \gamma v(s')] \right|$$
X. Como $a_1^*$ √© a a√ß√£o √≥tima para u(s) e $a_2^*$ para v(s), podemos deduzir que:
$$\sum_{s', r} p(s', r|s, a_1^*) [r + \gamma u(s')] \geq \sum_{s', r} p(s', r|s, a_2^*) [r + \gamma u(s')] $$ e
$$\sum_{s', r} p(s', r|s, a_2^*) [r + \gamma v(s')] \geq \sum_{s', r} p(s', r|s, a_1^*) [r + \gamma v(s')] $$
Portanto,
$$\sum_{s', r} p(s', r|s, a_1^*) [r + \gamma v(s')] - \sum_{s', r} p(s', r|s, a_2^*) [r + \gamma v(s')] \leq 0 $$
e
$$\sum_{s', r} p(s', r|s, a_2^*) [r + \gamma u(s')] - \sum_{s', r} p(s', r|s, a_1^*) [r + \gamma u(s')] \leq 0 $$

XI. Usando o fato de que $\sum_{s', r} p(s', r|s, a) = 1$, temos:
       $$||Tu - Tv|| \le \max_{s} \gamma \sum_{s', r} p(s', r|s, a_1^*) |u(s') - v(s')|$$
       $$||Tu - Tv|| \le \gamma \max_{s'} |u(s') - v(s')| \sum_{s', r} p(s', r|s, a_1^*) $$
       $$||Tu - Tv|| \le \gamma \max_{s'} |u(s') - v(s')| $$
       $$||Tu - Tv|| \le \gamma ||u - v||$$

XII. Como $0 \le \gamma < 1$, $T$ √© uma contra√ß√£o.

XIII. Pelo Teorema do Ponto Fixo de Banach, $T$ tem um ponto fixo √∫nico $v_*$, que √© a value function √≥tima, ou seja, $Tv_* = v_*$.

XIV. Portanto, a itera√ß√£o de valor converge para a value function √≥tima $v_*$. ‚ñ†

Al√©m disso, podemos expressar a rela√ß√£o entre $v_*(s)$ e $q_*(s, a)$ de forma expl√≠cita, o que √© √∫til para algoritmos que aprendem uma e usam a outra.

**Lema 2** [Rela√ß√£o entre $v_*(s)$ e $q_*(s, a)$]: A state-value function √≥tima pode ser expressa em termos da action-value function √≥tima como:

$$v_*(s) = \max_a q_*(s, a)$$

*Prova:* Esta rela√ß√£o decorre diretamente da defini√ß√£o de $v_*(s)$ como o valor m√°ximo que pode ser obtido a partir do estado $s$, e da defini√ß√£o de $q_*(s, a)$ como o valor de se tomar a a√ß√£o $a$ no estado $s$ e seguir a pol√≠tica √≥tima a partir da√≠. Portanto, o valor m√°ximo que pode ser obtido a partir de $s$ √© simplesmente o valor da melhor a√ß√£o em $s$.

Para formalizar a prova do Lema 2, podemos fornecer a seguinte prova:

**Prova do Lema 2:**

I.  Defini√ß√£o de $v_*(s)$: O valor √≥timo de um estado $s$ √© o retorno esperado m√°ximo que pode ser obtido seguindo a pol√≠tica √≥tima $\pi_*$.

II. Defini√ß√£o de $q_*(s, a)$: O valor √≥timo de tomar a a√ß√£o $a$ no estado $s$ √© o retorno esperado que se obt√©m ao tomar a a√ß√£o $a$ e, em seguida, seguir a pol√≠tica √≥tima $\pi_*$.

III. Pela defini√ß√£o de pol√≠tica √≥tima, a pol√≠tica √≥tima $\pi_*(s)$ no estado $s$ √© a a√ß√£o que maximiza o valor esperado:
    $$\pi_*(s) = \arg\max_a q_*(s, a)$$

IV. Portanto, o valor √≥timo do estado $s$ √© o valor esperado de seguir a pol√≠tica √≥tima:
    $$v_*(s) = q_*(s, \pi_*(s))$$

V.  Substituindo $\pi_*(s)$ pela defini√ß√£o em (III):
    $$v_*(s) = q_*(s, \arg\max_a q_*(s, a))$$

VI.  Como estamos maximizando sobre todas as a√ß√µes $a$, podemos escrever:
    $$v_*(s) = \max_a q_*(s, a)$$

VII. Portanto, a rela√ß√£o entre $v_*(s)$ e $q_*(s, a)$ √© dada por:
     $$v_*(s) = \max_a q_*(s, a)$$ ‚ñ†

Similarmente, podemos expressar $q_*(s, a)$ em termos de $v_*(s')$:

**Lema 3** [Rela√ß√£o entre $q_*(s, a)$ e $v_*(s')$]: A action-value function √≥tima pode ser expressa em termos da state-value function √≥tima como:

$$q_*(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')] $$

*Prova:* Esta rela√ß√£o √© uma consequ√™ncia direta da equa√ß√£o (4.2) e da defini√ß√£o de $v_*(s')$. Substituindo $v_*(s')$ na equa√ß√£o de Bellman para $q_*(s, a)$, obtemos a rela√ß√£o desejada.

Para formalizar a prova do Lema 3, podemos fornecer a seguinte prova:

**Prova do Lema 3:**

I.  Come√ßamos com a equa√ß√£o de Bellman para $q_*(s, a)$:

    $$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t=s, A_t=a]$$

II. Usando a defini√ß√£o de $v_*(s)$ do Lema 2, temos:

    $$v_*(s') = \max_{a'} q_*(s', a')$$

III. Substitu√≠mos $v_*(s')$ na equa√ß√£o de Bellman para $q_*(s, a)$:

     $$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s, A_t=a]$$

IV. Expandimos o valor esperado em termos da probabilidade de transi√ß√£o:

    $$q_*(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')] $$

V. Portanto, a rela√ß√£o entre $q_*(s, a)$ e $v_*(s')$ √© dada por:

     $$q_*(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')] $$ ‚ñ†

**Exemplo:**
Considere um ambiente simples onde um agente pode se mover para a esquerda ou para a direita. Se o agente estiver em um estado que o leve ao estado terminal, o agente recebe +1 se for bem sucedido ou -1 se fracassar. As probabilidades de transi√ß√£o s√£o conhecidas, e o fator de desconto $\gamma$ √© 0.9. Podemos usar as equa√ß√µes de Bellman para calcular iterativamente o valor √≥timo de cada estado.

> üí° **Exemplo Num√©rico:** Vamos considerar um problema de grade world simples com 4 estados (1, 2, 3, 4) e duas a√ß√µes (esquerda, direita). O objetivo √© alcan√ßar o estado 4, que tem uma recompensa de +1. Todas as outras transi√ß√µes t√™m uma recompensa de 0. Se o agente tentar ir para a esquerda no estado 1 ou para a direita no estado 4, ele permanece no mesmo estado. O fator de desconto √© $\gamma = 0.9$.
>
> Podemos inicializar os valores dos estados como 0: $v(1) = v(2) = v(3) = v(4) = 0$. Vamos realizar uma itera√ß√£o de valor:
>
> *   **Itera√ß√£o 1:**
>
>     *   $v(1) = \max \begin{cases} q(1, \text{esquerda}) \\ q(1, \text{direita}) \end{cases}$
>
>         *   $q(1, \text{esquerda}) = 0 + 0.9 \cdot v(1) = 0$
>         *   $q(1, \text{direita}) = 0 + 0.9 \cdot v(2) = 0$
>         *   $v(1) = 0$
>     *   $v(2) = \max \begin{cases} q(2, \text{esquerda}) \\ q(2, \text{direita}) \end{cases}$
>
>         *   $q(2, \text{esquerda}) = 0 + 0.9 \cdot v(1) = 0$
>         *   $q(2, \text{direita}) = 0 + 0.9 \cdot v(3) = 0$
>         *   $v(2) = 0$
>     *   $v(3) = \max \begin{cases} q(3, \text{esquerda}) \\ q(3, \text{direita}) \end{cases}$
>
>         *   $q(3, \text{esquerda}) = 0 + 0.9 \cdot v(2) = 0$
>         *   $q(3, \text{direita}) = 1 + 0.9 \cdot v(4) = 1$
>         *   $v(3) = 1$
>     *   $v(4) = \max \begin{cases} q(4, \text{esquerda}) \\ q(4, \text{direita}) \end{cases}$
>
>         *   $q(4, \text{esquerda}) = 0 + 0.9 \cdot v(3) = 0.9$
>         *   $q(4, \text{direita}) = 0 + 0.9 \cdot v(4) = 0$
>         *   $v(4) = 0.9$
>
> *   **Itera√ß√£o 2:**
>
>     *   $v(1) = \max \begin{cases} q(1, \text{esquerda}) \\ q(1, \text{direita}) \end{cases}$
>
>         *   $q(1, \text{esquerda}) = 0 + 0.9 \cdot v(1) = 0$
>         *   $q(1, \text{direita}) = 0 + 0.9 \cdot v(2) = 0$
>         *   $v(1) = 0$
>     *   $v(2) = \max \begin{cases} q(2, \text{esquerda}) \\ q(2, \text{direita}) \end{cases}$
>
>         *   $q(2, \text{esquerda}) = 0 + 0.9 \cdot v(1) = 0$
>         *   $q(2, \text{direita}) = 0 + 0.9 \cdot v(3) = 0.9$
>         *   $v(2) = 0.9$
>     *   $v(3) = \max \begin{cases} q(3, \text{esquerda}) \\ q(3, \text{direita}) \end{cases}$
>
>         *   $q(3, \text{esquerda}) = 0 + 0.9 \cdot v(2) = 0.81$
>         *   $q(3, \text{direita}) = 1 + 0.9 \cdot v(4) = 1.81$
>         *   $v(3) = 1.81$
>     *   $v(4) = \max \begin{cases} q(4, \text{esquerda}) \\ q(4, \text{direita}) \end{cases}$
>
>         *   $q(4, \text{esquerda}) = 0 + 0.9 \cdot v(3) = 1.629$
>         *   $q(4, \text{direita}) = 0 + 0.9 \cdot v(4) = 0.81$
>         *   $v(4) = 1.629$
>
> Podemos observar que os valores dos estados est√£o come√ßando a convergir. Ap√≥s v√°rias itera√ß√µes, os valores convergir√£o para os valores √≥timos.

Para ilustrar o Teorema 1, podemos simular a itera√ß√£o de valor neste ambiente simples e observar a converg√™ncia das estimativas de valor para os valores √≥timos reais.

Este c√≥digo simula a itera√ß√£o de valor para um grid world simples e mostra como os valores dos estados convergem ao longo das itera√ß√µes. O gr√°fico gerado visualiza essa converg√™ncia, demonstrando o Teorema 1 na pr√°tica.
Um exemplo pr√°tico que pode ajudar na visualiza√ß√£o √© o *gridworld*.

![Illustration of a 4x4 gridworld environment with rewards and actions for dynamic programming example.](./../images/image9.png)

A imagem acima ilustra um gridworld 4x4, onde cada c√©lula representa um estado. As setas indicam as poss√≠veis a√ß√µes (cima, baixo, esquerda, direita) e o objetivo √© encontrar a pol√≠tica √≥tima que maximiza o retorno. As c√©lulas sombreadas s√£o estados terminais.

Outro exemplo que pode ser √∫til √© a converg√™ncia da avalia√ß√£o iterativa da pol√≠tica.

![Convergence of iterative policy evaluation on a gridworld, showing improvement from random to optimal policy.](./../images/image7.png)

Esta figura demonstra a converg√™ncia da avalia√ß√£o iterativa de pol√≠tica em um gridworld 4x4. A coluna da esquerda mostra as aproxima√ß√µes sucessivas da fun√ß√£o valor-estado para uma pol√≠tica aleat√≥ria, enquanto a coluna da direita ilustra as pol√≠ticas *greedy* correspondentes. Observa-se a melhoria iterativa das pol√≠ticas, progredindo de uma pol√≠tica aleat√≥ria para uma pol√≠tica √≥tima.

Um exemplo mais complexo √© o problema do aluguel de carros de Jack.

![Policy iteration for Jack's car rental problem, showing policy improvements and the final state-value function.](./../images/image8.png)

Esta figura ilustra a sequ√™ncia de pol√≠ticas encontradas pela itera√ß√£o de pol√≠tica no problema de aluguel de carros de Jack. Os cinco primeiros diagramas representam as pol√≠ticas de \(\pi_0\) a \(\pi_4\), indicando o n√∫mero de carros a serem movidos entre dois locais. O gr√°fico 3D mostra a fun√ß√£o valor-estado final \(v_{\pi_4}\).

O algoritmo de itera√ß√£o de pol√≠ticas √© um m√©todo para encontrar a pol√≠tica √≥tima.

![Policy Iteration algorithm: iterative process of policy evaluation and improvement for optimal policy estimation.](./../images/image3.png)

A imagem descreve o algoritmo de Itera√ß√£o de Pol√≠tica, que itera entre a avalia√ß√£o da pol√≠tica e a melhoria da pol√≠tica at√© a converg√™ncia.

O algoritmo de itera√ß√£o de valor √© outro m√©todo para encontrar a pol√≠tica √≥tima.

![Pseudocode for Value Iteration algorithm, a method for estimating the optimal policy in an MDP.](./../images/image4.png)

A imagem mostra o pseudoc√≥digo para a Itera√ß√£o de Valor, um algoritmo que combina os passos de avalia√ß√£o e melhoria da pol√≠tica em cada varredura, levando a uma converg√™ncia mais r√°pida.

A avalia√ß√£o iterativa de pol√≠tica √© um algoritmo usado para estimar a fun√ß√£o de valor de estado.

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

Esta imagem apresenta o pseudoc√≥digo para Avalia√ß√£o Iterativa de Pol√≠tica, um algoritmo usado para estimar a fun√ß√£o valor-estado V para uma dada pol√≠tica œÄ.

A itera√ß√£o de pol√≠tica generalizada (GPI) √© um conceito chave que envolve a intera√ß√£o entre avalia√ß√£o e melhoria de pol√≠ticas.

![Diagrama da itera√ß√£o da pol√≠tica generalizada (GPI) mostrando o ciclo entre avalia√ß√£o e melhoria da pol√≠tica.](./../images/image2.png)

A imagem acima representa o diagrama da GPI, mostrando o ciclo iterativo entre a avalia√ß√£o da pol√≠tica e a melhoria da pol√≠tica.

![Diagrama representando a intera√ß√£o entre avalia√ß√£o e melhoria de pol√≠ticas na itera√ß√£o da pol√≠tica generalizada (GPI).](./../images/image6.png)

Esta imagem tamb√©m ilustra o conceito de GPI, enfatizando a intera√ß√£o entre os processos de avalia√ß√£o e melhoria da pol√≠tica.

Um exemplo concreto de aplica√ß√£o dos conceitos apresentados √© o problema do apostador.

![Value function sweeps and final policy for the gambler's problem with ph = 0.4.](./../images/image1.png)

A imagem ilustra a solu√ß√£o para o Problema do Apostador com ph=0.4. O gr√°fico superior mostra as fun√ß√µes de valor obtidas atrav√©s de varreduras sucessivas de itera√ß√£o de valor e o gr√°fico inferior representa a pol√≠tica final.

### Conclus√£o
As value functions e as equa√ß√µes de otimalidade de Bellman s√£o os pilares da programa√ß√£o din√¢mica e fornecem um framework poderoso para encontrar pol√≠ticas √≥timas em Markov Decision Processes (MDPs) [^1]. A habilidade de decompor o problema de otimiza√ß√£o em subproblemas menores, juntamente com os m√©todos iterativos para resolver as equa√ß√µes de Bellman, torna a programa√ß√£o din√¢mica uma t√©cnica valiosa para uma ampla gama de aplica√ß√µes em reinforcement learning.

### Refer√™ncias
[^1]: Dynamic Programming, Richard S. Sutton and Andrew G. Barto, Reinforcement Learning: An Introduction, 2nd Edition, 2020.
<!-- END -->