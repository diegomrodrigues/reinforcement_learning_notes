## A Equação de Otimalidade de Bellman para a Função de Valor de Estado

### Introdução

Este capítulo aprofunda-se no conceito fundamental da **equação de otimalidade de Bellman** para a **função de valor de estado**, denotada por $v_*(s)$ [^1]. Esta equação é uma peça central na programação dinâmica (DP) e fornece uma condição necessária para a otimalidade em processos de decisão de Markov (MDPs). Exploraremos como essa equação captura a essência de encontrar a melhor política, equilibrando recompensas imediatas com valores futuros descontados [^1].

### Conceitos Fundamentais

A **equação de otimalidade de Bellman** para a função de valor de estado $v_*(s)$ é expressa como [^1]:

$$ v_*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] $$

Esta equação afirma que o *valor de um estado sob uma política ótima* ($v_*(s)$) é igual ao *valor esperado da recompensa imediata* ($R_{t+1}$) mais o *valor futuro descontado* ($\gamma v_*(S_{t+1})$), *maximizado sobre todas as ações possíveis* ($a$) no estado $s$ [^1].

**Desmembrando a equação:**

*   $v_*(s)$: Representa o valor ótimo do estado $s$, ou seja, a recompensa total esperada que o agente pode acumular seguindo a política ótima a partir do estado $s$.
*   $\max_a$: Este operador enfatiza que estamos interessados na ação que maximiza o valor esperado. A política ótima sempre escolherá a ação que leva ao melhor resultado esperado.
*   $\mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$:  Representa o valor esperado da soma da recompensa imediata ($R_{t+1}$) e do valor descontado do próximo estado ($v_*(S_{t+1})$), dado que estamos no estado $S_t = s$ e tomamos a ação $A_t = a$.
*   $R_{t+1}$: A recompensa que o agente recebe imediatamente após realizar a ação $a$ no estado $s$.
*   $\gamma$: O fator de desconto, onde $0 \leq \gamma \leq 1$. Ele determina o quanto os valores futuros são descontados. Um $\gamma$ próximo de 0 torna o agente míope, enquanto um $\gamma$ próximo de 1 torna o agente mais focado em recompensas de longo prazo.
*   $S_{t+1}$: O próximo estado em que o agente se encontrará após realizar a ação $a$ no estado $s$.
*   $v_*(S_{t+1})$: O valor ótimo do próximo estado $S_{t+1}$.

> 💡 **Exemplo Numérico:** Imagine um agente em um grid world. No estado $s$, o agente pode ir para a direita ou para a esquerda. Ir para a direita dá uma recompensa imediata de $R_{t+1} = 1$ e leva ao estado $s'$, com um valor ótimo de $v_*(s') = 10$. Ir para a esquerda dá uma recompensa imediata de $R_{t+1} = 0$ e leva ao estado $s''$, com um valor ótimo de $v_*(s'') = 5$. Se o fator de desconto for $\gamma = 0.9$, então:
>
> Valor de ir para a direita: $1 + 0.9 * 10 = 10$
>
> Valor de ir para a esquerda: $0 + 0.9 * 5 = 4.5$
>
> Portanto, $v_*(s) = \max(10, 4.5) = 10$. O agente deve ir para a direita.
>
> ```python
> import numpy as np
>
> # Define the parameters
> reward_right = 1
> next_state_value_right = 10
> reward_left = 0
> next_state_value_left = 5
> gamma = 0.9
>
> # Calculate the value of each action
> value_right = reward_right + gamma * next_state_value_right
> value_left = reward_left + gamma * next_state_value_left
>
> # Determine the optimal value and action
> optimal_value = max(value_right, value_left)
> optimal_action = "right" if value_right > value_left else "left"
>
> print(f"Value of going right: {value_right}")
> print(f"Value of going left: {value_left}")
> print(f"Optimal value: {optimal_value}")
> print(f"Optimal action: {optimal_action}")
> ```

A equação de otimalidade de Bellman pode ser reescrita utilizando a dinâmica do MDP, $p(s', r | s, a)$, que define a probabilidade de transição para o estado $s'$ e receber recompensa $r$ ao tomar a ação $a$ no estado $s$ [^1]:

$$ v_*(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$

Esta formulação explicita a dependência do valor esperado sobre as probabilidades de transição e recompensas inerentes ao ambiente.

> 💡 **Exemplo Numérico:** Considere um MDP simples com dois estados ($s_1, s_2$) e duas ações ($a_1, a_2$) no estado $s_1$.  As probabilidades de transição e recompensas são dadas por:
>
> *   $p(s_1, 5 | s_1, a_1) = 0.8$, $p(s_2, 5 | s_1, a_1) = 0.2$
> *   $p(s_2, 10 | s_1, a_2) = 1.0$
>
> Seja $\gamma = 0.9$ e $v_*(s_2) = 20$. Queremos calcular $v_*(s_1)$. Assumimos que $v_*(s_2)$ já é conhecido (por exemplo, por iteração de valor).
>
> Para a ação $a_1$:
> $\sum_{s', r} p(s', r | s_1, a_1) [r + \gamma v_*(s')] = 0.8 * [5 + 0.9 * v_*(s_1)] + 0.2 * [5 + 0.9 * 20]$
>
> Para a ação $a_2$:
> $\sum_{s', r} p(s', r | s_1, a_2) [r + \gamma v_*(s')] = 1.0 * [10 + 0.9 * 20] = 28$
>
> Assumindo que $v_*(s_1)$ é conhecido, podemos calcular o valor exato da ação $a_1$, se não conhecemos $v_*(s_1)$ ainda, podemos iterar até convergência usando Bellman Iteration.
>
> Se, por exemplo, $v_*(s_1) = 25$:
>
> Para a ação $a_1$:
> $\sum_{s', r} p(s', r | s_1, a_1) [r + \gamma v_*(s')] = 0.8 * [5 + 0.9 * 25] + 0.2 * [5 + 0.9 * 18] = 0.8 * 27.5 + 0.2 * 21.2 = 22 + 4.24= 26.24$
>
> Portanto, $v_*(s_1) = \max(26.6, 28) = 28$, e a ação ótima é $a_2$.
>
> ```python
> import numpy as np
>
> # Define the parameters
> gamma = 0.9
> v_s2 = 20
> v_s1 = 25  # Example value for v_*(s1)
>
> # Action a1 parameters
> p_s1_a1 = 0.8
> p_s2_a1 = 0.2
> r_s1_a1 = 5
> r_s2_a1 = 5
>
> # Action a2 parameters
> p_s2_a2 = 1.0
> r_s2_a2 = 10
>
> # Calculate the value of action a1
> value_a1 = p_s1_a1 * (r_s1_a1 + gamma * v_s1) + p_s2_a1 * (r_s2_a1 + gamma * v_s2)
>
> # Calculate the value of action a2
> value_a2 = p_s2_a2 * (r_s2_a2 + gamma * v_s2)
>
> # Determine the optimal value and action
> optimal_value = max(value_a1, value_a2)
> optimal_action = "a1" if value_a1 > value_a2 else "a2"
>
> print(f"Value of action a1: {value_a1}")
> print(f"Value of action a2: {value_a2}")
> print(f"Optimal value: {optimal_value}")
> print(f"Optimal action: {optimal_action}")
> ```

**Intuição:**

A equação de otimalidade de Bellman essencialmente afirma que, para encontrar o valor ótimo de um estado, devemos considerar todas as ações possíveis e selecionar aquela que leva à melhor combinação de recompensa imediata e valor futuro descontado. Ela decompõe o problema de otimização em uma série de subproblemas menores, que podem ser resolvidos iterativamente [^1].

Para solidificar nossa compreensão, podemos derivar a equação de otimalidade de Bellman passo a passo, partindo da definição de valor ótimo.

**Derivação da Equação de Otimalidade de Bellman:**

O valor ótimo de um estado $s$ é definido como o máximo retorno esperado que pode ser obtido a partir de $s$, seguindo qualquer política $\pi$:

$$ v_*(s) = \max_\pi \mathbb{E}_\pi[G_t | S_t = s] $$

onde $G_t$ é o retorno, dado por:

$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots $$

Podemos reescrever o retorno como:

$$ G_t = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \ldots) = R_{t+1} + \gamma G_{t+1} $$

Substituindo na equação do valor ótimo:

$$ v_*(s) = \max_\pi \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s] $$

Agora, usamos o fato de que, sob a política ótima, o valor esperado do retorno futuro é o valor ótimo do próximo estado:

$$ v_*(s) = \max_\pi \mathbb{E}_\pi[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s] $$

Finalmente, a maximização sobre todas as políticas se traduz na maximização sobre todas as ações possíveis no estado $s$:

$$ v_*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] $$

Este resultado é a equação de otimalidade de Bellman para a função de valor de estado.

**Prova:**

Provaremos formalmente a derivação da Equação de Otimalidade de Bellman:

I. Defina o valor ótimo de um estado $s$ como o retorno esperado máximo possível a partir desse estado sob qualquer política $\pi$:
    $$v_*(s) = \max_{\pi} \mathbb{E}_{\pi} [G_t | S_t = s]$$
    onde $G_t$ representa o retorno a partir do tempo $t$.

II. Expresse o retorno $G_t$ recursivamente:
    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) = R_{t+1} + \gamma G_{t+1}$$

III. Substitua a expressão recursiva de $G_t$ na definição de $v_*(s)$:
     $$v_*(s) = \max_{\pi} \mathbb{E}_{\pi} [R_{t+1} + \gamma G_{t+1} | S_t = s]$$

IV. Reconheça que, seguindo a política ótima, o valor esperado de $\gamma G_{t+1}$ dado $S_t = s$ é equivalente a $\gamma v_*(S_{t+1})$:
    $$v_*(s) = \max_{\pi} \mathbb{E}_{\pi} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s]$$

V. Reescreva a esperança sobre todas as políticas como uma esperança sobre todas as ações possíveis $a$ no estado $s$:
   $$v_*(s) = \max_{a} \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$

VI. Portanto, chegamos à equação de otimalidade de Bellman para a função de valor de estado:
    $$v_*(s) = \max_{a} \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$ ■

**Relação com a função Q-valor:**

A equação de otimalidade de Bellman também pode ser expressa em termos da função Q-valor, $q_*(s, a)$, que representa o valor de tomar a ação $a$ no estado $s$ e seguir a política ótima a partir daí [^1]:

$$ q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a] $$

ou

$$ q_*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a')] $$

> 💡 **Exemplo Numérico:**  Suponha que estejamos em um estado $s$ e considerando a ação $a$. Após tomar a ação $a$, o agente transita para o estado $s'$ com probabilidade 1, e recebe uma recompensa de $r = 2$. No estado $s'$, o agente tem duas ações disponíveis, $a_1'$ e $a_2'$, com valores Q $q_*(s', a_1') = 5$ e $q_*(s', a_2') = 8$, respectivamente. Usando um fator de desconto $\gamma = 0.9$, o valor Q ótimo $q_*(s, a)$ é calculado como:
>
> $q_*(s, a) = 2 + 0.9 * \max(5, 8) = 2 + 0.9 * 8 = 2 + 7.2 = 9.2$
>
> ```python
> import numpy as np
>
> # Define the parameters
> reward = 2
> q_s_prime_a1 = 5
> q_s_prime_a2 = 8
> gamma = 0.9
>
> # Calculate the maximum Q-value in the next state
> max_q_s_prime = max(q_s_prime_a1, q_s_prime_a2)
>
> # Calculate the Q-value for the current state and action
> q_s_a = reward + gamma * max_q_s_prime
>
> print(f"Q-value for the current state and action: {q_s_a}")
> ```

Neste caso, $v_*(s)$ pode ser obtido maximizando $q_*(s, a)$ sobre todas as ações $a$:

$$ v_*(s) = \max_a q_*(s, a) $$

Podemos também expressar a equação de otimalidade de Bellman para a função Q-valor em termos da função de valor de estado $v_*(s')$ do próximo estado:

$$ q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] $$

ou

$$ q_*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$

> 💡 **Exemplo Numérico:** Considere o mesmo exemplo anterior, mas agora conhecemos o valor ótimo do estado seguinte $s'$, que é $v_*(s') = 8$. Tomando a ação $a$ no estado $s$ nos dá uma recompensa de $r = 2$.
>
> Então, o valor Q ótimo $q_*(s, a)$ é:
>
> $q_*(s, a) = 2 + 0.9 * 8 = 9.2$
>
> ```python
> import numpy as np
>
> # Define the parameters
> reward = 2
> v_s_prime = 8
> gamma = 0.9
>
> # Calculate the Q-value for the current state and action
> q_s_a = reward + gamma * v_s_prime
>
> print(f"Q-value for the current state and action: {q_s_a}")
> ```

Esta formulação explicita a relação dual entre a função Q-valor e a função de valor de estado.

**Prova:**

Mostraremos como a função Q-valor $q_*(s, a)$ está relacionada com a função de valor de estado $v_*(s)$:

I. Defina a função Q-valor $q_*(s, a)$ como o retorno esperado máximo obtido começando no estado $s$, tomando ação $a$ e seguindo a política ótima a partir daí:
   $$q_*(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a, \pi = \pi_*]$$

II.  Reescreva o retorno $G_t$ recursivamente:
    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) = R_{t+1} + \gamma G_{t+1}$$

III. Substitua na definição da função Q-valor:
    $$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a, \pi = \pi_*]$$

IV.  Reconheça que, seguindo a política ótima, o valor esperado de $\gamma G_{t+1}$ dado $S_t = s$ e $A_t = a$ é equivalente a $\gamma v_*(S_{t+1})$, onde $v_*(S_{t+1})$ é o valor ótimo do próximo estado:
     $$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$

V. Para expressar $v_*(s)$ em termos de $q_*(s, a)$, reconhecemos que o valor ótimo de um estado é o valor esperado máximo da função Q-valor sobre todas as ações possíveis:
   $$v_*(s) = \max_a q_*(s, a)$$

VI. Substituindo a expressão de $q_*(s, a)$, obtemos:
 $$v_*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$ ■

**Teorema 1** [Existência e Unicidade da Solução] A equação de otimalidade de Bellman possui uma solução única para $v_*$ e $q_*$, sob a condição de que o ambiente seja um MDP com um número finito de estados e ações, e que $0 \leq \gamma < 1$.

*Prova (Esboço)*: A prova da existência e unicidade da solução para a equação de Bellman geralmente envolve mostrar que o operador de Bellman é uma contração em um espaço de Banach, o que garante a convergência para uma solução única.

**Importância:**

A equação de otimalidade de Bellman é crucial porque fornece uma base para calcular políticas ótimas. Resolvendo essa equação (ou suas variantes), podemos determinar os valores ótimos dos estados e, em seguida, derivar uma política ótima que alcança esses valores.

### Conclusão

A equação de otimalidade de Bellman para a função de valor de estado é uma ferramenta poderosa na programação dinâmica. Ela encapsula a ideia de que uma política ótima deve equilibrar recompensas imediatas com valores futuros descontados. Ao resolver esta equação, podemos encontrar políticas ótimas para processos de decisão de Markov finitos [^1]. Este conceito forma a base para muitos algoritmos de reinforcement learning, permitindo que agentes aprendam a tomar decisões ótimas em ambientes complexos. $\blacksquare$

### Referências

[^1]: Dynamic Programming, Capítulo 4, Advanced Study of Reinforcement Learning Fundamentals
<!-- END -->