## A Equa√ß√£o de Otimalidade de Bellman para a Fun√ß√£o de Valor de Estado

### Introdu√ß√£o

Este cap√≠tulo aprofunda-se no conceito fundamental da **equa√ß√£o de otimalidade de Bellman** para a **fun√ß√£o de valor de estado**, denotada por $v_*(s)$ [^1]. Esta equa√ß√£o √© uma pe√ßa central na programa√ß√£o din√¢mica (DP) e fornece uma condi√ß√£o necess√°ria para a otimalidade em processos de decis√£o de Markov (MDPs). Exploraremos como essa equa√ß√£o captura a ess√™ncia de encontrar a melhor pol√≠tica, equilibrando recompensas imediatas com valores futuros descontados [^1].

### Conceitos Fundamentais

A **equa√ß√£o de otimalidade de Bellman** para a fun√ß√£o de valor de estado $v_*(s)$ √© expressa como [^1]:

$$ v_*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] $$

Esta equa√ß√£o afirma que o *valor de um estado sob uma pol√≠tica √≥tima* ($v_*(s)$) √© igual ao *valor esperado da recompensa imediata* ($R_{t+1}$) mais o *valor futuro descontado* ($\gamma v_*(S_{t+1})$), *maximizado sobre todas as a√ß√µes poss√≠veis* ($a$) no estado $s$ [^1].

**Desmembrando a equa√ß√£o:**

*   $v_*(s)$: Representa o valor √≥timo do estado $s$, ou seja, a recompensa total esperada que o agente pode acumular seguindo a pol√≠tica √≥tima a partir do estado $s$.
*   $\max_a$: Este operador enfatiza que estamos interessados na a√ß√£o que maximiza o valor esperado. A pol√≠tica √≥tima sempre escolher√° a a√ß√£o que leva ao melhor resultado esperado.
*   $\mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$:  Representa o valor esperado da soma da recompensa imediata ($R_{t+1}$) e do valor descontado do pr√≥ximo estado ($v_*(S_{t+1})$), dado que estamos no estado $S_t = s$ e tomamos a a√ß√£o $A_t = a$.
*   $R_{t+1}$: A recompensa que o agente recebe imediatamente ap√≥s realizar a a√ß√£o $a$ no estado $s$.
*   $\gamma$: O fator de desconto, onde $0 \leq \gamma \leq 1$. Ele determina o quanto os valores futuros s√£o descontados. Um $\gamma$ pr√≥ximo de 0 torna o agente m√≠ope, enquanto um $\gamma$ pr√≥ximo de 1 torna o agente mais focado em recompensas de longo prazo.
*   $S_{t+1}$: O pr√≥ximo estado em que o agente se encontrar√° ap√≥s realizar a a√ß√£o $a$ no estado $s$.
*   $v_*(S_{t+1})$: O valor √≥timo do pr√≥ximo estado $S_{t+1}$.

> üí° **Exemplo Num√©rico:** Imagine um agente em um grid world. No estado $s$, o agente pode ir para a direita ou para a esquerda. Ir para a direita d√° uma recompensa imediata de $R_{t+1} = 1$ e leva ao estado $s'$, com um valor √≥timo de $v_*(s') = 10$. Ir para a esquerda d√° uma recompensa imediata de $R_{t+1} = 0$ e leva ao estado $s''$, com um valor √≥timo de $v_*(s'') = 5$. Se o fator de desconto for $\gamma = 0.9$, ent√£o:
>
> Valor de ir para a direita: $1 + 0.9 * 10 = 10$
>
> Valor de ir para a esquerda: $0 + 0.9 * 5 = 4.5$
>
> Portanto, $v_*(s) = \max(10, 4.5) = 10$. O agente deve ir para a direita.
>
> ```python
> import numpy as np
>
> # Define the parameters
> reward_right = 1
> next_state_value_right = 10
> reward_left = 0
> next_state_value_left = 5
> gamma = 0.9
>
> # Calculate the value of each action
> value_right = reward_right + gamma * next_state_value_right
> value_left = reward_left + gamma * next_state_value_left
>
> # Determine the optimal value and action
> optimal_value = max(value_right, value_left)
> optimal_action = "right" if value_right > value_left else "left"
>
> print(f"Value of going right: {value_right}")
> print(f"Value of going left: {value_left}")
> print(f"Optimal value: {optimal_value}")
> print(f"Optimal action: {optimal_action}")
> ```

A equa√ß√£o de otimalidade de Bellman pode ser reescrita utilizando a din√¢mica do MDP, $p(s', r | s, a)$, que define a probabilidade de transi√ß√£o para o estado $s'$ e receber recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$ [^1]:

$$ v_*(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$

Esta formula√ß√£o explicita a depend√™ncia do valor esperado sobre as probabilidades de transi√ß√£o e recompensas inerentes ao ambiente.

> üí° **Exemplo Num√©rico:** Considere um MDP simples com dois estados ($s_1, s_2$) e duas a√ß√µes ($a_1, a_2$) no estado $s_1$.  As probabilidades de transi√ß√£o e recompensas s√£o dadas por:
>
> *   $p(s_1, 5 | s_1, a_1) = 0.8$, $p(s_2, 5 | s_1, a_1) = 0.2$
> *   $p(s_2, 10 | s_1, a_2) = 1.0$
>
> Seja $\gamma = 0.9$ e $v_*(s_2) = 20$. Queremos calcular $v_*(s_1)$. Assumimos que $v_*(s_2)$ j√° √© conhecido (por exemplo, por itera√ß√£o de valor).
>
> Para a a√ß√£o $a_1$:
> $\sum_{s', r} p(s', r | s_1, a_1) [r + \gamma v_*(s')] = 0.8 * [5 + 0.9 * v_*(s_1)] + 0.2 * [5 + 0.9 * 20]$
>
> Para a a√ß√£o $a_2$:
> $\sum_{s', r} p(s', r | s_1, a_2) [r + \gamma v_*(s')] = 1.0 * [10 + 0.9 * 20] = 28$
>
> Assumindo que $v_*(s_1)$ √© conhecido, podemos calcular o valor exato da a√ß√£o $a_1$, se n√£o conhecemos $v_*(s_1)$ ainda, podemos iterar at√© converg√™ncia usando Bellman Iteration.
>
> Se, por exemplo, $v_*(s_1) = 25$:
>
> Para a a√ß√£o $a_1$:
> $\sum_{s', r} p(s', r | s_1, a_1) [r + \gamma v_*(s')] = 0.8 * [5 + 0.9 * 25] + 0.2 * [5 + 0.9 * 18] = 0.8 * 27.5 + 0.2 * 21.2 = 22 + 4.24= 26.24$
>
> Portanto, $v_*(s_1) = \max(26.6, 28) = 28$, e a a√ß√£o √≥tima √© $a_2$.
>
> ```python
> import numpy as np
>
> # Define the parameters
> gamma = 0.9
> v_s2 = 20
> v_s1 = 25  # Example value for v_*(s1)
>
> # Action a1 parameters
> p_s1_a1 = 0.8
> p_s2_a1 = 0.2
> r_s1_a1 = 5
> r_s2_a1 = 5
>
> # Action a2 parameters
> p_s2_a2 = 1.0
> r_s2_a2 = 10
>
> # Calculate the value of action a1
> value_a1 = p_s1_a1 * (r_s1_a1 + gamma * v_s1) + p_s2_a1 * (r_s2_a1 + gamma * v_s2)
>
> # Calculate the value of action a2
> value_a2 = p_s2_a2 * (r_s2_a2 + gamma * v_s2)
>
> # Determine the optimal value and action
> optimal_value = max(value_a1, value_a2)
> optimal_action = "a1" if value_a1 > value_a2 else "a2"
>
> print(f"Value of action a1: {value_a1}")
> print(f"Value of action a2: {value_a2}")
> print(f"Optimal value: {optimal_value}")
> print(f"Optimal action: {optimal_action}")
> ```

**Intui√ß√£o:**

A equa√ß√£o de otimalidade de Bellman essencialmente afirma que, para encontrar o valor √≥timo de um estado, devemos considerar todas as a√ß√µes poss√≠veis e selecionar aquela que leva √† melhor combina√ß√£o de recompensa imediata e valor futuro descontado. Ela decomp√µe o problema de otimiza√ß√£o em uma s√©rie de subproblemas menores, que podem ser resolvidos iterativamente [^1].

Para solidificar nossa compreens√£o, podemos derivar a equa√ß√£o de otimalidade de Bellman passo a passo, partindo da defini√ß√£o de valor √≥timo.

**Deriva√ß√£o da Equa√ß√£o de Otimalidade de Bellman:**

O valor √≥timo de um estado $s$ √© definido como o m√°ximo retorno esperado que pode ser obtido a partir de $s$, seguindo qualquer pol√≠tica $\pi$:

$$ v_*(s) = \max_\pi \mathbb{E}_\pi[G_t | S_t = s] $$

onde $G_t$ √© o retorno, dado por:

$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots $$

Podemos reescrever o retorno como:

$$ G_t = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \ldots) = R_{t+1} + \gamma G_{t+1} $$

Substituindo na equa√ß√£o do valor √≥timo:

$$ v_*(s) = \max_\pi \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s] $$

Agora, usamos o fato de que, sob a pol√≠tica √≥tima, o valor esperado do retorno futuro √© o valor √≥timo do pr√≥ximo estado:

$$ v_*(s) = \max_\pi \mathbb{E}_\pi[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s] $$

Finalmente, a maximiza√ß√£o sobre todas as pol√≠ticas se traduz na maximiza√ß√£o sobre todas as a√ß√µes poss√≠veis no estado $s$:

$$ v_*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] $$

Este resultado √© a equa√ß√£o de otimalidade de Bellman para a fun√ß√£o de valor de estado.

**Prova:**

Provaremos formalmente a deriva√ß√£o da Equa√ß√£o de Otimalidade de Bellman:

I. Defina o valor √≥timo de um estado $s$ como o retorno esperado m√°ximo poss√≠vel a partir desse estado sob qualquer pol√≠tica $\pi$:
    $$v_*(s) = \max_{\pi} \mathbb{E}_{\pi} [G_t | S_t = s]$$
    onde $G_t$ representa o retorno a partir do tempo $t$.

II. Expresse o retorno $G_t$ recursivamente:
    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) = R_{t+1} + \gamma G_{t+1}$$

III. Substitua a express√£o recursiva de $G_t$ na defini√ß√£o de $v_*(s)$:
     $$v_*(s) = \max_{\pi} \mathbb{E}_{\pi} [R_{t+1} + \gamma G_{t+1} | S_t = s]$$

IV. Reconhe√ßa que, seguindo a pol√≠tica √≥tima, o valor esperado de $\gamma G_{t+1}$ dado $S_t = s$ √© equivalente a $\gamma v_*(S_{t+1})$:
    $$v_*(s) = \max_{\pi} \mathbb{E}_{\pi} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s]$$

V. Reescreva a esperan√ßa sobre todas as pol√≠ticas como uma esperan√ßa sobre todas as a√ß√µes poss√≠veis $a$ no estado $s$:
   $$v_*(s) = \max_{a} \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$

VI. Portanto, chegamos √† equa√ß√£o de otimalidade de Bellman para a fun√ß√£o de valor de estado:
    $$v_*(s) = \max_{a} \mathbb{E} [R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$ ‚ñ†

**Rela√ß√£o com a fun√ß√£o Q-valor:**

A equa√ß√£o de otimalidade de Bellman tamb√©m pode ser expressa em termos da fun√ß√£o Q-valor, $q_*(s, a)$, que representa o valor de tomar a a√ß√£o $a$ no estado $s$ e seguir a pol√≠tica √≥tima a partir da√≠ [^1]:

$$ q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a] $$

ou

$$ q_*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a')] $$

> üí° **Exemplo Num√©rico:**  Suponha que estejamos em um estado $s$ e considerando a a√ß√£o $a$. Ap√≥s tomar a a√ß√£o $a$, o agente transita para o estado $s'$ com probabilidade 1, e recebe uma recompensa de $r = 2$. No estado $s'$, o agente tem duas a√ß√µes dispon√≠veis, $a_1'$ e $a_2'$, com valores Q $q_*(s', a_1') = 5$ e $q_*(s', a_2') = 8$, respectivamente. Usando um fator de desconto $\gamma = 0.9$, o valor Q √≥timo $q_*(s, a)$ √© calculado como:
>
> $q_*(s, a) = 2 + 0.9 * \max(5, 8) = 2 + 0.9 * 8 = 2 + 7.2 = 9.2$
>
> ```python
> import numpy as np
>
> # Define the parameters
> reward = 2
> q_s_prime_a1 = 5
> q_s_prime_a2 = 8
> gamma = 0.9
>
> # Calculate the maximum Q-value in the next state
> max_q_s_prime = max(q_s_prime_a1, q_s_prime_a2)
>
> # Calculate the Q-value for the current state and action
> q_s_a = reward + gamma * max_q_s_prime
>
> print(f"Q-value for the current state and action: {q_s_a}")
> ```

Neste caso, $v_*(s)$ pode ser obtido maximizando $q_*(s, a)$ sobre todas as a√ß√µes $a$:

$$ v_*(s) = \max_a q_*(s, a) $$

Podemos tamb√©m expressar a equa√ß√£o de otimalidade de Bellman para a fun√ß√£o Q-valor em termos da fun√ß√£o de valor de estado $v_*(s')$ do pr√≥ximo estado:

$$ q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] $$

ou

$$ q_*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$

> üí° **Exemplo Num√©rico:** Considere o mesmo exemplo anterior, mas agora conhecemos o valor √≥timo do estado seguinte $s'$, que √© $v_*(s') = 8$. Tomando a a√ß√£o $a$ no estado $s$ nos d√° uma recompensa de $r = 2$.
>
> Ent√£o, o valor Q √≥timo $q_*(s, a)$ √©:
>
> $q_*(s, a) = 2 + 0.9 * 8 = 9.2$
>
> ```python
> import numpy as np
>
> # Define the parameters
> reward = 2
> v_s_prime = 8
> gamma = 0.9
>
> # Calculate the Q-value for the current state and action
> q_s_a = reward + gamma * v_s_prime
>
> print(f"Q-value for the current state and action: {q_s_a}")
> ```

Esta formula√ß√£o explicita a rela√ß√£o dual entre a fun√ß√£o Q-valor e a fun√ß√£o de valor de estado.

**Prova:**

Mostraremos como a fun√ß√£o Q-valor $q_*(s, a)$ est√° relacionada com a fun√ß√£o de valor de estado $v_*(s)$:

I. Defina a fun√ß√£o Q-valor $q_*(s, a)$ como o retorno esperado m√°ximo obtido come√ßando no estado $s$, tomando a√ß√£o $a$ e seguindo a pol√≠tica √≥tima a partir da√≠:
   $$q_*(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a, \pi = \pi_*]$$

II.  Reescreva o retorno $G_t$ recursivamente:
    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) = R_{t+1} + \gamma G_{t+1}$$

III. Substitua na defini√ß√£o da fun√ß√£o Q-valor:
    $$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a, \pi = \pi_*]$$

IV.  Reconhe√ßa que, seguindo a pol√≠tica √≥tima, o valor esperado de $\gamma G_{t+1}$ dado $S_t = s$ e $A_t = a$ √© equivalente a $\gamma v_*(S_{t+1})$, onde $v_*(S_{t+1})$ √© o valor √≥timo do pr√≥ximo estado:
     $$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$

V. Para expressar $v_*(s)$ em termos de $q_*(s, a)$, reconhecemos que o valor √≥timo de um estado √© o valor esperado m√°ximo da fun√ß√£o Q-valor sobre todas as a√ß√µes poss√≠veis:
   $$v_*(s) = \max_a q_*(s, a)$$

VI. Substituindo a express√£o de $q_*(s, a)$, obtemos:
 $$v_*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$ ‚ñ†

**Teorema 1** [Exist√™ncia e Unicidade da Solu√ß√£o] A equa√ß√£o de otimalidade de Bellman possui uma solu√ß√£o √∫nica para $v_*$ e $q_*$, sob a condi√ß√£o de que o ambiente seja um MDP com um n√∫mero finito de estados e a√ß√µes, e que $0 \leq \gamma < 1$.

*Prova (Esbo√ßo)*: A prova da exist√™ncia e unicidade da solu√ß√£o para a equa√ß√£o de Bellman geralmente envolve mostrar que o operador de Bellman √© uma contra√ß√£o em um espa√ßo de Banach, o que garante a converg√™ncia para uma solu√ß√£o √∫nica.

**Import√¢ncia:**

A equa√ß√£o de otimalidade de Bellman √© crucial porque fornece uma base para calcular pol√≠ticas √≥timas. Resolvendo essa equa√ß√£o (ou suas variantes), podemos determinar os valores √≥timos dos estados e, em seguida, derivar uma pol√≠tica √≥tima que alcan√ßa esses valores.

### Conclus√£o

A equa√ß√£o de otimalidade de Bellman para a fun√ß√£o de valor de estado √© uma ferramenta poderosa na programa√ß√£o din√¢mica. Ela encapsula a ideia de que uma pol√≠tica √≥tima deve equilibrar recompensas imediatas com valores futuros descontados. Ao resolver esta equa√ß√£o, podemos encontrar pol√≠ticas √≥timas para processos de decis√£o de Markov finitos [^1]. Este conceito forma a base para muitos algoritmos de reinforcement learning, permitindo que agentes aprendam a tomar decis√µes √≥timas em ambientes complexos. $\blacksquare$

### Refer√™ncias

[^1]: Dynamic Programming, Cap√≠tulo 4, Advanced Study of Reinforcement Learning Fundamentals
<!-- END -->