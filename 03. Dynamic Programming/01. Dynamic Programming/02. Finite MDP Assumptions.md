## Din√¢mica em Processos de Decis√£o de Markov Finitos

### Introdu√ß√£o
Este cap√≠tulo explora a aplica√ß√£o da **programa√ß√£o din√¢mica (DP)** em **processos de decis√£o de Markov (MDPs) finitos**, conforme definido no contexto de *Advanced Study of Reinforcement Learning Fundamentals* [^1]. A DP oferece um conjunto de algoritmos para calcular pol√≠ticas √≥timas quando um modelo perfeito do ambiente est√° dispon√≠vel. Em MDPs finitos, a DP se torna particularmente trat√°vel devido √† natureza discreta dos espa√ßos de estado, a√ß√£o e recompensa [^1].

### Conceitos Fundamentais

Um **MDP finito** √© caracterizado por conjuntos finitos de estados ($S$), a√ß√µes ($A$) e recompensas ($R$). A din√¢mica do ambiente √© definida por um conjunto de probabilidades de transi√ß√£o $p(s', r|s, a)$, que especificam a probabilidade de transitar para o estado $s'$ e receber uma recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$ [^1]. Formalmente, temos:

*   $S = \{s_1, s_2, \ldots, s_n\}$ onde $n$ √© o n√∫mero de estados.
*   $A(s) = \{a_1, a_2, \ldots, a_m\}$ onde $m$ √© o n√∫mero de a√ß√µes dispon√≠veis no estado $s$.
*   $R$ √© um conjunto finito de recompensas poss√≠veis.
*   $p(s', r|s, a) = P(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$.

> üí° **Exemplo Num√©rico:** Considere um MDP com dois estados $S = \{s_1, s_2\}$ e duas a√ß√µes em cada estado $A(s) = \{a_1, a_2\}$. As recompensas poss√≠veis s√£o $R = \{0, 1\}$. A probabilidade de transi√ß√£o ao tomar a a√ß√£o $a_1$ no estado $s_1$ √© $p(s_1, 0|s_1, a_1) = 0.8$ e $p(s_2, 1|s_1, a_1) = 0.2$. Isso significa que, ao tomar a a√ß√£o $a_1$ no estado $s_1$, h√° 80% de chance de permanecer em $s_1$ e receber uma recompensa de 0, e 20% de chance de transitar para $s_2$ e receber uma recompensa de 1.

A condi√ß√£o de **episodicidade** simplifica ainda mais a aplica√ß√£o de DP. Em um MDP epis√≥dico, a intera√ß√£o entre o agente e o ambiente se divide em epis√≥dios, cada um terminando em um estado terminal $S^+$ [^1]. Esta estrutura permite o uso de t√©cnicas que exploram a natureza finita do horizonte temporal.

A chave para a aplica√ß√£o de DP, e do aprendizado por refor√ßo em geral, √© o uso de **fun√ß√µes de valor** [^1]. Estas fun√ß√µes organizam e estruturam a busca por boas pol√≠ticas. As fun√ß√µes de valor √≥timas $v_*(s)$ e $q_*(s, a)$ satisfazem as equa√ß√µes de otimalidade de Bellman [^1]:

$$
v_*(s) = \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]
$$

$$
v_*(s) = \max_{a \in A(s)} \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')]
$$

ou

$$
q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a]
$$

$$
q_*(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma \max_{a'} q_*(s', a')]
$$

onde $\gamma$ √© o fator de desconto ($0 \leq \gamma \leq 1$) [^1].

> üí° **Exemplo Num√©rico:** Considere um estado $s$ com duas a√ß√µes $a_1$ e $a_2$. Suponha que $\gamma = 0.9$. Se ao tomar a a√ß√£o $a_1$, a recompensa esperada √© 2 e o valor do pr√≥ximo estado $s'$ √© 10, e ao tomar a a√ß√£o $a_2$, a recompensa esperada √© 5 e o valor do pr√≥ximo estado $s'$ √© 8, ent√£o $q_*(s, a_1) = 2 + 0.9 \cdot 10 = 11$ e $q_*(s, a_2) = 5 + 0.9 \cdot 8 = 12.2$. Portanto, $v_*(s) = \max(11, 12.2) = 12.2$, e a a√ß√£o √≥tima neste estado seria $a_2$.

Al√©m das equa√ß√µes de Bellman para a fun√ß√£o de valor √≥tima, podemos definir equa√ß√µes de Bellman para uma pol√≠tica arbitr√°ria $\pi$:

$$
v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]
$$

$$
v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')]
$$

e

$$
q_\pi(s, a) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a]
$$

$$
q_\pi(s, a) =  \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')]
$$

Essas equa√ß√µes s√£o fundamentais para a avalia√ß√£o de pol√≠tica.

**Policy Evaluation (Prediction)**

O objetivo da avalia√ß√£o de pol√≠tica √© computar a fun√ß√£o de valor de estado $v_\pi(s)$ para uma pol√≠tica arbitr√°ria $\pi$ [^2]. A **avalia√ß√£o de pol√≠tica iterativa** utiliza a seguinte regra de atualiza√ß√£o:

$$
v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s] = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')]
$$

para todo $s \in S$ [^2].  Esta itera√ß√£o converge para $v_\pi$ conforme $k \rightarrow \infty$ sob certas condi√ß√µes [^2].

> üí° **Exemplo Num√©rico:** Suponha que temos um MDP com dois estados $S = \{s_1, s_2\}$ e uma pol√≠tica $\pi$ que sempre escolhe a a√ß√£o $a_1$ em ambos os estados. Seja $\gamma = 0.9$. As probabilidades de transi√ß√£o e recompensas s√£o:
>
> *   $p(s_1, 0 | s_1, a_1) = 0.6$
> *   $p(s_2, 1 | s_1, a_1) = 0.4$
> *   $p(s_1, 0 | s_2, a_1) = 0.3$
> *   $p(s_2, 1 | s_2, a_1) = 0.7$
>
> Inicializamos $v_0(s_1) = 0$ e $v_0(s_2) = 0$. Ap√≥s a primeira itera√ß√£o ($k=0$):
>
> $v_1(s_1) = 0.6 * (0 + 0.9 * 0) + 0.4 * (1 + 0.9 * 0) = 0.4$
> $v_1(s_2) = 0.3 * (0 + 0.9 * 0) + 0.7 * (1 + 0.9 * 0) = 0.7$
>
> Ap√≥s a segunda itera√ß√£o ($k=1$):
>
> $v_2(s_1) = 0.6 * (0 + 0.9 * 0.4) + 0.4 * (1 + 0.9 * 0.7) = 0.6 * 0.36 + 0.4 * 1.63 = 0.216 + 0.652 = 0.868$
> $v_2(s_2) = 0.3 * (0 + 0.9 * 0.4) + 0.7 * (1 + 0.9 * 0.7) = 0.3 * 0.36 + 0.7 * 1.63 = 0.108 + 1.141 = 1.249$
>
> Este processo continua at√© a converg√™ncia.

![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

Para garantir a converg√™ncia da avalia√ß√£o de pol√≠tica iterativa, podemos usar a norma de Banach.

**Teorema 1** A avalia√ß√£o de pol√≠tica iterativa converge para $v_\pi$ sob a norma do supremo, isto √©, $||v_{k+1} - v_\pi||_\infty \le \gamma ||v_k - v_\pi||_\infty$.

*Proof:*
Provaremos que a avalia√ß√£o de pol√≠tica iterativa converge para $v_\pi$ sob a norma do supremo.

I. Subtraindo $v_\pi(s)$ de ambos os lados da equa√ß√£o de atualiza√ß√£o iterativa, temos:
   $$v_{k+1}(s) - v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] - v_\pi(s)$$

II. Como $v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')]$, podemos substituir $v_\pi(s)$ na equa√ß√£o acima:
    $$v_{k+1}(s) - v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] - \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')] $$
    $$v_{k+1}(s) - v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) \gamma [v_k(s') - v_\pi(s')] $$

III. Tomando o valor absoluto de ambos os lados:
     $$|v_{k+1}(s) - v_\pi(s)| = |\sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) \gamma [v_k(s') - v_\pi(s')]|$$

IV. Aplicando a desigualdade triangular:
    $$|v_{k+1}(s) - v_\pi(s)| \le \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) \gamma |v_k(s') - v_\pi(s')|$$

V. Como $\sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) = 1$ e $|v_k(s') - v_\pi(s')| \le ||v_k - v_\pi||_\infty$ para todo $s'$:
   $$|v_{k+1}(s) - v_\pi(s)| \le \gamma ||v_k - v_\pi||_\infty$$

VI. Portanto, $||v_{k+1} - v_\pi||_\infty = \max_s |v_{k+1}(s) - v_pi(s)| \le \gamma ||v_k - v_\pi||_\infty$. Como $\gamma < 1$, a avalia√ß√£o de pol√≠tica iterativa converge geometricamente. ‚ñ†

**Policy Improvement**

Ap√≥s determinar a fun√ß√£o de valor $v_\pi$ para uma pol√≠tica determin√≠stica arbitr√°ria $\pi$, o objetivo passa a ser encontrar uma pol√≠tica melhor [^4]. Para um estado $s$, considera-se mudar a pol√≠tica para escolher deterministicamente uma a√ß√£o $a \neq \pi(s)$. A **policy improvement theorem** estabelece que se $q_\pi(s, a) \geq v_\pi(s)$ para todo $s \in S$, ent√£o a pol√≠tica $\pi'$ deve ser t√£o boa quanto, ou melhor que $\pi$ [^6].

A pol√≠tica gulosa (greedy policy) $\pi'$ √© dada por [^7]:

$$
\pi'(s) = \arg \max_{a} q_\pi(s, a) = \arg \max_{a} \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a]
$$

Podemos expandir essa express√£o para mostrar a rela√ß√£o entre a fun√ß√£o Q e a fun√ß√£o de valor:

$$
\pi'(s) = \arg \max_{a} \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')]
$$

Esta pol√≠tica gulosa √© garantida como uma melhoria sobre a pol√≠tica original, ou pelo menos t√£o boa quanto.

> üí° **Exemplo Num√©rico:** Continuemos com o exemplo anterior ap√≥s a converg√™ncia de $v_\pi$. Digamos que $v_\pi(s_1) = 1.2$ e $v_\pi(s_2) = 1.8$. Agora, precisamos verificar se podemos melhorar a pol√≠tica $\pi$ (que sempre escolhe $a_1$). Para fazer isso, precisamos calcular $q_\pi(s, a)$ para todas as a√ß√µes. J√° conhecemos os valores para $a_1$. Precisamos calcular para $a_2$. Suponha que:
>
> *   $p(s_1, 0 | s_1, a_2) = 0.2$
> *   $p(s_2, 2 | s_1, a_2) = 0.8$
> *   $p(s_1, 1 | s_2, a_2) = 0.5$
> *   $p(s_2, 0 | s_2, a_2) = 0.5$
>
> Ent√£o, $q_\pi(s_1, a_2) = 0.2 * (0 + 0.9 * 1.2) + 0.8 * (2 + 0.9 * 1.8) = 0.2 * 1.08 + 0.8 * 3.62 = 0.216 + 2.896 = 3.112$.
> Como $q_\pi(s_1, a_2) = 3.112 > v_\pi(s_1) = 1.2$, podemos melhorar a pol√≠tica no estado $s_1$ escolhendo $a_2$ em vez de $a_1$.
>
> Similarmente, $q_\pi(s_2, a_2) = 0.5 * (1 + 0.9 * 1.2) + 0.5 * (0 + 0.9 * 1.8) = 0.5 * 2.08 + 0.5 * 1.62 = 1.04 + 0.81 = 1.85$.
> Como $q_\pi(s_2, a_2) = 1.85 > v_\pi(s_2) = 1.8$, podemos melhorar a pol√≠tica no estado $s_2$ escolhendo $a_2$ em vez de $a_1$ (embora a melhoria seja marginal).

**Policy Iteration**

A itera√ß√£o de pol√≠tica consiste em alternar entre a avalia√ß√£o de pol√≠tica e a melhoria de pol√≠tica para gerar uma sequ√™ncia de pol√≠ticas e fun√ß√µes de valor monotonicamente crescentes [^8]:

$$
\pi_0 \xrightarrow{E} v_{\pi_0} \xrightarrow{I} \pi_1 \xrightarrow{E} v_{\pi_1} \xrightarrow{I} \pi_2 \xrightarrow{E} \ldots \xrightarrow{I} \pi_* \xrightarrow{E} v_*
$$

onde $E$ denota a avalia√ß√£o de pol√≠tica e $I$ denota a melhoria de pol√≠tica [^8]. Cada pol√≠tica √© garantida como sendo uma melhoria estrita sobre a anterior, a menos que j√° seja √≥tima [^8].

![Policy Iteration algorithm: iterative process of policy evaluation and improvement for optimal policy estimation.](./../images/image3.png)

**Teorema 2** A itera√ß√£o de pol√≠tica converge para a pol√≠tica √≥tima $\pi_*$ em um n√∫mero finito de itera√ß√µes em MDPs finitos.

*Proof:*

Provaremos que a itera√ß√£o de pol√≠tica converge para a pol√≠tica √≥tima $\pi_*$ em um n√∫mero finito de itera√ß√µes em MDPs finitos.

I. Em um MDP finito, o n√∫mero de pol√≠ticas poss√≠veis √© finito. Seja $N$ o n√∫mero total de pol√≠ticas poss√≠veis.

II. A cada itera√ß√£o da itera√ß√£o de pol√≠tica, ou a pol√≠tica permanece a mesma (se j√° for √≥tima), ou √© melhorada estritamente.  "Melhorada estritamente" significa que existe pelo menos um estado $s$ onde a nova pol√≠tica $\pi'(s)$ tem um valor esperado maior do que a pol√≠tica anterior $\pi(s)$, i.e., $v_{\pi'}(s) > v_{\pi}(s)$.

III. Como o n√∫mero de pol√≠ticas poss√≠veis √© finito e cada itera√ß√£o produz uma pol√≠tica estritamente melhor (a menos que j√° seja √≥tima), a itera√ß√£o de pol√≠tica n√£o pode continuar indefinidamente.

IV. Portanto, a itera√ß√£o de pol√≠tica deve convergir para a pol√≠tica √≥tima $\pi_*$ em um n√∫mero finito de itera√ß√µes (no m√°ximo $N$ itera√ß√µes). ‚ñ†





**Value Iteration**

A itera√ß√£o de valor trunca o passo de avalia√ß√£o de pol√≠tica da itera√ß√£o de pol√≠tica [^10]. Uma itera√ß√£o da itera√ß√£o de valor combina os passos de melhoria de pol√≠tica e avalia√ß√£o de pol√≠tica truncada [^11]:

$$
v_{k+1}(s) = \max_{a} \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a] = \max_{a} \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')]
$$

> üí° **Exemplo Num√©rico:** Vamos considerar um MDP simples com 3 estados: {inicio, trabalho, fim}, onde 'fim' √© o estado terminal. As a√ß√µes poss√≠veis s√£o {irTrabalhar, ficarEmCasa}. As recompensas s√£o:
>
> * Ir para o trabalho: Recompensa = 10
> * Ficar em casa: Recompensa = 5
>
> As transi√ß√µes s√£o determin√≠sticas:
> * inicio -> irTrabalhar -> trabalho
> * inicio -> ficarEmCasa -> fim
> * trabalho -> irTrabalhar -> fim
> * trabalho -> ficarEmCasa -> fim
>
> Inicializamos V(inicio) = 0, V(trabalho) = 0, V(fim) = 0 e definimos gamma = 0.9.
>
> Itera√ß√£o 1:
> * V(inicio) = max [10 + 0.9 * V(trabalho), 5 + 0.9 * V(fim)] = max [10 + 0, 5 + 0] = 10
> * V(trabalho) = max [10 + 0.9 * V(fim), 5 + 0.9 * V(fim)] = max [10 + 0, 5 + 0] = 10
> * V(fim) = 0
>
> Itera√ß√£o 2:
> * V(inicio) = max [10 + 0.9 * 10, 5 + 0.9 * 0] = max [19, 5] = 19
> * V(trabalho) = max [10 + 0.9 * 0, 5 + 0.9 * 0] = max [10, 5] = 10
> * V(fim) = 0
>
> Itera√ß√£o 3:
> * V(inicio) = max [10 + 0.9 * 10, 5 + 0.9 * 0] = max [19, 5] = 19
> * V(trabalho) = max [10 + 0.9 * 0, 5 + 0.9 * 0] = max [10, 5] = 10
> * V(fim) = 0
>
> Neste caso, os valores convergem rapidamente. A pol√≠tica √≥tima a partir do estado 'inicio' √© 'irTrabalhar'.

![Pseudocode for Value Iteration algorithm, a method for estimating the optimal policy in an MDP.](./../images/image4.png)

A itera√ß√£o de valor tamb√©m possui uma rela√ß√£o com a norma de Banach:

**Teorema 3** A itera√ß√£o de valor converge para $v_*$ sob a norma do supremo, isto √©, $||v_{k+1} - v_*||_\infty \le \gamma ||v_k - v_*||_\infty$.

*Proof:*

Provaremos que a itera√ß√£o de valor converge para $v_*$ sob a norma do supremo.

I. Subtraindo $v_*(s)$ de ambos os lados da equa√ß√£o de atualiza√ß√£o iterativa, temos:

   $$v_{k+1}(s) - v_*(s) = \max_{a} \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] - v_*(s)$$

II. Como $v_*(s) = \max_{a} \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')]$, podemos reescrever:

    $$v_{k+1}(s) - v_*(s) = \max_{a} \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] - \max_{a} \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')] $$

III. Definindo $Q_k(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')]$ e $Q_*(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')]$, temos:

     $$v_{k+1}(s) - v_*(s) = \max_a Q_k(s, a) - \max_a Q_*(s, a)$$

IV. Tomando o valor absoluto de ambos os lados e usando a desigualdade $ | \max_a f(a) - \max_a g(a) | \le \max_a |f(a) - g(a)|$:

    $$|v_{k+1}(s) - v_*(s)| = |\max_a Q_k(s, a) - \max_a Q_*(s, a)| \le \max_a |Q_k(s, a) - Q_*(s, a)|$$

V. Substituindo as defini√ß√µes de $Q_k(s, a)$ e $Q_*(s, a)$:

   $$|v_{k+1}(s) - v_*(s)| \le \max_a |\sum_{s', r} p(s', r|s, a) [r + \gamma v_k(s')] - \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')]|$$
   $$|v_{k+1}(s) - v_*(s)| \le \max_a |\sum_{s', r} p(s', r|s, a) \gamma [v_k(s') - v_*(s')]|$$

VI. Como $\sum_{s', r} p(s', r|s, a) = 1$:

    $$|v_{k+1}(s) - v_*(s)| \le \gamma \max_{s'} |v_k(s') - v_*(s')|$$

VII. Portanto, $||v_{k+1} - v_*||_\infty = \max_s |v_{k+1}(s) - v_*(s)| \le \gamma ||v_k - v_*||_\infty$. Como $\gamma < 1$, a itera√ß√£o de valor converge geometricamente. ‚ñ†
<!-- END -->