## Transformando Equa√ß√µes de Bellman em Regras de Atualiza√ß√£o em Dynamic Programming

### Introdu√ß√£o
O conceito de **Dynamic Programming (DP)**, conforme introduzido no Cap√≠tulo 4 [^1], √© um conjunto de algoritmos utilizados para calcular pol√≠ticas √≥timas dado um modelo perfeito do ambiente, representado como um Processo de Decis√£o de Markov (MDP). Uma das principais ideias em DP e no aprendizado por refor√ßo em geral √© o uso de fun√ß√µes de valor para organizar e estruturar a busca por boas pol√≠ticas [^1]. Este cap√≠tulo explora como as equa√ß√µes de Bellman, que definem as fun√ß√µes de valor √≥timas $v_*(s)$ e $q_*(s, a)$ [^1], podem ser transformadas em regras de atualiza√ß√£o iterativas para aproximar essas fun√ß√µes de valor.

### Conceitos Fundamentais

A ess√™ncia dos algoritmos de DP reside na transforma√ß√£o das equa√ß√µes de Bellman em *regras de atribui√ß√£o*, tamb√©m conhecidas como *regras de atualiza√ß√£o*. Essas regras s√£o aplicadas iterativamente para melhorar as aproxima√ß√µes das fun√ß√µes de valor desejadas [^2]. O processo iterativo de atualiza√ß√£o √© fundamental para a converg√™ncia das fun√ß√µes de valor para seus valores √≥timos.

**Equa√ß√µes de Bellman e Atualiza√ß√µes Esperadas:**
As equa√ß√µes de Bellman, como a equa√ß√£o de Bellman para $v_*(s)$ [^1]:

$$v_*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$
$$ = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$

e a equa√ß√£o de Bellman para $q_*(s, a)$ [^1]:

$$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a]$$
$$ = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a')] $$

s√£o a base para as regras de atualiza√ß√£o em DP. Essencialmente, DP transforma essas equa√ß√µes em atribui√ß√µes iterativas, onde a estimativa atual do valor de um estado (ou par estado-a√ß√£o) √© atualizada com base nos valores estimados dos estados sucessores. Estas atualiza√ß√µes s√£o denominadas *atualiza√ß√µes esperadas* porque s√£o baseadas em uma expectativa sobre todos os poss√≠veis estados subsequentes [^3].

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP simples com 3 estados (S1, S2, S3) e 2 a√ß√µes (A1, A2). Suponha que estamos tentando calcular $v_*(S1)$. A partir do estado S1, a a√ß√£o A1 leva ao estado S2 com recompensa 5, e a a√ß√£o A2 leva ao estado S3 com recompensa 10. Suponha tamb√©m que $\gamma = 0.9$. As fun√ß√µes de valor √≥timas para S2 e S3 s√£o $v_*(S2) = 20$ e $v_*(S3) = 30$, respectivamente.
>
> Usando a equa√ß√£o de Bellman:
>
> $v_*(S1) = \max_a \sum_{s', r} p(s', r | S1, a) [r + \gamma v_*(s')] $
>
> Para a a√ß√£o A1:
>
> $\sum_{s', r} p(s', r | S1, A1) [r + \gamma v_*(s')] = 1 * [5 + 0.9 * 20] = 5 + 18 = 23$
>
> Para a a√ß√£o A2:
>
> $\sum_{s', r} p(s', r | S1, A2) [r + \gamma v_*(s')] = 1 * [10 + 0.9 * 30] = 10 + 27 = 37$
>
> Portanto, $v_*(S1) = \max(23, 37) = 37$. A a√ß√£o √≥tima no estado S1 √© A2.

**Lema 1:** *Contraction Mapping*. Se as recompensas forem limitadas e $0 \leq \gamma < 1$, ent√£o o operador de Bellman √© uma contra√ß√£o. Isso significa que aplicar o operador de Bellman repetidamente a uma fun√ß√£o de valor arbitraria ir√° convergir para a fun√ß√£o de valor √≥tima.

*Proof.* Considere duas fun√ß√µes de valor arbitr√°rias $v$ e $v'$. Aplicar o operador de Bellman a ambas resulta em novas fun√ß√µes de valor. Podemos mostrar que a dist√¢ncia m√°xima entre as fun√ß√µes de valor diminui a cada aplica√ß√£o, provando que o operador √© uma contra√ß√£o. A prova formal envolve usar a desigualdade triangular e a propriedade de desconto $\gamma$.

Para provar que o operador de Bellman √© uma contra√ß√£o, vamos seguir estes passos:
I. Sejam $v$ e $v'$ duas fun√ß√µes de valor arbitr√°rias.
II. Defina o operador de Bellman $T$ tal que $(Tv)(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s, A_t = a]$.
III. Queremos mostrar que $||Tv - Tv'||_\infty \leq \gamma ||v - v'||_\infty$, onde $||\cdot||_\infty$ denota a norma do supremo.
IV. Observe que:
    $$ (Tv)(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s, A_t = a] $$
    $$ (Tv')(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v'(S_{t+1}) | S_t = s, A_t = a] $$

V. Ent√£o:
    $$ |(Tv)(s) - (Tv')(s)| = |\max_a \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s, A_t = a] - \max_a \mathbb{E}[R_{t+1} + \gamma v'(S_{t+1}) | S_t = s, A_t = a]| $$

VI. Usando a propriedade de que $|\max_a f(a) - \max_a g(a)| \leq \max_a |f(a) - g(a)|$, temos:
$$ |(Tv)(s) - (Tv')(s)| \leq \max_a |\mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s, A_t = a] - \mathbb{E}[R_{t+1} + \gamma v'(S_{t+1}) | S_t = s, A_t = a]| $$

VII. Simplificando:
    $$ |(Tv)(s) - (Tv')(s)| \leq \max_a |\mathbb{E}[\gamma (v(S_{t+1}) - v'(S_{t+1})) | S_t = s, A_t = a]| $$
    $$ |(Tv)(s) - (Tv')(s)| \leq \gamma \max_a \mathbb{E}[|v(S_{t+1}) - v'(S_{t+1})| | S_t = s, A_t = a] $$

VIII. Como $|v(s) - v'(s)| \leq ||v - v'||_\infty$ para todo $s$:
$$ |(Tv)(s) - (Tv')(s)| \leq \gamma ||v - v'||_\infty $$

IX. Tomando o supremo sobre todos os estados $s$:
$$ ||Tv - Tv'||_\infty = \sup_s |(Tv)(s) - (Tv')(s)| \leq \gamma ||v - v'||_\infty $$

X. Portanto, $||Tv - Tv'||_\infty \leq \gamma ||v - v'||_\infty$, mostrando que o operador de Bellman $T$ √© uma contra√ß√£o com fator de contra√ß√£o $\gamma$ quando $0 \leq \gamma < 1$. Isso garante que a aplica√ß√£o repetida do operador de Bellman converge para a fun√ß√£o de valor √≥tima. ‚ñ†

**Policy Evaluation Iterativa (Predi√ß√£o):**
Um exemplo crucial de transformar a equa√ß√£o de Bellman em uma regra de atualiza√ß√£o √© o algoritmo de *Policy Evaluation Iterativa* [^2]. Este algoritmo calcula a fun√ß√£o de valor de estado $v_\pi$ para uma pol√≠tica arbitr√°ria $\pi$. A equa√ß√£o iterativa de atualiza√ß√£o √© derivada diretamente da equa√ß√£o de Bellman para $v_\pi(s)$:

$$v_{\pi}(s) = \mathbb{E}_\pi[G_t | S_t = s]$$
$$= \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s]$$
$$= \mathbb{E}_\pi[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]$$
$$= \sum_a \pi(a|s) \sum_{s',r} p(s', r | s, a) [r + \gamma v_{\pi}(s')] $$

A regra de atualiza√ß√£o iterativa √©:

$$v_{k+1}(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s]$$
$$= \sum_a \pi(a|s) \sum_{s',r} p(s', r | s, a) [r + \gamma v_k(s')] \qquad \text{[4.5]}$$

Aqui, $v_k(s)$ representa a estimativa do valor do estado $s$ na *$k$*-√©sima itera√ß√£o. O algoritmo substitui iterativamente o valor antigo de *s* por um novo valor obtido dos valores antigos dos estados sucessores de *s*, e as recompensas imediatas esperadas, em todas as transi√ß√µes de um passo poss√≠veis, seguindo a pol√≠tica $\pi$ [^2]. Este tipo de opera√ß√£o √© chamado de *expected update* [^2].

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP com dois estados (S1 e S2) e uma pol√≠tica $\pi$ que sempre escolhe a a√ß√£o A1. As probabilidades de transi√ß√£o e recompensas s√£o as seguintes:
> *   De S1, A1 leva a S2 com probabilidade 1.0 e recompensa 2.
> *   De S2, A1 leva a S1 com probabilidade 1.0 e recompensa -1.
>
> Seja $\gamma = 0.9$. Inicializamos $v_0(S1) = 0$ e $v_0(S2) = 0$. Vamos realizar duas itera√ß√µes de Policy Evaluation.
>
> *Itera√ß√£o 1:*
>
> $v_1(S1) = \sum_{s', r} p(s', r | S1, A1) [r + \gamma v_0(s')] = 1.0 * [2 + 0.9 * 0] = 2$
>
> $v_1(S2) = \sum_{s', r} p(s', r | S2, A1) [r + \gamma v_0(s')] = 1.0 * [-1 + 0.9 * 0] = -1$
>
> *Itera√ß√£o 2:*
>
> $v_2(S1) = \sum_{s', r} p(s', r | S1, A1) [r + \gamma v_1(s')] = 1.0 * [2 + 0.9 * (-1)] = 2 - 0.9 = 1.1$
>
> $v_2(S2) = \sum_{s', r} p(s', r | S2, A1) [r + \gamma v_1(s')] = 1.0 * [-1 + 0.9 * 2] = -1 + 1.8 = 0.8$
>
> Ap√≥s duas itera√ß√µes, as estimativas de valor s√£o $v_2(S1) = 1.1$ e $v_2(S2) = 0.8$.  Este processo continua at√© a converg√™ncia para $v_\pi$.



![Pseudocode for Iterative Policy Evaluation, an algorithm for estimating state values under a given policy.](./../images/image5.png)

**Teorema 1:** *Converg√™ncia da Policy Evaluation Iterativa*. Para qualquer pol√≠tica fixa $\pi$, a sequ√™ncia de fun√ß√µes de valor $\{v_k\}$ gerada pela Policy Evaluation Iterativa converge para $v_\pi$ conforme $k \rightarrow \infty$.

*Proof.* A prova se baseia no fato de que a atualiza√ß√£o iterativa √© uma contra√ß√£o sob a norma do supremo, garantindo que as sucessivas estimativas de valor se aproximem cada vez mais de $v_\pi$.

Para provar formalmente a converg√™ncia da Policy Evaluation Iterativa, precisamos demonstrar que a regra de atualiza√ß√£o √© uma contra√ß√£o sob a norma do supremo.

I. Seja $T_\pi$ o operador de Bellman para a pol√≠tica $\pi$, definido como:
$$(T_\pi v)(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] = \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] $$

II. Queremos mostrar que $||T_\pi v - T_\pi v'||_\infty \leq \gamma ||v - v'||_\infty$ para quaisquer fun√ß√µes de valor $v$ e $v'$.
III. Observe que:
$$(T_\pi v)(s) = \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] $$
$$(T_\pi v')(s) = \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v'(s')] $$

IV. Ent√£o:
$$|(T_\pi v)(s) - (T_\pi v')(s)| = |\sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] - \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v'(s')]|$$

V. Simplificando:
$$|(T_\pi v)(s) - (T_\pi v')(s)| = |\sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) [\gamma (v(s') - v'(s'))]|$$
$$|(T_\pi v)(s) - (T_\pi v')(s)| = \gamma |\sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) [v(s') - v'(s')]|$$

VI. Usando a desigualdade triangular:
$$|(T_\pi v)(s) - (T_\pi v')(s)| \leq \gamma \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) |v(s') - v'(s')|$$

VII. Como $|v(s') - v'(s')| \leq ||v - v'||_\infty$ para todo $s'$:
$$|(T_\pi v)(s) - (T_\pi v')(s)| \leq \gamma ||v - v'||_\infty \sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a)$$

VIII. Note que $\sum_a \pi(a|s) \sum_{s', r} p(s', r | s, a) = 1$, pois √© uma soma sobre todas as probabilidades poss√≠veis:
$$|(T_\pi v)(s) - (T_\pi v')(s)| \leq \gamma ||v - v'||_\infty$$

IX. Tomando o supremo sobre todos os estados $s$:
$$||T_\pi v - T_\pi v'||_\infty = \sup_s |(T_\pi v)(s) - (T_\pi v')(s)| \leq \gamma ||v - v'||_\infty$$

X. Portanto, $||T_\pi v - T_\pi v'||_\infty \leq \gamma ||v - v'||_\infty$, mostrando que o operador de Bellman $T_\pi$ √© uma contra√ß√£o com fator de contra√ß√£o $\gamma$ quando $0 \leq \gamma < 1$. Isso garante que a aplica√ß√£o repetida do operador $T_\pi$ converge para a fun√ß√£o de valor $v_\pi$. ‚ñ†

**Value Iteration:**
Outro exemplo importante √© o algoritmo de *Value Iteration* [^11]. Este algoritmo √© derivado da equa√ß√£o de otimalidade de Bellman, mostrada acima. A regra de atualiza√ß√£o iterativa √©:

$$v_{k+1}(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1}) | S_t = s, A_t = a]$$
$$= \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')] \qquad \text{[4.10]}$$

Para um $v_0$ arbitr√°rio, a sequ√™ncia {$v_k$} converge para $v_*$ sob as mesmas condi√ß√µes que garantem a exist√™ncia de $v_*$ [^11].

> üí° **Exemplo Num√©rico:**
>
> Considere um ambiente com dois estados (S1 e S2) e duas a√ß√µes (A1 e A2). As probabilidades e recompensas s√£o:
> * S1, A1 -> S2 (p=0.8, r=10), S1, A2 -> S1 (p=0.5, r=2), S2 (p=0.5, r=4)
> * S2, A1 -> S1 (p=0.6, r=8), S2, A2 -> S2 (p=1.0, r=6)
>
> Seja $\gamma = 0.9$. Inicializamos $v_0(S1) = 0$ e $v_0(S2) = 0$.
>
> *Itera√ß√£o 1:*
>
> $v_1(S1) = \max \begin{cases}
> 0.8 * (10 + 0.9 * 0) = 8 \\
> 0.5 * (2 + 0.9 * 0) + 0.5 * (4 + 0.9 * 0) = 1 + 2 = 3
> \end{cases} = \max(8, 3) = 8$
>
> $v_1(S2) = \max \begin{cases}
> 0.6 * (8 + 0.9 * 0) = 4.8 \\
> 1.0 * (6 + 0.9 * 0) = 6
> \end{cases} = \max(4.8, 6) = 6$
>
> *Itera√ß√£o 2:*
>
> $v_2(S1) = \max \begin{cases}
> 0.8 * (10 + 0.9 * 6) = 0.8 * (10 + 5.4) = 0.8 * 15.4 = 12.32 \\
> 0.5 * (2 + 0.9 * 8) + 0.5 * (4 + 0.9 * 6) = 0.5 * (2 + 7.2) + 0.5 * (4 + 5.4) = 0.5 * 9.2 + 0.5 * 9.4 = 4.6 + 4.7 = 9.3
> \end{cases} = \max(12.32, 9.3) = 12.32$
>
> $v_2(S2) = \max \begin{cases}
> 0.6 * (8 + 0.9 * 8) = 0.6 * (8 + 7.2) = 0.6 * 15.2 = 9.12 \\
> 1.0 * (6 + 0.9 * 6) = 6 + 5.4 = 11.4
> \end{cases} = \max(9.12, 11.4) = 11.4$
>
> Ap√≥s duas itera√ß√µes, $v_2(S1) = 12.32$ e $v_2(S2) = 11.4$. Este processo continua at√© a converg√™ncia para $v_*$.



![Pseudocode for Value Iteration algorithm, a method for estimating the optimal policy in an MDP.](./../images/image4.png)

**Teorema 2:** *Converg√™ncia da Value Iteration*. A sequ√™ncia de fun√ß√µes de valor $\{v_k\}$ gerada pela Value Iteration converge para $v_*$ conforme $k \rightarrow \infty$.

*Proof.* A prova √© similar √† da Policy Evaluation Iterativa, mostrando que a atualiza√ß√£o da Value Iteration √© uma contra√ß√£o sob a norma do supremo.

Para provar formalmente a converg√™ncia da Value Iteration, mostraremos que a atualiza√ß√£o de Value Iteration √© uma contra√ß√£o sob a norma do supremo.

I. Seja $T$ o operador de Bellman de otimalidade, definido como:
$$(Tv)(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s, A_t = a] = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] $$

II. Queremos mostrar que $||Tv - Tv'||_\infty \leq \gamma ||v - v'||_\infty$ para quaisquer fun√ß√µes de valor $v$ e $v'$.
III. Observe que:
$$(Tv)(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] $$
$$(Tv')(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v'(s')] $$

IV. Ent√£o:
$$|(Tv)(s) - (Tv')(s)| = |\max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] - \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v'(s')]|$$

V. Usando a propriedade que $|\max_a f(a) - \max_a g(a)| \leq \max_a |f(a) - g(a)|$, temos:
$$|(Tv)(s) - (Tv')(s)| \leq \max_a |\sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] - \sum_{s', r} p(s', r | s, a) [r + \gamma v'(s')]|$$

VI. Simplificando:
$$|(Tv)(s) - (Tv')(s)| \leq \max_a |\sum_{s', r} p(s', r | s, a) [\gamma (v(s') - v'(s'))]|$$
$$|(Tv)(s) - (Tv')(s)| \leq \gamma \max_a |\sum_{s', r} p(s', r | s, a) [v(s') - v'(s')]|$$

VII. Usando a desigualdade triangular:
$$|(Tv)(s) - (Tv')(s)| \leq \gamma \max_a \sum_{s', r} p(s', r | s, a) |v(s') - v'(s')|$$

VIII. Como $|v(s') - v'(s')| \leq ||v - v'||_\infty$ para todo $s'$:
$$|(Tv)(s) - (Tv')(s)| \leq \gamma ||v - v'||_\infty \max_a \sum_{s', r} p(s', r | s, a)$$

IX. Note que $\sum_{s', r} p(s', r | s, a) = 1$, pois √© uma soma sobre todas as probabilidades poss√≠veis:
$$|(Tv)(s) - (Tv')(s)| \leq \gamma ||v - v'||_\infty$$

X. Tomando o supremo sobre todos os estados $s$:
$$||Tv - Tv'||_\infty = \sup_s |(Tv)(s) - (Tv')(s)| \leq \gamma ||v - v'||_\infty$$

XI. Portanto, $||Tv - Tv'||_\infty \leq \gamma ||v - v'||_\infty$, mostrando que o operador de Bellman $T$ √© uma contra√ß√£o com fator de contra√ß√£o $\gamma$ quando $0 \leq \gamma < 1$. Isso garante que a aplica√ß√£o repetida do operador $T$ converge para a fun√ß√£o de valor √≥tima $v_*$. ‚ñ†

**Corol√°rio 2.1:** A Value Iteration encontra a pol√≠tica √≥tima $\pi_*$ em um n√∫mero finito de itera√ß√µes se o espa√ßo de estados e o espa√ßo de a√ß√µes forem finitos.

*Proof.* Como o n√∫mero de pol√≠ticas poss√≠veis √© finito, e a Value Iteration converge para a fun√ß√£o de valor √≥tima $v_*$, a pol√≠tica √≥tima correspondente $\pi_*$ √© encontrada em um n√∫mero finito de passos.

Para formalizar a prova:
I. Suponha que o espa√ßo de estados $S$ e o espa√ßo de a√ß√µes $A$ s√£o finitos.
II. O n√∫mero de pol√≠ticas poss√≠veis √© finito, j√° que uma pol√≠tica $\pi$ √© um mapeamento de estados para a√ß√µes, e existem $|A|^{|S|}$ poss√≠veis mapeamentos.
III. A Value Iteration converge para a fun√ß√£o de valor √≥tima $v_*$. Isso significa que para algum $k$, $v_k$ √© "pr√≥ximo o suficiente" de $v_*$, tal que a pol√≠tica derivada de $v_k$ √© √≥tima.
IV. Seja $\pi_k$ a pol√≠tica greedy em rela√ß√£o a $v_k$, definida como $\pi_k(s) = \arg \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')]$.
V. Como a Value Iteration converge para $v_*$, existe um $K$ tal que para todo $k \geq K$, $\pi_k$ √© uma pol√≠tica √≥tima $\pi_*$.
VI. Portanto, a Value Iteration encontra a pol√≠tica √≥tima $\pi_*$ em um n√∫mero finito de itera√ß√µes. ‚ñ†

**Expected Updates:**
As atualiza√ß√µes realizadas nos algoritmos de DP s√£o chamadas de *expected updates* porque s√£o baseadas em uma expectativa sobre todos os poss√≠veis estados seguintes, em vez de uma amostra do pr√≥ximo estado [^3].

> üí° **Exemplo Num√©rico:**
>
> Considere um estado S1 com duas a√ß√µes poss√≠veis: A1 e A2. A a√ß√£o A1 leva a tr√™s poss√≠veis estados sucessores: S2 (recompensa 1), S3 (recompensa 2), e S4 (recompensa 3) com probabilidades 0.2, 0.3, e 0.5, respectivamente. A a√ß√£o A2 leva a apenas um estado sucessor: S5 (recompensa 5) com probabilidade 1.
>
> Para calcular a expected update para a a√ß√£o A1 no estado S1, precisamos somar sobre todos os poss√≠veis estados sucessores ponderados por suas probabilidades e recompensas:
>
> $E[A1] = 0.2 * (1 + \gamma * V(S2)) + 0.3 * (2 + \gamma * V(S3)) + 0.5 * (3 + \gamma * V(S4))$
>
> Similarmente, para a a√ß√£o A2:
>
> $E[A2] = 1 * (5 + \gamma * V(S5))$
>
> Se $\gamma = 0.9$ e $V(S2) = 10$, $V(S3) = 15$, $V(S4) = 20$, e $V(S5) = 25$, ent√£o:
>
> $E[A1] = 0.2 * (1 + 0.9 * 10) + 0.3 * (2 + 0.9 * 15) + 0.5 * (3 + 0.9 * 20) = 0.2 * 10 + 0.3 * 15.5 + 0.5 * 21 = 2 + 4.65 + 10.5 = 17.15$
>
> $E[A2] = 1 * (5 + 0.9 * 25) = 5 + 22.5 = 27.5$
>
> Este exemplo ilustra como a expected update considera todos os poss√≠veis estados sucessores e suas probabilidades.

**Proposi√ß√£o 1:** As expected updates podem ser computacionalmente caras, especialmente para espa√ßos de estados grandes.

*Proof.* Calcular a expectativa requer somar sobre todos os poss√≠veis estados sucessores, o que pode ser impratic√°vel quando o n√∫mero de estados √© grande. Isso motiva o uso de m√©todos baseados em amostras, como Monte Carlo e Temporal Difference learning.

Para elaborar na prova:
I. Considere o c√°lculo da expected update para a Value Iteration:
$$v_{k+1}(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')] $$
II. Para calcular $v_{k+1}(s)$, √© necess√°rio somar sobre todos os poss√≠veis estados sucessores $s'$ e recompensas $r$.
III. A complexidade computacional de calcular $\sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')]$ √© proporcional ao n√∫mero de poss√≠veis pares $(s', r)$.
IV. Em um espa√ßo de estados grande, o n√∫mero de estados sucessores $|S'|$ pode ser muito grande. Similarmente, o n√∫mero de poss√≠veis recompensas $|R|$ pode ser grande.
V. Portanto, a complexidade de calcular a expected update para um √∫nico estado $s$ e a√ß√£o $a$ √© $O(|S'| \cdot |R|)$.
VI. Para atualizar todos os estados, a complexidade total seria $O(|S| \cdot |A| \cdot |S'| \cdot |R|)$, onde $|S|$ √© o n√∫mero de estados e $|A|$ √© o n√∫mero de a√ß√µes.
VII. Se o espa√ßo de estados √© muito grande, essa computa√ß√£o se torna impratic√°vel. Isso demonstra que as expected updates podem ser computacionalmente caras, especialmente para espa√ßos de estados grandes. ‚ñ†

### Conclus√£o

Em resumo, os algoritmos de DP dependem fortemente da transforma√ß√£o das equa√ß√µes de Bellman em regras de atualiza√ß√£o iterativas [^2]. Essas regras permitem a computa√ß√£o eficiente de fun√ß√µes de valor √≥timas atrav√©s da aplica√ß√£o repetida de *expected updates* [^2]. A converg√™ncia desses algoritmos √© garantida sob certas condi√ß√µes, tornando-os ferramentas poderosas para resolver problemas de decis√£o sequenciais [^2].

### Refer√™ncias
[^1]: Chapter 4: Dynamic Programming, Introduction
[^2]: Chapter 4: Dynamic Programming, 4.1 Policy Evaluation (Prediction)
[^3]: Chapter 4: Dynamic Programming, 4.1 Policy Evaluation (Prediction), Uk+1
[^11]: Chapter 4: Dynamic Programming, 4.4 Value Iteration
<!-- END -->