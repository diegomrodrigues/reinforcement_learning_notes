## Pol√≠ticas √ìtimas e Fun√ß√µes de Valor √ìtimas em MDPs Finitos

### Introdu√ß√£o

No √¢mbito do aprendizado por refor√ßo, o objetivo primordial √© discernir uma **pol√≠tica** que maximize a acumula√ß√£o de recompensas ao longo do tempo. Dentro da estrutura dos Processos de Decis√£o de Markov (MDPs) finitos, podemos definir rigorosamente o conceito de uma **pol√≠tica √≥tima** [^62]. Este cap√≠tulo se dedica a explorar as caracter√≠sticas e propriedades destas pol√≠ticas √≥timas, bem como as fun√ß√µes de valor associadas, estabelecendo as bases para a compreens√£o de algoritmos mais avan√ßados.

### Pol√≠ticas √ìtimas

Em MDPs finitos, o objetivo do aprendizado por refor√ßo pode ser formalizado como a busca por uma pol√≠tica que maximize o retorno acumulado. As **fun√ß√µes de valor** fornecem um meio de ordenar as pol√≠ticas, permitindo a identifica√ß√£o de uma pol√≠tica √≥tima. Formalmente, uma pol√≠tica $\pi$ √© definida como *melhor ou igual* a uma pol√≠tica $\pi'$ se, e somente se, seu retorno esperado for maior ou igual ao de $\pi'$ para todos os estados $s \in S$ [^62]. Matematicamente, isto se expressa como:

$$\pi \geq \pi' \iff v_{\pi}(s) \geq v_{\pi'}(s), \forall s \in S$$

√â crucial ressaltar que sempre existe pelo menos uma pol√≠tica que √© *melhor ou igual* a todas as outras pol√≠ticas poss√≠veis. Tal pol√≠tica √© designada como uma **pol√≠tica √≥tima**, denotada por $\pi_*$ [^62]. Embora possa haver m√∫ltiplas pol√≠ticas √≥timas para um determinado MDP, elas compartilham a mesma **fun√ß√£o de valor de estado √≥tima**, $v_*$, definida como [^62]:

$$v_*(s) = \max_{\pi} v_{\pi}(s), \forall s \in S$$

Essa fun√ß√£o de valor representa o m√°ximo retorno esperado que pode ser alcan√ßado a partir de um estado $s$, seguindo qualquer pol√≠tica $\pi$.

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP com dois estados, $S = \{s_1, s_2\}$, e duas pol√≠ticas, $\pi_1$ e $\pi_2$. Suponha que as fun√ß√µes de valor para essas pol√≠ticas sejam:
>
> - $v_{\pi_1}(s_1) = 10$, $v_{\pi_1}(s_2) = 5$
> - $v_{\pi_2}(s_1) = 7$, $v_{\pi_2}(s_2) = 8$
>
> Para determinar qual pol√≠tica √© melhor ou igual √† outra, comparamos os valores dos estados:
>
> - Para $s_1$: $v_{\pi_1}(s_1) = 10 > v_{\pi_2}(s_1) = 7$
> - Para $s_2$: $v_{\pi_1}(s_2) = 5 < v_{\pi_2}(s_2) = 8$
>
> Como $v_{\pi_1}(s) \geq v_{\pi_2}(s)$ n√£o √© v√°lido para todos os estados (√© falso para $s_2$), ent√£o $\pi_1 \ngeq \pi_2$.
>
> Similarmente, $v_{\pi_2}(s) \geq v_{\pi_1}(s)$ n√£o √© v√°lido para todos os estados (√© falso para $s_1$), ent√£o $\pi_2 \ngeq \pi_1$.
>
> Agora, suponha que existe uma terceira pol√≠tica $\pi_3$ com $v_{\pi_3}(s_1) = 12$ e $v_{\pi_3}(s_2) = 10$. Nesse caso:
>
> - $v_{\pi_3}(s_1) > v_{\pi_1}(s_1)$ e $v_{\pi_3}(s_2) > v_{\pi_1}(s_2)$
> - $v_{\pi_3}(s_1) > v_{\pi_2}(s_1)$ e $v_{\pi_3}(s_2) > v_{\pi_2}(s_2)$
>
> Portanto, $\pi_3$ √© melhor que $\pi_1$ e $\pi_2$. Se n√£o houver outra pol√≠tica com valores maiores, ent√£o $\pi_3$ √© uma pol√≠tica √≥tima $\pi_*$, e $v_*(s_1) = 12$ e $v_*(s_2) = 10$.

**Proposi√ß√£o 1** Se $\pi_*$ √© uma pol√≠tica √≥tima, ent√£o para qualquer estado $s$, seguir $\pi_*$ a partir de $s$ produz o mesmo retorno esperado que seguir qualquer outra pol√≠tica √≥tima $\pi'_*$ a partir de $s$.

*Prova*. Se $\pi_*$ e $\pi'_*$ s√£o ambas pol√≠ticas √≥timas, ent√£o $v_{\pi_*}(s) = v_*(s)$ e $v_{\pi'_*}(s) = v_*(s)$ para todo $s \in S$. Portanto, $v_{\pi_*}(s) = v_{\pi'_*}(s)$ para todo $s \in S$, o que implica que o retorno esperado ao seguir $\pi_*$ a partir de $s$ √© o mesmo que seguir $\pi'_*$ a partir de $s$. $\blacksquare$

### Fun√ß√µes de Valor de A√ß√£o √ìtimas

Al√©m da fun√ß√£o de valor de estado √≥tima, existe tamb√©m a **fun√ß√£o de valor de a√ß√£o √≥tima**, denotada por $q_*$, que √© definida como [^63]:

$$q_*(s, a) = \max_{\pi} q_{\pi}(s, a),$$

para todo $s \in S$ e $a \in A(s)$. Esta fun√ß√£o fornece o retorno esperado ao tomar a a√ß√£o $a$ no estado $s$ e, em seguida, seguir uma pol√≠tica √≥tima. Podemos relacionar $q_*$ com $v_*$ atrav√©s da seguinte equa√ß√£o [^63]:

$$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a].$$

> üí° **Exemplo Num√©rico:**
>
> Considere um estado $s$ com duas a√ß√µes poss√≠veis, $a_1$ e $a_2$. Suponha que, ao tomar a a√ß√£o $a_1$, o agente recebe uma recompensa de 2 e transita para o estado $s'$ com $v_*(s') = 6$. Ao tomar a a√ß√£o $a_2$, o agente recebe uma recompensa de 5 e permanece no estado $s$ com $v_*(s) = 7$. Assuma um fator de desconto $\gamma = 0.9$.
>
> Ent√£o, podemos calcular $q_*(s, a_1)$ e $q_*(s, a_2)$:
>
> $q_*(s, a_1) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a_1] = 2 + 0.9 * 6 = 2 + 5.4 = 7.4$
>
> $q_*(s, a_2) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a_2] = 5 + 0.9 * 7 = 5 + 6.3 = 11.3$
>
> Neste caso, $q_*(s, a_2) > q_*(s, a_1)$, ent√£o a a√ß√£o √≥tima no estado $s$ √© $a_2$.

**Proposi√ß√£o 2** Uma pol√≠tica $\pi$ √© √≥tima se, e somente se, para todo estado $s \in S$, a a√ß√£o $a$ selecionada por $\pi$ em $s$ maximiza $q_*(s, a)$.

*Prova*.
($\Rightarrow$) Se $\pi$ √© √≥tima, ent√£o $v_\pi(s) = v_*(s)$ para todo $s$. Se a a√ß√£o $a = \pi(s)$ n√£o maximizasse $q_*(s, a)$, ent√£o existiria uma a√ß√£o $a' \in A(s)$ tal que $q_*(s, a') > q_*(s, a)$. Isso implicaria que seguir $a'$ em $s$ e ent√£o seguir uma pol√≠tica √≥tima a partir de $S_{t+1}$ resultaria em um retorno esperado maior do que seguir $\pi$ a partir de $s$, contradizendo a otimalidade de $\pi$.

($\Leftarrow$) Se para todo $s$, a a√ß√£o $a = \pi(s)$ maximiza $q_*(s, a)$, ent√£o $q_\pi(s, \pi(s)) = q_*(s, \pi(s)) = \max_{a' \in A(s)} q_*(s, a')$.  Isso implica que seguir $\pi$ a partir de $s$ resulta no retorno esperado m√°ximo poss√≠vel, que √© $v_*(s)$. Portanto, $\pi$ √© uma pol√≠tica √≥tima. $\blacksquare$

### Exemplo: Golfe

Para ilustrar os conceitos de pol√≠ticas √≥timas e fun√ß√µes de valor √≥timas, considere o exemplo do jogo de golfe [^63]. A Figura 3.3 (n√£o mostrada aqui, consulte a refer√™ncia [^63] no contexto original) ilustra as curvas de n√≠vel de uma poss√≠vel fun√ß√£o de valor de a√ß√£o √≥tima $q_*(s, \text{driver})$. Estas curvas representam os valores de cada estado se o jogador inicialmente usar um *driver* e, posteriormente, escolher a melhor a√ß√£o (driver ou *putter*).





![State-value function for putting (upper) and optimal action-value function for using the driver (lower) in a golf scenario.](./../images/image8.png)

### A Equa√ß√£o de Otimalidade de Bellman

Uma vez que $v_*$ √© a fun√ß√£o de valor para uma pol√≠tica, ela deve satisfazer a condi√ß√£o de auto-consist√™ncia dada pela equa√ß√£o de Bellman para valores de estado [^59]. No entanto, como $v_*$ √© a fun√ß√£o de valor *√≥tima*, a sua condi√ß√£o de consist√™ncia pode ser expressa de uma forma especial, sem refer√™ncia a qualquer pol√≠tica espec√≠fica. Essa express√£o √© conhecida como a **equa√ß√£o de otimalidade de Bellman** [^63]. Intuitivamente, a equa√ß√£o de otimalidade de Bellman expressa o fato de que o valor de um estado sob uma pol√≠tica √≥tima deve ser igual ao retorno esperado para a melhor a√ß√£o a partir desse estado [^63]:

$$v_*(s) = \max_{a \in A(s)} q_{\pi_*}(s, a) = \max_{a} \mathbb{E}[G_t | S_t = s, A_t = a]$$

Expandindo a express√£o, obtemos [^63]:

$$v_*(s) = \max_{a} \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] = \max_{a} \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a].$$

Finalmente, a equa√ß√£o de otimalidade de Bellman pode ser escrita como [^63]:

$$v_*(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')].$$

> üí° **Exemplo Num√©rico:**
>
> Imagine um MDP simples com um estado $s$ e duas a√ß√µes $a_1$ e $a_2$. As probabilidades de transi√ß√£o e recompensas s√£o as seguintes:
>
> - A√ß√£o $a_1$: Transita para $s'$ com probabilidade 0.7, recompensa 1; transita para $s''$ com probabilidade 0.3, recompensa 0.
> - A√ß√£o $a_2$: Transita para $s'$ com probabilidade 0.2, recompensa 5; transita para $s''$ com probabilidade 0.8, recompensa -1.
>
> Suponha que $v_*(s') = 10$ e $v_*(s'') = 2$, e $\gamma = 0.9$.
>
> Calculamos $v_*(s)$ usando a equa√ß√£o de otimalidade de Bellman:
>
> $v_*(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]$
>
> Para $a_1$:
> $\sum_{s', r} p(s', r | s, a_1) [r + \gamma v_*(s')] = 0.7 * (1 + 0.9 * 10) + 0.3 * (0 + 0.9 * 2) = 0.7 * 10 + 0.3 * 1.8 = 6.3 + 0.54 = 7.64$
>
> Para $a_2$:
> $\sum_{s', r} p(s', r | s, a_2) [r + \gamma v_*(s')] = 0.2 * (5 + 0.9 * 10) + 0.8 * (-1 + 0.9 * 2) = 0.2 * 14 + 0.8 * 0.8 = 2.8 + 0.64 = 3.44$
>
> Portanto, $v_*(s) = \max(7.64, 3.44) = 7.64$.  A a√ß√£o √≥tima √© $a_1$.

De forma an√°loga, podemos obter a equa√ß√£o de otimalidade de Bellman para $q_*$ [^64]:

$$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a] = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a')].$$
 As figuras a seguir ilustram as equa√ß√µes para $v_*$ e $q_*$.



![Backup diagrams illustrating the update rules for state-value \(v_*\) and action-value \(q_*\) functions in MDPs.](./../images/image1.png)

> üí° **Exemplo Num√©rico:**
>
> Considere um estado $s$ e uma a√ß√£o $a$. Ap√≥s tomar a a√ß√£o $a$, o agente transita para o estado $s_1$ com probabilidade 0.6 e recebe uma recompensa de 3, ou transita para o estado $s_2$ com probabilidade 0.4 e recebe uma recompensa de 1. Seja $\gamma = 0.9$. Suponha que $q_*(s_1, a') = 5$ para todas as a√ß√µes $a'$ em $s_1$, e $q_*(s_2, a') = 2$ para todas as a√ß√µes $a'$ em $s_2$.
>
> Ent√£o,
>
> $q_*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a')] = 0.6 * (3 + 0.9 * 5) + 0.4 * (1 + 0.9 * 2) = 0.6 * 7.5 + 0.4 * 2.8 = 4.5 + 1.12 = 5.62$

**Teorema 3** (Unicidade da Solu√ß√£o da Equa√ß√£o de Otimalidade de Bellman). Para um MDP finito com fator de desconto $\gamma \in [0, 1)$, a equa√ß√£o de otimalidade de Bellman para $v_*$ tem uma √∫nica solu√ß√£o.

*Prova*. Considere o operador de Bellman √≥timo $T_*$ definido como:
$$(T_* v)(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')].$$
A equa√ß√£o de otimalidade de Bellman pode ent√£o ser escrita como $v_* = T_* v_*$. Para mostrar a unicidade da solu√ß√£o, precisamos mostrar que $T_*$ √© uma contra√ß√£o de Banach com fator de contra√ß√£o $\gamma$ sob a norma do supremo $\|v\| = \max_s |v(s)|$.

Para quaisquer duas fun√ß√µes de valor $v$ e $v'$, temos:
\begin{align*}
\|T_* v - T_* v'\| &= \max_s |(T_* v)(s) - (T_* v')(s)| \\
&= \max_s \left| \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] - \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v'(s')] \right| \\
&\leq \max_s \max_a \left| \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] - \sum_{s', r} p(s', r | s, a) [r + \gamma v'(s')] \right| \\
&= \max_s \max_a \left| \sum_{s', r} p(s', r | s, a) \gamma [v(s') - v'(s')] \right| \\
&\leq \max_s \max_a \sum_{s', r} p(s', r | s, a) \gamma |v(s') - v'(s')| \\
&\leq \gamma \max_s |v(s) - v'(s)| = \gamma \|v - v'\|.
\end{align*}
Portanto, $\|T_* v - T_* v'\| \leq \gamma \|v - v'\|$. Como $\gamma < 1$, $T_*$ √© uma contra√ß√£o de Banach e, pelo teorema do ponto fixo de Banach, existe um √∫nico ponto fixo, que √© a √∫nica solu√ß√£o para a equa√ß√£o de otimalidade de Bellman. $\blacksquare$

### Resolvendo a Equa√ß√£o de Otimalidade de Bellman

Para MDPs finitos, a equa√ß√£o de otimalidade de Bellman para $v_*$ representa um sistema de $n$ equa√ß√µes com $n$ inc√≥gnitas, onde $n$ √© o n√∫mero de estados [^64]. Se a din√¢mica do ambiente, $p$, for conhecida, este sistema de equa√ß√µes pode, em princ√≠pio, ser resolvido usando m√©todos para resolver sistemas de equa√ß√µes n√£o lineares [^64]. Da mesma forma, um conjunto de equa√ß√µes pode ser resolvido para $q_*$ [^64].

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP com dois estados $s_1$ e $s_2$, e duas a√ß√µes $a_1$ e $a_2$ em cada estado. As recompensas s√£o sempre 0.  As transi√ß√µes s√£o determin√≠sticas:
>
> - Em $s_1$: $a_1$ leva a $s_2$, $a_2$ leva a $s_1$.
> - Em $s_2$: $a_1$ leva a $s_1$, $a_2$ leva a $s_2$.
>
> Seja $\gamma = 0.5$.  Queremos encontrar $v_*(s_1)$ e $v_*(s_2)$.
>
> As equa√ß√µes de Bellman s√£o:
>
> $v_*(s_1) = \max\{0 + \gamma v_*(s_2), 0 + \gamma v_*(s_1)\} = \max\{0.5v_*(s_2), 0.5v_*(s_1)\}$
> $v_*(s_2) = \max\{0 + \gamma v_*(s_1), 0 + \gamma v_*(s_2)\} = \max\{0.5v_*(s_1), 0.5v_*(s_2)\}$
>
> Assumindo $v_*(s_1) > v_*(s_2)$, temos $v_*(s_1) = 0.5v_*(s_1)$, o que implica $v_*(s_1) = 0$.  Similarmente, $v_*(s_2) = 0$. A solu√ß√£o trivial √© $v_*(s_1) = v_*(s_2) = 0$.

Uma vez que $v_*$ √© conhecida, determinar uma pol√≠tica √≥tima torna-se relativamente simples [^64]. Para cada estado $s$, basta selecionar a a√ß√£o (ou a√ß√µes) que maximiza(m) o lado direito da equa√ß√£o de otimalidade de Bellman. Qualquer pol√≠tica que atribua probabilidade n√£o nula apenas a estas a√ß√µes √© uma pol√≠tica √≥tima. Em outras palavras, qualquer pol√≠tica *gulosa* com respeito √† fun√ß√£o de avalia√ß√£o √≥tima $v_*$ √© uma pol√≠tica √≥tima.

**Corol√°rio 3.1** Se $v_*$ √© a solu√ß√£o √∫nica da equa√ß√£o de otimalidade de Bellman, ent√£o qualquer pol√≠tica gulosa em rela√ß√£o a $v_*$ √© uma pol√≠tica √≥tima.

*Prova*. Seja $\pi$ uma pol√≠tica gulosa em rela√ß√£o a $v_*$. Isso significa que para todo $s \in S$, $\pi(s) = \arg\max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]$. Pela equa√ß√£o de otimalidade de Bellman, temos:
$$v_*(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')].$$
Como $\pi$ √© gulosa, temos:
$$v_*(s) = \sum_{s', r} p(s', r | s, \pi(s)) [r + \gamma v_*(s')].$$
Isso significa que $v_*$ √© a fun√ß√£o de valor para a pol√≠tica $\pi$, e como $v_*$ √© a fun√ß√£o de valor √≥tima, $\pi$ deve ser uma pol√≠tica √≥tima. $\blacksquare$

A seguir, uma figura com a pol√≠tica √≥tima e a fun√ß√£o de valor √≥tima para o exemplo *gridworld*:

![Optimal solutions to the gridworld example, illustrating the optimal policy and value function.](./../images/image2.png)

### Optimalidade e Aproxima√ß√£o

Embora tenhamos definido fun√ß√µes de valor e pol√≠ticas √≥timas, na pr√°tica, alcan√ßar a otimalidade completa √© raro. Para as classes de problemas que nos interessam, as pol√≠ticas √≥timas s√≥ podem ser geradas com um custo computacional proibitivo [^67]. Uma no√ß√£o bem definida de otimalidade organiza a abordagem de aprendizado que descrevemos neste livro e fornece uma maneira de entender as propriedades te√≥ricas de v√°rios algoritmos de aprendizado, mas √© um ideal que os agentes s√≥ podem se aproximar [^67].

<!-- END -->