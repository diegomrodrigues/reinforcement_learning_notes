## Bellman Optimality Equation e Solu√ß√£o para MDPs Finitos

### Introdu√ß√£o
No cap√≠tulo anterior, definimos **processos de decis√£o de Markov finitos (MDPs)** e introduzimos o conceito de **fun√ß√µes de valor**, que estimam qu√£o bom √© para um agente estar em um determinado estado ou realizar uma determinada a√ß√£o [^1]. Exploramos tamb√©m a **equa√ß√£o de Bellman** para a fun√ß√£o de valor de uma pol√≠tica arbitr√°ria, $v_\pi$ [^1]. Neste cap√≠tulo, avan√ßamos para o conceito de **pol√≠ticas √≥timas** e introduzimos a **equa√ß√£o de otimalidade de Bellman**, focando na sua aplica√ß√£o para encontrar a fun√ß√£o de valor √≥tima $v_*$ em MDPs finitos.

### Conceitos Fundamentais

A **equa√ß√£o de otimalidade de Bellman** √© uma condi√ß√£o de consist√™ncia que a fun√ß√£o de valor √≥tima deve satisfazer. Intuitivamente, ela expressa que o valor de um estado sob uma pol√≠tica √≥tima deve ser igual ao retorno esperado para a melhor a√ß√£o poss√≠vel a partir desse estado [^1]. Matematicamente, a equa√ß√£o para $v_*(s)$ √© dada por [^1]:
$$
v_*(s) = \max_{a \in A(s)} q_{\pi^*}(s, a) = \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s, A_t=a]
$$
onde:
*   $v_*(s)$ √© o valor √≥timo do estado $s$.
*   $A(s)$ √© o conjunto de a√ß√µes dispon√≠veis no estado $s$.
*   $q_{\pi^*}(s, a)$ √© o valor √≥timo da a√ß√£o, que representa o retorno esperado ao tomar a a√ß√£o $a$ no estado $s$ e, em seguida, seguir a pol√≠tica √≥tima $\pi^*$.
*   $\mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s, A_t=a]$ √© o retorno esperado ao tomar a a√ß√£o $a$ no estado $s$, obtendo uma recompensa $R_{t+1}$ e, em seguida, seguindo a pol√≠tica √≥tima a partir do pr√≥ximo estado $S_{t+1}$, com um fator de desconto $\gamma$.

> üí° **Exemplo Num√©rico:** Considere um MDP com dois estados, $S = \{s_1, s_2\}$, e duas a√ß√µes em cada estado, $A = \{a_1, a_2\}$. Suponha que $\gamma = 0.9$. A partir do estado $s_1$, a a√ß√£o $a_1$ leva deterministicamente a $s_2$ com uma recompensa de 10, enquanto a a√ß√£o $a_2$ leva deterministicamente a $s_1$ com uma recompensa de 2. A partir do estado $s_2$, a a√ß√£o $a_1$ leva deterministicamente a $s_1$ com uma recompensa de 5, enquanto a a√ß√£o $a_2$ leva deterministicamente a $s_2$ com uma recompensa de 3.  Para o estado $s_1$, a equa√ß√£o de otimalidade de Bellman ficaria:
> $$
> v_*(s_1) = \max \begin{cases}
> 10 + 0.9 \cdot v_*(s_2) \\
> 2 + 0.9 \cdot v_*(s_1)
> \end{cases}
> $$
> Similarmente, para o estado $s_2$:
> $$
> v_*(s_2) = \max \begin{cases}
> 5 + 0.9 \cdot v_*(s_1) \\
> 3 + 0.9 \cdot v_*(s_2)
> \end{cases}
> $$
> Este sistema de equa√ß√µes pode ser resolvido para encontrar $v_*(s_1)$ e $v_*(s_2)$. Resolver este sistema nos d√° uma ideia de como a equa√ß√£o de Bellman funciona na pr√°tica.

A equa√ß√£o acima pode ser expandida utilizando a defini√ß√£o da fun√ß√£o $q_*$ [^1]:
$$
v_*(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]
$$
Esta equa√ß√£o destaca que o valor √≥timo de um estado √© o m√°ximo, sobre todas as a√ß√µes poss√≠veis, da soma ponderada dos retornos esperados para cada poss√≠vel pr√≥ximo estado $s'$, ponderados pela probabilidade de transi√ß√£o $p(s', r | s, a)$ [^1].

**Unicidade da Solu√ß√£o**:
Para MDPs finitos, a equa√ß√£o de otimalidade de Bellman tem uma solu√ß√£o *√∫nica* [^1]. Isso significa que existe apenas uma fun√ß√£o de valor √≥tima $v_*$ que satisfaz a equa√ß√£o (3.19) simultaneamente para todos os estados $s \in S$ [^1].

**Interpreta√ß√£o como um Sistema de Equa√ß√µes**:
A equa√ß√£o de otimalidade de Bellman pode ser vista como um *sistema de equa√ß√µes*, uma para cada estado [^1]. Se houver $n$ estados, teremos $n$ equa√ß√µes com $n$ inc√≥gnitas (os valores de $v_*(s)$ para cada estado).

> üí° **Exemplo Num√©rico:** Suponha um MDP com tr√™s estados: $S = \{s_1, s_2, s_3\}$. A equa√ß√£o de otimalidade de Bellman pode ser representada como um sistema de tr√™s equa√ß√µes, uma para cada estado:
>
> $\begin{cases}
> v_*(s_1) = \max_{a \in A(s_1)} \sum_{s', r} p(s', r | s_1, a) [r + \gamma v_*(s')] \\
> v_*(s_2) = \max_{a \in A(s_2)} \sum_{s', r} p(s', r | s_2, a) [r + \gamma v_*(s')] \\
> v_*(s_3) = \max_{a \in A(s_3)} \sum_{s', r} p(s', r | s_3, a) [r + \gamma v_*(s')]
> \end{cases}$
>
> Cada equa√ß√£o mostra que o valor √≥timo de um estado √© uma fun√ß√£o dos valores √≥timos dos outros estados, ponderada pelas probabilidades de transi√ß√£o e recompensas. Resolver esse sistema nos d√° os valores √≥timos para cada estado.

**Lema 1:** *A contra√ß√£o da Equa√ß√£o de Bellman*.
A Equa√ß√£o de Bellman √© uma contra√ß√£o sob a norma do supremo, isto √©, a aplica√ß√£o repetida da Equa√ß√£o de Bellman converge para a solu√ß√£o √∫nica $v_*$.

*Prova (Esbo√ßo)*: A prova se baseia na demonstra√ß√£o de que o operador de Bellman √© uma contra√ß√£o de Banach, o que garante a converg√™ncia para um ponto fixo √∫nico, que √© a fun√ß√£o de valor √≥tima $v_*$. A demonstra√ß√£o envolve mostrar que a dist√¢ncia entre duas aplica√ß√µes sucessivas do operador de Bellman diminui a cada itera√ß√£o.

*Prova (Detalhada)*:
Para provar que a equa√ß√£o de Bellman √© uma contra√ß√£o sob a norma do supremo, precisamos demonstrar que o operador de Bellman, denotado por $T$, satisfaz a propriedade de contra√ß√£o. O operador de Bellman √© definido como:
$$(Tv)(s) = \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')]$$
onde $v$ √© uma fun√ß√£o de valor arbitr√°ria.

I. Sejam $v_1$ e $v_2$ duas fun√ß√µes de valor arbitr√°rias. Queremos mostrar que existe um fator $\gamma \in [0, 1)$ tal que:
    $$||Tv_1 - Tv_2||_\infty \leq \gamma ||v_1 - v_2||_\infty$$
    onde $||.||_\infty$ denota a norma do supremo, definida como $||v||_\infty = \max_{s \in S} |v(s)|$.

II. Consideremos a diferen√ßa entre as aplica√ß√µes do operador de Bellman em $v_1$ e $v_2$:
    $$|Tv_1(s) - Tv_2(s)| = \left| \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_1(s')] - \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_2(s')] \right|$$

III. Seja $a_1^*$ a a√ß√£o √≥tima para $Tv_1(s)$ e $a_2^*$ a a√ß√£o √≥tima para $Tv_2(s)$. Ent√£o podemos escrever:
    $$Tv_1(s) = \sum_{s', r} p(s', r | s, a_1^*) [r + \gamma v_1(s')] \quad \text{e} \quad Tv_2(s) = \sum_{s', r} p(s', r | s, a_2^*) [r + \gamma v_2(s')] $$

IV. Agora, consideremos a seguinte desigualdade:
    $$Tv_1(s) - Tv_2(s) \leq \sum_{s', r} p(s', r | s, a_1^*) [r + \gamma v_1(s')] - \sum_{s', r} p(s', r | s, a_1^*) [r + \gamma v_2(s')] = \gamma \sum_{s'} p(s' | s, a_1^*) [v_1(s') - v_2(s')] $$
    Porque $a_1^*$ √© a a√ß√£o que maximiza $Tv_1(s)$, ent√£o $Tv_2(s)$ tem que ser menor ou igual se usarmos $a_1^*$

V. Tomando o valor absoluto e usando a desigualdade triangular:
    $$|Tv_1(s) - Tv_2(s)| \leq \gamma \sum_{s'} p(s' | s, a_1^*) |v_1(s') - v_2(s')|$$

VI. Como $\sum_{s'} p(s' | s, a_1^*) = 1$, temos:
    $$|Tv_1(s) - Tv_2(s)| \leq \gamma \max_{s'} |v_1(s') - v_2(s')| = \gamma ||v_1 - v_2||_\infty$$

VII. Como esta desigualdade vale para todo $s \in S$, podemos tomar o m√°ximo sobre todos os estados:
    $$||Tv_1 - Tv_2||_\infty = \max_s |Tv_1(s) - Tv_2(s)| \leq \gamma ||v_1 - v_2||_\infty$$

VIII. Portanto, o operador de Bellman $T$ √© uma contra√ß√£o com fator $\gamma$. Pelo Teorema do Ponto Fixo de Banach, a aplica√ß√£o repetida de $T$ converge para um ponto fixo √∫nico, que √© a fun√ß√£o de valor √≥tima $v_*$. ‚ñ†

**Resolvendo a Equa√ß√£o de Otimalidade de Bellman**:
Se a din√¢mica do ambiente, $p(s', r | s, a)$, √© conhecida, em princ√≠pio, podemos resolver esse sistema de equa√ß√µes para $v_*$ usando uma variedade de m√©todos para resolver sistemas de equa√ß√µes n√£o lineares [^1]. Uma vez que $v_*$ √© conhecida, √© relativamente f√°cil determinar uma pol√≠tica √≥tima [^1]. Para cada estado $s$, haver√° uma ou mais a√ß√µes nas quais o m√°ximo √© obtido na equa√ß√£o de otimalidade de Bellman. Qualquer pol√≠tica que atribua probabilidade n√£o nula apenas a essas a√ß√µes √© uma pol√≠tica √≥tima [^1].

> üí° **Exemplo Num√©rico:** Considere um sistema de duas equa√ß√µes:
>
> $\begin{cases}
> v_*(s_1) = \max \{1 + 0.9v_*(s_2), 0.5 + 0.9v_*(s_1)\} \\
> v_*(s_2) = \max \{0.2 + 0.9v_*(s_1), 0.7 + 0.9v_*(s_2)\}
> \end{cases}$
>
> Resolvendo iterativamente, come√ßamos com $v_*(s_1) = 0$ e $v_*(s_2) = 0$.
>
> *   Itera√ß√£o 1:
>
>     *   $v_*(s_1) = \max \{1 + 0.9(0), 0.5 + 0.9(0)\} = 1$
>     *   $v_*(s_2) = \max \{0.2 + 0.9(0), 0.7 + 0.9(0)\} = 0.7$
> *   Itera√ß√£o 2:
>
>     *   $v_*(s_1) = \max \{1 + 0.9(0.7), 0.5 + 0.9(1)\} = \max\{1.63, 1.4\} = 1.63$
>     *   $v_*(s_2) = \max \{0.2 + 0.9(1), 0.7 + 0.9(0.7)\} = \max\{1.1, 1.33\} = 1.33$
>
> Repetindo essas itera√ß√µes, os valores convergem para uma solu√ß√£o. A solu√ß√£o final √© $v_*(s_1) \approx 6.02$ e $v_*(s_2) \approx 5.69$.
>
> Uma vez que conhecemos $v_*$, a pol√≠tica √≥tima pode ser determinada escolhendo a a√ß√£o que maximiza a equa√ß√£o de Bellman para cada estado.

**Teorema 1:** *Exist√™ncia de uma Pol√≠tica √ìtima Determin√≠stica*.
Para qualquer MDP finito, existe uma pol√≠tica √≥tima determin√≠stica.

*Prova (Esbo√ßo)*: Dada a unicidade de $v_*$, podemos construir uma pol√≠tica determin√≠stica $\pi^*$ que, para cada estado $s$, escolhe a a√ß√£o $a$ que maximiza $q_*(s, a)$. Essa pol√≠tica √© garantidamente √≥tima porque segue a a√ß√£o que leva ao melhor valor esperado a partir de cada estado, de acordo com $v_*$.

*Prova (Detalhada)*:
Provaremos que para qualquer MDP finito, existe uma pol√≠tica √≥tima determin√≠stica.

I. Seja $V_*$ a fun√ß√£o de valor √≥tima para o MDP. Pela equa√ß√£o de otimalidade de Bellman, sabemos que:
    $$v_*(s) = \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] \quad \forall s \in S$$

II. Definimos uma pol√≠tica determin√≠stica $\pi^*$ como se segue: para cada estado $s \in S$, escolhemos uma a√ß√£o $a^* \in A(s)$ que maximize a express√£o dentro do $\max$ na equa√ß√£o acima:
    $$a^*(s) = \arg\max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$
    Se houver v√°rias a√ß√µes que maximizem a express√£o, escolhemos uma arbitrariamente.

III. Agora, precisamos mostrar que essa pol√≠tica $\pi^*$ √© √≥tima.  Considere a fun√ß√£o de valor $v_{\pi^*}$ sob a pol√≠tica $\pi^*$. Ela satisfaz a equa√ß√£o de Bellman para $\pi^*$:
    $$v_{\pi^*}(s) = \sum_{s', r} p(s', r | s, \pi^*(s)) [r + \gamma v_{\pi^*}(s')] \quad \forall s \in S$$

IV. Como $\pi^*(s)$ foi escolhida para maximizar a soma ponderada de recompensas e valores descontados, temos:
    $$v_{\pi^*}(s) = \sum_{s', r} p(s', r | s, \pi^*(s)) [r + \gamma v_{\pi^*}(s')] = \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] = v_*(s)$$
    Isso significa que $v_{\pi^*}(s) = v_*(s)$ para todo $s \in S$.

V. Portanto, a pol√≠tica determin√≠stica $\pi^*$ alcan√ßa a fun√ß√£o de valor √≥tima $v_*$, o que implica que $\pi^*$ √© uma pol√≠tica √≥tima. ‚ñ†

**Pol√≠tica Gulosa (Greedy) com Rela√ß√£o a v***:
Qualquer pol√≠tica que seja *gulosa* com rela√ß√£o √† fun√ß√£o de avalia√ß√£o √≥tima $v_*$ √© uma pol√≠tica √≥tima [^1]. O termo "gulosa" significa que a pol√≠tica sempre escolhe a a√ß√£o que parece melhor a *curto prazo* [^1]. A beleza de $v_*$ √© que, se a usarmos para avaliar as consequ√™ncias de curto prazo das a√ß√µes - especificamente, as consequ√™ncias de um passo - ent√£o uma pol√≠tica gulosa √© realmente √≥tima no sentido de *longo prazo* em que estamos interessados [^1].

**Vantagens da Fun√ß√£o de Valor √ìtima:**
Ao usar a fun√ß√£o de valor √≥tima $v_*$, o retorno de longo prazo esperado √≥timo √© transformado em uma quantidade que √© local e imediatamente dispon√≠vel para cada estado [^1]. Portanto, uma busca de um passo √† frente produz as a√ß√µes √≥timas de longo prazo. Com $q_*$, o agente nem precisa fazer uma busca de um passo √† frente: para qualquer estado $s$, ele pode simplesmente encontrar qualquer a√ß√£o que maximize $q_*(s, a)$ [^1].

> üí° **Exemplo Num√©rico:** Considere um estado $s$ com duas a√ß√µes poss√≠veis: $a_1$ e $a_2$. Suponha que $v_*(s') = 10$ para o pr√≥ximo estado $s'$ se a a√ß√£o $a_1$ for tomada, e $v_*(s'') = 5$ para o pr√≥ximo estado $s''$ se a a√ß√£o $a_2$ for tomada. Assume tamb√©m que a recompensa $r$ para a a√ß√£o $a_1$ √© 1 e para a a√ß√£o $a_2$ √© 2. Se $\gamma = 0.9$, podemos calcular $q_*(s, a)$ para cada a√ß√£o:
>
> $q_*(s, a_1) = r + \gamma v_*(s') = 1 + 0.9 \cdot 10 = 10$
>
> $q_*(s, a_2) = r + \gamma v_*(s'') = 2 + 0.9 \cdot 5 = 6.5$
>
> Uma pol√≠tica gulosa escolheria a a√ß√£o $a_1$ porque $q_*(s, a_1) > q_*(s, a_2)$.

**Corol√°rio 1.1:** *Equival√™ncia entre Pol√≠ticas √ìtimas Gulosa e Pol√≠ticas √ìtimas*.
Uma pol√≠tica √© √≥tima se e somente se ela √© gulosa com rela√ß√£o a $v_*$.

*Prova*: Se uma pol√≠tica √© √≥tima, ent√£o por defini√ß√£o, ela atinge $v_*$. Uma pol√≠tica gulosa com rela√ß√£o a $v_*$ sempre escolhe a a√ß√£o que maximiza o retorno esperado, que, por defini√ß√£o de $v_*$, √© a a√ß√£o √≥tima. Portanto, a pol√≠tica gulosa tamb√©m √© √≥tima.

*Prova (Formal)*:
Provaremos a equival√™ncia entre pol√≠ticas √≥timas e pol√≠ticas gulosas com rela√ß√£o a $v_*$.

I. **Parte 1: Se uma pol√≠tica $\pi$ √© √≥tima, ent√£o ela √© gulosa com rela√ß√£o a $v_*$**.

   Seja $\pi$ uma pol√≠tica √≥tima. Ent√£o, por defini√ß√£o, $v_\pi(s) = v_*(s)$ para todo estado $s$.

II. Para que $\pi$ seja gulosa com rela√ß√£o a $v_*$, para cada estado $s$, a a√ß√£o $\pi(s)$ deve maximizar o valor esperado de seguir $\pi$ por um passo e, em seguida, seguir $v_*$:
    $$\pi(s) = \arg\max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$

III. Como $v_*(s) = v_\pi(s)$, podemos escrever:
     $$v_*(s) = v_\pi(s) = \sum_{s', r} p(s', r | s, \pi(s)) [r + \gamma v_\pi(s')] = \sum_{s', r} p(s', r | s, \pi(s)) [r + \gamma v_*(s')] $$

IV. Isso significa que a a√ß√£o $\pi(s)$ alcan√ßa o m√°ximo na equa√ß√£o de otimalidade de Bellman, ent√£o $\pi$ √© gulosa com rela√ß√£o a $v_*$.

V. **Parte 2: Se uma pol√≠tica $\pi$ √© gulosa com rela√ß√£o a $v_*$, ent√£o ela √© √≥tima**.

   Suponha que $\pi$ seja gulosa com rela√ß√£o a $v_*$. Ent√£o, para cada estado $s$:
   $$\pi(s) = \arg\max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$

VI. Isso significa que:
    $$v_\pi(s) = \sum_{s', r} p(s', r | s, \pi(s)) [r + \gamma v_\pi(s')] = \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] = v_*(s)$$

VII. Como $v_\pi(s) = v_*(s)$ para todo estado $s$, $\pi$ √© uma pol√≠tica √≥tima. ‚ñ†

**Exemplo: Resolvendo o Gridworld:**
No Exemplo 3.5, Figura 3.2 [^1], a fun√ß√£o de valor √≥tima, œÖ*, para este pol√≠tica, para o caso de recompensa descontada com Œ≥ = 0,9, foi calculado resolvendo o sistema de equa√ß√µes lineares (3.14) [^1]. A Figura 3.5 [^1] mostra a fun√ß√£o de valor √≥tima e as pol√≠ticas √≥timas correspondentes [^1].



![Optimal solutions to the gridworld example, illustrating the optimal policy and value function.](./../images/image2.png)

**Exemplo: Equa√ß√µes de Otimalidade de Bellman para o Rob√¥ de Reciclagem:**
Usando a equa√ß√£o (3.19) [^1], podemos dar explicitamente a equa√ß√£o de otimalidade de Bellman para o exemplo do rob√¥ de reciclagem [^1]. Para tornar as coisas mais compactas, abreviamos os estados alta e baixa, e as a√ß√µes pesquisar, esperar e recarregar, respectivamente, por h, l, s, w e re [^1]. Como existem apenas dois estados, a equa√ß√£o de otimalidade de Bellman consiste em duas equa√ß√µes [^1]. A equa√ß√£o para v‚àó(h) pode ser escrita como [^1]:
$$
v_*(h) = \max \begin{cases}
p(h|h, s)[r(h, s, h) + \gamma v_*(h)] + p(l|h, s)[r(h, s, l) + \gamma v_*(l)], \\
p(h|h, w)[r(h, w, h) + \gamma v_*(h)] + p(l|h, w)[r(h, w, l) + \gamma v_*(l)]
\end{cases}
$$
Substituindo os valores e as probabilidades de transi√ß√£o do Exemplo 3.3 [^1]:
$$
v_*(h) = \max \begin{cases}
\alpha[r_s + \gamma v_*(h)] + (1 - \alpha)[r_s + \gamma v_*(l)], \\
r_w + \gamma v_*(h)
\end{cases}
$$
Seguindo o mesmo procedimento para v‚àó(l) [^1]:
$$
v_*(l) = \max \begin{cases}
\beta r_s + (1-\beta)(-3) + \gamma[(1-\beta)v_*(h) + \beta v_*(l)], \\
r_w + \gamma v_*(l), \\
\gamma v_*(h)
\end{cases}
$$



![Representa√ß√£o do sistema de coleta de latas como um MDP finito, ilustrando as transi√ß√µes de estado e recompensas.](./../images/image4.png)

> üí° **Exemplo Num√©rico:** Para tornar este exemplo mais concreto, vamos atribuir alguns valores. Seja $\alpha = 0.7$, $\beta = 0.6$, $r_s = 5$, $r_w = 1$, e $\gamma = 0.9$. As equa√ß√µes se tornam:
>
> $$
> v_*(h) = \max \begin{cases}
> 0.7[5 + 0.9 v_*(h)] + 0.3[5 + 0.9 v_*(l)], \\
> 1 + 0.9 v_*(h)
> \end{cases}
> $$
>
> $$
> v_*(l) = \max \begin{cases}
> 0.6 \cdot 5 + 0.4 \cdot (-3) + 0.9[0.4 v_*(h) + 0.6 v_*(l)], \\
> 1 + 0.9 v_*(l), \\
> 0.9 v_*(h)
> \end{cases}
> $$
>
> Simplificando:
>
> $$
> v_*(h) = \max \begin{cases}
> 3.5 + 0.63 v_*(h) + 1.5 + 0.27 v_*(l), \\
> 1 + 0.9 v_*(h)
> \end{cases} = \max \begin{cases}
> 5 + 0.63 v_*(h) + 0.27 v_*(l), \\
> 1 + 0.9 v_*(h)
> \end{cases}
> $$
>
> $$
> v_*(l) = \max \begin{cases}
> 3 - 1.2 + 0.36 v_*(h) + 0.54 v_*(l), \\
> 1 + 0.9 v_*(l), \\
> 0.9 v_*(h)
> \end{cases} = \max \begin{cases}
> 1.8 + 0.36 v_*(h) + 0.54 v_*(l), \\
> 1 + 0.9 v_*(l), \\
> 0.9 v_*(h)
> \end{cases}
> $$
>
> Podemos resolver esse sistema de equa√ß√µes iterativamente para obter $v_*(h)$ e $v_*(l)$. Essas equa√ß√µes mostram como as probabilidades de transi√ß√£o, as recompensas e o fator de desconto influenciam os valores √≥timos dos estados.

### Conclus√£o

A equa√ß√£o de otimalidade de Bellman fornece uma caracteriza√ß√£o fundamental da fun√ß√£o de valor √≥tima em MDPs [^1]. Embora a solu√ß√£o direta dessa equa√ß√£o nem sempre seja pr√°tica devido a restri√ß√µes computacionais, ela serve como base para muitos algoritmos de reinforcement learning [^1]. Ao buscar solu√ß√µes aproximadas para a equa√ß√£o de Bellman, os agentes podem aprender pol√≠ticas que se aproximam do comportamento √≥timo em uma ampla gama de tarefas [^1].

### Refer√™ncias

[^1]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement Learning: An Introduction*. Cambridge, MA: MIT Press, 2018.
<!-- END -->