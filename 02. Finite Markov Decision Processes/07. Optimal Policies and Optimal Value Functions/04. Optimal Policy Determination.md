## Pol√≠ticas √ìtimas a Partir de Fun√ß√µes de Valor √ìtimas

### Introdu√ß√£o

Este cap√≠tulo explora as pol√≠ticas √≥timas e as fun√ß√µes de valor √≥timas dentro do contexto de **Processos de Decis√£o de Markov Finitos (MDPs)**. A se√ß√£o anterior [^58] introduziu o conceito de pol√≠ticas e fun√ß√µes de valor, demonstrando como as fun√ß√µes de valor estimam a "bondade" de estar em um determinado estado ou executar uma determinada a√ß√£o sob uma pol√≠tica espec√≠fica. Agora, investigaremos como, uma vez que a fun√ß√£o de valor *√≥tima* √© conhecida, podemos derivar a pol√≠tica *√≥tima* correspondente. Especificamente, esta se√ß√£o se concentra em como uma pol√≠tica pode ser facilmente determinada quando a fun√ß√£o de valor √≥tima $v_*$ √© conhecida [^64].

### Determina√ß√£o de Pol√≠ticas √ìtimas

Conforme definido anteriormente [^62], uma pol√≠tica $\pi$ √© considerada *melhor ou igual* a uma pol√≠tica $\pi'$ se o retorno esperado de $\pi$ √© maior ou igual ao de $\pi'$ para todos os estados $s \in S$. Uma pol√≠tica *√≥tima* ($\pi_*$) √© aquela que √© melhor ou igual a todas as outras pol√≠ticas [^62]. √â importante notar que, embora possa haver m√∫ltiplas pol√≠ticas √≥timas, elas compartilham a mesma fun√ß√£o de valor de estado *√≥tima* ($v_*$) [^62].

Uma vez que a fun√ß√£o de valor de estado √≥tima $v_*$ √© conhecida, uma pol√≠tica √≥tima pode ser determinada selecionando a√ß√µes que *maximizam* a Equa√ß√£o de Optimalidade de Bellman. Intuitivamente, isso significa que a pol√≠tica √≥tima em um estado deve selecionar a a√ß√£o que leva ao melhor retorno esperado, levando em conta tanto a recompensa imediata quanto o valor do estado resultante. Formalmente, isso pode ser expresso como [^64]:

$$
\pi_*(s) = \argmax_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]
$$

onde:

*   $\pi_*(s)$ √© a pol√≠tica √≥tima no estado *s*
*   $A(s)$ √© o conjunto de a√ß√µes dispon√≠veis no estado *s*
*   $p(s', r | s, a)$ √© a probabilidade de transi√ß√£o para o estado *s'* e receber recompensa *r* ap√≥s tomar a a√ß√£o *a* no estado *s*
*   $\gamma$ √© o fator de desconto
*   $v_*(s')$ √© a fun√ß√£o de valor de estado √≥tima para o estado *s'*

Em outras palavras, uma pol√≠tica √≥tima atribui probabilidade diferente de zero apenas √†s a√ß√µes que atingem o m√°ximo na equa√ß√£o da Optimalidade de Bellman [^64]. Qualquer pol√≠tica que satisfa√ßa esta condi√ß√£o √© uma pol√≠tica √≥tima.

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP simples com 3 estados (S1, S2, S3) e 2 a√ß√µes em cada estado (A1, A2). Suponha que determinamos a fun√ß√£o de valor √≥tima $v_*$ como:
>
> *   $v_*(S1) = 10$
> *   $v_*(S2) = 5$
> *   $v_*(S3) = 0$
>
> Seja o fator de desconto $\gamma = 0.9$. Agora, estamos no estado S1 e queremos determinar qual a√ß√£o (A1 ou A2) seguir de acordo com a pol√≠tica √≥tima. Precisamos calcular o valor esperado de cada a√ß√£o usando a equa√ß√£o da Optimalidade de Bellman.
>
> Suponha que as probabilidades de transi√ß√£o e recompensas sejam as seguintes:
>
> *   **A√ß√£o A1:**
>     *   $p(S2, 2 | S1, A1) = 0.8$ (transi√ß√£o para S2 com recompensa 2)
>     *   $p(S3, 0 | S1, A1) = 0.2$ (transi√ß√£o para S3 com recompensa 0)
> *   **A√ß√£o A2:**
>     *   $p(S2, 1 | S1, A2) = 0.5$ (transi√ß√£o para S2 com recompensa 1)
>     *   $p(S3, 1 | S1, A2) = 0.5$ (transi√ß√£o para S3 com recompensa 1)
>
> Agora, calcule o valor esperado para cada a√ß√£o:
>
> *   **A√ß√£o A1:**  $0.8 * (2 + 0.9 * 5) + 0.2 * (0 + 0.9 * 0) = 0.8 * (2 + 4.5) + 0 = 0.8 * 6.5 = 5.2$
> *   **A√ß√£o A2:**  $0.5 * (1 + 0.9 * 5) + 0.5 * (1 + 0.9 * 0) = 0.5 * (1 + 4.5) + 0.5 * (1 + 0) = 0.5 * 5.5 + 0.5 * 1 = 2.75 + 0.5 = 3.25$
>
> Como o valor esperado de A√ß√£o A1 (5.2) √© maior que o valor esperado de A√ß√£o A2 (3.25), a pol√≠tica √≥tima no estado S1 √© selecionar A√ß√£o A1.
>
> $$\pi_*(S1) = A1$$
>
> Este exemplo demonstra como, conhecendo a fun√ß√£o de valor √≥tima e as probabilidades de transi√ß√£o/recompensas, podemos determinar a a√ß√£o √≥tima para um estado espec√≠fico.

Al√©m disso, qualquer pol√≠tica que seja *greedy* em rela√ß√£o √† fun√ß√£o de avalia√ß√£o √≥tima $v_*$ √© uma pol√≠tica √≥tima [^64]. Uma pol√≠tica *greedy* √© aquela que seleciona a√ß√µes com base apenas em suas consequ√™ncias locais ou imediatas, sem considerar a possibilidade de que tal sele√ß√£o possa impedir o acesso futuro a alternativas ainda melhores. Isso √© poss√≠vel porque $v_*$ j√° leva em conta as consequ√™ncias da recompensa de todo o comportamento futuro poss√≠vel [^64].

Para formalizar a no√ß√£o de pol√≠tica *greedy*, podemos definir o operador *greedy* $G$ em rela√ß√£o a uma fun√ß√£o de valor $v$ como:

$$
G(v)(s) = \argmax_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')]
$$

Portanto, uma pol√≠tica $\pi$ √© *greedy* em rela√ß√£o a $v$ se $\pi(s) = G(v)(s)$ para todo $s \in S$.

**Teorema 1** Se $\pi$ √© uma pol√≠tica *greedy* em rela√ß√£o a $v_*$, ent√£o $\pi$ √© uma pol√≠tica √≥tima.

*Prova:* A prova segue diretamente da defini√ß√£o da Equa√ß√£o de Optimalidade de Bellman. Se $\pi$ √© *greedy* em rela√ß√£o a $v_*$, ent√£o para cada estado $s$, a a√ß√£o selecionada por $\pi$ maximiza o lado direito da Equa√ß√£o de Optimalidade de Bellman. Portanto, $\pi$ satisfaz a condi√ß√£o para ser uma pol√≠tica √≥tima.

Para maior clareza, apresentamos uma prova passo a passo:

I. Por defini√ß√£o, uma pol√≠tica $\pi$ *greedy* em rela√ß√£o a $v_*$ seleciona, para cada estado $s$, uma a√ß√£o $a$ que maximiza o valor esperado da soma da recompensa imediata e o valor descontado do pr√≥ximo estado, de acordo com $v_*$.  Matematicamente:
    $$\pi(s) = \argmax_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]$$

II. A Equa√ß√£o de Optimalidade de Bellman para fun√ß√µes de valor de estado √≥timas √© dada por:
$$v_*(s) = \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$

III.  Como $\pi$ √© *greedy* em rela√ß√£o a $v_*$,  para cada estado $s$, a a√ß√£o selecionada por $\pi$ alcan√ßa o m√°ximo no lado direito da Equa√ß√£o de Optimalidade de Bellman.  Portanto, podemos substituir $\pi(s)$ no lugar do $\argmax$ na Equa√ß√£o de Optimalidade de Bellman:
$$v_*(s) = \sum_{s', r} p(s', r | s, \pi(s)) [r + \gamma v_*(s')] $$

IV. A equa√ß√£o acima demonstra que a fun√ß√£o de valor de estado sob a pol√≠tica $\pi$ √© igual √† fun√ß√£o de valor de estado √≥tima $v_*$.  Como $v_*$ √©, por defini√ß√£o, a maior fun√ß√£o de valor que pode ser alcan√ßada por qualquer pol√≠tica, $\pi$ deve ser uma pol√≠tica √≥tima.  Seja $v_\pi(s)$ a fun√ß√£o de valor de estado para a pol√≠tica $\pi$. Sabemos que $v_*(s) \geq v_\pi(s)$ para todas as pol√≠ticas $\pi$ e todos os estados $s$. J√° mostramos que $v_*(s) = v_\pi(s)$, ent√£o $\pi$ √© uma pol√≠tica √≥tima. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Usando o mesmo MDP do exemplo anterior, vamos verificar se a pol√≠tica que sempre escolhe A1 no estado S1 √© *greedy* em rela√ß√£o a $v_*$.
>
> J√° calculamos que, no estado S1:
>
> *   Valor esperado de A1: 5.2
> *   Valor esperado de A2: 3.25
>
> Portanto, $G(v_*)(S1) = \argmax_{a \in A(S1)} \sum_{s', r} p(s', r | S1, a) [r + \gamma v_*(s')] = A1$
>
> Se a pol√≠tica $\pi$ define $\pi(S1) = A1$, ent√£o $\pi$ √© *greedy* em rela√ß√£o a $v_*$ no estado S1. Podemos repetir esse processo para todos os estados para verificar se a pol√≠tica √© *greedy* para todos os estados e, portanto, √≥tima.
>
> Para S2, suponha:
>
> *   $p(S1, 3 | S2, A1) = 0.6$
> *   $p(S3, 1 | S2, A1) = 0.4$
> *   $p(S1, 2 | S2, A2) = 0.3$
> *   $p(S2, 2 | S2, A2) = 0.7$
>
> *   **A√ß√£o A1:**  $0.6 * (3 + 0.9 * 10) + 0.4 * (1 + 0.9 * 0) = 0.6 * 12 + 0.4 * 1 = 7.2 + 0.4 = 7.6$
> *   **A√ß√£o A2:**  $0.3 * (2 + 0.9 * 10) + 0.7 * (2 + 0.9 * 5) = 0.3 * 11 + 0.7 * 6.5 = 3.3 + 4.55 = 7.85$
>
> Aqui, a a√ß√£o A2 tem um valor esperado maior, ent√£o $G(v_*)(S2) = A2$. Se $\pi(S2) = A2$, ent√£o $\pi$ √© *greedy* em S2.

### Utilizando Fun√ß√µes de Valor de A√ß√£o √ìtimas (q*)

Conforme mencionado anteriormente, ter a fun√ß√£o de valor de a√ß√£o √≥tima $q_*(s, a)$ simplifica ainda mais o processo de escolha de a√ß√µes √≥timas [^65]. Como $q_*(s, a)$ fornece o retorno esperado para tomar uma a√ß√£o espec√≠fica *a* em um estado *s* e, em seguida, seguir uma pol√≠tica √≥tima, o agente simplesmente precisa selecionar a a√ß√£o que maximiza $q_*(s, a)$ [^65].

$$
\pi_*(s) = \argmax_{a \in A(s)} q_*(s, a)
$$

Com $q_*$, o agente n√£o precisa nem mesmo realizar uma pesquisa antecipada de um passo; para qualquer estado *s*, pode simplesmente encontrar qualquer a√ß√£o que maximize $q_*(s, a)$. A fun√ß√£o de valor de a√ß√£o armazena em cache de forma eficaz os resultados de todas as pesquisas antecipadas de um passo [^65].

Essa abordagem oferece diversas vantagens:

*   **Efici√™ncia:** Escolher uma a√ß√£o torna-se uma simples consulta [^65].
*   **Independ√™ncia do Modelo:** Nenhuma informa√ß√£o sobre os poss√≠veis estados sucessores e seus valores √© necess√°ria [^65].

No entanto, essa conveni√™ncia tem um custo: a fun√ß√£o de valor da a√ß√£o $q_*$ requer a representa√ß√£o de uma fun√ß√£o de pares estado-a√ß√£o, em vez de apenas estados [^65].

> üí° **Exemplo Num√©rico:**
>
> Usando o mesmo MDP, suponha que tenhamos calculado a fun√ß√£o de valor de a√ß√£o √≥tima $q_*(s, a)$ como:
>
> *   $q_*(S1, A1) = 5.2$
> *   $q_*(S1, A2) = 3.25$
> *   $q_*(S2, A1) = 7.6$
> *   $q_*(S2, A2) = 7.85$
> *   $q_*(S3, A1) = -1$
> *   $q_*(S3, A2) = -2$
>
> Se estamos no estado S1, a pol√≠tica √≥tima √© $\pi_*(S1) = \argmax_{a \in A(S1)} q_*(S1, a) = A1$ porque $q_*(S1, A1) > q_*(S1, A2)$.  De forma semelhante, no estado S2, $\pi_*(S2) = A2$ porque $q_*(S2, A2) > q_*(S2, A1)$, e no estado S3, $\pi_*(S3) = A1$ because $q_*(S3, A1) > $q_*(S3, A2)$.
>
> Observe que n√£o precisamos conhecer as probabilidades de transi√ß√£o ou recompensas para tomar essa decis√£o.

√â importante notar que a fun√ß√£o de valor de estado √≥tima $v_*(s)$ pode ser recuperada da fun√ß√£o de valor de a√ß√£o √≥tima $q_*(s, a)$ atrav√©s da seguinte rela√ß√£o:

$$
v_*(s) = \max_{a \in A(s)} q_*(s, a)
$$

Esta rela√ß√£o surge diretamente da defini√ß√£o de $v_*(s)$ como o m√°ximo retorno esperado que pode ser obtido a partir do estado *s*, seguindo uma pol√≠tica √≥tima. Como $q_*(s, a)$ representa o retorno esperado ao tomar a a√ß√£o *a* no estado *s* e, em seguida, seguindo a pol√≠tica √≥tima, o valor √≥timo do estado deve ser o valor m√°ximo de a√ß√£o poss√≠vel naquele estado.

**Teorema 1.1** Seja $\pi_*$ uma pol√≠tica √≥tima e $q_*$ sua fun√ß√£o de valor de a√ß√£o √≥tima correspondente. Ent√£o, para qualquer estado $s$, $v_*(s) = \max_{a \in A(s)} q_*(s, a)$.

*Prova:* Seja $a^* = \argmax_{a \in A(s)} q_*(s, a)$. Ent√£o, $q_*(s, a^*)$ √© o retorno esperado m√°ximo que pode ser obtido a partir do estado $s$. Por defini√ß√£o de pol√≠tica √≥tima, esse retorno m√°ximo √© igual a $v_*(s)$.

Para maior clareza, apresentamos uma prova passo a passo:

I. Por defini√ß√£o, $q_*(s, a)$ representa o valor esperado de iniciar no estado $s$, tomar a a√ß√£o $a$ e, em seguida, seguir uma pol√≠tica √≥tima.

II. $v_*(s)$ representa o valor esperado m√°ximo que pode ser obtido a partir do estado $s$, seguindo uma pol√≠tica √≥tima.

III. Portanto, para encontrar $v_*(s)$, precisamos encontrar a a√ß√£o $a$ no estado $s$ que maximiza $q_*(s, a)$. Isso √© representado por $\max_{a \in A(s)} q_*(s, a)$.

IV. Como $v_*(s)$ √© o valor m√°ximo que pode ser obtido a partir do estado $s$, ele deve ser igual ao valor m√°ximo de $q_*(s, a)$ sobre todas as a√ß√µes poss√≠veis $a$ no estado $s$.

V. Portanto, $v_*(s) = \max_{a \in A(s)} q_*(s, a)$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Usando a fun√ß√£o $q_*(s, a)$ do exemplo anterior:
>
> *   $v_*(S1) = \max(q_*(S1, A1), q_*(S1, A2)) = \max(5.2, 3.25) = 5.2$
> *   $v_*(S2) = \max(q_*(S2, A1), q_*(S2, A2)) = \max(7.6, 7.85) = 7.85$
> *   $v_*(S3) = \max(q_*(S3, A1), q_*(S3, A2)) = \max(-1, -2) = -1$
>
> Observe que esses valores de $v_*(s)$ s√£o consistentes com a pol√≠tica √≥tima derivada anteriormente usando $q_*(s, a)$.

### Conclus√£o

Este cap√≠tulo demonstrou como, uma vez que a fun√ß√£o de valor √≥tima (tanto no estado como na a√ß√£o) √© conhecida, a determina√ß√£o de uma pol√≠tica √≥tima torna-se relativamente direta [^64]. Seja selecionando a√ß√µes que maximizem a Equa√ß√£o de Optimalidade de Bellman utilizando $v_*$, ou simplesmente selecionando a a√ß√£o que maximiza $q_*$, o agente pode tomar decis√µes √≥timas sem precisar conhecer a din√¢mica do ambiente [^65]. A capacidade de derivar uma pol√≠tica √≥tima de uma fun√ß√£o de valor √≥tima √© uma pedra angular do aprendizado por refor√ßo, lan√ßando as bases para muitos algoritmos que ser√£o explorados nos cap√≠tulos seguintes.

### Refer√™ncias

[^58]: Se√ß√£o 3.5, "Policies and Value Functions," Richard S. Sutton and Andrew G. Barto, "Reinforcement Learning: An Introduction," 2nd edition, 2018.
[^62]: Se√ß√£o 3.6, "Optimal Policies and Optimal Value Functions," Richard S. Sutton and Andrew G. Barto, "Reinforcement Learning: An Introduction," 2nd edition, 2018.
[^64]: Se√ß√£o 3.6, "Optimal Policies and Optimal Value Functions," Richard S. Sutton and Andrew G. Barto, "Reinforcement Learning: An Introduction," 2nd edition, 2018.
[^65]: Se√ß√£o 3.6, "Optimal Policies and Optimal Value Functions," Richard S. Sutton and Andrew G. Barto, "Reinforcement Learning: An Introduction," 2nd edition, 2018.
<!-- END -->