## Pol√≠ticas √ìtimas e Fun√ß√µes de Valor √ìtimas em Processos de Decis√£o de Markov Finitos

### Introdu√ß√£o
Neste cap√≠tulo, exploramos os conceitos de pol√≠ticas √≥timas e fun√ß√µes de valor √≥timas em processos de decis√£o de Markov finitos (MDPs finitos). Expandindo o conceito de **processos de decis√£o de Markov (MDPs)**, que s√£o uma formaliza√ß√£o cl√°ssica da tomada de decis√£o sequencial, onde as a√ß√µes influenciam n√£o apenas as recompensas imediatas, mas tamb√©m as situa√ß√µes subsequentes, ou estados, e, por meio delas, recompensas futuras [^1], focaremos em como definir e encontrar as melhores estrat√©gias de tomada de decis√£o.

### Pol√≠ticas √ìtimas e Fun√ß√µes de Valor √ìtimas
Resolver uma tarefa de aprendizado por refor√ßo significa, em termos gerais, encontrar uma pol√≠tica que alcance uma grande quantidade de recompensa ao longo do tempo [^16]. Para MDPs finitos, podemos definir precisamente uma **pol√≠tica √≥tima**. Fun√ß√µes de valor definem uma ordena√ß√£o parcial sobre as pol√≠ticas. Uma pol√≠tica $\pi$ √© definida como melhor ou igual a uma pol√≠tica $\pi'$ se seu retorno esperado for maior ou igual ao de $\pi'$ para todos os estados. Em outras palavras, $\pi \geq \pi'$ se e somente se $v_\pi(s) \geq v_{\pi'}(s)$ para todo $s \in S$ [^16].

Existe sempre pelo menos uma pol√≠tica que √© melhor ou igual a todas as outras pol√≠ticas. Essa √© uma **pol√≠tica √≥tima**. Embora possa haver mais de uma, denotamos todas as pol√≠ticas √≥timas por $\pi^*$. Elas compartilham a mesma **fun√ß√£o de valor de estado**, chamada **fun√ß√£o de valor de estado √≥tima**, denotada por $v_*$, e definida como [^16]:

$$
v_*(s) = \max_{\pi} v_\pi(s), \forall s \in S \quad [3.15]
$$

> üí° **Exemplo Num√©rico:** Considere um MDP com dois estados, $S = \{s_1, s_2\}$, e duas pol√≠ticas, $\pi_1$ e $\pi_2$. Suponha que $v_{\pi_1}(s_1) = 5$, $v_{\pi_1}(s_2) = 3$, $v_{\pi_2}(s_1) = 4$, e $v_{\pi_2}(s_2) = 6$.  Ent√£o $v_*(s_1) = \max(5, 4) = 5$ e $v_*(s_2) = \max(3, 6) = 6$. Portanto, $v_*(s_1) = 5$ e $v_*(s_2) = 6$. A pol√≠tica √≥tima para o estado $s_1$ seria $\pi_1$, e para o estado $s_2$ seria $\pi_2$. Se definirmos uma nova pol√≠tica $\pi_3$ onde $v_{\pi_3}(s_1) = 5$ e $v_{\pi_3}(s_2) = 6$, ent√£o $\pi_3$ seria uma pol√≠tica √≥tima global.

As pol√≠ticas √≥timas tamb√©m compartilham a mesma **fun√ß√£o de valor de a√ß√£o √≥tima**, denotada por $q_*$, e definida como [^17]:

$$
q_*(s, a) = \max_{\pi} q_\pi(s, a), \quad [3.16]
$$

para todo $s \in S$ e $a \in A(s)$. Para o par estado-a√ß√£o $(s, a)$, essa fun√ß√£o fornece o retorno esperado para realizar a a√ß√£o $a$ no estado $s$ e, subsequentemente, seguir uma pol√≠tica √≥tima. Assim, podemos escrever $q_*$ em termos de $v_*$ como segue:

$$
q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \quad [3.17]
$$

> üí° **Exemplo Num√©rico:** Suponha que em um estado $s$, temos duas a√ß√µes poss√≠veis, $a_1$ e $a_2$. Assuma que $R_{t+1} = 1$ para ambas as a√ß√µes.  Se escolhermos $a_1$, vamos para o estado $s_1$ com $v_*(s_1) = 10$. Se escolhermos $a_2$, vamos para o estado $s_2$ com $v_*(s_2) = 5$. Seja $\gamma = 0.9$.
> Ent√£o, $q_*(s, a_1) = 1 + 0.9 * 10 = 10$ e $q_*(s, a_2) = 1 + 0.9 * 5 = 5.5$. Portanto, a a√ß√£o √≥tima seria $a_1$, pois maximiza o valor esperado.

**Proposi√ß√£o 1**
Uma pol√≠tica $\pi$ √© √≥tima se e somente se $v_\pi(s) = v_*(s)$ para todo $s \in S$. Similarmente, $\pi$ √© √≥tima se e somente se $q_\pi(s, a) = q_*(s, a)$ para todo $s \in S$ e $a \in A(s)$.

*Prova:*
Se $\pi$ √© √≥tima, ent√£o por defini√ß√£o $v_\pi(s) = v_*(s)$ e $q_\pi(s, a) = q_*(s, a)$.
Reciprocamente, se $v_\pi(s) = v_*(s)$ para todo $s$, ent√£o $\pi$ √© √≥tima porque $v_*(s)$ √© o valor m√°ximo que qualquer pol√≠tica pode alcan√ßar em qualquer estado. O argumento √© an√°logo para $q_\pi(s, a) = q_*(s, a)$. $\blacksquare$

#### Exemplo: Fun√ß√µes de Valor √ìtimas para Golfe
A parte inferior da Figura 3.3 [n√£o presente aqui, mas mencionada na refer√™ncia original] mostra os contornos de uma poss√≠vel fun√ß√£o de valor de a√ß√£o √≥tima $q_*(s, driver)$. Estes s√£o os valores de cada estado se primeiro jogarmos uma tacada com o driver e depois selecionarmos o driver ou o putter, o que for melhor [^17]. O driver nos permite acertar a bola mais longe, mas com menos precis√£o. Podemos chegar ao buraco em uma tacada usando o driver apenas se j√° estivermos muito perto; assim, o contorno -1 para $q_*(s, driver)$ cobre apenas uma pequena por√ß√£o do green [^17]. Se tivermos duas tacadas, no entanto, podemos chegar ao buraco de muito mais longe, como mostrado pelo contorno -2. Nesse caso, n√£o temos que dirigir at√© o pequeno contorno -1, mas apenas para qualquer lugar no green; de l√° podemos usar o putter [^17]. A fun√ß√£o de valor de a√ß√£o √≥tima fornece os valores ap√≥s se comprometer com uma primeira a√ß√£o espec√≠fica, neste caso, para o driver, mas depois usando as a√ß√µes que forem melhores. O contorno -3 ainda est√° mais longe e inclui o tee de partida [^17]. Do tee, a melhor sequ√™ncia de a√ß√µes s√£o dois drives e um putt, afundando a bola em tr√™s tacadas.





![State-value function for putting (upper) and optimal action-value function for using the driver (lower) in a golf scenario.](./../images/image8.png)

#### A Equa√ß√£o de otimalidade de Bellman

Como $v_*$ √© a fun√ß√£o de valor para uma pol√≠tica, ela deve satisfazer a condi√ß√£o de autoconsist√™ncia dada pela equa√ß√£o de Bellman para valores de estado [^17] (3.14). Como √© a fun√ß√£o de valor √≥tima, no entanto, a condi√ß√£o de consist√™ncia de $v_*$ pode ser escrita de uma forma especial, sem refer√™ncia a nenhuma pol√≠tica espec√≠fica. Esta √© a **equa√ß√£o de Bellman para $v_*$**, ou a **equa√ß√£o de otimalidade de Bellman**. Intuitivamente, a equa√ß√£o de otimalidade de Bellman expressa o fato de que o valor de um estado sob uma pol√≠tica √≥tima deve ser igual ao retorno esperado para a melhor a√ß√£o a partir desse estado [^17]:

$$
\begin{aligned}
v_*(s) &= \max_{a \in A(s)} q_{\pi^*}(s, a) \\
&= \max_{a} \mathbb{E}[G_t | S_t = s, A_t = a] \\
&= \max_{a} \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] \\
&= \max_{a} \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\
&= \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] \quad [3.19]
\end{aligned}
$$

> üí° **Exemplo Num√©rico:** Considere um estado $s$ com duas a√ß√µes $a_1$ e $a_2$.  As probabilidades de transi√ß√£o e recompensas s√£o:
>
> *   A√ß√£o $a_1$: $p(s_1, r_1 | s, a_1) = 0.7$, onde $r_1 = 2$ e $v_*(s_1) = 8$; $p(s_2, r_2 | s, a_1) = 0.3$, onde $r_2 = -1$ e $v_*(s_2) = 4$.
> *   A√ß√£o $a_2$: $p(s_3, r_3 | s, a_2) = 0.5$, onde $r_3 = 1$ e $v_*(s_3) = 6$; $p(s_4, r_4 | s, a_2) = 0.5$, onde $r_4 = 0$ e $v_*(s_4) = 2$.
>
> Seja $\gamma = 0.9$.  Calculamos o valor esperado para cada a√ß√£o:
>
> *   $Q(s, a_1) = 0.7 * (2 + 0.9 * 8) + 0.3 * (-1 + 0.9 * 4) = 0.7 * 9.2 + 0.3 * 2.6 = 6.44 + 0.78 = 7.22$
> *   $Q(s, a_2) = 0.5 * (1 + 0.9 * 6) + 0.5 * (0 + 0.9 * 2) = 0.5 * 6.4 + 0.5 * 1.8 = 3.2 + 0.9 = 4.1$
>
> Portanto, $v_*(s) = \max(7.22, 4.1) = 7.22$, e a a√ß√£o √≥tima √© $a_1$.

**Prova da Equa√ß√£o (3.19)**
Vamos provar a equa√ß√£o de otimalidade de Bellman para $v_*$:
$$
v_*(s) = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]
$$

I. Come√ßamos com a defini√ß√£o da fun√ß√£o de valor de estado √≥tima:
   $$v_*(s) = \max_{\pi} v_\pi(s)$$

II. Usando a equa√ß√£o de Bellman para $v_\pi(s)$:
    $$v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]$$

III. Para encontrar o valor √≥timo, maximizamos sobre todas as pol√≠ticas:
     $$v_*(s) = \max_{\pi} \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]$$

IV. Uma vez que estamos maximizando sobre todas as pol√≠ticas, isso significa que estamos escolhendo a melhor a√ß√£o $a$ no estado $s$:
    $$v_*(s) = \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]$$

V. Expressando a expectativa em termos de probabilidades de transi√ß√£o:
   $$v_*(s) = \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$

VI. Portanto, a equa√ß√£o de otimalidade de Bellman para $v_*$ √©:
    $$v_*(s) = \max_{a} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$
$\blacksquare$

As duas √∫ltimas equa√ß√µes s√£o duas formas da equa√ß√£o de otimalidade de Bellman para $v_*$. A equa√ß√£o de otimalidade de Bellman para $q_*$ √© [^18]:

$$
\begin{aligned}
q_*(s, a) &= \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a] \\
&= \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a')] \quad [3.20]
\end{aligned}
$$

> üí° **Exemplo Num√©rico:** Considere um estado $s$ e uma a√ß√£o $a$. Depois de tomar a a√ß√£o $a$, existem dois poss√≠veis estados seguintes: $s_1$ e $s_2$, com probabilidades $p(s_1, r_1 | s, a) = 0.6$ e $p(s_2, r_2 | s, a) = 0.4$, e recompensas $r_1 = 5$ e $r_2 = -3$, respectivamente. Assume que $q_*(s_1, a'_1) = 10$ e $q_*(s_1, a'_2) = 6$, ent√£o $\max_{a'} q_*(s_1, a') = 10$. Similarmente, $q_*(s_2, a'_1) = 4$ e $q_*(s_2, a'_2) = 8$, ent√£o $\max_{a'} q_*(s_2, a') = 8$. Se $\gamma = 0.9$, ent√£o:
>
> $q_*(s, a) = 0.6 * (5 + 0.9 * 10) + 0.4 * (-3 + 0.9 * 8) = 0.6 * 14 + 0.4 * 4.2 = 8.4 + 1.68 = 10.08$

**Prova da Equa√ß√£o (3.20)**
Vamos provar a equa√ß√£o de otimalidade de Bellman para $q_*$:
$$
q_*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a')]
$$

I. Come√ßamos com a defini√ß√£o da fun√ß√£o de valor de a√ß√£o √≥tima:
   $$q_*(s, a) = \max_{\pi} q_\pi(s, a)$$

II. Usando a equa√ß√£o de Bellman para $q_\pi(s, a)$:
    $$q_\pi(s, a) = \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a]$$

III. Expressando $v_\pi(S_{t+1})$ em termos de $q_\pi(S_{t+1}, a')$ e maximizando sobre as a√ß√µes:
     $$v_*(s') = \max_{a'} q_*(s', a')$$

IV. Substituindo isso na equa√ß√£o de valor de a√ß√£o √≥tima:
    $$q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a]$$

V. Expressando a expectativa em termos de probabilidades de transi√ß√£o:
   $$q_*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a')] $$

VI. Portanto, a equa√ß√£o de otimalidade de Bellman para $q_*$ √©:
    $$q_*(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q_*(s', a')] $$
$\blacksquare$

√â importante notar que a equa√ß√£o de otimalidade de Bellman pode ser vista como um operador de backup.  Definimos o operador de Bellman otimista $\mathcal{T}^*$ para fun√ß√µes de valor de estado como:
$$
(\mathcal{T}^* v)(s) = \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] \quad [3.21]
$$
> üí° **Exemplo Num√©rico:** Usando os dados do exemplo anterior para a equa√ß√£o 3.19, vamos calcular $(\mathcal{T}^* v)(s)$:
>
> $ (\mathcal{T}^* v)(s) = \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v(s')] = \max(Q(s, a_1), Q(s, a_2)) = \max(7.22, 4.1) = 7.22 $
>
> Portanto, $(\mathcal{T}^* v)(s) = 7.22$.

Similarmente, definimos o operador de Bellman otimista $\mathcal{Q}^*$ para fun√ß√µes de valor de a√ß√£o como:
$$
(\mathcal{Q}^* q)(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q(s', a')] \quad [3.22]
$$
> üí° **Exemplo Num√©rico:** Usando os dados do exemplo anterior para a equa√ß√£o 3.20, vamos calcular $(\mathcal{Q}^* q)(s, a)$:
>
> $ (\mathcal{Q}^* q)(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma \max_{a'} q(s', a')] = 10.08 $
>
> Portanto, $(\mathcal{Q}^* q)(s, a) = 10.08$.

Com essa nota√ß√£o, a equa√ß√£o de otimalidade de Bellman pode ser escrita concisamente como $v_* = \mathcal{T}^* v_*$ e $q_* = \mathcal{Q}^* q_*$.

### Diagramas de Backup para $v_*$ e $q_*$
Os diagramas de backup na figura 3.4 [n√£o presente aqui, mas mencionada na refer√™ncia original] mostram graficamente os alcances de estados futuros e a√ß√µes considerados nas equa√ß√µes de otimalidade de Bellman para $v_*$ e $q_*$ [^18]. Estes s√£o os mesmos que os diagramas de backup para $v_\pi$ e $q_\pi$ apresentados anteriormente, exceto que arcos foram adicionados nos pontos de escolha do agente para representar que o m√°ximo sobre essa escolha √© tomado em vez do valor esperado dado alguma pol√≠tica. O diagrama de backup √† esquerda representa graficamente a equa√ß√£o de otimalidade de Bellman (3.19) e o diagrama de backup √† direita representa graficamente (3.20).



![Backup diagrams illustrating the update rules for state-value \(v_*\) and action-value \(q_*\) functions in MDPs.](./../images/image1.png)

### Resolu√ß√£o da Equa√ß√£o de Otimalidade de Bellman
Para MDPs finitos, a equa√ß√£o de otimalidade de Bellman para $v_*$ (3.19) tem uma solu√ß√£o √∫nica [^18]. A equa√ß√£o de otimalidade de Bellman √©, na verdade, um sistema de equa√ß√µes, uma para cada estado, de modo que se houver $n$ estados, ent√£o h√° $n$ equa√ß√µes em $n$ inc√≥gnitas [^18]. Se a din√¢mica $p$ do ambiente for conhecida, ent√£o, em princ√≠pio, pode-se resolver este sistema de equa√ß√µes para $v_*$ usando qualquer um de uma variedade de m√©todos para resolver sistemas de equa√ß√µes n√£o lineares [^18]. Pode-se resolver um conjunto relacionado de equa√ß√µes para $q_*$.

> üí° **Exemplo Num√©rico:** Considere um MDP com dois estados $s_1$ e $s_2$. Para simplificar, vamos assumir que existe apenas uma a√ß√£o poss√≠vel em cada estado. As equa√ß√µes de otimalidade de Bellman s√£o:
>
> $v_*(s_1) = \sum_{s', r} p(s', r | s_1, a) [r + \gamma v_*(s')]$
> $v_*(s_2) = \sum_{s', r} p(s', r | s_2, a) [r + \gamma v_*(s')]$
>
> Suponha que:
>
> *   $p(s_1, 2 | s_1, a) = 0.8$, $p(s_2, 1 | s_1, a) = 0.2$
> *   $p(s_2, 3 | s_2, a) = 0.6$, $p(s_1, -1 | s_2, a) = 0.4$
>
> E $\gamma = 0.9$. As equa√ß√µes se tornam:
>
> $v_*(s_1) = 0.8 * (2 + 0.9 * v_*(s_1)) + 0.2 * (1 + 0.9 * v_*(s_2))$
> $v_*(s_2) = 0.6 * (3 + 0.9 * v_*(s_2)) + 0.4 * (-1 + 0.9 * v_*(s_1))$
>
> Simplificando:
>
> $v_*(s_1) = 1.6 + 0.72 * v_*(s_1) + 0.2 + 0.18 * v_*(s_2)$
> $v_*(s_2) = 1.8 + 0.54 * v_*(s_2) - 0.4 + 0.36 * v_*(s_1)$
>
> Rearranjando:
>
> $0.28 * v_*(s_1) - 0.18 * v_*(s_2) = 1.8$
> $-0.36 * v_*(s_1) + 0.46 * v_*(s_2) = 1.4$
>
> Resolvendo este sistema de equa√ß√µes lineares (por exemplo, usando substitui√ß√£o ou m√©todos matriciais) resulta em valores aproximados:
>
> $v_*(s_1) \approx 10.22$
> $v_*(s_2) \approx 8.65$

Uma vez que se tem $v_*$, √© relativamente f√°cil determinar uma pol√≠tica √≥tima [^18]. Para cada estado $s$, haver√° uma ou mais a√ß√µes nas quais o m√°ximo √© obtido na equa√ß√£o de otimalidade de Bellman. Qualquer pol√≠tica que atribua probabilidade diferente de zero apenas a essas a√ß√µes √© uma pol√≠tica √≥tima [^18]. Pode-se pensar nisso como uma busca de um passo. Se voc√™ tem a fun√ß√£o de valor √≥tima, $v_*$, ent√£o as a√ß√µes que parecem melhores ap√≥s uma busca de um passo ser√£o a√ß√µes √≥timas.

**Teorema 2** (Pol√≠tica Gulosa)
Dada a fun√ß√£o de valor de estado √≥tima $v_*$, uma pol√≠tica $\pi$ que age greedy com respeito a $v_*$ √© uma pol√≠tica √≥tima.  Especificamente, para cada estado $s \in S$, a pol√≠tica $\pi$ escolhe uma a√ß√£o $a$ que maximiza o valor esperado do pr√≥ximo estado:
$$
\pi(a|s) = \begin{cases}
1, & \text{se } a = \arg\max_{a' \in A(s)} \sum_{s', r} p(s', r | s, a') [r + \gamma v_*(s')] \\
0, & \text{caso contr√°rio}
\end{cases}
$$
Qualquer pol√≠tica que satisfa√ßa essa condi√ß√£o √© uma pol√≠tica √≥tima.

> üí° **Exemplo Num√©rico:** Suponha que temos um estado $s$ com duas a√ß√µes poss√≠veis $a_1$ e $a_2$. J√° calculamos $v_*(s)$ para todos os estados poss√≠veis. Considere:
>
> *   Para $a_1$: $\sum_{s', r} p(s', r | s, a_1) [r + \gamma v_*(s')] = 7.5$
> *   Para $a_2$: $\sum_{s', r} p(s', r | s, a_2) [r + \gamma v_*(s')] = 6.0$
>
> Ent√£o, $\arg\max_{a' \in A(s)} \sum_{s', r} p(s', r | s, a') [r + \gamma v_*(s')] = a_1$. De acordo com o teorema da pol√≠tica gananciosa, a pol√≠tica √≥tima $\pi(a|s)$ atribuir√° probabilidade 1 √† a√ß√£o $a_1$ e probabilidade 0 √† a√ß√£o $a_2$.

*Prova:*
Seja $\pi$ uma pol√≠tica que age greedy com respeito a $v_*$. Ent√£o, para cada estado $s$, temos:
$$
\begin{aligned}
v_\pi(s) &= \mathbb{E}_\pi[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s] \\
&= \sum_{a \in A(s)} \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] \\
&= \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] \\
&= v_*(s)
\end{aligned}
$$
Como $v_\pi(s) = v_*(s)$ para todo $s$, ent√£o $\pi$ √© uma pol√≠tica √≥tima (pela Proposi√ß√£o 1). $\blacksquare$

**Prova do Teorema 2**
Para provar que uma pol√≠tica gananciosa com respeito a $v_*$ √© √≥tima, mostramos que o valor de estado dessa pol√≠tica √© igual a $v_*$ para todos os estados.

I. Seja $\pi$ uma pol√≠tica gananciosa com respeito a $v_*$. Isso significa que para cada estado $s$, $\pi(a|s) = 1$ para a a√ß√£o $a$ que maximiza $\sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')]$.

II. O valor de $v_\pi(s)$ pode ser expresso usando a equa√ß√£o de Bellman:
   $$v_\pi(s) = \mathbb{E}_\pi[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s]$$

III. Expandindo a expectativa sobre todas as a√ß√µes poss√≠veis sob $\pi$:
   $$v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$

IV. Como $\pi$ √© gananciosa, $\pi(a|s) = 1$ para a a√ß√£o que maximiza a express√£o. Assim, a soma sobre $a$ se reduz ao m√°ximo:
   $$v_\pi(s) = \max_{a \in A(s)} \sum_{s', r} p(s', r | s, a) [r + \gamma v_*(s')] $$

V. Pela equa√ß√£o de otimalidade de Bellman, o lado direito √© exatamente $v_*(s)$:
   $$v_\pi(s) = v_*(s)$$

VI. Uma vez que $v_\pi(s) = v_*(s)$ para todos os estados $s$, $\pi$ √© uma pol√≠tica √≥tima (pela Proposi√ß√£o 1).
$\blacksquare$

### Conclus√£o
Neste cap√≠tulo, exploramos os conceitos de pol√≠ticas √≥timas e fun√ß√µes de valor √≥timas, fornecendo um arcabou√ßo fundamental para entender e resolver problemas de aprendizado por refor√ßo em MDPs finitos [^1]. Embora encontrar pol√≠ticas √≥timas possa ser computacionalmente desafiador, entender esses conceitos √© crucial para projetar algoritmos de aprendizado eficazes que se aproximem do comportamento √≥timo em ambientes complexos [^18].

### Refer√™ncias
[^1]: Chapter 3: Finite Markov Decision Processes
[^16]: Section 3.6 Optimal Policies and Optimal Value Functions
[^17]: Example 3.7: Optimal Value Functions for Golf
[^18]: For finite MDPs, the Bellman optimality equation for  v‚àó  (3.19) has a unique solution.
<!-- END -->