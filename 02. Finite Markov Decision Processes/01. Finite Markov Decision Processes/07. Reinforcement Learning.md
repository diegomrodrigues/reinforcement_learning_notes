## O Aprendizado por ReforÃ§o e a InteraÃ§Ã£o Adaptativa

### IntroduÃ§Ã£o

Este capÃ­tulo explora o aprendizado por reforÃ§o (RL) no contexto de processos de decisÃ£o de Markov (MDPs) finitos, onde o objetivo Ã© aprender a se comportar de forma otimizada atravÃ©s da interaÃ§Ã£o com um ambiente [^1]. O RL difere de outras abordagens para resolver MDPs devido Ã  sua natureza *online*, permitindo que o agente foque em estados frequentemente encontrados, tornando-o adaptÃ¡vel e eficiente. A presente seÃ§Ã£o aborda o conceito fundamental do aprendizado por reforÃ§o, detalhando sua aplicabilidade, desafios e as consideraÃ§Ãµes essenciais para sua implementaÃ§Ã£o efetiva.

### Conceitos Fundamentais
O aprendizado por reforÃ§o se distingue por sua capacidade de aprender a partir da interaÃ§Ã£o direta com o ambiente para atingir um objetivo especÃ­fico [^68]. Nesse contexto, a natureza *online* do RL se torna uma vantagem crucial. Diferentemente de mÃ©todos que exigem um modelo completo do ambiente (como programaÃ§Ã£o dinÃ¢mica), o RL pode se concentrar em aprender boas decisÃµes para os estados mais frequentemente encontrados, alocando recursos computacionais de forma adaptativa [^68].

A aplicaÃ§Ã£o do RL enfrenta um dilema inerente: a *tensÃ£o entre a amplitude de aplicabilidade e a tratabilidade matemÃ¡tica* [^1]. Enquanto os MDPs fornecem um arcabouÃ§o matemÃ¡tico rigoroso, nem todos os problemas do mundo real se encaixam perfeitamente nesse modelo. Estender o RL para alÃ©m dos MDPs, como serÃ¡ discutido no CapÃ­tulo 17, introduz desafios adicionais, exigindo compromissos e consideraÃ§Ãµes cuidadosas.

O aprendizado por reforÃ§o envolve feedback *avaliativo*, onde o agente recebe sinais de recompensa indicando a qualidade de suas aÃ§Ãµes, e um aspecto *associativo*, onde o agente aprende a escolher diferentes aÃ§Ãµes em diferentes situaÃ§Ãµes [^1]. Esta capacidade de associar estados a aÃ§Ãµes apropriadas Ã© fundamental para o sucesso do RL em ambientes complexos.

A escolha das representaÃ§Ãµes de estado e aÃ§Ã£o tem um impacto profundo no desempenho do aprendizado por reforÃ§o [^4]. Uma representaÃ§Ã£o inadequada pode obscurecer as caracterÃ­sticas relevantes do ambiente, tornando o aprendizado mais difÃ­cil ou impossÃ­vel. A seleÃ§Ã£o criteriosa de representaÃ§Ãµes Ã©, portanto, um aspecto crucial do design de sistemas de RL.

> ğŸ’¡ **Exemplo NumÃ©rico:** Imagine um agente aprendendo a jogar Pac-Man. Se o estado for representado apenas pela posiÃ§Ã£o atual do Pac-Man, sem considerar a posiÃ§Ã£o dos fantasmas ou a disposiÃ§Ã£o dos "dots" (bolinhas), o agente terÃ¡ muita dificuldade em aprender uma polÃ­tica eficaz. Uma representaÃ§Ã£o melhor incluiria a posiÃ§Ã£o do Pac-Man, a posiÃ§Ã£o dos fantasmas (talvez apenas os mais prÃ³ximos), a direÃ§Ã£o em que os fantasmas estÃ£o se movendo, e a localizaÃ§Ã£o dos "dots" mais prÃ³ximos. Isso permite que o agente associe diferentes estados (configuraÃ§Ãµes do jogo) a aÃ§Ãµes apropriadas (ir para um "dot", evitar um fantasma).
>
> ```python
> import numpy as np
>
> # RepresentaÃ§Ã£o de estado simplificada (apenas para ilustraÃ§Ã£o)
> class State:
>     def __init__(self, pacman_x, pacman_y, ghost_x, ghost_y):
>         self.pacman_x = pacman_x
>         self.pacman_y = pacman_y
>         self.ghost_x = ghost_x
>         self.ghost_y = ghost_y
>
> # Exemplo de um estado
> state = State(pacman_x=10, pacman_y=5, ghost_x=8, ghost_y=6)
>
> print(f"Estado: Pac-Man=({state.pacman_x}, {state.pacman_y}), Fantasma=({state.ghost_x}, {state.ghost_y})")
> ```

Ã‰ fundamental notar que o sinal de recompensa nÃ£o deve ser usado para transmitir conhecimento prÃ©vio sobre como atingir o objetivo [^54]. O agente deve ser recompensado apenas por atingir o objetivo, e nÃ£o por alcanÃ§ar sub-objetivos. O uso inadequado de recompensas pode levar a comportamentos indesejados, nos quais o agente explora brechas na estrutura de recompensas sem realmente atingir o objetivo final.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um agente treinado para limpar um quarto. Se o agente for recompensado por pegar cada pedaÃ§o de lixo individualmente, ele pode se concentrar em pegar o lixo e colocÃ¡-lo de volta no chÃ£o para pegar novamente e receber outra recompensa, em vez de realmente jogar o lixo fora. A recompensa deve ser dada apenas quando o quarto estiver completamente limpo.
>
> ```python
> # Exemplo de funÃ§Ã£o de recompensa inadequada
> def reward_bad(action):
>     if action == "pegar_lixo":
>         return 1
>     else:
>         return 0
>
> # Exemplo de funÃ§Ã£o de recompensa adequada
> def reward_good(room_is_clean):
>     if room_is_clean:
>         return 100
>     else:
>         return 0
>
> print("Recompensa inadequada:", reward_bad("pegar_lixo"))
> print("Recompensa adequada:", reward_good(True))
> ```

Para clarificar ainda mais este conceito, podemos introduzir a noÃ§Ã£o de *recompensa intrÃ­nseca*.

**DefiniÃ§Ã£o:** Recompensa IntrÃ­nseca Ã© um sinal de recompensa gerado internamente pelo agente, baseado em critÃ©rios como curiosidade, novidade ou progresso no aprendizado.

Ao contrÃ¡rio das recompensas extrÃ­nsecas, que sÃ£o definidas pelo ambiente, as recompensas intrÃ­nsecas podem ser usadas para incentivar a exploraÃ§Ã£o e o aprendizado em ambientes com recompensas esparsas ou atrasadas. No entanto, Ã© crucial equilibrar recompensas intrÃ­nsecas e extrÃ­nsecas para evitar que o agente se concentre apenas em explorar o ambiente sem atingir o objetivo final.

#### A InteraÃ§Ã£o Agente-Ambiente

No cerne do RL estÃ¡ a interaÃ§Ã£o contÃ­nua entre um *agente* e um *ambiente* [^1]. O agente percebe o estado do ambiente ($S_t$), escolhe uma aÃ§Ã£o ($A_t$), e recebe uma recompensa ($R_{t+1}$) [^48]. O ambiente, por sua vez, responde Ã  aÃ§Ã£o do agente, transitando para um novo estado ($S_{t+1}$) [^48]. Esse ciclo de percepÃ§Ã£o-aÃ§Ã£o-recompensa forma a base do processo de aprendizado por reforÃ§o.

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um robÃ´ aprendendo a navegar em um labirinto.
> 1.  **Estado ($S_t$):** A posiÃ§Ã£o atual do robÃ´ no labirinto (coordenadas x, y). Por exemplo, $S_t = (2, 3)$.
> 2.  **AÃ§Ã£o ($A_t$):** A aÃ§Ã£o que o robÃ´ escolhe realizar (mover para cima, para baixo, para a esquerda ou para a direita). Por exemplo, $A_t = \text{mover para cima}$.
> 3.  **Recompensa ($R_{t+1}$):** A recompensa que o robÃ´ recebe apÃ³s realizar a aÃ§Ã£o. Por exemplo, $R_{t+1} = -1$ se o robÃ´ nÃ£o atingir o objetivo, e $R_{t+1} = 10$ se o robÃ´ atingir o objetivo (chegar Ã  saÃ­da do labirinto).
> 4.  **Novo Estado ($S_{t+1}$):** A nova posiÃ§Ã£o do robÃ´ apÃ³s realizar a aÃ§Ã£o. Por exemplo, se o robÃ´ estava em (2, 3) e se moveu para cima, o novo estado pode ser $S_{t+1} = (2, 4)$.
>
> ```python
> # SimulaÃ§Ã£o simplificada da interaÃ§Ã£o agente-ambiente
> class Environment:
>     def __init__(self, grid_size=10):
>         self.grid_size = grid_size
>         self.agent_position = (0, 0)  # PosiÃ§Ã£o inicial do agente
>         self.goal_position = (grid_size - 1, grid_size - 1) # Objetivo
>
>     def step(self, action):
>         # Movimento do agente (simplificado)
>         if action == "up":
>             self.agent_position = (self.agent_position[0], min(self.agent_position[1] + 1, self.grid_size - 1))
>         elif action == "down":
>             self.agent_position = (self.agent_position[0], max(self.agent_position[1] - 1, 0))
>         elif action == "left":
>             self.agent_position = (max(self.agent_position[0] - 1, 0), self.agent_position[1])
>         elif action == "right":
>             self.agent_position = (min(self.agent_position[0] + 1, self.grid_size - 1), self.agent_position[1])
>
>         # Recompensa
>         if self.agent_position == self.goal_position:
>             reward = 10
>         else:
>             reward = -1
>
>         return self.agent_position, reward
>
> # InicializaÃ§Ã£o do ambiente
> env = Environment()
>
> # Agente realiza uma aÃ§Ã£o
> action = "up"
> new_state, reward = env.step(action)
>
> print(f"AÃ§Ã£o: {action}, Novo estado: {new_state}, Recompensa: {reward}")
> ```

Essa interaÃ§Ã£o contÃ­nua gera uma trajetÃ³ria ou sequÃªncia de estados, aÃ§Ãµes e recompensas: $S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots$ [^48]. A partir dessa experiÃªncia, o agente busca aprender uma *polÃ­tica*, que mapeia estados para aÃ§Ãµes, de forma a maximizar a recompensa cumulativa ao longo do tempo [^58].

Para formalizar a noÃ§Ã£o de polÃ­tica, podemos definir:

**DefiniÃ§Ã£o:** Uma polÃ­tica $\pi$ Ã© uma funÃ§Ã£o que mapeia estados para probabilidades de seleÃ§Ã£o de cada aÃ§Ã£o disponÃ­vel. Formalmente, $\pi(a|s) = P(A_t = a | S_t = s)$, onde $\pi(a|s)$ representa a probabilidade de selecionar a aÃ§Ã£o $a$ no estado $s$ no tempo $t$.

Assim, o objetivo do agente Ã© encontrar a polÃ­tica Ã³tima $\pi^*$ que maximize o retorno esperado.

#### Recompensas e Objetivos

Em RL, o *objetivo* do agente Ã© formalizado em termos de um sinal especial chamado *recompensa* [^53]. A recompensa Ã© um nÃºmero simples, $R_t \in \mathbb{R}$, que o agente recebe do ambiente a cada passo de tempo [^53]. O objetivo do agente Ã© maximizar a quantidade total de recompensa que recebe ao longo do tempo [^53].

O *retorno* ($G_t$) Ã© a funÃ§Ã£o das recompensas futuras que o agente busca maximizar. Em sua forma mais simples, o retorno Ã© a soma das recompensas futuras:

$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \ldots + R_T$$ [^54]

onde $T$ Ã© o passo de tempo final. Esta formulaÃ§Ã£o Ã© apropriada para *tarefas episÃ³dicas*, nas quais a interaÃ§Ã£o agente-ambiente se divide naturalmente em episÃ³dios [^54].

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um jogo como o Tetris. Cada jogo Ã© um episÃ³dio. Se o agente recebe recompensas de +1 por cada linha que completa e 0 no resto do tempo e o jogo termina apÃ³s 1000 passos, entÃ£o:
>
> *   $R_1 = 0$ (nenhuma linha completada no primeiro passo)
> *   $R_2 = 1$ (uma linha completada no segundo passo)
> *   $R_3 = 0$ (nenhuma linha completada no terceiro passo)
> *   ...
> *   $R_{1000} = 2$ (duas linhas completadas no Ãºltimo passo)
>
> O retorno $G_0$ (o retorno no inÃ­cio do episÃ³dio) seria a soma de todas as recompensas: $G_0 = 0 + 1 + 0 + \ldots + 2$.  Se o agente completou um total de 50 linhas durante o jogo, entÃ£o $G_0 = 50$.

Para *tarefas contÃ­nuas*, nas quais a interaÃ§Ã£o nÃ£o se divide naturalmente em episÃ³dios, o retorno Ã© geralmente definido usando um fator de desconto, $\gamma$, onde $0 \le \gamma \le 1$:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$ [^55]

O fator de desconto determina o valor presente de recompensas futuras [^55]. Um $\gamma$ prÃ³ximo de 0 faz com que o agente se concentre apenas em recompensas imediatas, enquanto um $\gamma$ prÃ³ximo de 1 faz com que o agente considere recompensas futuras com maior peso [^55].

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um robÃ´ aprendendo a gerenciar a energia de uma bateria. A recompensa pode ser o uso da energia disponÃ­vel ao longo do tempo, e queremos que o robÃ´ equilibre o uso imediato da energia com a necessidade de manter energia para o futuro. Se definirmos $\gamma = 0.9$, isso significa que uma recompensa recebida no prÃ³ximo passo vale 90% de uma recompensa recebida agora. Se o robÃ´ receber as seguintes recompensas:
>
> *   $R_1 = 1$ (uso moderado de energia no primeiro passo)
> *   $R_2 = 2$ (uso um pouco maior de energia no segundo passo)
> *   $R_3 = 0.5$ (uso baixo de energia no terceiro passo)
> *   $R_4 = 1.5$ (uso um pouco maior de energia no quarto passo)
>
> O retorno $G_0$ seria:
>
> $G_0 = 1 + 0.9 \cdot 2 + 0.9^2 \cdot 0.5 + 0.9^3 \cdot 1.5 + \ldots$
> $G_0 = 1 + 1.8 + 0.405 + 1.0935 + \ldots \approx 4.2985 + \ldots$
>
> Quanto maior for o $\gamma$, mais o agente considerarÃ¡ recompensas futuras, e vice-versa.
>
> ```python
> import numpy as np
>
> gamma = 0.9
> rewards = [1, 2, 0.5, 1.5]
>
> # Calcula o retorno G_0 para os primeiros 4 recompensas
> G_0 = sum([gamma**k * rewards[k] for k in range(len(rewards))])
>
> print(f"Retorno G_0: {G_0}")
> ```

Uma formulaÃ§Ã£o alternativa do retorno, que facilita a anÃ¡lise e implementaÃ§Ã£o de algoritmos de RL, Ã© a definiÃ§Ã£o recursiva:

$$G_t = R_{t+1} + \gamma G_{t+1}$$

Esta equaÃ§Ã£o expressa o retorno no tempo $t$ em termos da recompensa imediata $R_{t+1}$ e do retorno no tempo $t+1$, $G_{t+1}$. Esta forma recursiva Ã© fundamental para algoritmos como Q-learning e SARSA.

> ğŸ’¡ **Exemplo NumÃ©rico:** Usando o mesmo exemplo do robÃ´ gerenciando a energia da bateria com $\gamma = 0.9$ e as recompensas $R_1 = 1, R_2 = 2, R_3 = 0.5, R_4 = 1.5$, vamos calcular $G_2$ usando a definiÃ§Ã£o recursiva. Primeiro, precisamos de $G_3$:
>
> $G_3 = R_4 + \gamma G_4$. Assumindo que $G_4$ (o retorno a partir do passo 4) Ã© 0 para simplificar (o robÃ´ para de usar a bateria apÃ³s o passo 4), entÃ£o $G_3 = 1.5 + 0.9 \cdot 0 = 1.5$.
>
> Agora, podemos calcular $G_2$:
>
> $G_2 = R_3 + \gamma G_3 = 0.5 + 0.9 \cdot 1.5 = 0.5 + 1.35 = 1.85$.
>
> Este cÃ¡lculo recursivo demonstra como o retorno no tempo $t$ depende do retorno no tempo $t+1$ e da recompensa imediata.
>
> ```python
> gamma = 0.9
> rewards = [1, 2, 0.5, 1.5]
>
> # Calcula G_3 e G_2 recursivamente
> G_4 = 0  # Assumindo que o retorno apÃ³s o passo 4 Ã© 0
> G_3 = rewards[3] + gamma * G_4
> G_2 = rewards[2] + gamma * G_3
>
> print(f"Retorno G_2: {G_2}")
> ```

**Prova:** Demonstraremos que a definiÃ§Ã£o recursiva do retorno Ã© equivalente Ã  definiÃ§Ã£o de soma descontada infinita.

I. Partindo da definiÃ§Ã£o recursiva:
   $$G_t = R_{t+1} + \gamma G_{t+1}$$

II. Expandindo $G_{t+1}$ usando a mesma definiÃ§Ã£o recursiva:
    $$G_{t+1} = R_{t+2} + \gamma G_{t+2}$$
    Substituindo na equaÃ§Ã£o original:
    $$G_t = R_{t+1} + \gamma (R_{t+2} + \gamma G_{t+2}) = R_{t+1} + \gamma R_{t+2} + \gamma^2 G_{t+2}$$

III. Continuando a expansÃ£o recursivamente por $n$ passos:
     $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots + \gamma^n R_{t+n+1} + \gamma^{n+1} G_{t+n+1}$$

IV. Tomando o limite quando $n$ tende ao infinito, assumindo que $\gamma < 1$ e que o retorno $G_t$ Ã© limitado:
    $$\lim_{n \to \infty} \gamma^{n+1} G_{t+n+1} = 0$$

V. Portanto, no limite, obtemos a soma descontada infinita:
   $$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$ â– 

### ConclusÃ£o

Em resumo, o aprendizado por reforÃ§o oferece um arcabouÃ§o poderoso para agentes aprenderem a se comportar de forma otimizada em ambientes complexos atravÃ©s da interaÃ§Ã£o e do feedback avaliativo. A natureza *online* do RL permite que ele se adapte a ambientes dinÃ¢micos e se concentre em estados relevantes, tornando-o uma abordagem promissora para uma ampla gama de problemas de tomada de decisÃ£o sequencial. A escolha cuidadosa das representaÃ§Ãµes de estado e aÃ§Ã£o, bem como o design adequado de sinais de recompensa, sÃ£o essenciais para o sucesso do RL. Este capÃ­tulo serviu como uma introduÃ§Ã£o aos conceitos fundamentais do aprendizado por reforÃ§o, preparando o terreno para a exploraÃ§Ã£o de algoritmos e tÃ©cnicas mais avanÃ§adas nos capÃ­tulos subsequentes.

### ReferÃªncias
[^1]: Chapter 3: Finite Markov Decision Processes
[^4]: Chapter 3: Finite Markov Decision Processes
[^5]: Chapter 3: Finite Markov Decision Processes
[^48]: Chapter 3: Finite Markov Decision Processes
[^53]: Chapter 3: Finite Markov Decision Processes
[^54]: Chapter 3: Finite Markov Decision Processes
[^55]: Chapter 3: Finite Markov Decision Processes
[^68]: Chapter 3: Finite Markov Decision Processes
<!-- END -->