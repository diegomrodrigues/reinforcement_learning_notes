## O Aprendizado por Refor√ßo e a Intera√ß√£o Adaptativa

### Introdu√ß√£o

Este cap√≠tulo explora o aprendizado por refor√ßo (RL) no contexto de processos de decis√£o de Markov (MDPs) finitos, onde o objetivo √© aprender a se comportar de forma otimizada atrav√©s da intera√ß√£o com um ambiente [^1]. O RL difere de outras abordagens para resolver MDPs devido √† sua natureza *online*, permitindo que o agente foque em estados frequentemente encontrados, tornando-o adapt√°vel e eficiente. A presente se√ß√£o aborda o conceito fundamental do aprendizado por refor√ßo, detalhando sua aplicabilidade, desafios e as considera√ß√µes essenciais para sua implementa√ß√£o efetiva.

### Conceitos Fundamentais
O aprendizado por refor√ßo se distingue por sua capacidade de aprender a partir da intera√ß√£o direta com o ambiente para atingir um objetivo espec√≠fico [^68]. Nesse contexto, a natureza *online* do RL se torna uma vantagem crucial. Diferentemente de m√©todos que exigem um modelo completo do ambiente (como programa√ß√£o din√¢mica), o RL pode se concentrar em aprender boas decis√µes para os estados mais frequentemente encontrados, alocando recursos computacionais de forma adaptativa [^68].

A aplica√ß√£o do RL enfrenta um dilema inerente: a *tens√£o entre a amplitude de aplicabilidade e a tratabilidade matem√°tica* [^1]. Enquanto os MDPs fornecem um arcabou√ßo matem√°tico rigoroso, nem todos os problemas do mundo real se encaixam perfeitamente nesse modelo. Estender o RL para al√©m dos MDPs, como ser√° discutido no Cap√≠tulo 17, introduz desafios adicionais, exigindo compromissos e considera√ß√µes cuidadosas.

O aprendizado por refor√ßo envolve feedback *avaliativo*, onde o agente recebe sinais de recompensa indicando a qualidade de suas a√ß√µes, e um aspecto *associativo*, onde o agente aprende a escolher diferentes a√ß√µes em diferentes situa√ß√µes [^1]. Esta capacidade de associar estados a a√ß√µes apropriadas √© fundamental para o sucesso do RL em ambientes complexos.

A escolha das representa√ß√µes de estado e a√ß√£o tem um impacto profundo no desempenho do aprendizado por refor√ßo [^4]. Uma representa√ß√£o inadequada pode obscurecer as caracter√≠sticas relevantes do ambiente, tornando o aprendizado mais dif√≠cil ou imposs√≠vel. A sele√ß√£o criteriosa de representa√ß√µes √©, portanto, um aspecto crucial do design de sistemas de RL.

> üí° **Exemplo Num√©rico:** Imagine um agente aprendendo a jogar Pac-Man. Se o estado for representado apenas pela posi√ß√£o atual do Pac-Man, sem considerar a posi√ß√£o dos fantasmas ou a disposi√ß√£o dos "dots" (bolinhas), o agente ter√° muita dificuldade em aprender uma pol√≠tica eficaz. Uma representa√ß√£o melhor incluiria a posi√ß√£o do Pac-Man, a posi√ß√£o dos fantasmas (talvez apenas os mais pr√≥ximos), a dire√ß√£o em que os fantasmas est√£o se movendo, e a localiza√ß√£o dos "dots" mais pr√≥ximos. Isso permite que o agente associe diferentes estados (configura√ß√µes do jogo) a a√ß√µes apropriadas (ir para um "dot", evitar um fantasma).
>
> ```python
> import numpy as np
>
> # Representa√ß√£o de estado simplificada (apenas para ilustra√ß√£o)
> class State:
>     def __init__(self, pacman_x, pacman_y, ghost_x, ghost_y):
>         self.pacman_x = pacman_x
>         self.pacman_y = pacman_y
>         self.ghost_x = ghost_x
>         self.ghost_y = ghost_y
>
> # Exemplo de um estado
> state = State(pacman_x=10, pacman_y=5, ghost_x=8, ghost_y=6)
>
> print(f"Estado: Pac-Man=({state.pacman_x}, {state.pacman_y}), Fantasma=({state.ghost_x}, {state.ghost_y})")
> ```

√â fundamental notar que o sinal de recompensa n√£o deve ser usado para transmitir conhecimento pr√©vio sobre como atingir o objetivo [^54]. O agente deve ser recompensado apenas por atingir o objetivo, e n√£o por alcan√ßar sub-objetivos. O uso inadequado de recompensas pode levar a comportamentos indesejados, nos quais o agente explora brechas na estrutura de recompensas sem realmente atingir o objetivo final.

> üí° **Exemplo Num√©rico:** Considere um agente treinado para limpar um quarto. Se o agente for recompensado por pegar cada peda√ßo de lixo individualmente, ele pode se concentrar em pegar o lixo e coloc√°-lo de volta no ch√£o para pegar novamente e receber outra recompensa, em vez de realmente jogar o lixo fora. A recompensa deve ser dada apenas quando o quarto estiver completamente limpo.
>
> ```python
> # Exemplo de fun√ß√£o de recompensa inadequada
> def reward_bad(action):
>     if action == "pegar_lixo":
>         return 1
>     else:
>         return 0
>
> # Exemplo de fun√ß√£o de recompensa adequada
> def reward_good(room_is_clean):
>     if room_is_clean:
>         return 100
>     else:
>         return 0
>
> print("Recompensa inadequada:", reward_bad("pegar_lixo"))
> print("Recompensa adequada:", reward_good(True))
> ```

Para clarificar ainda mais este conceito, podemos introduzir a no√ß√£o de *recompensa intr√≠nseca*.

**Defini√ß√£o:** Recompensa Intr√≠nseca √© um sinal de recompensa gerado internamente pelo agente, baseado em crit√©rios como curiosidade, novidade ou progresso no aprendizado.

Ao contr√°rio das recompensas extr√≠nsecas, que s√£o definidas pelo ambiente, as recompensas intr√≠nsecas podem ser usadas para incentivar a explora√ß√£o e o aprendizado em ambientes com recompensas esparsas ou atrasadas. No entanto, √© crucial equilibrar recompensas intr√≠nsecas e extr√≠nsecas para evitar que o agente se concentre apenas em explorar o ambiente sem atingir o objetivo final.

#### A Intera√ß√£o Agente-Ambiente

No cerne do RL est√° a intera√ß√£o cont√≠nua entre um *agente* e um *ambiente* [^1]. O agente percebe o estado do ambiente ($S_t$), escolhe uma a√ß√£o ($A_t$), e recebe uma recompensa ($R_{t+1}$) [^48]. O ambiente, por sua vez, responde √† a√ß√£o do agente, transitando para um novo estado ($S_{t+1}$) [^48]. Esse ciclo de percep√ß√£o-a√ß√£o-recompensa forma a base do processo de aprendizado por refor√ßo.

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

> üí° **Exemplo Num√©rico:** Considere um rob√¥ aprendendo a navegar em um labirinto.
> 1.  **Estado ($S_t$):** A posi√ß√£o atual do rob√¥ no labirinto (coordenadas x, y). Por exemplo, $S_t = (2, 3)$.
> 2.  **A√ß√£o ($A_t$):** A a√ß√£o que o rob√¥ escolhe realizar (mover para cima, para baixo, para a esquerda ou para a direita). Por exemplo, $A_t = \text{mover para cima}$.
> 3.  **Recompensa ($R_{t+1}$):** A recompensa que o rob√¥ recebe ap√≥s realizar a a√ß√£o. Por exemplo, $R_{t+1} = -1$ se o rob√¥ n√£o atingir o objetivo, e $R_{t+1} = 10$ se o rob√¥ atingir o objetivo (chegar √† sa√≠da do labirinto).
> 4.  **Novo Estado ($S_{t+1}$):** A nova posi√ß√£o do rob√¥ ap√≥s realizar a a√ß√£o. Por exemplo, se o rob√¥ estava em (2, 3) e se moveu para cima, o novo estado pode ser $S_{t+1} = (2, 4)$.
>
> ```python
> # Simula√ß√£o simplificada da intera√ß√£o agente-ambiente
> class Environment:
>     def __init__(self, grid_size=10):
>         self.grid_size = grid_size
>         self.agent_position = (0, 0)  # Posi√ß√£o inicial do agente
>         self.goal_position = (grid_size - 1, grid_size - 1) # Objetivo
>
>     def step(self, action):
>         # Movimento do agente (simplificado)
>         if action == "up":
>             self.agent_position = (self.agent_position[0], min(self.agent_position[1] + 1, self.grid_size - 1))
>         elif action == "down":
>             self.agent_position = (self.agent_position[0], max(self.agent_position[1] - 1, 0))
>         elif action == "left":
>             self.agent_position = (max(self.agent_position[0] - 1, 0), self.agent_position[1])
>         elif action == "right":
>             self.agent_position = (min(self.agent_position[0] + 1, self.grid_size - 1), self.agent_position[1])
>
>         # Recompensa
>         if self.agent_position == self.goal_position:
>             reward = 10
>         else:
>             reward = -1
>
>         return self.agent_position, reward
>
> # Inicializa√ß√£o do ambiente
> env = Environment()
>
> # Agente realiza uma a√ß√£o
> action = "up"
> new_state, reward = env.step(action)
>
> print(f"A√ß√£o: {action}, Novo estado: {new_state}, Recompensa: {reward}")
> ```

Essa intera√ß√£o cont√≠nua gera uma trajet√≥ria ou sequ√™ncia de estados, a√ß√µes e recompensas: $S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots$ [^48]. A partir dessa experi√™ncia, o agente busca aprender uma *pol√≠tica*, que mapeia estados para a√ß√µes, de forma a maximizar a recompensa cumulativa ao longo do tempo [^58].

Para formalizar a no√ß√£o de pol√≠tica, podemos definir:

**Defini√ß√£o:** Uma pol√≠tica $\pi$ √© uma fun√ß√£o que mapeia estados para probabilidades de sele√ß√£o de cada a√ß√£o dispon√≠vel. Formalmente, $\pi(a|s) = P(A_t = a | S_t = s)$, onde $\pi(a|s)$ representa a probabilidade de selecionar a a√ß√£o $a$ no estado $s$ no tempo $t$.

Assim, o objetivo do agente √© encontrar a pol√≠tica √≥tima $\pi^*$ que maximize o retorno esperado.

#### Recompensas e Objetivos

Em RL, o *objetivo* do agente √© formalizado em termos de um sinal especial chamado *recompensa* [^53]. A recompensa √© um n√∫mero simples, $R_t \in \mathbb{R}$, que o agente recebe do ambiente a cada passo de tempo [^53]. O objetivo do agente √© maximizar a quantidade total de recompensa que recebe ao longo do tempo [^53].

O *retorno* ($G_t$) √© a fun√ß√£o das recompensas futuras que o agente busca maximizar. Em sua forma mais simples, o retorno √© a soma das recompensas futuras:

$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \ldots + R_T$$ [^54]

onde $T$ √© o passo de tempo final. Esta formula√ß√£o √© apropriada para *tarefas epis√≥dicas*, nas quais a intera√ß√£o agente-ambiente se divide naturalmente em epis√≥dios [^54].

> üí° **Exemplo Num√©rico:** Considere um jogo como o Tetris. Cada jogo √© um epis√≥dio. Se o agente recebe recompensas de +1 por cada linha que completa e 0 no resto do tempo e o jogo termina ap√≥s 1000 passos, ent√£o:
>
> *   $R_1 = 0$ (nenhuma linha completada no primeiro passo)
> *   $R_2 = 1$ (uma linha completada no segundo passo)
> *   $R_3 = 0$ (nenhuma linha completada no terceiro passo)
> *   ...
> *   $R_{1000} = 2$ (duas linhas completadas no √∫ltimo passo)
>
> O retorno $G_0$ (o retorno no in√≠cio do epis√≥dio) seria a soma de todas as recompensas: $G_0 = 0 + 1 + 0 + \ldots + 2$.  Se o agente completou um total de 50 linhas durante o jogo, ent√£o $G_0 = 50$.

Para *tarefas cont√≠nuas*, nas quais a intera√ß√£o n√£o se divide naturalmente em epis√≥dios, o retorno √© geralmente definido usando um fator de desconto, $\gamma$, onde $0 \le \gamma \le 1$:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$ [^55]

O fator de desconto determina o valor presente de recompensas futuras [^55]. Um $\gamma$ pr√≥ximo de 0 faz com que o agente se concentre apenas em recompensas imediatas, enquanto um $\gamma$ pr√≥ximo de 1 faz com que o agente considere recompensas futuras com maior peso [^55].

> üí° **Exemplo Num√©rico:** Considere um rob√¥ aprendendo a gerenciar a energia de uma bateria. A recompensa pode ser o uso da energia dispon√≠vel ao longo do tempo, e queremos que o rob√¥ equilibre o uso imediato da energia com a necessidade de manter energia para o futuro. Se definirmos $\gamma = 0.9$, isso significa que uma recompensa recebida no pr√≥ximo passo vale 90% de uma recompensa recebida agora. Se o rob√¥ receber as seguintes recompensas:
>
> *   $R_1 = 1$ (uso moderado de energia no primeiro passo)
> *   $R_2 = 2$ (uso um pouco maior de energia no segundo passo)
> *   $R_3 = 0.5$ (uso baixo de energia no terceiro passo)
> *   $R_4 = 1.5$ (uso um pouco maior de energia no quarto passo)
>
> O retorno $G_0$ seria:
>
> $G_0 = 1 + 0.9 \cdot 2 + 0.9^2 \cdot 0.5 + 0.9^3 \cdot 1.5 + \ldots$
> $G_0 = 1 + 1.8 + 0.405 + 1.0935 + \ldots \approx 4.2985 + \ldots$
>
> Quanto maior for o $\gamma$, mais o agente considerar√° recompensas futuras, e vice-versa.
>
> ```python
> import numpy as np
>
> gamma = 0.9
> rewards = [1, 2, 0.5, 1.5]
>
> # Calcula o retorno G_0 para os primeiros 4 recompensas
> G_0 = sum([gamma**k * rewards[k] for k in range(len(rewards))])
>
> print(f"Retorno G_0: {G_0}")
> ```

Uma formula√ß√£o alternativa do retorno, que facilita a an√°lise e implementa√ß√£o de algoritmos de RL, √© a defini√ß√£o recursiva:

$$G_t = R_{t+1} + \gamma G_{t+1}$$

Esta equa√ß√£o expressa o retorno no tempo $t$ em termos da recompensa imediata $R_{t+1}$ e do retorno no tempo $t+1$, $G_{t+1}$. Esta forma recursiva √© fundamental para algoritmos como Q-learning e SARSA.

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo do rob√¥ gerenciando a energia da bateria com $\gamma = 0.9$ e as recompensas $R_1 = 1, R_2 = 2, R_3 = 0.5, R_4 = 1.5$, vamos calcular $G_2$ usando a defini√ß√£o recursiva. Primeiro, precisamos de $G_3$:
>
> $G_3 = R_4 + \gamma G_4$. Assumindo que $G_4$ (o retorno a partir do passo 4) √© 0 para simplificar (o rob√¥ para de usar a bateria ap√≥s o passo 4), ent√£o $G_3 = 1.5 + 0.9 \cdot 0 = 1.5$.
>
> Agora, podemos calcular $G_2$:
>
> $G_2 = R_3 + \gamma G_3 = 0.5 + 0.9 \cdot 1.5 = 0.5 + 1.35 = 1.85$.
>
> Este c√°lculo recursivo demonstra como o retorno no tempo $t$ depende do retorno no tempo $t+1$ e da recompensa imediata.
>
> ```python
> gamma = 0.9
> rewards = [1, 2, 0.5, 1.5]
>
> # Calcula G_3 e G_2 recursivamente
> G_4 = 0  # Assumindo que o retorno ap√≥s o passo 4 √© 0
> G_3 = rewards[3] + gamma * G_4
> G_2 = rewards[2] + gamma * G_3
>
> print(f"Retorno G_2: {G_2}")
> ```

**Prova:** Demonstraremos que a defini√ß√£o recursiva do retorno √© equivalente √† defini√ß√£o de soma descontada infinita.

I. Partindo da defini√ß√£o recursiva:
   $$G_t = R_{t+1} + \gamma G_{t+1}$$

II. Expandindo $G_{t+1}$ usando a mesma defini√ß√£o recursiva:
    $$G_{t+1} = R_{t+2} + \gamma G_{t+2}$$
    Substituindo na equa√ß√£o original:
    $$G_t = R_{t+1} + \gamma (R_{t+2} + \gamma G_{t+2}) = R_{t+1} + \gamma R_{t+2} + \gamma^2 G_{t+2}$$

III. Continuando a expans√£o recursivamente por $n$ passos:
     $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots + \gamma^n R_{t+n+1} + \gamma^{n+1} G_{t+n+1}$$

IV. Tomando o limite quando $n$ tende ao infinito, assumindo que $\gamma < 1$ e que o retorno $G_t$ √© limitado:
    $$\lim_{n \to \infty} \gamma^{n+1} G_{t+n+1} = 0$$

V. Portanto, no limite, obtemos a soma descontada infinita:
   $$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$ ‚ñ†

### Conclus√£o

Em resumo, o aprendizado por refor√ßo oferece um arcabou√ßo poderoso para agentes aprenderem a se comportar de forma otimizada em ambientes complexos atrav√©s da intera√ß√£o e do feedback avaliativo. A natureza *online* do RL permite que ele se adapte a ambientes din√¢micos e se concentre em estados relevantes, tornando-o uma abordagem promissora para uma ampla gama de problemas de tomada de decis√£o sequencial. A escolha cuidadosa das representa√ß√µes de estado e a√ß√£o, bem como o design adequado de sinais de recompensa, s√£o essenciais para o sucesso do RL. Este cap√≠tulo serviu como uma introdu√ß√£o aos conceitos fundamentais do aprendizado por refor√ßo, preparando o terreno para a explora√ß√£o de algoritmos e t√©cnicas mais avan√ßadas nos cap√≠tulos subsequentes.

### Refer√™ncias
[^1]: Chapter 3: Finite Markov Decision Processes
[^4]: Chapter 3: Finite Markov Decision Processes
[^5]: Chapter 3: Finite Markov Decision Processes
[^48]: Chapter 3: Finite Markov Decision Processes
[^53]: Chapter 3: Finite Markov Decision Processes
[^54]: Chapter 3: Finite Markov Decision Processes
[^55]: Chapter 3: Finite Markov Decision Processes
[^68]: Chapter 3: Finite Markov Decision Processes
<!-- END -->