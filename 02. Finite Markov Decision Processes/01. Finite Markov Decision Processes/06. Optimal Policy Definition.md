## Pol√≠ticas √ìtimas e Fun√ß√µes de Valor √ìtimas em MDPs Finitos

### Introdu√ß√£o
Este cap√≠tulo dedica-se ao estudo das pol√≠ticas √≥timas e das fun√ß√µes de valor √≥timas no contexto dos Processos de Decis√£o de Markov (MDPs) finitos. A busca por uma pol√≠tica √≥tima √© o cerne do aprendizado por refor√ßo, e entender as propriedades dessas pol√≠ticas e como elas se relacionam com as fun√ß√µes de valor √© fundamental para o desenvolvimento de algoritmos eficazes. Construiremos sobre os conceitos introduzidos nas se√ß√µes anteriores, como o agente-ambiente interface, recompensas, retornos e, especialmente, a defini√ß√£o formal de pol√≠ticas e fun√ß√µes de valor [^1].

### Conceitos Fundamentais

Em um problema de aprendizado por refor√ßo, o objetivo principal √© encontrar uma pol√≠tica que maximize a recompensa acumulada ao longo do tempo [^7]. No contexto de MDPs finitos, podemos definir precisamente o conceito de uma **pol√≠tica √≥tima**.

*   **Defini√ß√£o de Pol√≠tica √ìtima**: Uma pol√≠tica $\pi$ √© definida como *melhor ou igual* a uma pol√≠tica $\pi'$ se, e somente se, o retorno esperado de $\pi$ √© maior ou igual ao de $\pi'$ para todos os estados $s \in S$ [^16]. Formalmente, $\pi \geq \pi'$ se e somente se $v_{\pi}(s) \geq v_{\pi'}(s)$ para todo $s \in S$. Uma pol√≠tica $\pi^*$ √© considerada **√≥tima** se ela √© melhor ou igual a todas as outras pol√≠ticas poss√≠veis. Matematicamente, $v_{\pi^*}(s) = \max_{\pi} v_{\pi}(s)$ para todo $s \in S$. Note que, embora possa existir mais de uma pol√≠tica √≥tima, todas elas compartilham a mesma **fun√ß√£o de valor de estado √≥tima**, denotada por $v_*(s)$ [^16].

$$
v_*(s) = \max_{\pi} v_{\pi}(s), \forall s \in S \qquad (3.15)
$$

> üí° **Exemplo Num√©rico:** Considere um MDP com dois estados, $S = \{s_1, s_2\}$, e duas pol√≠ticas, $\pi_1$ e $\pi_2$. Suponha que as fun√ß√µes de valor para estas pol√≠ticas sejam: $v_{\pi_1}(s_1) = 10$, $v_{\pi_1}(s_2) = 5$ e $v_{\pi_2}(s_1) = 8$, $v_{\pi_2}(s_2) = 7$.  Neste caso, $\pi_1$ √© melhor que $\pi_2$ no estado $s_1$, mas $\pi_2$ √© melhor que $\pi_1$ no estado $s_2$. N√£o podemos dizer que $\pi_1 \geq \pi_2$ ou $\pi_2 \geq \pi_1$ porque a condi√ß√£o $v_{\pi}(s) \geq v_{\pi'}(s)$ deve valer para todos os estados $s \in S$. Se existisse uma pol√≠tica $\pi^*$ tal que $v_{\pi^*}(s_1) = 12$ e $v_{\pi^*}(s_2) = 7$, ent√£o $\pi^*$ seria uma pol√≠tica √≥tima porque $v_{\pi^*}(s) \geq v_{\pi}(s)$ para toda pol√≠tica $\pi$ e para todo estado $s$.

De forma an√°loga, podemos definir a **fun√ß√£o de valor de a√ß√£o √≥tima** [^16]:

$$
q_*(s, a) = \max_{\pi} q_{\pi}(s, a), \forall s \in S, a \in A(s) \qquad (3.16)
$$

A fun√ß√£o $q_*(s, a)$ representa o retorno esperado ao iniciar no estado $s$, tomar a a√ß√£o $a$, e seguir a pol√≠tica √≥tima a partir de ent√£o. Podemos expressar $q_*(s, a)$ em termos de $v_*(s)$ como [^17]:

$$
q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \qquad (3.17)
$$

> üí° **Exemplo Num√©rico:** Suponha que em um estado $s$, tomar a a√ß√£o $a_1$ leva a um estado $s'$ com $v_*(s') = 20$ e recompensa $r = 5$, enquanto tomar a a√ß√£o $a_2$ leva a um estado $s''$ com $v_*(s'') = 15$ e recompensa $r = 10$. Se $\gamma = 0.9$, ent√£o:
> $q_*(s, a_1) = \mathbb{E}[5 + 0.9 \cdot 20] = 5 + 18 = 23$
> $q_*(s, a_2) = \mathbb{E}[10 + 0.9 \cdot 15] = 10 + 13.5 = 23.5$
> Neste caso, a a√ß√£o √≥tima no estado $s$ seria $a_2$, pois $q_*(s, a_2) > q_*(s, a_1)$.

*   **Equa√ß√£o de otimalidade de Bellman**: A equa√ß√£o de otimalidade de Bellman √© uma rela√ß√£o recursiva que expressa o valor de um estado (ou par estado-a√ß√£o) em termos dos valores de seus sucessores sob uma pol√≠tica √≥tima [^13]. Ela fornece uma maneira de calcular a fun√ß√£o de valor √≥tima $v_*$ e, consequentemente, encontrar uma pol√≠tica √≥tima. Intuitivamente, ela afirma que o valor de um estado sob uma pol√≠tica √≥tima deve ser igual ao retorno esperado para a melhor a√ß√£o naquele estado. A equa√ß√£o de otimalidade de Bellman para $v_*$ √© dada por [^17]:

$$
v_*(s) = \max_{a \in A(s)} \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] = \max_a \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')] \qquad (3.18, 3.19)
$$

onde $p(s', r|s, a)$ √© a probabilidade de transi√ß√£o para o estado $s'$ e receber a recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$ [^2]. A equa√ß√£o de otimalidade de Bellman para $q_*$ √© [^18]:

$$
q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') | S_t = s, A_t = a] = \sum_{s', r} p(s', r|s, a) [r + \gamma \max_{a'} q_*(s', a')] \qquad (3.20)
$$

> üí° **Exemplo Num√©rico:**  Considere um MDP com um estado $s$ e duas a√ß√µes $a_1$ e $a_2$.  Ao tomar a a√ß√£o $a_1$, o agente transita para o estado $s_1$ com recompensa $r_1 = 10$ e probabilidade $p(s_1, r_1|s, a_1) = 0.7$, e para o estado $s_2$ com recompensa $r_2 = 5$ e probabilidade $p(s_2, r_2|s, a_1) = 0.3$.  Ao tomar a a√ß√£o $a_2$, o agente transita para o estado $s_3$ com recompensa $r_3 = 8$ e probabilidade $p(s_3, r_3|s, a_2) = 0.6$, e para o estado $s_4$ com recompensa $r_4 = 12$ e probabilidade $p(s_4, r_4|s, a_2) = 0.4$. Seja $\gamma = 0.9$, $v_*(s_1) = 20$, $v_*(s_2) = 15$, $v_*(s_3) = 18$, e $v_*(s_4) = 22$. Ent√£o, usando a Equa√ß√£o 3.19:
>
> $v_*(s) = \max \begin{cases}
> 0.7 \cdot (10 + 0.9 \cdot 20) + 0.3 \cdot (5 + 0.9 \cdot 15) \\
> 0.6 \cdot (8 + 0.9 \cdot 18) + 0.4 \cdot (12 + 0.9 \cdot 22)
> \end{cases}$
>
> $v_*(s) = \max \begin{cases}
> 0.7 \cdot (10 + 18) + 0.3 \cdot (5 + 13.5) \\
> 0.6 \cdot (8 + 16.2) + 0.4 \cdot (12 + 19.8)
> \end{cases}$
>
> $v_*(s) = \max \begin{cases}
> 0.7 \cdot 28 + 0.3 \cdot 18.5 \\
> 0.6 \cdot 24.2 + 0.4 \cdot 31.8
> \end{cases}$
>
> $v_*(s) = \max \begin{cases}
> 19.6 + 5.55 \\
> 14.52 + 12.72
> \end{cases}$
>
> $v_*(s) = \max \begin{cases}
> 25.15 \\
> 27.24
> \end{cases} = 27.24$
>
> Portanto, $v_*(s) = 27.24$, e a a√ß√£o √≥tima √© $a_2$.

**Teorema 1** [Formas alternativas da Equa√ß√£o de Otimalidade de Bellman]: A Equa√ß√£o de Otimalidade de Bellman para $v_*(s)$ (Equa√ß√£o 3.19) pode ser expressa em termos de $q_*(s,a)$ e vice-versa. Especificamente:

$$
v_*(s) = \max_{a \in A(s)} q_*(s, a) \qquad (3.21)
$$
e
$$
q_*(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')] \qquad (3.22)
$$

*Proof*:
A equa√ß√£o (3.21) segue diretamente da defini√ß√£o de $v_*(s)$ como o m√°ximo valor que pode ser obtido a partir do estado $s$ sob qualquer pol√≠tica, o que implica que esse valor deve ser o m√°ximo valor de $q_*(s, a)$ sobre todas as a√ß√µes $a$ dispon√≠veis em $s$. A equa√ß√£o (3.22) √© simplesmente a reescrita da equa√ß√£o (3.17) usando a nota√ß√£o expl√≠cita da probabilidade de transi√ß√£o.

*   **Unicidade da Solu√ß√£o**: Para MDPs finitos, a equa√ß√£o de otimalidade de Bellman para $v_*$ (Equa√ß√£o 3.19) possui uma solu√ß√£o √∫nica [^18]. Isso significa que existe uma √∫nica fun√ß√£o de valor de estado √≥tima. Como a equa√ß√£o de otimalidade √© um sistema de $n$ equa√ß√µes n√£o lineares com $n$ inc√≥gnitas (onde $n$ √© o n√∫mero de estados), √© poss√≠vel, em princ√≠pio, resolver esse sistema usando m√©todos num√©ricos.

**Lema 1**. A contra√ß√£o da Equa√ß√£o de Bellman. O operador de Bellman √© uma contra√ß√£o sob a norma do supremo.

*Proof*. Seja $B$ o operador de Bellman definido por
$$
(Bv)(s) = \max_{a \in A(s)} \sum_{s', r} p(s', r|s, a) [r + \gamma v(s')]
$$
Para mostrar que $B$ √© uma contra√ß√£o, precisamos mostrar que existe um $\gamma \in [0,1)$ tal que para qualquer fun√ß√£o de valor $v_1$ e $v_2$:
$$
||Bv_1 - Bv_2||_\infty \leq \gamma ||v_1 - v_2||_\infty
$$
onde $||v||_\infty = \max_{s \in S} |v(s)|$.
Seja $s^* = \arg \max_s |(Bv_1)(s) - (Bv_2)(s)|$, ent√£o:
$$
\begin{aligned}
|(Bv_1)(s^*) - (Bv_2)(s^*)| &= |\max_a \sum_{s',r} p(s', r|s^*, a) [r + \gamma v_1(s')] - \max_a \sum_{s',r} p(s', r|s^*, a) [r + \gamma v_2(s')]| \\
&\leq \max_a |\sum_{s',r} p(s', r|s^*, a) [r + \gamma v_1(s')] - \sum_{s',r} p(s', r|s^*, a) [r + \gamma v_2(s')]| \\
&= \max_a |\sum_{s',r} p(s', r|s^*, a) \gamma [v_1(s') - v_2(s')]| \\
&\leq \gamma \max_{s'} |v_1(s') - v_2(s')| \sum_{s',r} p(s', r|s^*, a) \\
&= \gamma ||v_1 - v_2||_\infty
\end{aligned}
$$
Portanto, $||Bv_1 - Bv_2||_\infty \leq \gamma ||v_1 - v_2||_\infty$, o que demonstra que $B$ √© uma contra√ß√£o com fator $\gamma$.

*   **Obten√ß√£o da Pol√≠tica √ìtima**: Uma vez que a fun√ß√£o de valor √≥tima $v_*(s)$ √© conhecida, a determina√ß√£o de uma pol√≠tica √≥tima √© relativamente direta. Para cada estado $s$, podemos escolher uma a√ß√£o $a$ que maximize o lado direito da equa√ß√£o de otimalidade de Bellman [^18]. Qualquer pol√≠tica que atribua probabilidade n√£o nula apenas a essas a√ß√µes √© uma pol√≠tica √≥tima. Em outras palavras, uma pol√≠tica *gulosa* (greedy) em rela√ß√£o √† fun√ß√£o de valor √≥tima $v_*(s)$ √© necessariamente uma pol√≠tica √≥tima [^18].
    Formalmente, uma pol√≠tica $\pi^*$ √© √≥tima se, para todo estado $s \in S$:

    $$
    \pi^*(a|s) =
    \begin{cases}
    1, & \text{se } a = \arg\max_{a' \in A(s)} \sum_{s', r} p(s', r|s, a') [r + \gamma v_*(s')] \\
    0, & \text{caso contr√°rio}
    \end{cases}
    $$



![Backup diagrams illustrating the update rules for state-value \(v_*\) and action-value \(q_*\) functions in MDPs.](./../images/image1.png)

*Exemplo*: No contexto do exemplo do Gridworld [^14], uma vez que a fun√ß√£o de valor √≥tima $v_*(s)$ √© conhecida (como mostrado na Figura 3.5), uma pol√≠tica √≥tima pode ser facilmente constru√≠da escolhendo, para cada estado $s$, a a√ß√£o que leva ao estado sucessor $s'$ com o maior valor $v_*(s')$.

![Optimal solutions to the gridworld example, illustrating the optimal policy and value function.](./../images/image2.png)

> üí° **Exemplo Num√©rico:**  Considere um Gridworld simplificado com 3 estados: `Start`, `A`, e `Goal`.  O agente come√ßa em `Start`, pode ir para `A` ou diretamente para `Goal`. Se vai para `A`, pode depois ir para `Goal`.  Recompensas: `Start` -> `Goal`: +10, `Start` -> `A`: 0, `A` -> `Goal`: +10. Seja $\gamma = 0.9$. Suponha que j√° calculamos $v_*(\text{Goal}) = 0$ (estado terminal), e $v_*(\text{A}) = 9$.
>
> No estado `Start`, temos duas a√ß√µes:
> 1.  Ir direto para `Goal`: $q_*(\text{Start}, \text{Goal}) = 10 + \gamma \cdot v_*(\text{Goal}) = 10 + 0.9 * 0 = 10$
> 2.  Ir para `A`: $q_*(\text{Start}, \text{A}) = 0 + \gamma \cdot v_*(\text{A}) = 0 + 0.9 * 9 = 8.1$
>
> Assim, $v_*(\text{Start}) = \max(10, 8.1) = 10$.  A pol√≠tica √≥tima em `Start` seria ir diretamente para `Goal`.

**Proposi√ß√£o 1** [Multiplicidade de Pol√≠ticas √ìtimas]: Em um MDP, pode existir mais de uma pol√≠tica √≥tima, mas todas essas pol√≠ticas alcan√ßam a mesma fun√ß√£o de valor √≥tima $v_*(s)$.

*Proof:*
I.  Suponha que existam duas pol√≠ticas √≥timas distintas, $\pi_1^*$ e $\pi_2^*$.
II. Pela defini√ß√£o de otimalidade, temos $v_{\pi_1^*}(s) = v_*(s)$ e $v_{\pi_2^*}(s) = v_*(s)$ para todo $s \in S$.
III. Portanto, $v_{\pi_1^*}(s) = v_{\pi_2^*}(s)$ para todo $s \in S$, demonstrando que ambas as pol√≠ticas alcan√ßam a mesma fun√ß√£o de valor √≥tima, embora possam diferir nas a√ß√µes escolhidas em alguns estados. ‚ñ†

### Conclus√£o

A caracteriza√ß√£o das pol√≠ticas √≥timas e das fun√ß√µes de valor √≥timas √© um passo crucial na teoria do aprendizado por refor√ßo. A equa√ß√£o de otimalidade de Bellman fornece uma base te√≥rica para algoritmos que buscam encontrar pol√≠ticas √≥timas, e a unicidade da solu√ß√£o garante que, sob certas condi√ß√µes, podemos convergir para uma solu√ß√£o √≥tima bem definida. Embora a solu√ß√£o direta da equa√ß√£o de otimalidade possa ser computacionalmente invi√°vel para grandes MDPs, os conceitos e resultados apresentados nesta se√ß√£o s√£o fundamentais para o desenvolvimento de m√©todos aproximados que ser√£o explorados em cap√≠tulos posteriores.

### Refer√™ncias

[^2]: p(s',r|s,a) = Pr{St=s',R‚ÇÅ=r | St‚àí1=s, At‚àí1=a},

[^7]: The agent's objective is to maximize the amount of reward it receives over time.

[^13]: A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy recursive relationships similar to that which we have already established for the return (3.9).

[^14]: Example 3.5: Gridworld Figure 3.2 (left) shows a rectangular gridworld representation of a simple finite MDP. The cells of the grid correspond to the states of the environment.

[^16]: Value functions define a partial ordering over policies. A policy œÄis defined to be better than or equal to a policy œÄ' if its expected return is greater than or equal to that of œÄ' for all states.

[^17]:  q*(s, a) = E[Rt+1 + Œ≥œÖ*(St+1) | St=s, At=a]

[^18]: Once one has v‚àó, it is relatively easy to determine an optimal policy. For each state s, there will be one or more actions at which the maximum is obtained in the Bellman optimality equation. Any policy that assigns nonzero probability only to these actions is an optimal policy.
<!-- END -->