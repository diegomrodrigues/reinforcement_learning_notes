## Caracteriza√ß√£o e Propriedades de Processos de Decis√£o de Markov Finitos

### Introdu√ß√£o
Este cap√≠tulo explora em profundidade os Processos de Decis√£o de Markov (MDPs) finitos, um framework fundamental no campo do *reinforcement learning*. Nos deteremos nas caracter√≠sticas que definem um MDP finito, incluindo a finitude dos conjuntos de estados, a√ß√µes e recompensas, bem como na fun√ß√£o de probabilidade que governa a din√¢mica do ambiente. Exploraremos tamb√©m o conceito da propriedade de Markov e sua import√¢ncia na defini√ß√£o do estado.

### Conceitos Fundamentais

Em um **Processo de Decis√£o de Markov Finito (MDP finito)**, os conjuntos de estados ($\mathcal{S}$), a√ß√µes ($\mathcal{A}$) e recompensas ($\mathcal{R}$) s√£o todos finitos [^1]. Isso significa que o n√∫mero de estados poss√≠veis que o agente pode ocupar, o n√∫mero de a√ß√µes que o agente pode executar em cada estado e o n√∫mero de recompensas diferentes que o agente pode receber s√£o todos limitados e discretos. Essa condi√ß√£o de finitude simplifica a an√°lise e permite a aplica√ß√£o de certos algoritmos que n√£o seriam vi√°veis em espa√ßos cont√≠nuos ou infinitos.

> üí° **Exemplo Num√©rico:** Imagine um rob√¥ aspirador em um ambiente com apenas 4 c√¥modos (estados): Sala, Cozinha, Quarto, Banheiro, $\mathcal{S} = \{Sala, Cozinha, Quarto, Banheiro\}$. O rob√¥ pode realizar 2 a√ß√µes: Mover para o pr√≥ximo c√¥modo ou Carregar a bateria, $\mathcal{A} = \{Mover, Carregar\}$. A cada a√ß√£o, ele pode receber recompensas: +1 se aspirar sujeira, 0 se n√£o houver sujeira ou -1 se bater em um obst√°culo, $\mathcal{R} = \{-1, 0, +1\}$. Este √© um MDP finito porque $|\mathcal{S}| = 4$, $|\mathcal{A}| = 2$ e $|\mathcal{R}| = 3$.

A **din√¢mica do ambiente** √© completamente caracterizada pela fun√ß√£o de probabilidade $p(s', r | s, a)$, definida como [^1]:
$$
p(s', r | s, a) = \Pr\{S_t = s', R_{t+1} = r | S_{t-1} = s, A_{t-1} = a\}
$$
Essa fun√ß√£o representa a probabilidade de transi√ß√£o para o estado $s'$ e receber a recompensa $r$ no tempo $t$, dado que o agente estava no estado $s$ e executou a a√ß√£o $a$ no tempo $t-1$. Em outras palavras, $p(s', r | s, a)$ especifica a probabilidade condicional de observar um pr√≥ximo estado $s'$ e recompensa $r$, dado o estado atual $s$ e a a√ß√£o $a$.

> üí° **Exemplo Num√©rico:** No exemplo do rob√¥ aspirador, suponha que ele esteja na Sala e execute a a√ß√£o Mover. A fun√ß√£o $p(s', r | s, a)$ poderia ser definida como:
>
> *   $p(Cozinha, 0 | Sala, Mover) = 0.7$: 70% de chance de ir para a Cozinha e n√£o encontrar sujeira.
> *   $p(Cozinha, 1 | Sala, Mover) = 0.2$: 20% de chance de ir para a Cozinha e encontrar sujeira.
> *   $p(Sala, -1 | Sala, Mover) = 0.1$: 10% de chance de permanecer na Sala e bater em algo (recebendo recompensa -1).
>
> Isso ilustra como $p(s', r | s, a)$ quantifica a probabilidade de diferentes resultados ap√≥s uma a√ß√£o.

A fun√ß√£o de probabilidade $p(s', r | s, a)$ permite a deriva√ß√£o de outras fun√ß√µes √∫teis, como a **probabilidade de transi√ß√£o de estado** $p(s' | s, a)$, que indica a probabilidade de ir para o estado $s'$ ap√≥s executar a a√ß√£o $a$ no estado $s$ [^1]:
$$
p(s' | s, a) = \sum_{r \in \mathcal{R}} p(s', r | s, a)
$$

**Proposi√ß√£o 2** A probabilidade de transi√ß√£o de estado $p(s' | s, a)$ √© a marginaliza√ß√£o da fun√ß√£o de probabilidade $p(s', r | s, a)$ sobre todas as recompensas poss√≠veis.

*Proof:*
I.  Come√ßamos com a defini√ß√£o da probabilidade conjunta de transi√ß√£o para o estado $s'$ e receber a recompensa $r$ dado o estado $s$ e a√ß√£o $a$: $p(s', r | s, a)$.

II. A probabilidade de transi√ß√£o para o estado $s'$ dado o estado $s$ e a√ß√£o $a$, $p(s' | s, a)$, pode ser obtida somando (marginalizando) a probabilidade conjunta sobre todos os valores poss√≠veis de $r \in \mathcal{R}$.

III. Portanto, $p(s' | s, a) = \sum_{r \in \mathcal{R}} p(s', r | s, a)$. ‚ñ†

> üí° **Exemplo Num√©rico:** Usando os dados do exemplo anterior, para calcular $p(Cozinha | Sala, Mover)$, somamos as probabilidades de ir para a Cozinha com cada recompensa poss√≠vel:
>
> $p(Cozinha | Sala, Mover) = p(Cozinha, 0 | Sala, Mover) + p(Cozinha, 1 | Sala, Mover) = 0.7 + 0.2 = 0.9$
>
> Isso significa que h√° 90% de chance do rob√¥ ir para a Cozinha se ele estiver na Sala e executar a a√ß√£o Mover.

Al√©m disso, podemos definir a **recompensa esperada** para um par estado-a√ß√£o $r(s, a)$ [^1]:
$$
r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \cdot p(s', r | s, a)
$$

**Proposi√ß√£o 3** A recompensa esperada $r(s, a)$ √© a m√©dia ponderada das recompensas poss√≠veis, ponderada pela probabilidade de cada recompensa ocorrer ap√≥s tomar a a√ß√£o $a$ no estado $s$.

*Proof:*
I.  Come√ßamos com a defini√ß√£o formal da recompensa esperada dado um estado $s$ e a√ß√£o $a$: $r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a]$.

II. Pela defini√ß√£o de esperan√ßa, podemos expressar isso como a soma sobre todos os valores poss√≠veis de recompensa, multiplicados por suas respectivas probabilidades de ocorr√™ncia dado $s$ e $a$.

III.  A probabilidade de obter a recompensa $r$ dado $s$ e $a$ pode ser expressa como a soma sobre todos os poss√≠veis estados seguintes $s'$ da probabilidade conjunta de obter $r$ e transitar para $s'$.

IV. Portanto, $r(s, a) = \sum_{r \in \mathcal{R}} r \cdot Pr(R_t = r | S_{t-1} = s, A_{t-1} = a) =  \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \cdot p(s', r | s, a)$. ‚ñ†

> üí° **Exemplo Num√©rico:**  Continuando com o exemplo do rob√¥, vamos calcular $r(Sala, Mover)$:
>
> $r(Sala, Mover) = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \cdot p(s', r | Sala, Mover) $
>
> $= (-1) \cdot p(Sala, -1 | Sala, Mover) + (0) \cdot p(Cozinha, 0 | Sala, Mover) + (1) \cdot p(Cozinha, 1 | Sala, Mover)$
>
> $= (-1)(0.1) + (0)(0.7) + (1)(0.2) = -0.1 + 0 + 0.2 = 0.1$
>
> A recompensa esperada de mover da Sala √© 0.1.

E a **recompensa esperada para uma tripla estado-a√ß√£o-pr√≥ximo estado** $r(s, a, s')$ [^1]:
$$
r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \cdot \frac{p(s', r | s, a)}{p(s' | s, a)}
$$
Note que, para a defini√ß√£o de $r(s, a, s')$ ser v√°lida, devemos ter $p(s' | s, a) > 0$.

**Proposi√ß√£o 1** Se $p(s' | s, a) = 0$, ent√£o $r(s, a, s')$ pode ser definida arbitrariamente sem afetar o comportamento do MDP, pois essa transi√ß√£o nunca ocorrer√°.

*Proof:*
Se $p(s' | s, a) = 0$, a probabilidade de transitar para o estado $s'$ ao tomar a a√ß√£o $a$ no estado $s$ √© nula. Portanto, a recompensa esperada $r(s, a, s')$ associada a essa transi√ß√£o n√£o contribui para o valor esperado de nenhuma pol√≠tica, e podemos atribuir qualquer valor a ela sem alterar o comportamento do MDP.

> üí° **Exemplo Num√©rico:** Vamos calcular $r(Sala, Mover, Cozinha)$:
>
> $r(Sala, Mover, Cozinha) = \sum_{r \in \mathcal{R}} r \cdot \frac{p(Cozinha, r | Sala, Mover)}{p(Cozinha | Sala, Mover)}$
>
> $ = (-1) \cdot \frac{p(Cozinha, -1 | Sala, Mover)}{0.9} + (0) \cdot \frac{p(Cozinha, 0 | Sala, Mover)}{0.9} + (1) \cdot \frac{p(Cozinha, 1 | Sala, Mover)}{0.9}$
>
>  Dado que $p(Cozinha, -1 | Sala, Mover)$ √© 0 (n√£o h√° probabilidade de ir para a cozinha e bater em algo),
>
> $r(Sala, Mover, Cozinha) = (0) \cdot \frac{0.7}{0.9} + (1) \cdot \frac{0.2}{0.9} = \frac{0.2}{0.9} \approx 0.22$
>
> A recompensa esperada ao mover da Sala para a Cozinha √© aproximadamente 0.22.

A **propriedade de Markov** √© uma caracter√≠stica essencial dos MDPs [^1]. Ela estabelece que o pr√≥ximo estado e recompensa dependem apenas do estado e da a√ß√£o imediatamente anteriores, e n√£o do hist√≥rico de estados e a√ß√µes passadas. Formalmente:
$$
\Pr\{S_t = s', R_{t+1} = r | S_{t-1} = s, A_{t-1} = a, H_{t-2}\} = \Pr\{S_t = s', R_{t+1} = r | S_{t-1} = s, A_{t-1} = a\}
$$
onde $H_{t-2}$ representa o hist√≥rico de estados e a√ß√µes at√© o tempo $t-2$. Isso implica que o estado deve incluir todas as informa√ß√µes relevantes sobre o passado que podem influenciar o futuro. Essencialmente, o estado serve como um sum√°rio suficiente do hist√≥rico.

> üí° **Exemplo Num√©rico:** Suponha que, no nosso exemplo do rob√¥ aspirador, a probabilidade de encontrar sujeira em um c√¥modo dependa apenas do c√¥modo atual e da a√ß√£o de aspirar (ou n√£o) o c√¥modo.  Se a probabilidade de encontrar sujeira dependesse tamb√©m de quando o c√¥modo foi aspirado pela √∫ltima vez (hist√≥rico), a representa√ß√£o do estado (apenas o c√¥modo atual) n√£o seria Markoviana. Para tornar o problema Markoviano, o estado precisaria incluir informa√ß√µes como "dias desde a √∫ltima limpeza deste c√¥modo".

> **Destaque:** A propriedade de Markov n√£o √© uma restri√ß√£o sobre o ambiente em si, mas sim uma restri√ß√£o sobre a *representa√ß√£o* do ambiente atrav√©s do estado. Se o ambiente real n√£o √© Markoviano, podemos muitas vezes construir um estado que *seja* Markoviano, incorporando informa√ß√µes relevantes do hist√≥rico no estado. [^1]



![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

Al√©m da fun√ß√£o de probabilidade de transi√ß√£o e da recompensa esperada, outra fun√ß√£o importante que podemos derivar √© o **fator de desconto** $\gamma$. O fator de desconto $\gamma \in [0, 1]$ representa a import√¢ncia relativa das recompensas futuras em rela√ß√£o √†s recompensas imediatas. Um valor de $\gamma$ pr√≥ximo de 0 faz com que o agente se concentre em obter recompensas imediatas, enquanto um valor de $\gamma$ pr√≥ximo de 1 faz com que o agente valorize as recompensas futuras tanto quanto as recompensas imediatas.

> üí° **Exemplo Num√©rico:** Se $\gamma = 0.9$, uma recompensa de +1 recebida 2 passos no futuro vale $0.9^2 * 1 = 0.81$ hoje.  Se $\gamma = 0.1$, a mesma recompensa valeria apenas $0.1^2 * 1 = 0.01$ hoje. Isso demonstra como $\gamma$ controla o horizonte de planejamento do agente.

Com o fator de desconto, podemos definir o **retorno** $G_t$ como a soma descontada das recompensas futuras:
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

**Teorema 1** (Equa√ß√£o de Bellman para o Retorno) O retorno $G_t$ pode ser decomposto recursivamente como:

$$ G_t = R_{t+1} + \gamma G_{t+1} $$

*Proof:*
Come√ßando com a defini√ß√£o de retorno:

$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots $

Podemos fatorar $\gamma$ dos termos a partir de $R_{t+2}$:

$G_t = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots)$

O termo entre par√™nteses √© precisamente $G_{t+1}$:

$G_t = R_{t+1} + \gamma G_{t+1}$

Essa decomposi√ß√£o recursiva √© fundamental para muitos algoritmos de *reinforcement learning*, pois permite calcular o retorno de forma eficiente, reutilizando os resultados de c√°lculos anteriores.

> üí° **Exemplo Num√©rico:** Suponha a seguinte sequ√™ncia de recompensas: $R_{t+1} = 1, R_{t+2} = 0, R_{t+3} = -1, R_{t+4} = 1$.  Com $\gamma = 0.5$:
>
> $G_t = 1 + 0.5 * 0 + 0.5^2 * (-1) + 0.5^3 * 1 = 1 + 0 - 0.25 + 0.125 = 0.875$
>
> Usando a equa√ß√£o de Bellman:
>
> $G_{t+1} = 0 + 0.5 * (-1) + 0.5^2 * 1 = 0 - 0.5 + 0.25 = -0.25$
>
> $G_t = R_{t+1} + \gamma G_{t+1} = 1 + 0.5 * (-0.25) = 1 - 0.125 = 0.875$
>
> Os dois m√©todos resultam no mesmo valor para $G_t$, demonstrando a validade da equa√ß√£o de Bellman.

### Conclus√£o

A defini√ß√£o formal de um MDP finito, com seus conjuntos finitos de estados, a√ß√µes e recompensas, e a fun√ß√£o de probabilidade que descreve a din√¢mica do ambiente, fornece um arcabou√ßo matem√°tico robusto para modelar problemas de tomada de decis√£o sequencial. A propriedade de Markov simplifica a an√°lise e permite o desenvolvimento de algoritmos eficientes. Compreender esses conceitos fundamentais √© crucial para o estudo e a aplica√ß√£o do *reinforcement learning*.

### Refer√™ncias
[^1]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. Cambridge, MA: MIT press, 2018.
<!-- END -->