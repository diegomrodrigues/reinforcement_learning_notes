## CaracterizaÃ§Ã£o e Propriedades de Processos de DecisÃ£o de Markov Finitos

### IntroduÃ§Ã£o
Este capÃ­tulo explora em profundidade os Processos de DecisÃ£o de Markov (MDPs) finitos, um framework fundamental no campo do *reinforcement learning*. Nos deteremos nas caracterÃ­sticas que definem um MDP finito, incluindo a finitude dos conjuntos de estados, aÃ§Ãµes e recompensas, bem como na funÃ§Ã£o de probabilidade que governa a dinÃ¢mica do ambiente. Exploraremos tambÃ©m o conceito da propriedade de Markov e sua importÃ¢ncia na definiÃ§Ã£o do estado.

### Conceitos Fundamentais

Em um **Processo de DecisÃ£o de Markov Finito (MDP finito)**, os conjuntos de estados ($\mathcal{S}$), aÃ§Ãµes ($\mathcal{A}$) e recompensas ($\mathcal{R}$) sÃ£o todos finitos [^1]. Isso significa que o nÃºmero de estados possÃ­veis que o agente pode ocupar, o nÃºmero de aÃ§Ãµes que o agente pode executar em cada estado e o nÃºmero de recompensas diferentes que o agente pode receber sÃ£o todos limitados e discretos. Essa condiÃ§Ã£o de finitude simplifica a anÃ¡lise e permite a aplicaÃ§Ã£o de certos algoritmos que nÃ£o seriam viÃ¡veis em espaÃ§os contÃ­nuos ou infinitos.

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine um robÃ´ aspirador em um ambiente com apenas 4 cÃ´modos (estados): Sala, Cozinha, Quarto, Banheiro, $\mathcal{S} = \{Sala, Cozinha, Quarto, Banheiro\}$. O robÃ´ pode realizar 2 aÃ§Ãµes: Mover para o prÃ³ximo cÃ´modo ou Carregar a bateria, $\mathcal{A} = \{Mover, Carregar\}$. A cada aÃ§Ã£o, ele pode receber recompensas: +1 se aspirar sujeira, 0 se nÃ£o houver sujeira ou -1 se bater em um obstÃ¡culo, $\mathcal{R} = \{-1, 0, +1\}$. Este Ã© um MDP finito porque $|\mathcal{S}| = 4$, $|\mathcal{A}| = 2$ e $|\mathcal{R}| = 3$.

A **dinÃ¢mica do ambiente** Ã© completamente caracterizada pela funÃ§Ã£o de probabilidade $p(s', r | s, a)$, definida como [^1]:
$$
p(s', r | s, a) = \Pr\{S_t = s', R_{t+1} = r | S_{t-1} = s, A_{t-1} = a\}
$$
Essa funÃ§Ã£o representa a probabilidade de transiÃ§Ã£o para o estado $s'$ e receber a recompensa $r$ no tempo $t$, dado que o agente estava no estado $s$ e executou a aÃ§Ã£o $a$ no tempo $t-1$. Em outras palavras, $p(s', r | s, a)$ especifica a probabilidade condicional de observar um prÃ³ximo estado $s'$ e recompensa $r$, dado o estado atual $s$ e a aÃ§Ã£o $a$.

> ðŸ’¡ **Exemplo NumÃ©rico:** No exemplo do robÃ´ aspirador, suponha que ele esteja na Sala e execute a aÃ§Ã£o Mover. A funÃ§Ã£o $p(s', r | s, a)$ poderia ser definida como:
>
> *   $p(Cozinha, 0 | Sala, Mover) = 0.7$: 70% de chance de ir para a Cozinha e nÃ£o encontrar sujeira.
> *   $p(Cozinha, 1 | Sala, Mover) = 0.2$: 20% de chance de ir para a Cozinha e encontrar sujeira.
> *   $p(Sala, -1 | Sala, Mover) = 0.1$: 10% de chance de permanecer na Sala e bater em algo (recebendo recompensa -1).
>
> Isso ilustra como $p(s', r | s, a)$ quantifica a probabilidade de diferentes resultados apÃ³s uma aÃ§Ã£o.

A funÃ§Ã£o de probabilidade $p(s', r | s, a)$ permite a derivaÃ§Ã£o de outras funÃ§Ãµes Ãºteis, como a **probabilidade de transiÃ§Ã£o de estado** $p(s' | s, a)$, que indica a probabilidade de ir para o estado $s'$ apÃ³s executar a aÃ§Ã£o $a$ no estado $s$ [^1]:
$$
p(s' | s, a) = \sum_{r \in \mathcal{R}} p(s', r | s, a)
$$

**ProposiÃ§Ã£o 2** A probabilidade de transiÃ§Ã£o de estado $p(s' | s, a)$ Ã© a marginalizaÃ§Ã£o da funÃ§Ã£o de probabilidade $p(s', r | s, a)$ sobre todas as recompensas possÃ­veis.

*Proof:*
I.  ComeÃ§amos com a definiÃ§Ã£o da probabilidade conjunta de transiÃ§Ã£o para o estado $s'$ e receber a recompensa $r$ dado o estado $s$ e aÃ§Ã£o $a$: $p(s', r | s, a)$.

II. A probabilidade de transiÃ§Ã£o para o estado $s'$ dado o estado $s$ e aÃ§Ã£o $a$, $p(s' | s, a)$, pode ser obtida somando (marginalizando) a probabilidade conjunta sobre todos os valores possÃ­veis de $r \in \mathcal{R}$.

III. Portanto, $p(s' | s, a) = \sum_{r \in \mathcal{R}} p(s', r | s, a)$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Usando os dados do exemplo anterior, para calcular $p(Cozinha | Sala, Mover)$, somamos as probabilidades de ir para a Cozinha com cada recompensa possÃ­vel:
>
> $p(Cozinha | Sala, Mover) = p(Cozinha, 0 | Sala, Mover) + p(Cozinha, 1 | Sala, Mover) = 0.7 + 0.2 = 0.9$
>
> Isso significa que hÃ¡ 90% de chance do robÃ´ ir para a Cozinha se ele estiver na Sala e executar a aÃ§Ã£o Mover.

AlÃ©m disso, podemos definir a **recompensa esperada** para um par estado-aÃ§Ã£o $r(s, a)$ [^1]:
$$
r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \cdot p(s', r | s, a)
$$

**ProposiÃ§Ã£o 3** A recompensa esperada $r(s, a)$ Ã© a mÃ©dia ponderada das recompensas possÃ­veis, ponderada pela probabilidade de cada recompensa ocorrer apÃ³s tomar a aÃ§Ã£o $a$ no estado $s$.

*Proof:*
I.  ComeÃ§amos com a definiÃ§Ã£o formal da recompensa esperada dado um estado $s$ e aÃ§Ã£o $a$: $r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a]$.

II. Pela definiÃ§Ã£o de esperanÃ§a, podemos expressar isso como a soma sobre todos os valores possÃ­veis de recompensa, multiplicados por suas respectivas probabilidades de ocorrÃªncia dado $s$ e $a$.

III.  A probabilidade de obter a recompensa $r$ dado $s$ e $a$ pode ser expressa como a soma sobre todos os possÃ­veis estados seguintes $s'$ da probabilidade conjunta de obter $r$ e transitar para $s'$.

IV. Portanto, $r(s, a) = \sum_{r \in \mathcal{R}} r \cdot Pr(R_t = r | S_{t-1} = s, A_{t-1} = a) =  \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \cdot p(s', r | s, a)$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**  Continuando com o exemplo do robÃ´, vamos calcular $r(Sala, Mover)$:
>
> $r(Sala, Mover) = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \cdot p(s', r | Sala, Mover) $
>
> $= (-1) \cdot p(Sala, -1 | Sala, Mover) + (0) \cdot p(Cozinha, 0 | Sala, Mover) + (1) \cdot p(Cozinha, 1 | Sala, Mover)$
>
> $= (-1)(0.1) + (0)(0.7) + (1)(0.2) = -0.1 + 0 + 0.2 = 0.1$
>
> A recompensa esperada de mover da Sala Ã© 0.1.

E a **recompensa esperada para uma tripla estado-aÃ§Ã£o-prÃ³ximo estado** $r(s, a, s')$ [^1]:
$$
r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \cdot \frac{p(s', r | s, a)}{p(s' | s, a)}
$$
Note que, para a definiÃ§Ã£o de $r(s, a, s')$ ser vÃ¡lida, devemos ter $p(s' | s, a) > 0$.

**ProposiÃ§Ã£o 1** Se $p(s' | s, a) = 0$, entÃ£o $r(s, a, s')$ pode ser definida arbitrariamente sem afetar o comportamento do MDP, pois essa transiÃ§Ã£o nunca ocorrerÃ¡.

*Proof:*
Se $p(s' | s, a) = 0$, a probabilidade de transitar para o estado $s'$ ao tomar a aÃ§Ã£o $a$ no estado $s$ Ã© nula. Portanto, a recompensa esperada $r(s, a, s')$ associada a essa transiÃ§Ã£o nÃ£o contribui para o valor esperado de nenhuma polÃ­tica, e podemos atribuir qualquer valor a ela sem alterar o comportamento do MDP.

> ðŸ’¡ **Exemplo NumÃ©rico:** Vamos calcular $r(Sala, Mover, Cozinha)$:
>
> $r(Sala, Mover, Cozinha) = \sum_{r \in \mathcal{R}} r \cdot \frac{p(Cozinha, r | Sala, Mover)}{p(Cozinha | Sala, Mover)}$
>
> $ = (-1) \cdot \frac{p(Cozinha, -1 | Sala, Mover)}{0.9} + (0) \cdot \frac{p(Cozinha, 0 | Sala, Mover)}{0.9} + (1) \cdot \frac{p(Cozinha, 1 | Sala, Mover)}{0.9}$
>
>  Dado que $p(Cozinha, -1 | Sala, Mover)$ Ã© 0 (nÃ£o hÃ¡ probabilidade de ir para a cozinha e bater em algo),
>
> $r(Sala, Mover, Cozinha) = (0) \cdot \frac{0.7}{0.9} + (1) \cdot \frac{0.2}{0.9} = \frac{0.2}{0.9} \approx 0.22$
>
> A recompensa esperada ao mover da Sala para a Cozinha Ã© aproximadamente 0.22.

A **propriedade de Markov** Ã© uma caracterÃ­stica essencial dos MDPs [^1]. Ela estabelece que o prÃ³ximo estado e recompensa dependem apenas do estado e da aÃ§Ã£o imediatamente anteriores, e nÃ£o do histÃ³rico de estados e aÃ§Ãµes passadas. Formalmente:
$$
\Pr\{S_t = s', R_{t+1} = r | S_{t-1} = s, A_{t-1} = a, H_{t-2}\} = \Pr\{S_t = s', R_{t+1} = r | S_{t-1} = s, A_{t-1} = a\}
$$
onde $H_{t-2}$ representa o histÃ³rico de estados e aÃ§Ãµes atÃ© o tempo $t-2$. Isso implica que o estado deve incluir todas as informaÃ§Ãµes relevantes sobre o passado que podem influenciar o futuro. Essencialmente, o estado serve como um sumÃ¡rio suficiente do histÃ³rico.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que, no nosso exemplo do robÃ´ aspirador, a probabilidade de encontrar sujeira em um cÃ´modo dependa apenas do cÃ´modo atual e da aÃ§Ã£o de aspirar (ou nÃ£o) o cÃ´modo.  Se a probabilidade de encontrar sujeira dependesse tambÃ©m de quando o cÃ´modo foi aspirado pela Ãºltima vez (histÃ³rico), a representaÃ§Ã£o do estado (apenas o cÃ´modo atual) nÃ£o seria Markoviana. Para tornar o problema Markoviano, o estado precisaria incluir informaÃ§Ãµes como "dias desde a Ãºltima limpeza deste cÃ´modo".

> **Destaque:** A propriedade de Markov nÃ£o Ã© uma restriÃ§Ã£o sobre o ambiente em si, mas sim uma restriÃ§Ã£o sobre a *representaÃ§Ã£o* do ambiente atravÃ©s do estado. Se o ambiente real nÃ£o Ã© Markoviano, podemos muitas vezes construir um estado que *seja* Markoviano, incorporando informaÃ§Ãµes relevantes do histÃ³rico no estado. [^1]



![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

AlÃ©m da funÃ§Ã£o de probabilidade de transiÃ§Ã£o e da recompensa esperada, outra funÃ§Ã£o importante que podemos derivar Ã© o **fator de desconto** $\gamma$. O fator de desconto $\gamma \in [0, 1]$ representa a importÃ¢ncia relativa das recompensas futuras em relaÃ§Ã£o Ã s recompensas imediatas. Um valor de $\gamma$ prÃ³ximo de 0 faz com que o agente se concentre em obter recompensas imediatas, enquanto um valor de $\gamma$ prÃ³ximo de 1 faz com que o agente valorize as recompensas futuras tanto quanto as recompensas imediatas.

> ðŸ’¡ **Exemplo NumÃ©rico:** Se $\gamma = 0.9$, uma recompensa de +1 recebida 2 passos no futuro vale $0.9^2 * 1 = 0.81$ hoje.  Se $\gamma = 0.1$, a mesma recompensa valeria apenas $0.1^2 * 1 = 0.01$ hoje. Isso demonstra como $\gamma$ controla o horizonte de planejamento do agente.

Com o fator de desconto, podemos definir o **retorno** $G_t$ como a soma descontada das recompensas futuras:
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

**Teorema 1** (EquaÃ§Ã£o de Bellman para o Retorno) O retorno $G_t$ pode ser decomposto recursivamente como:

$$ G_t = R_{t+1} + \gamma G_{t+1} $$

*Proof:*
ComeÃ§ando com a definiÃ§Ã£o de retorno:

$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots $

Podemos fatorar $\gamma$ dos termos a partir de $R_{t+2}$:

$G_t = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots)$

O termo entre parÃªnteses Ã© precisamente $G_{t+1}$:

$G_t = R_{t+1} + \gamma G_{t+1}$

Essa decomposiÃ§Ã£o recursiva Ã© fundamental para muitos algoritmos de *reinforcement learning*, pois permite calcular o retorno de forma eficiente, reutilizando os resultados de cÃ¡lculos anteriores.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha a seguinte sequÃªncia de recompensas: $R_{t+1} = 1, R_{t+2} = 0, R_{t+3} = -1, R_{t+4} = 1$.  Com $\gamma = 0.5$:
>
> $G_t = 1 + 0.5 * 0 + 0.5^2 * (-1) + 0.5^3 * 1 = 1 + 0 - 0.25 + 0.125 = 0.875$
>
> Usando a equaÃ§Ã£o de Bellman:
>
> $G_{t+1} = 0 + 0.5 * (-1) + 0.5^2 * 1 = 0 - 0.5 + 0.25 = -0.25$
>
> $G_t = R_{t+1} + \gamma G_{t+1} = 1 + 0.5 * (-0.25) = 1 - 0.125 = 0.875$
>
> Os dois mÃ©todos resultam no mesmo valor para $G_t$, demonstrando a validade da equaÃ§Ã£o de Bellman.

### ConclusÃ£o

A definiÃ§Ã£o formal de um MDP finito, com seus conjuntos finitos de estados, aÃ§Ãµes e recompensas, e a funÃ§Ã£o de probabilidade que descreve a dinÃ¢mica do ambiente, fornece um arcabouÃ§o matemÃ¡tico robusto para modelar problemas de tomada de decisÃ£o sequencial. A propriedade de Markov simplifica a anÃ¡lise e permite o desenvolvimento de algoritmos eficientes. Compreender esses conceitos fundamentais Ã© crucial para o estudo e a aplicaÃ§Ã£o do *reinforcement learning*.

### ReferÃªncias
[^1]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. Cambridge, MA: MIT press, 2018.
<!-- END -->