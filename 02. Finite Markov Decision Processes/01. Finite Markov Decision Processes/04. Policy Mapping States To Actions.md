## Pol√≠ticas e Fun√ß√µes de Valor em Processos de Decis√£o de Markov Finitos

### Introdu√ß√£o
Este cap√≠tulo aprofunda o conceito de **Processos de Decis√£o de Markov Finitos (MDPs)**, introduzidos anteriormente, focando especificamente nas **pol√≠ticas** e **fun√ß√µes de valor**. Em continuidade ao conceito de intera√ß√£o agente-ambiente [^1], onde o agente seleciona a√ß√µes e o ambiente responde com novos estados e recompensas, exploraremos como as pol√≠ticas guiam o agente na sele√ß√£o de a√ß√µes e como as fun√ß√µes de valor quantificam a "qualidade" de estar em um determinado estado ou executar uma determinada a√ß√£o sob uma pol√≠tica espec√≠fica. Este cap√≠tulo √© crucial para a compreens√£o dos algoritmos de *reinforcement learning* (RL), que visam otimizar essas pol√≠ticas e fun√ß√µes de valor para alcan√ßar o m√°ximo retorno acumulado [^53].

### Pol√≠ticas: O Guia do Agente
Em *reinforcement learning*, o **agente** age de acordo com uma **pol√≠tica**, que define como ele seleciona a√ß√µes em cada estado. Formalmente, uma pol√≠tica √© uma *mapping* de estados para probabilidades de sele√ß√£o de cada poss√≠vel a√ß√£o [^58].

**Defini√ß√£o:** Uma **pol√≠tica** $\pi$ √© uma fun√ß√£o $\pi(a|s)$ que especifica a probabilidade de o agente selecionar a a√ß√£o $a$ no estado $s$ no tempo $t$, ou seja, $P(A_t = a | S_t = s)$. Formalmente,
$$
\pi(a|s) = Pr\{A_t = a | S_t = s\}
$$
para $a \in A(s)$ e $s \in S$. [^58]

Assim como a fun√ß√£o de din√¢mica $p$ [^48], $\pi$ √© uma fun√ß√£o ordin√°ria. O s√≠mbolo ‚Äò|‚Äô em $\pi(a|s)$ serve para lembrar que essa fun√ß√£o define uma distribui√ß√£o de probabilidade sobre $a \in A(s)$ para cada $s \in S$ [^58].

As pol√≠ticas podem ser **determin√≠sticas** ou **estoc√°sticas**.
*   Uma **pol√≠tica determin√≠stica** atribui uma √∫nica a√ß√£o a cada estado.
*   Uma **pol√≠tica estoc√°stica** atribui uma distribui√ß√£o de probabilidade sobre as a√ß√µes para cada estado.

O *reinforcement learning* especifica como a pol√≠tica do agente √© alterada como resultado de sua experi√™ncia [^58].

**Proposi√ß√£o 1:** Uma pol√≠tica determin√≠stica pode ser vista como um caso especial de uma pol√≠tica estoc√°stica onde a probabilidade da a√ß√£o escolhida √© 1 e as demais s√£o 0.

*Prova:* Seja $\pi(s)$ uma pol√≠tica determin√≠stica que mapeia o estado $s$ para a a√ß√£o $a$. Provaremos que $\pi(s)$ pode ser representada como um caso especial de uma pol√≠tica estoc√°stica.

I.  Definimos uma pol√≠tica estoc√°stica $\pi'(a'|s)$ como:
$$
\pi'(a'|s) =
\begin{cases}
1, & \text{se } a' = \pi(s) \\
0, & \text{se } a' \neq \pi(s)
\end{cases}
$$

II. Verificamos que $\pi'(a'|s)$ √© uma distribui√ß√£o de probabilidade v√°lida:
$$\sum_{a' \in A(s)} \pi'(a'|s) = \pi'(\pi(s)|s) + \sum_{a' \neq \pi(s)} \pi'(a'|s) = 1 + 0 = 1$$

III. Portanto, $\pi'(a'|s)$ define uma distribui√ß√£o de probabilidade sobre as a√ß√µes para cada estado $s$, onde a a√ß√£o $\pi(s)$ tem probabilidade 1 e todas as outras a√ß√µes t√™m probabilidade 0. Isso demonstra que uma pol√≠tica determin√≠stica √© um caso especial de uma pol√≠tica estoc√°stica. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP com dois estados, $S = \{s_1, s_2\}$, e duas a√ß√µes, $A = \{a_1, a_2\}$.
>
> 1.  **Pol√≠tica Determin√≠stica:** Seja $\pi(s_1) = a_1$ e $\pi(s_2) = a_2$. Isso significa que no estado $s_1$ o agente sempre escolher√° a a√ß√£o $a_1$, e no estado $s_2$ sempre escolher√° a a√ß√£o $a_2$.
>
> 2.  **Pol√≠tica Estoc√°stica Equivalente:** Podemos representar essa pol√≠tica determin√≠stica como uma pol√≠tica estoc√°stica $\pi'(a|s)$ onde:
>
>     *   $\pi'(a_1|s_1) = 1$ e $\pi'(a_2|s_1) = 0$
>     *   $\pi'(a_1|s_2) = 0$ e $\pi'(a_2|s_2) = 1$
>
> 3.  **Outra Pol√≠tica Estoc√°stica:** Agora, considere uma pol√≠tica estoc√°stica diferente:
>
>     *   $\pi(a_1|s_1) = 0.7$ e $\pi(a_2|s_1) = 0.3$
>     *   $\pi(a_1|s_2) = 0.2$ e $\pi(a_2|s_2) = 0.8$
>
>     Essa pol√≠tica implica que no estado $s_1$, o agente escolher√° a a√ß√£o $a_1$ com probabilidade 0.7 e a a√ß√£o $a_2$ com probabilidade 0.3. Similarmente, no estado $s_2$, ele escolher√° $a_1$ com probabilidade 0.2 e a a√ß√£o $a_2$ com probabilidade 0.8.
>
> Este exemplo demonstra como pol√≠ticas determin√≠sticas s√£o um caso espec√≠fico de pol√≠ticas estoc√°sticas, onde a probabilidade de uma a√ß√£o √© 1 e a das outras √© 0. Pol√≠ticas estoc√°sticas, por outro lado, fornecem uma distribui√ß√£o de probabilidade sobre as a√ß√µes para cada estado, permitindo uma explora√ß√£o mais flex√≠vel do ambiente.

<!-- NEW ADDITION END -->

### Fun√ß√µes de Valor: Avaliando o Desempenho
As **fun√ß√µes de valor** s√£o ferramentas essenciais para avaliar a qualidade de uma pol√≠tica. Elas estimam o "qu√£o bom" √© para o agente estar em um determinado estado ou executar uma determinada a√ß√£o em um determinado estado, em termos de *retorno esperado* [^58]. O retorno esperado √© definido em termos de recompensas futuras que podem ser esperadas [^58]. As fun√ß√µes de valor dependem das a√ß√µes que o agente realiza [^58].

Existem dois tipos principais de fun√ß√µes de valor:
1.  **Fun√ß√£o de valor de estado ($v_\pi(s)$)**: Representa o valor de estar em um estado $s$ seguindo a pol√≠tica $\pi$. [^58]
2.  **Fun√ß√£o de valor de a√ß√£o ($q_\pi(s, a)$)**: Representa o valor de tomar uma a√ß√£o $a$ em um estado $s$ seguindo a pol√≠tica $\pi$. [^58]

**Defini√ß√£o:** A **fun√ß√£o de valor de estado** $v_\pi(s)$ sob uma pol√≠tica $\pi$ √© o retorno esperado ao iniciar no estado $s$ e, posteriormente, seguir a pol√≠tica $\pi$. Para MDPs, podemos definir $v_\pi$ formalmente por

$$
v_\pi(s) = E_\pi[G_t | S_t = s] = E_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg| S_t = s \right], \text{ para todos } s \in S
$$

onde $E_\pi[\cdot]$ denota o valor esperado de uma vari√°vel aleat√≥ria dado que o agente segue a pol√≠tica $\pi$ e $t$ √© um passo de tempo qualquer. O valor do estado terminal, se houver, √© sempre zero [^58]. Chamamos a fun√ß√£o $v_\pi$ de **fun√ß√£o de valor de estado para a pol√≠tica $\pi$** [^58].

**Defini√ß√£o:** Similarmente, definimos o valor de executar a a√ß√£o $a$ no estado $s$ sob uma pol√≠tica $\pi$, denotado por $q_\pi(s,a)$, como o retorno esperado ao iniciar em $s$, executar a a√ß√£o $a$ e, subsequentemente, seguir a pol√≠tica $\pi$.

$$
q_\pi(s, a) = E_\pi[G_t | S_t = s, A_t = a] = E_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg| S_t = s, A_t = a \right] \text{ .}
$$

Chamamos $q_\pi$ de **fun√ß√£o de valor de a√ß√£o para a pol√≠tica $\pi$** [^58].

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP simples com um √∫nico estado $s$ e duas a√ß√µes $a_1$ e $a_2$. Seja $\gamma = 0.9$. Suponha que seguindo uma pol√≠tica $\pi$:
>
> *   $q_\pi(s, a_1) = 10$:  Tomar a a√ß√£o $a_1$ no estado $s$ resulta em um retorno esperado de 10.
> *   $q_\pi(s, a_2) = 5$: Tomar a a√ß√£o $a_2$ no estado $s$ resulta em um retorno esperado de 5.
>
> Agora, suponha que a pol√≠tica $\pi$ √© estoc√°stica: $\pi(a_1|s) = 0.6$ e $\pi(a_2|s) = 0.4$.
>
> Podemos calcular a fun√ß√£o de valor de estado $v_\pi(s)$ usando o Lema 2:
>
> $v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) q_\pi(s, a) = \pi(a_1|s)q_\pi(s, a_1) + \pi(a_2|s)q_\pi(s, a_2)$
> $v_\pi(s) = (0.6)(10) + (0.4)(5) = 6 + 2 = 8$
>
> Isso significa que o valor de estar no estado $s$ seguindo a pol√≠tica $\pi$ √© 8. Este valor √© uma m√©dia ponderada dos valores das a√ß√µes, ponderada pela probabilidade de cada a√ß√£o ser escolhida sob a pol√≠tica $\pi$.

**Lema 2:** A fun√ß√£o de valor de estado $v_\pi(s)$ pode ser expressa em termos da fun√ß√£o de valor de a√ß√£o $q_\pi(s, a)$ da seguinte forma:

$$
v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) q_\pi(s, a)
$$

*Prova:* Provaremos que a fun√ß√£o de valor de estado $v_\pi(s)$ pode ser expressa em termos da fun√ß√£o de valor de a√ß√£o $q_\pi(s, a)$ como mostrado acima.

I.  Come√ßamos com a defini√ß√£o da fun√ß√£o de valor de estado:
$$v_\pi(s) = E_\pi[G_t | S_t = s]$$

II. Expandimos a expectativa condicionando na a√ß√£o $A_t = a$:
$$v_\pi(s) = \sum_{a \in A(s)} P(A_t = a | S_t = s) E_\pi[G_t | S_t = s, A_t = a]$$

III. Reconhecemos que $P(A_t = a | S_t = s)$ √© a pol√≠tica $\pi(a|s)$ e $E_\pi[G_t | S_t = s, A_t = a]$ √© a fun√ß√£o de valor de a√ß√£o $q_\pi(s, a)$:
$$v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) q_\pi(s, a)$$

IV. Portanto, a fun√ß√£o de valor de estado $v_\pi(s)$ √© a m√©dia ponderada da fun√ß√£o de valor de a√ß√£o $q_\pi(s, a)$ sobre todas as a√ß√µes poss√≠veis, ponderada pela probabilidade de cada a√ß√£o de acordo com a pol√≠tica $\pi$. ‚ñ†

<!-- NEW ADDITION END -->

### A Equa√ß√£o de Bellman: Recursividade nas Fun√ß√µes de Valor
Uma propriedade fundamental das fun√ß√µes de valor, utilizada amplamente em *reinforcement learning* e programa√ß√£o din√¢mica, √© que elas satisfazem rela√ß√µes recursivas, semelhantes √†quelas que j√° estabelecemos para o retorno [^59]. Para qualquer pol√≠tica $\pi$ e qualquer estado $s$, a seguinte condi√ß√£o de consist√™ncia se mant√©m entre o valor de $s$ e o valor de seus poss√≠veis estados sucessores [^59]:
$$
v_\pi(s) = E_\pi[G_t | S_t = s]
$$
$$
= E_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s]
$$
$$
= \sum_a \pi(a|s) \sum_{s', r} p(s',r|s,a) [r+E_\pi[G_{t+1}|S_{t+1}=s']]
$$
$$
= \sum_a \pi(a|s) \sum_{s', r} p(s',r|s,a) [r + \gamma v_\pi(s')], \text{ para todos } s \in S,
$$

onde est√° impl√≠cito que as a√ß√µes, $a$, s√£o tomadas do conjunto $A(s)$, que os pr√≥ximos estados, $s'$, s√£o tomados do conjunto $S$ (ou de $S^+$ no caso de um problema epis√≥dico) e que as recompensas, $r$, s√£o tomadas do conjunto $R$ [^59]. Note tamb√©m como na √∫ltima equa√ß√£o n√≥s combinamos as duas somas, uma sobre todos os valores de $s'$ e a outra sobre todos os valores de $r$, em uma soma sobre todos os poss√≠veis valores de ambos [^59]. Usamos este tipo de soma combinada frequentemente para simplificar as f√≥rmulas [^59]. Note como a express√£o final pode ser lida facilmente como um valor esperado [^59]. √â realmente uma soma sobre todos os valores das tr√™s vari√°veis, $a$, $s'$, e $r$ [^59]. Para cada tripla, computamos sua probabilidade, $\pi(a|s)p(s',r|s,a)$, ponderamos a quantidade entre colchetes por essa probabilidade e, ent√£o, somamos sobre todas as possibilidades para obter um valor esperado [^59].

A equa√ß√£o (3.14) [^59] √© a **Equa√ß√£o de Bellman para $v_\pi$**. Ela expressa uma rela√ß√£o entre o valor de um estado e os valores de seus estados sucessores [^59]. Imagine olhar para frente de um estado para seus poss√≠veis estados sucessores [^59].

![Diagrama de backup para a fun√ß√£o de valor \(v_\pi\), ilustrando a rela√ß√£o entre um estado e seus sucessores sob uma pol√≠tica \(\pi\).](./../images/image3.png)

A imagem √© um diagrama de backup para a fun√ß√£o de valor \(v_\pi\), como discutido na p√°gina 59 do Cap√≠tulo 3 do documento. O diagrama ilustra a rela√ß√£o entre o valor de um estado \(s\) e os valores de seus poss√≠veis estados sucessores ap√≥s tomar a√ß√µes de acordo com uma pol√≠tica \(\pi\), considerando a probabilidade de transi√ß√£o \(p\) e a recompensa \(r\). Os n√≥s abertos representam estados, enquanto os n√≥s fechados representam pares estado-a√ß√£o; as setas indicam transi√ß√µes, com \(\pi\) indicando a sele√ß√£o da a√ß√£o e \(p\) e \(r\) indicando a din√¢mica do ambiente e recompensas resultantes.

Cada c√≠rculo aberto representa um estado e cada c√≠rculo s√≥lido representa um par estado-a√ß√£o [^59]. Come√ßando pelo estado $s$, o n√≥ raiz no topo, o agente poderia tomar qualquer a√ß√£o de um conjunto de a√ß√µes ‚Äî tr√™s s√£o mostradas no diagrama ‚Äî com base em sua pol√≠tica $\pi$ [^59]. A partir de cada uma dessas, o ambiente poderia responder com um de diversos estados seguintes, $s'$ (dois s√£o mostradas na figura), juntamente com uma recompensa $r$, dependendo de suas din√¢micas dadas pela fun√ß√£o $p$ [^59]. A Equa√ß√£o de Bellman (3.14) [^59] faz a m√©dia sobre todas as possibilidades, ponderando cada uma por sua probabilidade de ocorr√™ncia [^59]. Ela afirma que o valor do estado inicial deve ser igual ao valor (descontado) do pr√≥ximo estado esperado, mais a recompensa esperada ao longo do caminho [^59].

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP com dois estados, $S = \{s_1, s_2\}$, e uma a√ß√£o em cada estado, $A(s_1) = \{a_1\}$ e $A(s_2) = \{a_2\}$. Seja $\gamma = 0.9$. As din√¢micas do ambiente s√£o:
>
> *   $p(s_2, 10 | s_1, a_1) = 1$:  Tomar a a√ß√£o $a_1$ no estado $s_1$ sempre leva ao estado $s_2$ com uma recompensa de 10.
> *   $p(s_1, 5 | s_2, a_2) = 1$: Tomar a a√ß√£o $a_2$ no estado $s_2$ sempre leva ao estado $s_1$ com uma recompensa de 5.
>
> A pol√≠tica $\pi$ √© determin√≠stica: $\pi(a_1|s_1) = 1$ e $\pi(a_2|s_2) = 1$.
>
> Usando a Equa√ß√£o de Bellman para $v_\pi(s)$:
>
> *   $v_\pi(s_1) = \sum_{s', r} p(s', r|s_1, a_1) [r + \gamma v_\pi(s')] = p(s_2, 10|s_1, a_1) [10 + \gamma v_\pi(s_2)] = 1 [10 + 0.9 v_\pi(s_2)]$
> *   $v_\pi(s_2) = \sum_{s', r} p(s', r|s_2, a_2) [r + \gamma v_\pi(s')] = p(s_1, 5|s_2, a_2) [5 + \gamma v_\pi(s_1)] = 1 [5 + 0.9 v_\pi(s_1)]$
>
> Temos um sistema de duas equa√ß√µes com duas inc√≥gnitas:
>
> *   $v_\pi(s_1) = 10 + 0.9 v_\pi(s_2)$
> *   $v_\pi(s_2) = 5 + 0.9 v_\pi(s_1)$
>
> Resolvendo o sistema:
>
> Substituindo a segunda equa√ß√£o na primeira:
> $v_\pi(s_1) = 10 + 0.9(5 + 0.9 v_\pi(s_1)) = 10 + 4.5 + 0.81 v_\pi(s_1)$
> $0.19 v_\pi(s_1) = 14.5$
> $v_\pi(s_1) = \frac{14.5}{0.19} \approx 76.32$
>
> Substituindo o valor de $v_\pi(s_1)$ na segunda equa√ß√£o:
> $v_\pi(s_2) = 5 + 0.9(76.32) = 5 + 68.688 \approx 73.69$
>
> Portanto, $v_\pi(s_1) \approx 76.32$ e $v_\pi(s_2) \approx 73.69$. Isso significa que o valor de estar no estado $s_1$ √© aproximadamente 76.32, e o valor de estar no estado $s_2$ √© aproximadamente 73.69, seguindo a pol√≠tica $\pi$. A Equa√ß√£o de Bellman permite calcular esses valores de forma recursiva, considerando as recompensas imediatas e os valores dos estados sucessores.

**Teorema 3:** A Equa√ß√£o de Bellman para $q_\pi$ √© dada por:

$$
q_\pi(s, a) = \sum_{s', r} p(s', r|s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') q_\pi(s', a') \right], \text{ para todos } s \in S, a \in A(s)
$$

*Prova:* Demonstraremos a equa√ß√£o de Bellman para $q_\pi$.

I. Come√ßamos com a defini√ß√£o da fun√ß√£o de valor de a√ß√£o:
$$q_\pi(s, a) = E_\pi[G_t | S_t = s, A_t = a]$$

II. Expandimos o retorno $G_t$ em termos da recompensa imediata e do retorno descontado:
$$q_\pi(s, a) = E_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$$

III. Condicionamos sobre o pr√≥ximo estado $s'$ e a recompensa $r$, usando a fun√ß√£o de transi√ß√£o $p(s', r|s, a)$:
$$q_\pi(s, a) = \sum_{s', r} p(s', r|s, a) E_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a, S_{t+1} = s', R_{t+1} = r]$$

IV. Simplificamos a expectativa, notando que $R_{t+1} = r$ √© determin√≠stico dado $s, a, s'$:
$$q_\pi(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma E_\pi[G_{t+1} | S_{t+1} = s']]$$

V. Reconhecemos que $E_\pi[G_{t+1} | S_{t+1} = s']$ √© a fun√ß√£o de valor de estado $v_\pi(s')$:
$$q_\pi(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')] $$

VI. Usamos o Lema 2 para substituir $v_\pi(s')$ por sua express√£o em termos de $q_\pi(s', a')$:
$$v_\pi(s') = \sum_{a'} \pi(a'|s') q_\pi(s', a')$$

VII. Substitu√≠mos $v_\pi(s')$ na equa√ß√£o anterior:
$$q_\pi(s, a) = \sum_{s', r} p(s', r|s, a) \left[ r + \gamma \sum_{a'} \pi(a'|s') q_\pi(s', a') \right]$$

VIII. Isso completa a deriva√ß√£o da Equa√ß√£o de Bellman para $q_\pi$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP com dois estados $S = \{s_1, s_2\}$ e duas a√ß√µes em cada estado $A(s) = \{a_1, a_2\}$ para $s \in S$. Seja $\gamma = 0.9$. As din√¢micas do ambiente e a pol√≠tica s√£o dadas por:
>
> *   $p(s_2, 10 | s_1, a_1) = 0.7$, $p(s_1, 2 | s_1, a_1) = 0.3$
> *   $p(s_1, 5 | s_1, a_2) = 0.5$, $p(s_2, 0 | s_1, a_2) = 0.5$
> *   $p(s_1, 8 | s_2, a_1) = 0.6$, $p(s_2, 3 | s_2, a_1) = 0.4$
> *   $p(s_2, 12 | s_2, a_2) = 0.8$, $p(s_1, -1 | s_2, a_2) = 0.2$
>
> *   $\pi(a_1|s_1) = 0.6$, $\pi(a_2|s_1) = 0.4$
> *   $\pi(a_1|s_2) = 0.3$, $\pi(a_2|s_2) = 0.7$
>
> Vamos calcular $q_\pi(s_1, a_1)$ usando a Equa√ß√£o de Bellman para $q_\pi$:
>
> $q_\pi(s_1, a_1) = \sum_{s', r} p(s', r|s_1, a_1) \left[ r + \gamma \sum_{a'} \pi(a'|s') q_\pi(s', a') \right]$
>
> $q_\pi(s_1, a_1) = p(s_2, 10|s_1, a_1) [10 + \gamma (\pi(a_1|s_2)q_\pi(s_2, a_1) + \pi(a_2|s_2)q_\pi(s_2, a_2))] + p(s_1, 2|s_1, a_1) [2 + \gamma (\pi(a_1|s_1)q_\pi(s_1, a_1) + \pi(a_2|s_1)q_\pi(s_1, a_2))]$
>
> $q_\pi(s_1, a_1) = 0.7 [10 + 0.9(0.3q_\pi(s_2, a_1) + 0.7q_\pi(s_2, a_2))] + 0.3 [2 + 0.9(0.6q_\pi(s_1, a_1) + 0.4q_\pi(s_1, a_2))]$
>
> Para resolver completamente, precisar√≠amos de equa√ß√µes similares para $q_\pi(s_1, a_2)$, $q_\pi(s_2, a_1)$, e $q_\pi(s_2, a_2)$ e resolver o sistema de equa√ß√µes. Este exemplo demonstra como a Equa√ß√£o de Bellman para $q_\pi$ relaciona o valor de uma a√ß√£o em um estado ao retorno esperado, considerando as probabilidades de transi√ß√£o para os pr√≥ximos estados e as a√ß√µes subsequentes tomadas nesses estados de acordo com a pol√≠tica $\pi$.



![Diagrama de backup qœÄ ilustrando as rela√ß√µes entre estados, a√ß√µes e recompensas em reinforcement learning.](./../images/image10.png)

A imagem √© um diagrama de backup de qœÄ, que √© uma representa√ß√£o gr√°fica utilizada em reinforcement learning para ilustrar as rela√ß√µes entre estados, a√ß√µes e recompensas. O diagrama mostra que a partir de um estado s e uma a√ß√£o a, existem duas poss√≠veis transi√ß√µes: uma com probabilidade p e outra com recompensa r, levando a um novo estado s'. No novo estado s', a pol√≠tica œÄ √© utilizada para selecionar a pr√≥xima a√ß√£o a'. Este tipo de diagrama √© utilizado para visualizar e analisar algoritmos de reinforcement learning, como explicado na Se√ß√£o 3.5 do documento.

### Conclus√£o

Este cap√≠tulo forneceu uma base s√≥lida para a compreens√£o das pol√≠ticas e fun√ß√µes de valor em MDPs finitos. Ao definir formalmente as pol√≠ticas como *mappings* de estados para a√ß√µes e as fun√ß√µes de valor como estimativas do retorno esperado, estabelecemos as bases para algoritmos que visam otimizar o comportamento do agente em um ambiente incerto [^1]. A equa√ß√£o de Bellman [^59] √© uma ferramenta fundamental para calcular e atualizar as fun√ß√µes de valor de forma recursiva. Os conceitos apresentados neste cap√≠tulo s√£o essenciais para a compreens√£o dos cap√≠tulos subsequentes, que explorar√£o diferentes t√©cnicas para encontrar pol√≠ticas √≥timas em MDPs [^1].

### Refer√™ncias

[^1]: Cap√≠tulo 3: Finite Markov Decision Processes.
[^48]: Se√ß√£o 3.1: The Agent-Environment Interface.
[^53]: Se√ß√£o 3.2: Goals and Rewards.
[^58]: Se√ß√£o 3.5: Policies and Value Functions.
[^59]: Se√ß√£o 3.5: Policies and Value Functions.
<!-- END -->