## Modelos de Decis√£o de Markov Finitos: Uma Formaliza√ß√£o da Tomada de Decis√£o Sequencial

### Introdu√ß√£o
Este cap√≠tulo introduz a formaliza√ß√£o dos **Processos de Decis√£o de Markov Finitos (MDPs)**, um modelo matem√°tico para descrever ambientes de tomada de decis√£o sequencial [^1]. Em contraste com os problemas de *bandit*, que focam na avalia√ß√£o de a√ß√µes independentes, os MDPs incorporam um aspecto associativo, onde a escolha de a√ß√µes influencia n√£o apenas a recompensa imediata, mas tamb√©m os estados futuros e, consequentemente, as recompensas futuras [^1]. Este aspecto de *delayed reward* requer que o agente aprenda a equilibrar recompensas imediatas e futuras.

### Conceitos Fundamentais

Um MDP √© definido por um conjunto de estados $S$, um conjunto de a√ß√µes $A(s)$ dispon√≠veis em cada estado $s \in S$, um conjunto de recompensas $R$, e uma fun√ß√£o de din√¢mica $p(s', r|s, a)$ que especifica a probabilidade de transi√ß√£o para o estado $s'$ e receber recompensa $r$ ao tomar a√ß√£o $a$ no estado $s$ [^2].

> üí° **Exemplo Num√©rico:** Considere um MDP simples com dois estados $S = \{s_1, s_2\}$ e duas a√ß√µes $A(s) = \{a_1, a_2\}$ para cada estado. Suponha que as recompensas poss√≠veis s√£o $R = \{0, 1\}$. A fun√ß√£o de din√¢mica $p(s', r|s, a)$ poderia ser definida como segue (valores hipot√©ticos):
>
> *   $p(s_1, 0|s_1, a_1) = 0.7$
> *   $p(s_2, 0|s_1, a_1) = 0.3$
> *   $p(s_1, 1|s_1, a_2) = 0.2$
> *   $p(s_2, 1|s_1, a_2) = 0.8$
> *   $p(s_1, 0|s_2, a_1) = 0.9$
> *   $p(s_2, 0|s_2, a_1) = 0.1$
> *   $p(s_1, 1|s_2, a_2) = 0.5$
> *   $p(s_2, 1|s_2, a_2) = 0.5$
>
> Isso significa que, por exemplo, ao tomar a a√ß√£o $a_1$ no estado $s_1$, h√° uma probabilidade de 0.7 de permanecer no estado $s_1$ e receber recompensa 0, e uma probabilidade de 0.3 de transitar para o estado $s_2$ e receber recompensa 0.

A intera√ß√£o entre o agente e o ambiente em um MDP ocorre em *discrete time steps* $t = 0, 1, 2, 3, \ldots$ [^2]. No instante $t$, o agente observa o estado do ambiente $S_t \in S$ e seleciona uma a√ß√£o $A_t \in A(S_t)$. Como resultado dessa a√ß√£o, o agente recebe uma recompensa num√©rica $R_{t+1} \in R$ e transita para um novo estado $S_{t+1}$. Esta intera√ß√£o gera uma sequ√™ncia, ou trajet√≥ria, da forma:

$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots$$ [^2]

> üí° **Exemplo Num√©rico (continua√ß√£o):** Uma poss√≠vel trajet√≥ria de intera√ß√£o nesse MDP poderia ser:
>
> $s_1, a_1, 0, s_1, a_2, 1, s_2, a_2, 1, s_2, a_1, 0, s_1, \ldots$
>
> Aqui, o agente come√ßou no estado $s_1$, tomou a a√ß√£o $a_1$ e recebeu recompensa 0, permanecendo no estado $s_1$. Em seguida, tomou a a√ß√£o $a_2$ e recebeu recompensa 1, transicionando para o estado $s_2$, e assim por diante.

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

Em um *finite MDP*, os conjuntos de estados, a√ß√µes e recompensas (S, A, e R) possuem um n√∫mero finito de elementos [^2]. Isso implica que as vari√°veis aleat√≥rias $R_t$ e $S_t$ t√™m distribui√ß√µes de probabilidade discretas bem definidas, que dependem apenas do estado e a√ß√£o precedentes [^2]. Formalmente, a fun√ß√£o de din√¢mica √© dada por:

$$p(s', r|s, a) = Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\}$$ [^2]

para todo $s', s \in S$, $r \in R$, e $a \in A(s)$. Essa fun√ß√£o $p: S \times R \times S \times A \rightarrow [0, 1]$ define a din√¢mica do MDP e √© uma fun√ß√£o determin√≠stica de quatro argumentos [^2].

A soma sobre todos os poss√≠veis estados seguintes e recompensas deve ser igual a 1:
$$\sum_{s' \in S} \sum_{r \in R} p(s', r|s, a) = 1, \text{ para todo } s \in S, a \in A(s)$$ [^3]

> üí° **Exemplo Num√©rico (valida√ß√£o da soma):** Usando os valores do exemplo anterior, vamos verificar se a soma das probabilidades para o estado $s_1$ e a√ß√£o $a_1$ √© igual a 1:
>
> $\sum_{s' \in S} \sum_{r \in R} p(s', r|s_1, a_1) = p(s_1, 0|s_1, a_1) + p(s_2, 0|s_1, a_1) + p(s_1, 1|s_1, a_1) + p(s_2, 1|s_1, a_1)$
>
> $= 0.7 + 0.3 + 0 + 0 = 1.0$
>
> Observe que $p(s_1, 1|s_1, a_1) = 0$ e $p(s_2, 1|s_1, a_1) = 0$ porque, de acordo com a defini√ß√£o inicial do exemplo, a a√ß√£o $a_1$ no estado $s_1$ nunca resulta na recompensa 1.

Num processo de decis√£o de Markov, as probabilidades dadas por *p* caracterizam completamente as din√¢micas do ambiente [^3]. Isto √©, a probabilidade de cada valor poss√≠vel para $S_t$ e $R_t$ depende do estado e da a√ß√£o imediatamente anteriores, $S_{t-1}$ e $A_{t-1}$ [^3].

Para simplificar a nota√ß√£o e facilitar a an√°lise, √© comum definir fun√ß√µes relacionadas √† din√¢mica do MDP. Uma delas √© a fun√ß√£o de transi√ß√£o de estado, dada por:

$$p(s'|s, a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum_{r \in R} p(s', r|s, a)$$

Essa fun√ß√£o representa a probabilidade de transi√ß√£o para o estado $s'$ dado que a a√ß√£o $a$ foi tomada no estado $s$, independentemente da recompensa recebida.

> üí° **Exemplo Num√©rico:**  Usando novamente o exemplo anterior, calculemos a fun√ß√£o de transi√ß√£o de estado $p(s'|s, a)$ para $s = s_1$ e $a = a_1$:
>
> $p(s_1|s_1, a_1) = \sum_{r \in R} p(s_1, r|s_1, a_1) = p(s_1, 0|s_1, a_1) + p(s_1, 1|s_1, a_1) = 0.7 + 0 = 0.7$
>
> $p(s_2|s_1, a_1) = \sum_{r \in R} p(s_2, r|s_1, a_1) = p(s_2, 0|s_1, a_1) + p(s_2, 1|s_1, a_1) = 0.3 + 0 = 0.3$
>
> Isso significa que, ao tomar a a√ß√£o $a_1$ no estado $s_1$, a probabilidade de permanecer no estado $s_1$ √© 0.7 e a probabilidade de transitar para o estado $s_2$ √© 0.3.

Outra fun√ß√£o √∫til √© a fun√ß√£o de recompensa esperada, definida como:

$$r(s, a) = E[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in R} r \sum_{s' \in S} p(s', r|s, a) = \sum_{r \in R} r Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a\}$$

Esta fun√ß√£o representa o valor esperado da recompensa obtida ao se tomar a a√ß√£o $a$ no estado $s$.

> üí° **Exemplo Num√©rico:**  Calculemos a fun√ß√£o de recompensa esperada $r(s, a)$ para $s = s_1$ e $a = a_2$ usando os valores do exemplo anterior:
>
> $r(s_1, a_2) = \sum_{r \in R} r \sum_{s' \in S} p(s', r|s_1, a_2) = 0 * (p(s_1, 0|s_1, a_2) + p(s_2, 0|s_1, a_2)) + 1 * (p(s_1, 1|s_1, a_2) + p(s_2, 1|s_1, a_2))$
>
> $r(s_1, a_2) = 0 * (0 + 0) + 1 * (0.2 + 0.8) = 1.0$
>
> Isso significa que, ao tomar a a√ß√£o $a_2$ no estado $s_1$, o valor esperado da recompensa √© 1.

**Proposi√ß√£o 1:** A fun√ß√£o de recompensa esperada $r(s, a)$ pode ser reescrita como uma m√©dia ponderada das recompensas, onde os pesos s√£o as probabilidades de transi√ß√£o e recompensa.

*Proof:* A defini√ß√£o de $r(s, a)$ √© dada por $r(s, a) = \sum_{r \in R} r \sum_{s' \in S} p(s', r|s, a)$. Esta express√£o √© exatamente a m√©dia ponderada das recompensas $r$, ponderadas pelas probabilidades conjuntas $p(s', r|s, a)$ de transi√ß√£o para o estado $s'$ e recebimento da recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$.

**Corol√°rio 1:** Se o conjunto de recompensas $R$ for unit√°rio, ou seja, $R = \{r\}$, ent√£o $r(s, a) = r$ para todo $s \in S$ e $a \in A(s)$.

*Proof:*
I.  Come√ßamos com a defini√ß√£o da fun√ß√£o de recompensa esperada:
    $$r(s, a) = \sum_{r' \in R} r' \sum_{s' \in S} p(s', r'|s, a)$$

II. Dado que $R = \{r\}$, a soma sobre $r' \in R$ se reduz a um √∫nico termo, onde $r' = r$:
     $$r(s, a) = r \sum_{s' \in S} p(s', r|s, a)$$

III. Podemos fatorar $r$ da somat√≥ria, pois n√£o depende de $s'$:
      $$r(s, a) = r \sum_{s' \in S} p(s', r|s, a)$$

IV. A soma $\sum_{s' \in S} p(s', r|s, a)$ representa a probabilidade de receber a recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$, independentemente do estado seguinte $s'$.  Dado que $R = \{r\}$, receber alguma recompensa ao tomar a a√ß√£o $a$ no estado $s$ significa que a recompensa $r$ foi recebida *com certeza*. Portanto, a soma sobre todos os estados seguintes deve ser igual a 1:
$$\sum_{s' \in S} p(s', r|s, a) = Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a\} = 1$$

V.  Substituindo isso na equa√ß√£o para $r(s, a)$, obtemos:
 $$r(s, a) = r \cdot 1 = r$$

Portanto, se $R = \{r\}$, ent√£o $r(s, a) = r$ para todo $s \in S$ e $a \in A(s)$. ‚ñ†

### Conclus√£o

Os MDPs fornecem uma estrutura matematicamente precisa para a modelagem de problemas de tomada de decis√£o sequencial [^1]. A formaliza√ß√£o do ambiente, do agente e da sua intera√ß√£o, bem como a defini√ß√£o da din√¢mica do sistema atrav√©s da fun√ß√£o *p*, permitem a an√°lise rigorosa e o desenvolvimento de algoritmos para a otimiza√ß√£o do comportamento do agente [^2]. A propriedade de Markov, que postula que o estado atual cont√©m todas as informa√ß√µes relevantes do passado para a tomada de decis√£o futura, simplifica a an√°lise e permite o desenvolvimento de m√©todos eficientes para a resolu√ß√£o de MDPs [^3]. Em cap√≠tulos futuros, exploraremos conceitos como *retornos*, *fun√ß√µes de valor* e as *equa√ß√µes de Bellman*, que se baseiam nesta formaliza√ß√£o inicial e s√£o essenciais para o desenvolvimento de algoritmos de *reinforcement learning*.

### Refer√™ncias
[^1]: Chapter 3: Finite Markov Decision Processes
[^2]: Chapter 3: Finite Markov Decision Processes, Section 3.1
[^3]: Chapter 3: Finite Markov Decision Processes, Section 3.1
<!-- END -->