## A Solu√ß√£o √önica da Equa√ß√£o de Bellman e Diagramas de Backup

### Introdu√ß√£o
Este cap√≠tulo explora em profundidade as **equa√ß√µes de Bellman** e como elas desempenham um papel fundamental na solu√ß√£o de **Processos de Decis√£o de Markov (MDPs)** finitos. Especificamente, focaremos na propriedade de que a fun√ß√£o de valor $v_\pi$ √© a *√∫nica solu√ß√£o* para sua equa√ß√£o de Bellman correspondente [^60]. Al√©m disso, examinaremos como os **diagramas de backup** fornecem uma representa√ß√£o visual das rela√ß√µes que formam a base para as opera√ß√µes de atualiza√ß√£o e backup, que s√£o centrais para os m√©todos de aprendizado por refor√ßo [^60].

### A Unicidade da Solu√ß√£o da Equa√ß√£o de Bellman
A **equa√ß√£o de Bellman** para $v_\pi$, como apresentada em [^59], expressa a rela√ß√£o entre o valor de um estado e os valores de seus estados sucessores, ponderados pelas probabilidades de transi√ß√£o e pelas recompensas esperadas. Formalmente, essa equa√ß√£o √© dada por:

$$
v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_\pi(s')] \quad \text{para todo } s \in S
$$

onde:

*   $v_\pi(s)$ √© o valor do estado $s$ sob a pol√≠tica $\pi$
*   $A(s)$ √© o conjunto de a√ß√µes dispon√≠veis no estado $s$
*   $\pi(a|s)$ √© a probabilidade de selecionar a a√ß√£o $a$ no estado $s$ sob a pol√≠tica $\pi$
*   $p(s', r|s, a)$ √© a probabilidade de transi√ß√£o para o estado $s'$ com recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$
*   $\gamma$ √© o fator de desconto

A afirma√ß√£o central √© que *existe apenas uma fun√ß√£o* $v_\pi$ que satisfaz essa equa√ß√£o para todos os estados $s \in S$. Para entender melhor, considere que a equa√ß√£o de Bellman pode ser vista como um sistema de equa√ß√µes lineares, onde o n√∫mero de equa√ß√µes corresponde ao n√∫mero de estados [^64].

**Teorema:** Para um MDP finito com uma pol√≠tica $\pi$ fixa, a equa√ß√£o de Bellman para $v_\pi$ tem uma solu√ß√£o √∫nica.

*Prova:*
A equa√ß√£o de Bellman pode ser escrita na forma matricial como:

$$v = R + \gamma P v$$

onde:
*   $v$ √© um vetor coluna representando os valores de todos os estados, $[v_\pi(s_1), v_\pi(s_2), \ldots, v_\pi(s_n)]^T$
*   $R$ √© um vetor coluna representando as recompensas esperadas, $[R(s_1), R(s_2), \ldots, R(s_n)]^T$
*   $P$ √© a matriz de transi√ß√£o de probabilidade sob a pol√≠tica $\pi$, onde $P_{ij}$ √© a probabilidade de transi√ß√£o do estado $i$ para o estado $j$.

> üí° **Exemplo Num√©rico:** Considere um MDP com 3 estados. A pol√≠tica $\pi$ √© fixa. Suponha que as recompensas esperadas para os estados 1, 2 e 3 sejam 2, -1 e 0 respectivamente. A matriz de transi√ß√£o sob a pol√≠tica $\pi$ √© dada por:
> $$
> P = \begin{bmatrix}
> 0.5 & 0.3 & 0.2 \\
> 0.1 & 0.6 & 0.3 \\
> 0.4 & 0.2 & 0.4
> \end{bmatrix}
> $$
> Seja $\gamma = 0.9$.  Ent√£o, $R = \begin{bmatrix} 2 \\ -1 \\ 0 \end{bmatrix}$. Queremos resolver $v = R + \gamma Pv$.

Reorganizando a equa√ß√£o, temos:

$$(I - \gamma P)v = R$$

onde $I$ √© a matriz identidade. Se a matriz $(I - \gamma P)$ for invert√≠vel, ent√£o a solu√ß√£o para $v$ √© √∫nica e dada por:

$$v = (I - \gamma P)^{-1} R$$

> üí° **Exemplo Num√©rico (cont.):** Continuando o exemplo anterior:
> $\text{Passo 1: Calcular } \gamma P$:
> $$
> \gamma P = 0.9 \times \begin{bmatrix}
> 0.5 & 0.3 & 0.2 \\
> 0.1 & 0.6 & 0.3 \\
> 0.4 & 0.2 & 0.4
> \end{bmatrix} = \begin{bmatrix}
> 0.45 & 0.27 & 0.18 \\
> 0.09 & 0.54 & 0.27 \\
> 0.36 & 0.18 & 0.36
> \end{bmatrix}
> $$
> $\text{Passo 2: Calcular } (I - \gamma P)$:
> $$
> (I - \gamma P) = \begin{bmatrix}
> 1 & 0 & 0 \\
> 0 & 1 & 0 \\
> 0 & 0 & 1
> \end{bmatrix} - \begin{bmatrix}
> 0.45 & 0.27 & 0.18 \\
> 0.09 & 0.54 & 0.27 \\
> 0.36 & 0.18 & 0.36
> \end{bmatrix} = \begin{bmatrix}
> 0.55 & -0.27 & -0.18 \\
> -0.09 & 0.46 & -0.27 \\
> -0.36 & -0.18 & 0.64
> \end{bmatrix}
> $$
> $\text{Passo 3: Calcular } (I - \gamma P)^{-1}$.  Usando Python com NumPy:
> ```python
> import numpy as np
>
> gamma = 0.9
> P = np.array([[0.5, 0.3, 0.2],
>               [0.1, 0.6, 0.3],
>               [0.4, 0.2, 0.4]])
> R = np.array([2, -1, 0])
>
> I = np.identity(3)
> A = I - gamma * P
> A_inv = np.linalg.inv(A)
>
> print("Inversa de (I - gamma * P):\n", A_inv)
> ```
>
> Isso resulta em:
> ```
> Inversa de (I - gamma * P):
>  [[ 2.14787701  1.33602151  0.92043011]
>  [ 1.22311828  2.87634409  1.5483871 ]
>  [ 1.36021505  0.98924731  2.06451613]]
> ```
> $\text{Passo 4: Calcular } v = (I - \gamma P)^{-1} R$:
> ```python
> v = np.dot(A_inv, R)
> print("Fun√ß√£o de valor v:\n", v)
> ```
> Isso resulta em:
> ```
> Fun√ß√£o de valor v:
>  [ 3.25806452 -0.22580645  1.12903226]
> ```
> Portanto, os valores dos estados 1, 2 e 3 sob a pol√≠tica $\pi$ s√£o aproximadamente 3.26, -0.23 e 1.13, respectivamente.  Este √© o *√∫nico* vetor de valor que satisfaz a equa√ß√£o de Bellman para esta pol√≠tica e MDP.

Para mostrar que $(I - \gamma P)$ √© invert√≠vel, devemos mostrar que seu determinante n√£o √© zero. Isso pode ser demonstrado mostrando que $(I - \gamma P)$ √© uma matriz *estritamente diagonal dominante* para $0 \leq \gamma < 1$, o que implica que √© invert√≠vel. Uma matriz √© estritamente diagonal dominante se, para cada linha, o valor absoluto do elemento diagonal √© maior que a soma dos valores absolutos dos outros elementos nessa linha. Formalmente:

$$|a_{ii}| > \sum_{j \neq i} |a_{ij}|$$

No nosso caso, $(I - \gamma P)$ tem elementos diagonais iguais a 1 e elementos fora da diagonal iguais a $-\gamma P_{ij}$.  Como $P$ √© uma matriz de transi√ß√£o de probabilidade, as linhas de $P$ somam 1. Assim, para cada linha $i$ de $(I - \gamma P)$:

$$1 > \gamma \sum_{j \neq i} P_{ij}$$

desde que $\gamma < 1$. Portanto, $(I - \gamma P)$ √© estritamente diagonal dominante e, consequentemente, invert√≠vel, o que implica que a solu√ß√£o para $v$ √© √∫nica. $\blacksquare$

**Corol√°rio:** Dada a unicidade da solu√ß√£o para a equa√ß√£o de Bellman, qualquer algoritmo que convirja para uma solu√ß√£o dessa equa√ß√£o necessariamente converge para a fun√ß√£o de valor $v_\pi$.

Dado que estabelecemos a unicidade da solu√ß√£o para $v_\pi$, podemos estender essa an√°lise para a fun√ß√£o de valor da a√ß√£o, $q_\pi(s, a)$.

**Teorema 1:** Para um MDP finito com uma pol√≠tica $\pi$ fixa, a equa√ß√£o de Bellman para $q_\pi$ tem uma solu√ß√£o √∫nica.

*Prova:*
A equa√ß√£o de Bellman para $q_\pi$ √© dada por:

$$
q_\pi(s, a) = \sum_{s',r} p(s', r|s, a) [r + \gamma \sum_{a' \in A(s')} \pi(a'|s') q_\pi(s', a')] \quad \text{para todo } s \in S, a \in A(s)
$$

De forma similar a $v_\pi$, podemos expressar essa equa√ß√£o na forma matricial:

$$q = R + \gamma P_{\pi} q$$

onde:

*   $q$ √© um vetor coluna representando os valores de todos os pares estado-a√ß√£o.
*   $R$ √© um vetor coluna representando as recompensas esperadas para cada par estado-a√ß√£o.
*   $P_{\pi}$ √© a matriz de transi√ß√£o de probabilidade para pares estado-a√ß√£o sob a pol√≠tica $\pi$. Cada entrada representa a probabilidade de transitar de um par estado-a√ß√£o para outro.

> üí° **Exemplo Num√©rico:**  Considere um MDP com 2 estados e 2 a√ß√µes por estado. Ent√£o o vetor $q$ tem dimens√£o 4.  Suponha que $\gamma = 0.9$ e que a matriz $P_\pi$ e o vetor $R$ sejam definidos como:
> $$P_\pi = \begin{bmatrix} 0.2 & 0.3 & 0.1 & 0.4 \\ 0.3 & 0.2 & 0.4 & 0.1 \\ 0.1 & 0.4 & 0.3 & 0.2 \\ 0.4 & 0.1 & 0.2 & 0.3 \end{bmatrix}, \quad R = \begin{bmatrix} 1 \\ -1 \\ 0.5 \\ -0.5 \end{bmatrix}$$
> Podemos ent√£o encontrar a fun√ß√£o de valor a√ß√£o $q_\pi$ resolvendo a equa√ß√£o $q = R + \gamma P_{\pi} q$.

A prova segue o mesmo racioc√≠nio que a prova para $v_\pi$. Reorganizando a equa√ß√£o, temos:

$$(I - \gamma P_{\pi})q = R$$

Se $(I - \gamma P_{\pi})$ for invert√≠vel, ent√£o a solu√ß√£o para $q$ √© √∫nica. A invertibilidade pode ser demonstrada mostrando que $(I - \gamma P_{\pi})$ √© estritamente diagonal dominante para $0 \leq \gamma < 1$, o que implica que seu determinante n√£o √© zero.  Como $P_{\pi}$ representa as probabilidades de transi√ß√£o de pares estado-a√ß√£o, suas linhas somam 1.  Portanto, para $\gamma < 1$, $(I - \gamma P_{\pi})$ √© estritamente diagonal dominante e invert√≠vel, garantindo a unicidade da solu√ß√£o para $q_\pi$. $\blacksquare$

**Corol√°rio 1.1:** Qualquer algoritmo que convirja para uma solu√ß√£o da equa√ß√£o de Bellman para $q_\pi$ converge necessariamente para a fun√ß√£o de valor a√ß√£o correta $q_\pi$.

### Diagramas de Backup
Os **diagramas de backup** s√£o representa√ß√µes gr√°ficas que ilustram as rela√ß√µes de atualiza√ß√£o nos algoritmos de aprendizado por refor√ßo [^60]. Eles mostram como o valor de um estado (ou par estado-a√ß√£o) √© atualizado com base nos valores estimados de seus sucessores e nas recompensas esperadas.

Considere o diagrama de backup para $v_\pi$ [^59]:

![Diagrama de backup para a fun√ß√£o de valor \(v_\pi\), ilustrando a rela√ß√£o entre um estado e seus sucessores sob uma pol√≠tica \(\pi\).](./../images/image3.png)

A imagem √© um diagrama de backup para a fun√ß√£o de valor \(v_\pi\), como discutido na p√°gina 59 do Cap√≠tulo 3 do documento. O diagrama ilustra a rela√ß√£o entre o valor de um estado \(s\) e os valores de seus poss√≠veis estados sucessores ap√≥s tomar a√ß√µes de acordo com uma pol√≠tica \(\pi\), considerando a probabilidade de transi√ß√£o \(p\) e a recompensa \(r\). Os n√≥s abertos representam estados, enquanto os n√≥s fechados representam pares estado-a√ß√£o; as setas indicam transi√ß√µes, com \(\pi\) indicando a sele√ß√£o da a√ß√£o e \(p\) e \(r\) indicando a din√¢mica do ambiente e recompensas resultantes.

* Cada **c√≠rculo aberto** representa um estado.
* Cada **c√≠rculo preenchido** representa um par estado-a√ß√£o.
* As **setas** indicam as transi√ß√µes de um estado para outro, ou de um estado para um par estado-a√ß√£o.

O diagrama de backup para a equa√ß√£o de Bellman (3.14) visualiza a rela√ß√£o entre um estado $s$ e seus poss√≠veis estados sucessores $s'$, ponderados pelas probabilidades de transi√ß√£o $p(s', r|s, a)$ e pela pol√≠tica $\pi(a|s)$. Cada ramo do diagrama representa uma poss√≠vel trajet√≥ria que o agente pode seguir a partir do estado $s$. A equa√ß√£o de Bellman calcula uma m√©dia sobre todas essas possibilidades, ponderando cada uma por sua probabilidade de ocorr√™ncia [^59].

**Exemplo:** No contexto do *Gridworld*, apresentado em [^60], o diagrama de backup ilustra como o valor de uma c√©lula na grade √© atualizado com base nos valores de suas c√©lulas vizinhas. A atualiza√ß√£o leva em conta a pol√≠tica do agente e as recompensas associadas √† transi√ß√£o para cada vizinho.

> üí° **Exemplo Num√©rico:** Imagine um Gridworld 2x2 onde um agente pode se mover para cima, baixo, esquerda ou direita.  Se o agente tenta sair da grade, ele permanece no mesmo estado e recebe uma recompensa de -1.  Todos os outros movimentos resultam em uma recompensa de 0. Seja $\gamma = 0.9$.  Suponha que a pol√≠tica $\pi$ seja escolher cada a√ß√£o com igual probabilidade (0.25).  O diagrama de backup para um estado espec√≠fico (por exemplo, o canto superior esquerdo) mostraria setas para cada um dos seus vizinhos (o estado abaixo e o estado √† direita, e ele mesmo se tentar sair da grade), com cada transi√ß√£o ponderada pela probabilidade da pol√≠tica e pela probabilidade de transi√ß√£o.  As recompensas associadas a cada transi√ß√£o seriam ent√£o usadas para atualizar o valor do estado usando a equa√ß√£o de Bellman.

A utilidade dos diagramas de backup reside na sua capacidade de fornecer uma compreens√£o visual das opera√ß√µes de atualiza√ß√£o, facilitando o projeto e a an√°lise de algoritmos de aprendizado por refor√ßo [^60]. Al√©m de visualizar as atualiza√ß√µes de valor para uma pol√≠tica fixa, os diagramas de backup s√£o cruciais para entender algoritmos de controle, como *Value Iteration* e *Policy Iteration*, que visam encontrar a pol√≠tica √≥tima.

**Exemplo:** Em *Value Iteration*, o diagrama de backup representa uma atualiza√ß√£o "completa" que considera todas as a√ß√µes poss√≠veis em cada estado e atualiza o valor do estado com o m√°ximo valor esperado. Isso contrasta com os diagramas de backup para $v_\pi$ ou $q_\pi$, que s√£o espec√≠ficos para uma determinada pol√≠tica.

> üí° **Exemplo Num√©rico:** No Gridworld 2x2, *Value Iteration* consideraria todas as quatro a√ß√µes (cima, baixo, esquerda, direita) para cada estado. O diagrama de backup para cada estado mostraria quatro ramos, um para cada a√ß√£o. O valor do estado seria atualizado para o m√°ximo valor esperado entre todas as a√ß√µes poss√≠veis, ou seja, $v(s) = \max_a \sum_{s',r} p(s', r|s, a) [r + \gamma v(s')]$.

Outros exemplos de diagramas de backup incluem:

![Backup diagrams illustrating the update rules for state-value \(v_*\) and action-value \(q_*\) functions in MDPs.](./../images/image1.png)

The image (Figure 3.4 from Chapter 3) illustrates backup diagrams for state-value function \(v_*\) and action-value function \(q_*\) in Markov Decision Processes (MDPs). On the left, the diagram for \(v_*\) shows the value of a state \(s\) being updated by considering the maximum value achievable from its possible successor states, each reached after taking an action. The diagram on the right, representing \(q_*\), shows the value of taking a specific action \(a\) in state \(s\), considering the immediate reward \(r\) and the value of the resulting next state \(s'\), which is further maximized over possible actions from \(s'\). These diagrams provide a graphical representation of how value functions are updated, forming the basis for update or backup operations used in reinforcement learning methods.

e

![Diagrama de backup qœÄ ilustrando as rela√ß√µes entre estados, a√ß√µes e recompensas em reinforcement learning.](./../images/image10.png)

A imagem √© um diagrama de backup de qœÄ, que √© uma representa√ß√£o gr√°fica utilizada em reinforcement learning para ilustrar as rela√ß√µes entre estados, a√ß√µes e recompensas. O diagrama mostra que a partir de um estado s e uma a√ß√£o a, existem duas poss√≠veis transi√ß√µes: uma com probabilidade p e outra com recompensa r, levando a um novo estado s'. No novo estado s', a pol√≠tica œÄ √© utilizada para selecionar a pr√≥xima a√ß√£o a'. Este tipo de diagrama √© utilizado para visualizar e analisar algoritmos de reinforcement learning, como explicado na Se√ß√£o 3.5 do documento.
### Conclus√£o
A equa√ß√£o de Bellman desempenha um papel crucial no aprendizado por refor√ßo, fornecendo uma base para o c√°lculo e a otimiza√ß√£o das fun√ß√µes de valor. A propriedade de unicidade da solu√ß√£o garante que os algoritmos que resolvem essa equa√ß√£o convergem para a fun√ß√£o de valor correta, o que √© fundamental para a garantia de converg√™ncia dos m√©todos de aprendizado por refor√ßo. Os diagramas de backup complementam essa compreens√£o ao fornecer uma visualiza√ß√£o intuitiva das rela√ß√µes de atualiza√ß√£o, auxiliando no desenvolvimento e na an√°lise de algoritmos.

### Refer√™ncias
[^59]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, 2018.
[^60]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, 2018.
[^64]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, 2018.
<!-- END -->