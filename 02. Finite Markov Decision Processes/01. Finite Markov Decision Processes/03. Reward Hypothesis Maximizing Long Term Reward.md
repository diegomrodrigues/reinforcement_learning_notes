## O Objetivo do Agente: Maximiza√ß√£o da Recompensa Cumulativa a Longo Prazo

### Introdu√ß√£o

Este cap√≠tulo aprofunda a formaliza√ß√£o do problema de **Processos de Decis√£o de Markov Finitos (MDPs)**, introduzido anteriormente, concentrando-se no objetivo fundamental do agente: maximizar a recompensa cumulativa a longo prazo [^1]. Essa busca pela recompensa m√°xima √© traduzida em termos matem√°ticos precisos, atrav√©s do conceito de *retorno*, que quantifica a recompensa total que o agente espera acumular ao longo do tempo. O conceito de retorno √© definido de forma diferente para tarefas *epis√≥dicas* e *cont√≠nuas*, levando √† introdu√ß√£o da taxa de desconto $\gamma$ para as √∫ltimas [^8].

### A Hip√≥tese da Recompensa e a Formaliza√ß√£o do Retorno

O cora√ß√£o do *reinforcement learning* reside na formaliza√ß√£o do objetivo do agente, que √© expresso na **hip√≥tese da recompensa**: *todos os objetivos e prop√≥sitos podem ser bem compreendidos como a maximiza√ß√£o do valor esperado da soma cumulativa de um sinal escalar recebido (chamado recompensa)* [^7]. Em outras palavras, o agente busca aprender a agir de forma a acumular a maior quantidade poss√≠vel de recompensa ao longo do tempo.

Para formalizar matematicamente essa ideia, define-se o conceito de **retorno** ($G_t$), que representa a recompensa total que o agente espera receber a partir de um determinado instante *t*. A defini√ß√£o precisa de $G_t$ varia dependendo da natureza da tarefa:

1.  **Tarefas Epis√≥dicas:** Em *tarefas epis√≥dicas*, a intera√ß√£o entre o agente e o ambiente se divide naturalmente em *epis√≥dios*, cada um terminando em um estado *terminal*. Seja *T* o instante final do epis√≥dio, o retorno $G_t$ √© definido como a soma das recompensas recebidas at√© o final do epis√≥dio [^8]:

    $$ G_t = R_{t+1} + R_{t+2} + \dots + R_T $$

    Nesse caso, o agente busca maximizar a recompensa total obtida ao longo de cada epis√≥dio.

    > üí° **Exemplo Num√©rico:** Imagine um agente aprendendo a jogar um jogo simples onde recebe recompensas por cada a√ß√£o que o aproxima do objetivo. Em um epis√≥dio, o agente recebe as seguintes recompensas: $R_1 = 2$, $R_2 = -1$, $R_3 = 3$, $R_4 = 1$. O retorno $G_0$ no in√≠cio do epis√≥dio √© a soma de todas as recompensas: $G_0 = 2 - 1 + 3 + 1 = 5$.

2.  **Tarefas Cont√≠nuas:** Em *tarefas cont√≠nuas*, a intera√ß√£o entre o agente e o ambiente n√£o se divide em epis√≥dios distintos, e o processo continua indefinidamente. Nesse caso, a soma simples das recompensas pode divergir, tornando a defini√ß√£o do retorno problem√°tica. Para contornar esse problema, introduz-se o conceito de **retorno descontado**, que atribui um peso menor √†s recompensas recebidas no futuro [^9]:

    $$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} $$

    O par√¢metro $\gamma$, denominado **taxa de desconto**, assume valores entre 0 e 1 ($0 \leq \gamma \leq$ 1) e determina o qu√£o o agente valoriza recompensas futuras em rela√ß√£o √†s recompensas imediatas. Quando $\gamma$ √© pr√≥ximo de 0, o agente √© *m√≠ope* e se concentra apenas em maximizar a recompensa imediata ($R_{t+1}$). Quando $\gamma$ √© pr√≥ximo de 1, o agente √© *mais perspicaz* e considera as recompensas futuras com mais peso [^9].

    > üí° **Exemplo Num√©rico:** Considere um agente em um ambiente cont√≠nuo que recebe uma recompensa constante de $R = 1$ em cada passo. Se a taxa de desconto $\gamma = 0.9$, o retorno $G_0$ √©:
    >
    > $G_0 = 1 + 0.9 \cdot 1 + 0.9^2 \cdot 1 + 0.9^3 \cdot 1 + \dots$
    >
    > Usando a f√≥rmula da s√©rie geom√©trica, $G_0 = \frac{1}{1 - 0.9} = \frac{1}{0.1} = 10$. Isso significa que, embora o agente receba uma recompensa de 1 a cada passo, o valor total descontado √© 10. Se $\gamma = 0.5$, ent√£o $G_0 = \frac{1}{1 - 0.5} = 2$. Com um $\gamma$ menor, o retorno total descontado √© menor, indicando que o agente valoriza menos as recompensas futuras.

    *Observa√ß√£o:* A introdu√ß√£o da taxa de desconto garante que o retorno $G_t$ seja finito, desde que a sequ√™ncia de recompensas {$R_k$} seja limitada e $\gamma <$ 1 [^9].

    **Proposi√ß√£o 1** Se $|R_k| \leq R_{max}$ para todo $k$ e $0 \leq \gamma < 1$, ent√£o $|G_t| \leq \frac{R_{max}}{1-\gamma}$.

    *Prova:*
    I.  Come√ßamos com a defini√ß√£o do valor absoluto do retorno $G_t$:
        $$ |G_t| = \left| \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right| $$
    II. Aplicamos a desigualdade triangular:
        $$ |G_t| \leq \sum_{k=0}^{\infty} \left| \gamma^k R_{t+k+1} \right| $$
    III. Como $\gamma^k$ √© n√£o negativo, podemos simplificar:
         $$ |G_t| \leq \sum_{k=0}^{\infty} \gamma^k |R_{t+k+1}| $$
    IV. Usamos a condi√ß√£o de que $|R_k| \leq R_{max}$ para todo *k*:
        $$ |G_t| \leq \sum_{k=0}^{\infty} \gamma^k R_{max} $$
    V.  Fatoramos $R_{max}$ para fora da soma:
        $$ |G_t| \leq R_{max} \sum_{k=0}^{\infty} \gamma^k $$
    VI. A soma √© uma s√©rie geom√©trica com raz√£o $\gamma$, que converge para $\frac{1}{1-\gamma}$ quando $0 \leq \gamma < 1$:
         $$ |G_t| \leq R_{max} \frac{1}{1-\gamma} $$
    VII. Portanto:
         $$ |G_t| \leq \frac{R_{max}}{1-\gamma} $$
    ‚ñ†

    > üí° **Exemplo Num√©rico:** Suponha que a recompensa m√°xima que o agente pode receber em qualquer passo √© $R_{max} = 10$, e a taxa de desconto √© $\gamma = 0.9$. Usando a proposi√ß√£o 1, podemos calcular o limite superior do retorno:
    >
    > $|G_t| \leq \frac{10}{1 - 0.9} = \frac{10}{0.1} = 100$
    >
    > Isso significa que, mesmo que o agente receba a recompensa m√°xima em todos os passos futuros, o retorno total descontado nunca exceder√° 100. Se $\gamma = 0.5$, o limite superior seria $|G_t| \leq \frac{10}{1 - 0.5} = 20$.

### Rela√ß√£o Recursiva entre Retornos

Uma propriedade fundamental do retorno, que ser√° crucial para o desenvolvimento de algoritmos de *reinforcement learning*, √© a sua rela√ß√£o recursiva. Essa rela√ß√£o expressa o retorno no instante *t* em termos da recompensa imediata e do retorno no instante *t+1* [^9]:

$$ G_t = R_{t+1} + \gamma G_{t+1} $$

Essa equa√ß√£o expressa que o retorno total no tempo *t* √© igual √† recompensa recebida imediatamente ($R_{t+1}$) mais o retorno futuro ($G_{t+1}$) descontado pela taxa $\gamma$. Essa rela√ß√£o recursiva √© v√°lida tanto para tarefas epis√≥dicas (com $G_T = 0$ no estado terminal) quanto para tarefas cont√≠nuas e √© fundamental para o desenvolvimento de algoritmos de *dynamic programming* e *temporal-difference learning*.

> üí° **Exemplo Num√©rico:** Imagine um agente que, no instante *t*, recebe uma recompensa $R_{t+1} = 5$. Se o retorno no instante *t+1* √© $G_{t+1} = 8$ e a taxa de desconto √© $\gamma = 0.8$, ent√£o o retorno no instante *t* √©:
>
> $G_t = 5 + 0.8 \cdot 8 = 5 + 6.4 = 11.4$
>
> Isso ilustra como o retorno atual √© uma combina√ß√£o da recompensa imediata e uma previs√£o do retorno futuro.

Para entender melhor essa rela√ß√£o recursiva, podemos expandi-la iterativamente.

**Proposi√ß√£o 2** A rela√ß√£o recursiva $G_t = R_{t+1} + \gamma G_{t+1}$ pode ser expandida para $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{n-1}R_{t+n} + \gamma^n G_{t+n}$ para qualquer $n \geq 1$.

*Prova:* Usaremos indu√ß√£o em $n$.

*   Caso base: $n = 1$. A equa√ß√£o se torna $G_t = R_{t+1} + \gamma G_{t+1}$, que √© a rela√ß√£o recursiva original.
*   Passo indutivo: Assumimos que a equa√ß√£o √© v√°lida para algum $n = k$, ou seja, $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k G_{t+k}$.

    Agora, vamos mostrar que a equa√ß√£o tamb√©m √© v√°lida para $n = k + 1$. Come√ßamos com o lado direito da equa√ß√£o para $n = k + 1$:

    $R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k R_{t+k+1} + \gamma^{k+1} G_{t+k+1}$

    Podemos reescrever isso como:

    $R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots + \gamma^{k-2}R_{t+k} + \gamma^{k-1} R_{t+k+1} + \gamma^{k} G_{t+k+1})$

    Pela rela√ß√£o recursiva original, sabemos que $G_{t+1} = R_{t+2} + \gamma G_{t+2}$. Generalizando, $G_{t+k} = R_{t+k+1} + \gamma G_{t+k+1}$. Substituindo isso na nossa hip√≥tese indutiva:

    $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k (R_{t+k+1} + \gamma G_{t+k+1}) = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k R_{t+k+1} + \gamma^{k+1} G_{t+k+1}$.

    Isso completa o passo indutivo. Portanto, a rela√ß√£o √© v√°lida para todo $n \geq 1$.

*Prova:*
I.  Come√ßamos com a rela√ß√£o recursiva b√°sica:
    $$G_t = R_{t+1} + \gamma G_{t+1}$$
II. Aplicamos a rela√ß√£o recursiva novamente a $G_{t+1}$:
    $$G_{t+1} = R_{t+2} + \gamma G_{t+2}$$
III. Substitu√≠mos a express√£o para $G_{t+1}$ na equa√ß√£o original:
     $$G_t = R_{t+1} + \gamma (R_{t+2} + \gamma G_{t+2}) = R_{t+1} + \gamma R_{t+2} + \gamma^2 G_{t+2}$$
IV. Continuamos expandindo iterativamente:
    $$G_{t+2} = R_{t+3} + \gamma G_{t+3}$$
    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 (R_{t+3} + \gamma G_{t+3}) = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 G_{t+3}$$
V.  Ap√≥s *n* itera√ß√µes, obtemos:
    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{n-1}R_{t+n} + \gamma^n G_{t+n}$$
‚ñ†

> üí° **Exemplo Num√©rico:** Suponha que um agente receba as seguintes recompensas em uma sequ√™ncia de passos: $R_{t+1} = 1$, $R_{t+2} = 2$, $R_{t+3} = -1$. Se $\gamma = 0.5$, podemos calcular $G_t$ expandindo a rela√ß√£o recursiva:
>
> $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 G_{t+3}$
>
> Assumindo que $G_{t+3} = 0$ (por exemplo, o epis√≥dio termina no passo $t+3$), ent√£o:
>
> $G_t = 1 + 0.5 \cdot 2 + 0.5^2 \cdot (-1) + 0.5^3 \cdot 0 = 1 + 1 - 0.25 + 0 = 1.75$

### Conclus√£o

A formaliza√ß√£o do objetivo do agente em termos da maximiza√ß√£o do retorno cumulativo a longo prazo √© um passo fundamental na defini√ß√£o do problema de *reinforcement learning*. A distin√ß√£o entre tarefas epis√≥dicas e cont√≠nuas leva a diferentes defini√ß√µes do retorno, sendo a taxa de desconto $\gamma$ um par√¢metro crucial para controlar a import√¢ncia das recompensas futuras. A rela√ß√£o recursiva entre retornos √© uma propriedade fundamental que ser√° explorada em detalhes nos cap√≠tulos subsequentes [^9].

### Refer√™ncias

[^1]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
[^7]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
[^8]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
[^9]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
<!-- END -->