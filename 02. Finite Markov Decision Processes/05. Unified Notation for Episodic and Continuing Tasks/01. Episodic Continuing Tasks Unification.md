## Unifica√ß√£o da Nota√ß√£o para Tarefas Epis√≥dicas e Cont√≠nuas via Absor√ß√£o

### Introdu√ß√£o
O conceito de **Processos de Decis√£o de Markov (MDPs)** √© fundamental no estudo do aprendizado por refor√ßo, permitindo a formaliza√ß√£o de problemas de tomada de decis√£o sequencial. Uma distin√ß√£o importante dentro dos MDPs √© entre tarefas *epis√≥dicas* e *cont√≠nuas*. Tarefas epis√≥dicas possuem um fim natural, dividindo a intera√ß√£o agente-ambiente em sequ√™ncias independentes, enquanto tarefas cont√≠nuas prosseguem indefinidamente. Este cap√≠tulo visa unificar o tratamento matem√°tico dessas duas categorias, essencial para o desenvolvimento de algoritmos de aprendizado por refor√ßo generaliz√°veis [^1]. A unifica√ß√£o proposta envolve a introdu√ß√£o de um estado *absorvente* especial que permite tratar tarefas epis√≥dicas como um caso particular de tarefas cont√≠nuas, simplificando a nota√ß√£o e facilitando a an√°lise [^11].

### Unifica√ß√£o via Estado Absorvente
Para unificar a nota√ß√£o e o tratamento de tarefas epis√≥dicas e cont√≠nuas, introduzimos o conceito de um **estado absorvente** [^11]. Em tarefas epis√≥dicas, ao final de cada epis√≥dio, o ambiente transita para este estado absorvente especial. Formalmente, este estado, denotado como $S_T$, possui as seguintes propriedades:
1.  Uma vez que o agente entra no estado $S_T$, ele permanece nesse estado para sempre. Ou seja, $p(S_T|S_T, a) = 1$ para qualquer a√ß√£o $a$ [^11].
2.  A recompensa recebida ao permanecer no estado $S_T$ √© sempre zero. Ou seja, $r(s, a, S_T) = 0$ para qualquer estado $s$ e a√ß√£o $a$ [^11].

Com essa conven√ß√£o, podemos tratar as tarefas epis√≥dicas como se fossem cont√≠nuas, pois a transi√ß√£o para o estado absorvente garante que a intera√ß√£o termine, e as recompensas subsequentes sejam zero, n√£o afetando o retorno acumulado.

> üí° **Exemplo Num√©rico:** Imagine um jogo simples onde o agente coleta moedas em um tabuleiro. O jogo termina quando o agente encontra um monstro. Podemos modelar isso com um estado absorvente. Seja $S_T$ o estado "encontrou o monstro". Uma vez que o agente entra nesse estado, ele permanece l√° ($p(S_T|S_T, a) = 1$ para qualquer a√ß√£o) e n√£o recebe mais moedas ($r(s, a, S_T) = 0$). Isso permite que o algoritmo trate esse jogo como uma tarefa cont√≠nua com um estado terminal.



![Diagrama de transi√ß√£o de estados ilustrando um MDP com um estado terminal absorvente.](./../images/image9.png)

**Proposi√ß√£o 1:** *A introdu√ß√£o do estado absorvente n√£o altera a otimalidade da pol√≠tica em tarefas epis√≥dicas.*

*Demonstra√ß√£o:* Seja $\pi^*$ a pol√≠tica √≥tima para a tarefa epis√≥dica original. A introdu√ß√£o do estado absorvente adiciona apenas estados e transi√ß√µes ap√≥s o t√©rmino do epis√≥dio, que por defini√ß√£o, n√£o influenciam o retorno acumulado at√© o final do epis√≥dio. Portanto, seguir $\pi^*$ at√© o estado terminal e ent√£o entrar no estado absorvente resulta no mesmo retorno que seguir $\pi^*$ na tarefa original. Qualquer outra pol√≠tica que desvie de $\pi^*$ antes de atingir o estado terminal ter√° um retorno menor ou igual ao de $\pi^*$, e a pol√≠tica que leva ao estado absorvente tamb√©m n√£o pode melhorar o retorno ap√≥s o t√©rmino do epis√≥dio.

**Prova da Proposi√ß√£o 1:**
I. Seja $\pi^*$ a pol√≠tica √≥tima na tarefa epis√≥dica original, sem o estado absorvente.
II. Seja $V^{\pi^*}(s)$ o valor √≥timo de iniciar no estado $s$ e seguir a pol√≠tica $\pi^*$ at√© o final do epis√≥dio na tarefa original.
III. Com a introdu√ß√£o do estado absorvente $S_T$, ap√≥s o t√©rmino do epis√≥dio (instante $T$), o agente transita para $S_T$ e recebe recompensa 0 para sempre.
IV. Seja $\pi' $ uma pol√≠tica que segue $\pi^*$ at√© o instante $T$ e ent√£o escolhe qualquer a√ß√£o $a$ que leva ao estado $S_T$.
V. O valor de seguir $\pi'$ √© o mesmo que seguir $\pi^*$, pois as recompensas ap√≥s $T$ s√£o todas zero. Portanto, $V^{\pi'}(s) = V^{\pi^*}(s)$.
VI. Qualquer pol√≠tica que se desvie de $\pi^*$ antes de $T$ resultar√° em um valor menor ou igual a $V^{\pi^*}(s)$, pois $\pi^*$ √© √≥tima por defini√ß√£o.
VII. Portanto, a introdu√ß√£o do estado absorvente n√£o altera a otimalidade da pol√≠tica $\pi^*$. ‚ñ†

**Retorno Generalizado:**

Com a introdu√ß√£o do estado absorvente, o retorno $G_t$ pode ser definido de forma unificada para ambos os tipos de tarefas usando a f√≥rmula do retorno descontado [^9]:

$$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

onde:
*   $G_t$ √© o retorno no instante $t$
*   $R_{t+k+1}$ √© a recompensa recebida no instante $t+k+1$
*   $\gamma$ √© a taxa de desconto, com $0 \leq \gamma \leq 1$

Essa formula√ß√£o unificada acomoda ambos os tipos de tarefas da seguinte maneira:

*   **Tarefas Epis√≥dicas:** Em tarefas epis√≥dicas, ap√≥s o agente atingir o estado terminal $S_T$ no instante $T$, todas as recompensas subsequentes $R_{t+k+1}$ ser√£o zero [^8]. Portanto, a soma infinita se torna uma soma finita at√© o instante $T$, resultando no retorno acumulado do epis√≥dio.

*   **Tarefas Cont√≠nuas:** Em tarefas cont√≠nuas, a intera√ß√£o prossegue indefinidamente. Se $\gamma < 1$, a soma infinita pode convergir para um valor finito, mesmo com recompensas n√£o nulas, desde que a sequ√™ncia de recompensas seja limitada. Se $\gamma = 1$ em tarefas cont√≠nuas, √© necess√°rio que as recompensas tendam a zero para garantir que o retorno n√£o seja infinito.

> üí° **Exemplo Num√©rico:** Considere uma tarefa epis√≥dica com 3 passos. As recompensas s√£o $R_1 = 2$, $R_2 = 3$, e $R_3 = 1$, e depois o agente entra no estado absorvente com recompensa 0 para sempre. Se $\gamma = 0.9$, o retorno $G_0$ √© calculado como:
>
> $G_0 = (0.9)^0 \cdot 2 + (0.9)^1 \cdot 3 + (0.9)^2 \cdot 1 + (0.9)^3 \cdot 0 + \ldots = 2 + 2.7 + 0.81 = 5.51$
>
> O estado absorvente garante que os termos subsequentes sejam zero, tornando a soma finita.

**Observa√ß√£o Importante:** √â crucial notar que a taxa de desconto $\gamma$ desempenha um papel fundamental na garantia da converg√™ncia do retorno em tarefas cont√≠nuas [^9]. Se $\gamma$ for muito pr√≥ximo de 1, o agente dar√° grande peso √†s recompensas futuras, o que pode levar a um comportamento inst√°vel.

**Lema 1.1:** *Para uma tarefa cont√≠nua com recompensas limitadas por $R_{max}$ e taxa de desconto $\gamma < 1$, o retorno esperado $G_t$ √© limitado.*

*Demonstra√ß√£o:* O valor absoluto do retorno esperado pode ser limitado da seguinte forma:

$$|G_t| = \left| \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right| \leq \sum_{k=0}^{\infty} \gamma^k |R_{t+k+1}| \leq \sum_{k=0}^{\infty} \gamma^k R_{max} = R_{max} \sum_{k=0}^{\infty} \gamma^k$$

Como $\gamma < 1$, a soma geom√©trica converge para $\frac{1}{1-\gamma}$. Portanto,

$$|G_t| \leq \frac{R_{max}}{1-\gamma}$$

Este limite superior garante que o retorno esperado √© finito.

**Prova do Lema 1.1:**
I. Assumimos que as recompensas s√£o limitadas, $|R_{t+k+1}| \leq R_{max}$ para todo $k$.
II. Usamos a desigualdade triangular: $| \sum_{k=0}^{\infty} a_k | \leq \sum_{k=0}^{\infty} |a_k|$.
III. Substitu√≠mos o limite superior da recompensa:
   $|G_t| = \left| \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right| \leq \sum_{k=0}^{\infty} \gamma^k |R_{t+k+1}| \leq \sum_{k=0}^{\infty} \gamma^k R_{max}$.
IV. Fatoramos $R_{max}$: $\sum_{k=0}^{\infty} \gamma^k R_{max} = R_{max} \sum_{k=0}^{\infty} \gamma^k$.
V. Usamos a f√≥rmula da soma geom√©trica: $\sum_{k=0}^{\infty} \gamma^k = \frac{1}{1-\gamma}$ para $0 \leq \gamma < 1$.
VI. Substitu√≠mos a soma geom√©trica: $R_{max} \sum_{k=0}^{\infty} \gamma^k = R_{max} \cdot \frac{1}{1-\gamma} = \frac{R_{max}}{1-\gamma}$.
VII. Portanto, $|G_t| \leq \frac{R_{max}}{1-\gamma}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Em uma tarefa cont√≠nua, suponha que $R_{max} = 1$ e $\gamma = 0.9$. Ent√£o, o retorno esperado √© limitado por $|G_t| \leq \frac{1}{1-0.9} = \frac{1}{0.1} = 10$. Se $\gamma$ fosse 0.99, o limite seria $|G_t| \leq \frac{1}{1-0.99} = 100$. Isso ilustra como um $\gamma$ mais pr√≥ximo de 1 pode aumentar significativamente o limite superior do retorno, tornando o aprendizado mais sens√≠vel a recompensas futuras.

**Exemplo:**

Considere uma tarefa epis√≥dica simples de navega√ß√£o em um grid, onde o objetivo √© alcan√ßar um estado terminal espec√≠fico. Definimos uma recompensa de +1 ao alcan√ßar o estado terminal e 0 em todos os outros estados. Ao alcan√ßar o estado terminal, o ambiente transita para o estado absorvente, e o agente recebe recompensas subsequentes de 0. O retorno $G_t$ ser√° a soma descontada das recompensas at√© alcan√ßar o estado terminal [^9], ap√≥s o qual n√£o haver√° contribui√ß√µes adicionais.

> üí° **Exemplo Num√©rico:** Imagine um grid 3x3 onde o agente come√ßa no canto superior esquerdo e o objetivo √© alcan√ßar o canto inferior direito. Cada passo tem uma recompensa de -0.1 (para incentivar o agente a ser r√°pido) e alcan√ßar o objetivo d√° +1. Se $\gamma = 0.9$, e o agente leva 6 passos para chegar ao objetivo, o retorno ser√°:
> $G_0 = (-0.1) + 0.9(-0.1) + 0.9^2(-0.1) + 0.9^3(-0.1) + 0.9^4(-0.1) + 0.9^5(-0.1) + 0.9^6(1)$.
> $G_0 = -0.1(1 + 0.9 + 0.81 + 0.729 + 0.6561 + 0.59049) + 0.531441 = -0.468559 + 0.531441 = 0.062882$. Ap√≥s chegar ao objetivo, o agente entra no estado absorvente, e todas as recompensas futuras s√£o 0.

**Teorema 2:** *A fun√ß√£o valor $V(s)$ e a fun√ß√£o Q-valor $Q(s,a)$ podem ser definidas recursivamente utilizando a nota√ß√£o unificada com o estado absorvente.*

*Demonstra√ß√£o:*  A fun√ß√£o valor $V(s)$ representa o valor esperado do retorno ao iniciar no estado $s$ e seguir uma pol√≠tica $\pi$:

$$V(s) = \mathbb{E}_{\pi} [G_t | S_t = s]$$

Usando a defini√ß√£o do retorno $G_t$ e a propriedade de Markov, podemos expressar $V(s)$ recursivamente como:

$$V(s) = \mathbb{E}_{\pi} [R_{t+1} + \gamma G_{t+1} | S_t = s] = \sum_{a} \pi(a|s) \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma V(s')] $$

Da mesma forma, a fun√ß√£o Q-valor $Q(s,a)$ representa o valor esperado do retorno ao iniciar no estado $s$, tomar a a√ß√£o $a$ e seguir uma pol√≠tica $\pi$:

$$Q(s,a) = \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a]$$

Recursivamente, $Q(s,a)$ pode ser expresso como:

$$Q(s,a) = \mathbb{E}_{\pi} [R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] = \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma \sum_{a'} \pi(a'|s') Q(s',a')] $$

Essas equa√ß√µes recursivas s√£o v√°lidas tanto para tarefas epis√≥dicas quanto cont√≠nuas, gra√ßas √† introdu√ß√£o do estado absorvente. No estado absorvente, $V(S_T) = 0$ e $Q(S_T, a) = 0$ para todas as a√ß√µes $a$, o que simplifica os c√°lculos e garante que o retorno seja bem definido para tarefas epis√≥dicas.

**Prova do Teorema 2:**
I. Defini√ß√£o de $V(s)$: $V(s) = \mathbb{E}_{\pi}[G_t | S_t = s]$.
II. Expandindo $G_t$: $V(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]$.
III. Usando a lei da expectativa total: $V(s) = \sum_{a} \pi(a|s) \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$.
IV. Expandindo a expectativa condicional sobre as poss√≠veis transi√ß√µes para o pr√≥ximo estado $s'$ e recompensa $r$: $V(s) = \sum_{a} \pi(a|s) \sum_{s'} \sum_{r} p(s', r | s, a) [r + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s']]$.
V. Substituindo $\mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s']$ por $V(s')$: $V(s) = \sum_{a} \pi(a|s) \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma V(s')]$.
VI. Defini√ß√£o de $Q(s,a)$: $Q(s,a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]$.
VII. Expandindo $G_t$: $Q(s,a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$.
VIII. Expandindo a expectativa condicional sobre as poss√≠veis transi√ß√µes para o pr√≥ximo estado $s'$ e recompensa $r$: $Q(s, a) = \sum_{s'} \sum_{r} p(s', r | s, a) [r + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s']]$.
IX. Usando a lei da expectativa total para expressar $\mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s']$ em termos de $Q(s', a')$: $\mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s'] = \sum_{a'} \pi(a'|s') Q(s', a')$.
X. Substituindo: $Q(s,a) = \sum_{s'} \sum_{r} p(s',r|s,a) [r + \gamma \sum_{a'} \pi(a'|s') Q(s',a')]$.
XI. No estado absorvente $S_T$, $V(S_T) = 0$ e $Q(S_T, a) = 0$ para todas as a√ß√µes $a$, o que simplifica os c√°lculos e garante que o retorno seja bem definido para tarefas epis√≥dicas. Portanto, as equa√ß√µes recursivas s√£o v√°lidas para ambos os tipos de tarefas. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um MDP com dois estados, $S_1$ e $S_2$, e duas a√ß√µes, $a_1$ e $a_2$. As probabilidades de transi√ß√£o e recompensas s√£o:
>
> *   $p(S_2, 1 | S_1, a_1) = 1$, $p(S_1, 0 | S_1, a_2) = 1$
> *   $p(S_T, 0 | S_2, a_1) = 1$, $p(S_T, 0 | S_2, a_2) = 1$  (onde $S_T$ √© o estado absorvente)
>
> Suponha que a pol√≠tica $\pi$ em $S_1$ seja $\pi(a_1|S_1) = 0.6$ e $\pi(a_2|S_1) = 0.4$.  Se $\gamma = 0.9$, podemos calcular $V(S_1)$ usando a equa√ß√£o recursiva:
>
> $V(S_1) = 0.6 \cdot [1 + 0.9 \cdot V(S_2)] + 0.4 \cdot [0 + 0.9 \cdot V(S_1)]$
>
> Como $S_2$ sempre leva ao estado absorvente $S_T$, $V(S_2) = 0$. Ent√£o:
>
> $V(S_1) = 0.6 \cdot [1 + 0] + 0.4 \cdot [0.9 \cdot V(S_1)]$
> $V(S_1) = 0.6 + 0.36 \cdot V(S_1)$
> $0.64 \cdot V(S_1) = 0.6$
> $V(S_1) = \frac{0.6}{0.64} = 0.9375$

### Vantagens da Nota√ß√£o Unificada
A ado√ß√£o desta conven√ß√£o unificada oferece diversas vantagens:

1.  **Simplicidade da Nota√ß√£o:** Elimina a necessidade de tratar tarefas epis√≥dicas e cont√≠nuas com f√≥rmulas diferentes, simplificando a apresenta√ß√£o e an√°lise de algoritmos [^11].
2.  **Generaliza√ß√£o de Algoritmos:** Permite o desenvolvimento de algoritmos que podem ser aplicados a ambos os tipos de tarefas sem modifica√ß√µes significativas [^11].
3.  **Fundamenta√ß√£o Te√≥rica:** Facilita a an√°lise te√≥rica das propriedades de converg√™ncia e otimalidade dos algoritmos [^11].

### Conclus√£o
A unifica√ß√£o da nota√ß√£o para tarefas epis√≥dicas e cont√≠nuas atrav√©s da introdu√ß√£o de um estado absorvente representa uma ferramenta poderosa no estudo do aprendizado por refor√ßo [^11]. Essa abordagem simplifica a formaliza√ß√£o matem√°tica, permitindo o desenvolvimento de algoritmos mais gerais e robustos, e facilita a an√°lise te√≥rica das propriedades desses algoritmos. Ao tratar as tarefas epis√≥dicas como um caso particular de tarefas cont√≠nuas, podemos construir uma base te√≥rica mais coesa e abrangente para o aprendizado por refor√ßo. $\blacksquare$

### Refer√™ncias
[^1]: Chapter 3: Finite Markov Decision Processes
[^8]: Section 3.3 Returns and Episodes
[^9]: Section 3.3. Returns and Episodes
[^11]: Section 3.4 Unified Notation for Episodic and Continuing Tasks
<!-- END -->