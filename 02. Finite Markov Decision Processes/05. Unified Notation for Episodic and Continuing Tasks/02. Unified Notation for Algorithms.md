## Nota√ß√£o Unificada para Tarefas Epis√≥dicas e Cont√≠nuas em MDPs Finitos

### Introdu√ß√£o
O conceito de **Processos de Decis√£o de Markov (MDPs)**, e em particular os MDPs finitos, fornece uma estrutura matem√°tica para modelar tomadas de decis√£o sequenciais em ambientes onde o resultado das a√ß√µes n√£o √© imediato, mas influencia recompensas futuras [^1]. No contexto do aprendizado por refor√ßo (RL), o objetivo √© encontrar uma *policy* que maximize a recompensa acumulada ao longo do tempo [^5]. Uma distin√ß√£o importante surge entre tarefas **epis√≥dicas**, que possuem um ponto final natural, e tarefas **cont√≠nuas**, que prosseguem indefinidamente. Este cap√≠tulo foca em uma nota√ß√£o unificada capaz de expressar ambas as situa√ß√µes de maneira concisa.

### Conceitos Fundamentais

A nota√ß√£o unificada √© essencial para simplificar a an√°lise e o desenvolvimento de algoritmos que podem ser aplicados tanto a tarefas epis√≥dicas quanto a cont√≠nuas. Nas se√ß√µes precedentes, √© descrita a diferen√ßa entre tarefas epis√≥dicas e cont√≠nuas [^3]. Tarefas epis√≥dicas s√£o divididas naturalmente em uma s√©rie de epis√≥dios, cada um consistindo em uma sequ√™ncia finita de passos de tempo. Tarefas cont√≠nuas, por outro lado, n√£o possuem um ponto final inerente.

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

Para unificar a nota√ß√£o, √© necess√°rio revisitar a defini√ß√£o do **retorno** (*return*), denotado por $G_t$ [^3]. Em tarefas epis√≥dicas, o retorno √© definido como a soma das recompensas recebidas ao longo do epis√≥dio:

$$
G_t = R_{t+1} + R_{t+2} + \dots + R_T
$$

onde $T$ √© o passo de tempo final do epis√≥dio. Para tarefas cont√≠nuas, essa formula√ß√£o √© problem√°tica, pois $T = \infty$, e o retorno pode ser infinito. Para lidar com isso, introduz-se o conceito de **desconto** (*discounting*) [^3]. O retorno descontado √© definido como:

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

onde $\gamma$ √© a taxa de desconto, com $0 \leq \gamma \leq 1$. O desconto d√° mais peso √†s recompensas imediatas e menos peso √†s recompensas futuras. Se $\gamma < 1$, a soma infinita pode ser finita, mesmo que as recompensas sejam limitadas.

> üí° **Exemplo Num√©rico:** Considere um agente em um ambiente cont√≠nuo recebendo recompensas $R_{t+1} = 1, R_{t+2} = 2, R_{t+3} = 3,...$. Sem desconto ($\gamma = 1$), o retorno $G_t$ seria infinito. No entanto, com um desconto de $\gamma = 0.9$, o retorno seria:
> $G_t = 1 + 0.9 * 2 + 0.9^2 * 3 + 0.9^3 * 4 + ... \approx 1 + 1.8 + 2.43 + 2.916 + ...$.
> A soma converge para um valor finito, tornando o problema trat√°vel.  Vamos calcular os primeiros 10 termos e ver como a soma parcial se comporta:
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> gamma = 0.9
> rewards = np.arange(1, 11)  # Recompensas de 1 a 10
>
> discounted_rewards = [rewards[k] * (gamma ** k) for k in range(len(rewards))]
> partial_sums = np.cumsum(discounted_rewards)
>
> plt.figure(figsize=(10, 6))
> plt.plot(range(1, len(partial_sums) + 1), partial_sums, marker='o')
> plt.title("Retorno Descontado Parcial ao Longo do Tempo")
> plt.xlabel("Passo de Tempo (k)")
> plt.ylabel("Retorno Descontado Parcial")
> plt.grid(True)
> plt.show()
> ```
> Isso mostra visualmente como o retorno descontado converge quando $\gamma < 1$.

Para unificar as tarefas epis√≥dicas e cont√≠nuas, o texto introduz a conven√ß√£o de tratar o t√©rmino do epis√≥dio como a entrada em um **estado absorvente** especial [^4]. Este estado absorvente transita apenas para si mesmo e gera recompensas zero. Desta forma, uma tarefa epis√≥dica pode ser vista como uma tarefa cont√≠nua que eventualmente entra em um estado absorvente.

> üí° **Exemplo Num√©rico:** Imagine um jogo simples com tr√™s estados: S1, S2 e S3. S3 √© o estado terminal (absorvente). As recompensas s√£o: R(S1) = 0, R(S2) = 1 e R(S3) = 0. Em uma tarefa epis√≥dica, o agente parte de S1 ou S2 e tenta chegar em S3. Em uma tarefa cont√≠nua, ap√≥s atingir S3, o agente permanece l√° indefinidamente, recebendo recompensas zero. Isso unifica a representa√ß√£o.

![Diagrama de transi√ß√£o de estados ilustrando um MDP com um estado terminal absorvente.](./../images/image9.png)

A partir dessa conven√ß√£o, o retorno pode ser definido de forma unificada como:

$$
G_t = \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k
$$

onde tanto $T = \infty$ (tarefas cont√≠nuas) quanto $\gamma = 1$ (tarefas epis√≥dicas sem desconto) s√£o permitidos, mas n√£o ambos simultaneamente [^4].

√â importante ressaltar que a introdu√ß√£o do fator de desconto $\gamma$ tem implica√ß√µes importantes na an√°lise da converg√™ncia de algoritmos de RL. Para complementar a defini√ß√£o do retorno, podemos definir a fun√ß√£o valor.

**Defini√ß√£o:** A **fun√ß√£o valor** $v_{\pi}(s)$ de um estado $s$ sob uma pol√≠tica $\pi$ √© o valor esperado do retorno, come√ßando em $s$ e seguindo a pol√≠tica $\pi$:

$$
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s\right]
$$

> üí° **Exemplo Num√©rico:** Considere um MDP simples com dois estados (A e B) e uma pol√≠tica $\pi$ que sempre move do estado A para o estado B e do estado B para o estado A. Suponha que a recompensa por ir de A para B √© 1 e por ir de B para A √© 0. Se $\gamma = 0.9$, ent√£o:
>
> $v_{\pi}(A) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(B) | S_t = A] = 1 + 0.9 * v_{\pi}(B)$
> $v_{\pi}(B) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(A) | S_t = B] = 0 + 0.9 * v_{\pi}(A)$
>
> Resolvendo este sistema de equa√ß√µes:
> $v_{\pi}(A) = 1 + 0.9 * (0.9 * v_{\pi}(A)) = 1 + 0.81 * v_{\pi}(A)$
> $0.19 * v_{\pi}(A) = 1$
> $v_{\pi}(A) = \frac{1}{0.19} \approx 5.26$
> $v_{\pi}(B) = 0.9 * v_{\pi}(A) = 0.9 * 5.26 \approx 4.74$

Analogamente, podemos definir a fun√ß√£o Q-valor.

**Defini√ß√£o:** A **fun√ß√£o Q-valor** $q_{\pi}(s, a)$ de um par estado-a√ß√£o $(s, a)$ sob uma pol√≠tica $\pi$ √© o valor esperado do retorno, come√ßando em $s$, tomando a a√ß√£o $a$ e seguindo a pol√≠tica $\pi$:

$$
q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a\right]
$$

> üí° **Exemplo Num√©rico:** Considere um MDP com um estado e duas a√ß√µes: A√ß√£o 1 e A√ß√£o 2. A pol√≠tica $\pi$ √© tomar a A√ß√£o 1 com probabilidade 0.6 e a A√ß√£o 2 com probabilidade 0.4. A recompensa para a A√ß√£o 1 √© 2, e para a A√ß√£o 2 √© 1. Se $\gamma = 0.9$:
>
> $q_{\pi}(s, A√ß√£o1) = \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(s, A_{t+1}) | S_t = s, A_t = A√ß√£o1] = 2 + 0.9 * (0.6 * q_{\pi}(s, A√ß√£o1) + 0.4 * q_{\pi}(s, A√ß√£o2))$
> $q_{\pi}(s, A√ß√£o2) = \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(s, A_{t+1}) | S_t = s, A_t = A√ß√£o2] = 1 + 0.9 * (0.6 * q_{\pi}(s, A√ß√£o1) + 0.4 * q_{\pi}(s, A√ß√£o2))$
>
> Resolvendo este sistema de equa√ß√µes lineares:
> Seja $q_1 = q_{\pi}(s, A√ß√£o1)$ e $q_2 = q_{\pi}(s, A√ß√£o2)$.
> $q_1 = 2 + 0.54q_1 + 0.36q_2$
> $q_2 = 1 + 0.54q_1 + 0.36q_2$
> $0.46q_1 - 0.36q_2 = 2$
> $-0.54q_1 + 0.64q_2 = 1$
> Resolvendo o sistema:
> ```python
> import numpy as np
>
> A = np.array([[0.46, -0.36], [-0.54, 0.64]])
> b = np.array([2, 1])
> q = np.linalg.solve(A, b)
> print(f"q(s, A√ß√£o1) = {q[0]:.2f}")
> print(f"q(s, A√ß√£o2) = {q[1]:.2f}")
> ```
> Portanto, $q_{\pi}(s, A√ß√£o1) \approx 7.43$ e $q_{\pi}(s, A√ß√£o2) \approx 7.07$.

Estas defini√ß√µes s√£o v√°lidas tanto para tarefas epis√≥dicas (considerando o estado absorvente) quanto para tarefas cont√≠nuas. A seguir, podemos derivar a equa√ß√£o de Bellman para a fun√ß√£o valor.

**Teorema 1 (Equa√ß√£o de Bellman para $v_{\pi}$):** A fun√ß√£o valor $v_{\pi}$ satisfaz a seguinte equa√ß√£o recursiva:

$$
v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]
$$

*Prova:*
Come√ßando pela defini√ß√£o de $v_{\pi}(s)$:

$$
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s\right]
$$

I. Podemos separar o primeiro termo da soma:
   $$
   v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} | S_t = s] + \mathbb{E}_{\pi}\left[\sum_{k=1}^{\infty} \gamma^k R_{t+k+1} | S_t = s\right]
   $$

II. Agora, fatoramos $\gamma$ da soma restante:
   $$
   v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} | S_t = s] + \mathbb{E}_{\pi}\left[\gamma \sum_{k=1}^{\infty} \gamma^{k-1} R_{t+k+1} | S_t = s\right]
   $$

III. Reconhecemos que $\sum_{k=1}^{\infty} \gamma^{k-1} R_{t+k+1}$ √© o retorno $G_{t+1}$ a partir do estado $S_{t+1}$ no tempo $t+1$:
   $$
   v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} | S_t = s] + \mathbb{E}_{\pi}[\gamma G_{t+1} | S_t = s]
   $$

IV. Pela defini√ß√£o da fun√ß√£o valor $v_{\pi}(S_{t+1}) = \mathbb{E}_{\pi}[G_{t+1} | S_{t+1}]$, substitu√≠mos $G_{t+1}$:
   $$
   v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} | S_t = s] + \mathbb{E}_{\pi}[\gamma v_{\pi}(S_{t+1}) | S_t = s]
   $$

V. Combinando os termos de expectativa, obtemos a equa√ß√£o de Bellman para $v_{\pi}$:
   $$
   v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]
   $$
‚ñ†

De forma an√°loga, √© poss√≠vel obter a equa√ß√£o de Bellman para $q_{\pi}$.

**Teorema 2 (Equa√ß√£o de Bellman para $q_{\pi}$):** A fun√ß√£o Q-valor $q_{\pi}$ satisfaz a seguinte equa√ß√£o recursiva:

$$
q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]
$$

*Prova:*
Come√ßando pela defini√ß√£o de $q_{\pi}(s, a)$:
$$
q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a\right]
$$

I. Podemos separar o primeiro termo da soma:
   $$
   q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} | S_t = s, A_t = a] + \mathbb{E}_{\pi}\left[\sum_{k=1}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a\right]
   $$

II. Agora, fatoramos $\gamma$ da soma restante:
   $$
   q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} | S_t = s, A_t = a] + \mathbb{E}_{\pi}\left[\gamma \sum_{k=1}^{\infty} \gamma^{k-1} R_{t+k+1} | S_t = s, A_t = a\right]
   $$

III. Reconhecemos que $\sum_{k=1}^{\infty} \gamma^{k-1} R_{t+k+1}$ √© o retorno $G_{t+1}$ a partir do estado $S_{t+1}$ e a√ß√£o $A_{t+1}$ no tempo $t+1$:
   $$
   q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} | S_t = s, A_t = a] + \mathbb{E}_{\pi}[\gamma G_{t+1} | S_t = s, A_t = a]
   $$

IV. Pela defini√ß√£o da fun√ß√£o Q-valor $q_{\pi}(S_{t+1}, A_{t+1}) = \mathbb{E}_{\pi}[G_{t+1} | S_{t+1}, A_{t+1}]$, substitu√≠mos $G_{t+1}$:
   $$
   q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} | S_t = s, A_t = a] + \mathbb{E}_{\pi}[\gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]
   $$

V. Combinando os termos de expectativa, obtemos a equa√ß√£o de Bellman para $q_{\pi}$:
   $$
   q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]
   $$
‚ñ†

### Conclus√£o

A nota√ß√£o unificada apresentada neste cap√≠tulo simplifica a express√£o de algoritmos de aprendizado por refor√ßo, permitindo que sejam aplicados tanto a tarefas epis√≥dicas quanto a tarefas cont√≠nuas com pequenas modifica√ß√µes [^4]. Ao tratar o t√©rmino do epis√≥dio como a entrada em um estado absorvente, as tarefas epis√≥dicas podem ser expressas usando a mesma estrutura matem√°tica das tarefas cont√≠nuas. Isso facilita a an√°lise, o desenvolvimento e a implementa√ß√£o de algoritmos de aprendizado por refor√ßo. As defini√ß√µes das fun√ß√µes valor e Q-valor, juntamente com as equa√ß√µes de Bellman, fornecem ferramentas adicionais para a an√°lise e o projeto de algoritmos de RL.

### Refer√™ncias
[^1]: Chapter 3: Finite Markov Decision Processes.
[^2]: Section 3.1: The Agent-Environment Interface.
[^3]: Section 3.3: Returns and Episodes.
[^4]: Section 3.4: Unified Notation for Episodic and Continuing Tasks.
[^5]: Section 3.8: Summary.
<!-- END -->