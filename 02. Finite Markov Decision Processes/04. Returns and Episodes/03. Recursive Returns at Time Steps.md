## Retornos Recursivos e a Equa√ß√£o de Bellman

### Introdu√ß√£o
A compreens√£o da rela√ß√£o recursiva entre retornos em sucessivos passos de tempo √© fundamental para a teoria e os algoritmos de *reinforcement learning*. Esta se√ß√£o explora essa rela√ß√£o, que leva diretamente √† *Bellman equation*, uma ferramenta essencial para a an√°lise e solu√ß√£o de *Markov decision processes (MDPs)*. o objetivo do agente √© maximizar o retorno cumulativo [^53].

### Conceitos Fundamentais

A rela√ß√£o recursiva entre retornos em passos de tempo sucessivos √© expressa pela equa√ß√£o [^55]:

$$
G_t = R_{t+1} + \gamma G_{t+1}
$$

onde $G_t$ √© o retorno no tempo *t*, $R_{t+1}$ √© a recompensa recebida no tempo *t+1*, e $\gamma$ √© a taxa de desconto, com $0 \leq \gamma \leq 1$. Essa equa√ß√£o demonstra que o retorno no tempo *t* √© a soma da recompensa imediata $R_{t+1}$ e o retorno descontado no pr√≥ximo passo de tempo, $\gamma G_{t+1}$.

> üí° **Exemplo Num√©rico:**
>
> Suponha que um agente esteja em um estado no tempo $t$. Ele recebe uma recompensa $R_{t+1} = 10$ no tempo $t+1$. Se a taxa de desconto $\gamma = 0.9$ e o retorno no tempo $t+1$ √© $G_{t+1} = 50$, ent√£o o retorno no tempo $t$ √©:
>
> $G_t = R_{t+1} + \gamma G_{t+1} = 10 + 0.9 \times 50 = 10 + 45 = 55$
>
> Isso significa que o retorno total esperado a partir do tempo $t$ √© 55, considerando a recompensa imediata e o retorno futuro descontado.

Essa rela√ß√£o √© crucial por v√°rias raz√µes:

1.  **Efici√™ncia Computacional:** A recurs√£o permite o c√°lculo eficiente dos retornos, evitando a necessidade de somar sequ√™ncias infinitas de recompensas diretamente [^55].
2.  **Base para Algoritmos:** Muitos algoritmos de *reinforcement learning*, como *dynamic programming* e *temporal-difference learning*, exploram essa recurs√£o para atualizar estimativas de valor e aprender pol√≠ticas √≥timas [^59, 60].
3.  **Fundamento Te√≥rico:** A rela√ß√£o recursiva √© a base da *Bellman equation*, que expressa a rela√ß√£o de consist√™ncia entre o valor de um estado e os valores de seus estados sucessores [^60, 63].

Al√©m dessas raz√µes, √© importante notar que a escolha da taxa de desconto $\gamma$ impacta significativamente o comportamento do agente. Valores de $\gamma$ pr√≥ximos de 0 tornam o agente m√≠ope, priorizando recompensas imediatas, enquanto valores pr√≥ximos de 1 fazem com que o agente considere recompensas futuras com quase o mesmo peso das recompensas imediatas, levando a um planejamento de longo prazo mais abrangente.

> üí° **Exemplo Num√©rico:**
>
> Considere dois cen√°rios:
>
> *   Cen√°rio 1: $\gamma = 0.1$. O agente recebe recompensas de 10, 20 e 30 em tr√™s passos de tempo subsequentes. O retorno no tempo $t=0$ seria $G_0 = 10 + 0.1 \times 20 + 0.1^2 \times 30 = 10 + 2 + 0.3 = 12.3$. O agente prioriza a recompensa imediata de 10.
> *   Cen√°rio 2: $\gamma = 0.9$. Com as mesmas recompensas, o retorno no tempo $t=0$ seria $G_0 = 10 + 0.9 \times 20 + 0.9^2 \times 30 = 10 + 18 + 24.3 = 52.3$. O agente considera fortemente as recompensas futuras.
>
> Isso demonstra como $\gamma$ influencia a import√¢ncia relativa das recompensas futuras em rela√ß√£o √†s recompensas imediatas.

A *Bellman equation* para a *state-value function* $v_\pi(s)$ sob uma pol√≠tica $\pi$ √© dada por [^63]:

$$
v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s]
$$

Essa equa√ß√£o pode ser expandida para:

$$
v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) \sum_{s',r} p(s', r|s, a) [r + \gamma v_\pi(s')]
$$

onde:

*   $A(s)$ √© o conjunto de a√ß√µes dispon√≠veis no estado *s*.
*   $\pi(a|s)$ √© a probabilidade de selecionar a a√ß√£o *a* no estado *s* sob a pol√≠tica $\pi$.
*   $p(s', r|s, a)$ √© a probabilidade de transi√ß√£o para o estado *s'* com recompensa *r*, dado o estado *s* e a√ß√£o *a*.

A *Bellman equation* expressa que o valor de um estado √© a soma ponderada dos retornos esperados para cada poss√≠vel a√ß√£o e estado sucessor [^63].  Essa equa√ß√£o √© fundamental para computar, aproximar e aprender $v_\pi$ [^60].  Ela estabelece uma condi√ß√£o de consist√™ncia entre o valor de um estado e o valor de seus poss√≠veis sucessores.  A solu√ß√£o √∫nica para a *Bellman equation* √© a *state-value function* $v_\pi$ [^60].

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP simples com dois estados ($s_1$ e $s_2$) e duas a√ß√µes ($a_1$ e $a_2$) em cada estado. Suponha que a pol√≠tica $\pi$ seja tal que $\pi(a_1|s_1) = 0.6$ e $\pi(a_2|s_1) = 0.4$. As probabilidades de transi√ß√£o e recompensas s√£o:
>
> *   $p(s_2, 5|s_1, a_1) = 1.0$ (Ao tomar a a√ß√£o $a_1$ em $s_1$, o agente vai para $s_2$ e recebe uma recompensa de 5)
> *   $p(s_1, 2|s_1, a_2) = 1.0$ (Ao tomar a a√ß√£o $a_2$ em $s_1$, o agente permanece em $s_1$ e recebe uma recompensa de 2)
>
> Se $\gamma = 0.8$ e $v_\pi(s_2) = 10$, podemos calcular $v_\pi(s_1)$ usando a *Bellman equation*:
>
> $v_\pi(s_1) = \pi(a_1|s_1) \sum_{s',r} p(s', r|s_1, a_1) [r + \gamma v_\pi(s')] + \pi(a_2|s_1) \sum_{s',r} p(s', r|s_1, a_2) [r + \gamma v_\pi(s')]$
>
> $v_\pi(s_1) = 0.6 \times (5 + 0.8 \times 10) + 0.4 \times (2 + 0.8 \times v_\pi(s_1))$
>
> $v_\pi(s_1) = 0.6 \times (5 + 8) + 0.4 \times (2 + 0.8 \times v_\pi(s_1))$
>
> $v_\pi(s_1) = 0.6 \times 13 + 0.4 \times (2 + 0.8 \times v_\pi(s_1))$
>
> $v_\pi(s_1) = 7.8 + 0.8 + 0.32 \times v_\pi(s_1)$
>
> $0.68 \times v_\pi(s_1) = 8.6$
>
> $v_\pi(s_1) = \frac{8.6}{0.68} \approx 12.65$
>
> Portanto, o valor do estado $s_1$ sob a pol√≠tica $\pi$ √© aproximadamente 12.65.

**Lema 1:** (Unicidade da Solu√ß√£o) A *Bellman equation* para $v_\pi$ tem uma √∫nica solu√ß√£o.

*Proof:* Podemos reescrever a Bellman equation na forma de um sistema de equa√ß√µes lineares. Seja **v** o vetor dos valores de estado, **R** o vetor das recompensas esperadas e **P** a matriz de transi√ß√£o ponderada pelas probabilidades de a√ß√£o sob a pol√≠tica $\pi$. A equa√ß√£o pode ser escrita como:

$$
\mathbf{v} = \mathbf{R} + \gamma \mathbf{P} \mathbf{v}
$$

Resolvendo para **v**, obtemos:

$$
\mathbf{v} = (\mathbf{I} - \gamma \mathbf{P})^{-1} \mathbf{R}
$$

Como $\gamma < 1$, a matriz $(\mathbf{I} - \gamma \mathbf{P})$ √© sempre invert√≠vel, garantindo a unicidade da solu√ß√£o para **v**.

Prova:

I.  Come√ßamos com a *Bellman equation* na forma de matriz:
    $$\mathbf{v} = \mathbf{R} + \gamma \mathbf{P} \mathbf{v}$$

II. Rearranjamos a equa√ß√£o para isolar $\mathbf{v}$:
    $$\mathbf{v} - \gamma \mathbf{P} \mathbf{v} = \mathbf{R}$$

III. Fatoramos $\mathbf{v}$ do lado esquerdo:
    $$(\mathbf{I} - \gamma \mathbf{P}) \mathbf{v} = \mathbf{R}$$
    onde $\mathbf{I}$ √© a matriz identidade.

IV. Multiplicamos ambos os lados pela inversa de $(\mathbf{I} - \gamma \mathbf{P})$, assumindo que ela exista:
    $$(\mathbf{I} - \gamma \mathbf{P})^{-1} (\mathbf{I} - \gamma \mathbf{P}) \mathbf{v} = (\mathbf{I} - \gamma \mathbf{P})^{-1} \mathbf{R}$$

V. Simplificamos o lado esquerdo:
    $$\mathbf{v} = (\mathbf{I} - \gamma \mathbf{P})^{-1} \mathbf{R}$$

VI. Para garantir que $(\mathbf{I} - \gamma \mathbf{P})^{-1}$ exista, precisamos mostrar que $(\mathbf{I} - \gamma \mathbf{P})$ √© invert√≠vel. Isso √© verdade se o determinante de $(\mathbf{I} - \gamma \mathbf{P})$ n√£o for zero. Como $\gamma < 1$, todos os autovalores de $\gamma \mathbf{P}$ s√£o menores que 1 em magnitude, o que garante que $(\mathbf{I} - \gamma \mathbf{P})$ √© invert√≠vel.

VII. Portanto, existe uma √∫nica solu√ß√£o para $\mathbf{v}$, dada por:
     $$\mathbf{v} = (\mathbf{I} - \gamma \mathbf{P})^{-1} \mathbf{R}$$ ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP com 3 estados.  A matriz de transi√ß√£o $\mathbf{P}$ e o vetor de recompensa $\mathbf{R}$ s√£o:
>
> $\mathbf{P} = \begin{bmatrix} 0.5 & 0.3 & 0.2 \\ 0.1 & 0.6 & 0.3 \\ 0.4 & 0.2 & 0.4 \end{bmatrix}$
>
> $\mathbf{R} = \begin{bmatrix} 10 \\ 5 \\ 2 \end{bmatrix}$
>
> Seja $\gamma = 0.9$.  Ent√£o, para encontrar o vetor de valor $\mathbf{v}$, precisamos calcular $(\mathbf{I} - \gamma \mathbf{P})^{-1}\mathbf{R}$:
>
> $\mathbf{I} - \gamma \mathbf{P} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} - 0.9 \times \begin{bmatrix} 0.5 & 0.3 & 0.2 \\ 0.1 & 0.6 & 0.3 \\ 0.4 & 0.2 & 0.4 \end{bmatrix} = \begin{bmatrix} 0.55 & -0.27 & -0.18 \\ -0.09 & 0.46 & -0.27 \\ -0.36 & -0.18 & 0.64 \end{bmatrix}$
>
> Usando Python com NumPy para calcular a inversa e o valor:
> ```python
> import numpy as np
>
> P = np.array([[0.5, 0.3, 0.2],
>               [0.1, 0.6, 0.3],
>               [0.4, 0.2, 0.4]])
> R = np.array([10, 5, 2])
> gamma = 0.9
>
> I = np.identity(3)
> A = I - gamma * P
> A_inv = np.linalg.inv(A)
> v = np.dot(A_inv, R)
>
> print("Vetor de valor v:", v)
> ```
>
> Este c√≥digo calcula a inversa da matriz $(\mathbf{I} - \gamma \mathbf{P})$ e multiplica pelo vetor de recompensas $\mathbf{R}$ para obter o vetor de valor $\mathbf{v}$.  A sa√≠da seria algo pr√≥ximo a `[34.79, 22.83, 23.15]`, que representa os valores dos estados 1, 2 e 3, respectivamente.
>
> Isso demonstra numericamente a unicidade da solu√ß√£o para a *Bellman equation*.

**Backup Diagrams:**

Os *backup diagrams* s√£o representa√ß√µes gr√°ficas da *Bellman equation*. Eles ilustram as rela√ß√µes que formam a base das opera√ß√µes de atualiza√ß√£o nos algoritmos de *reinforcement learning* [^60, 63]. Um *backup diagram* para $v_\pi(s)$ mostra que, a partir do estado *s*, o agente pode tomar diferentes a√ß√µes de acordo com a pol√≠tica $\pi$, e o ambiente responde com um novo estado *s'* e uma recompensa *r*, de acordo com a din√¢mica *p*. A *Bellman equation* calcula a m√©dia sobre todas essas possibilidades [^63].

```mermaid
graph LR
    A[s] --> B{"a ~ œÄ(a|s)"}
    B --> C("(s', r) ~ p(s', r|s, a)")
    C --> D["r + Œ≥v(s')"]
    style A fill:#f9f,stroke:#333,stroke-width:2px
```



![Diagrama de backup para a fun√ß√£o de valor \(v_\pi\), ilustrando a rela√ß√£o entre um estado e seus sucessores sob uma pol√≠tica \(\pi\).](./../images/image3.png)

> üí° **Exemplo Num√©rico:**
>
> O diagrama acima representa a atualiza√ß√£o do valor de um estado *s*. A partir de *s*, a a√ß√£o *a* √© selecionada de acordo com a pol√≠tica $\pi(a|s)$. O ambiente ent√£o transita para um novo estado *s'* com recompensa *r* de acordo com a din√¢mica $p(s', r|s, a)$. A atualiza√ß√£o do valor de *s* considera todas as a√ß√µes poss√≠veis e as transi√ß√µes resultantes, ponderadas por suas probabilidades.
> Imagine que, no estado *s*, temos duas a√ß√µes poss√≠veis: $a_1$ e $a_2$, com $\pi(a_1|s)=0.6$ e $\pi(a_2|s)=0.4$.
> Se tomarmos a a√ß√£o $a_1$, vamos para o estado $s_1'$ com recompensa $r_1=5$.
> Se tomarmos a a√ß√£o $a_2$, vamos para o estado $s_2'$ com recompensa $r_2=2$.
> Assumindo $\gamma = 0.9$, $v(s_1') = 10$, $v(s_2')=5$, o valor de *s* seria
> $v(s) = 0.6 * (5 + 0.9 * 10) + 0.4 * (2 + 0.9*5) = 0.6 * 14 + 0.4 * 6.5 = 8.4 + 2.6 = 11$.

**A Bellman Optimality Equation:**

A *Bellman optimality equation* expressa a rela√ß√£o que *value functions* √≥timas devem satisfazer. A *state-value function* √≥tima $v_*(s)$ √© definida como [^61]:

$$
v_*(s) = \max_\pi v_\pi(s)
$$

A *Bellman optimality equation* para $v_*(s)$ √© [^63, 64]:

$$
v_*(s) = \max_a \sum_{s',r} p(s', r|s, a) [r + \gamma v_*(s')]
$$

Essa equa√ß√£o estabelece que o valor de um estado sob uma pol√≠tica √≥tima √© o valor esperado do melhor poss√≠vel *imediato* recompensa mais o valor *descontado* do pr√≥ximo estado, assumindo que a partir desse ponto em diante a√ß√µes √≥timas s√£o tomadas. Um agente que se comporta *greedy* com respeito a $v_*$ √© otimamente garantido para atingir o melhor retorno a longo prazo.

> üí° **Exemplo Num√©rico:**
>
> Considere um estado *s* com duas a√ß√µes poss√≠veis, $a_1$ e $a_2$. As probabilidades de transi√ß√£o e recompensas s√£o:
>
> *   A√ß√£o $a_1$: $p(s_1', 5|s, a_1) = 0.7$ e $p(s_2', 2|s, a_1) = 0.3$
> *   A√ß√£o $a_2$: $p(s_1', 1|s, a_2) = 0.2$ e $p(s_2', 8|s, a_2) = 0.8$
>
> Se $\gamma = 0.9$, $v_*(s_1') = 20$ e $v_*(s_2') = 15$, podemos calcular o valor √≥timo de *s* usando a *Bellman optimality equation*:
>
> $v_*(s) = \max_a \sum_{s',r} p(s', r|s, a) [r + \gamma v_*(s')]
$$
Para a a√ß√£o $a_1$:
$$
\sum_{s',r} p(s', r|s, a_1) [r + \gamma v_*(s')] = 0.7 \times (5 + 0.9 \times 20) + 0.3 \times (2 + 0.9 \times 15) = 0.7 \times (5 + 18) + 0.3 \times (2 + 13.5) = 0.7 \times 23 + 0.3 \times 15.5 = 16.1 + 4.65 = 20.75
$$
Para a a√ß√£o $a_2$:
$$
\sum_{s',r} p(s', r|s, a_2) [r + \gamma v_*(s')] = 0.2 \times (1 + 0.9 \times 20) + 0.8 \times (8 + 0.9 \times 15) = 0.2 \times (1 + 18) + 0.8 \times (8 + 13.5) = 0.2 \times 19 + 0.8 \times 21.5 = 3.8 + 17.2 = 21
$$
$v_*(s) = \max(20.75, 21) = 21$

A a√ß√£o √≥tima √© $a_2$, e o valor √≥timo do estado *s* √© 21.

A *action-value function* √≥tima $q_*(s, a)$ √© definida como [^63]:
$$
q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a]
$$
e a *Bellman optimality equation* para $q_*(s, a)$ √© [^64]:
$$
q_*(s, a) = \sum_{s',r} p(s', r|s, a) [r + \gamma \max_{a'} q_*(s', a')]
$$


![Backup diagrams illustrating the update rules for state-value \(v_*\) and action-value \(q_*\) functions in MDPs.](./../images/image1.png)

Uma vez que $q_*(s, a)$ √© conhecida, uma pol√≠tica √≥tima pode ser facilmente obtida selecionando a a√ß√£o que maximiza $q_*(s, a)$ para cada estado *s*. Formalmente, a pol√≠tica √≥tima $\pi_*(s)$ √© dada por:
$$
\pi_*(s) = \arg\max_a q_*(s, a)
$$
> üí° **Exemplo Num√©rico:**
>
> Considere um estado *s* com duas a√ß√µes, *a1* e *a2*. Ap√≥s tomar a a√ß√£o *a1* no estado *s*, o agente transita para o estado *s'* com probabilidade 1 e recebe uma recompensa de 5. Ap√≥s tomar a a√ß√£o *a2* no estado *s*, o agente transita para o estado *s"* com probabilidade 1 e recebe uma recompensa de 10.
>
> Assumindo $\gamma = 0.9$, $q_*(s', a') = 20$ para toda a√ß√£o *a'* em *s'*, e $q_*(s", a") = 15$ para toda a√ß√£o *a"* em *s"*.
>
> $q_*(s, a_1) = 5 + 0.9 * 20 = 5 + 18 = 23$
>
> $q_*(s, a_2) = 10 + 0.9 * 15 = 10 + 13.5 = 23.5$
>
> Ent√£o, a pol√≠tica √≥tima $\pi_*(s) = \arg\max_a q_*(s, a) = a_2$. Isso significa que no estado *s*, a a√ß√£o √≥tima √© *a2* pois maximiza o valor esperado a longo prazo.

**Teorema 1:** (Exist√™ncia e Unicidade da $q_*$) Existe uma √∫nica fun√ß√£o $q_*(s, a)$ que satisfaz a *Bellman optimality equation* para $q_*(s, a)$.

*Proof:* A prova segue uma linha similar √† prova da unicidade da solu√ß√£o para $v_\pi$. A *Bellman optimality equation* para $q_*(s, a)$ pode ser vista como um operador de Bellman $T$ aplicado √† fun√ß√£o $q$, ou seja, $q_* = Tq_*$. Pode-se mostrar que o operador de Bellman √© uma contra√ß√£o em um espa√ßo de Banach, e, portanto, pelo teorema do ponto fixo de Banach, existe um √∫nico ponto fixo, que √© a solu√ß√£o $q_*(s, a)$.

Prova:

I. Considere o operador de Bellman $T$ definido como:
$$
   (Tq)(s, a) = \sum_{s', r} p(s', r | s, a) \left[r + \gamma \max_{a'} q(s', a')\right]
$$
   Este operador mapeia uma fun√ß√£o $q$ para outra fun√ß√£o $Tq$.

II. Para mostrar a unicidade, vamos provar que $T$ √© uma contra√ß√£o em um espa√ßo de Banach. Seja $q$ e $q'$ duas fun√ß√µes arbitr√°rias. Queremos mostrar que existe um $\alpha \in [0, 1)$ tal que:
$$
    ||Tq - Tq'||_\infty \leq \alpha ||q - q'||_\infty
    $$
    onde $|| \cdot ||_\infty$ √© a norma suprema, definida como $||q||_\infty = \max_{s, a} |q(s, a)|$.

III. Considere a diferen√ßa entre $Tq$ e $Tq'$:
     $$
     |(Tq)(s, a) - (Tq')(s, a)| = \left| \sum_{s', r} p(s', r | s, a) \left[r + \gamma \max_{a'} q(s', a')\right] - \sum_{s', r} p(s', r | s, a) \left[r + \gamma \max_{a'} q'(s', a')\right] \right|
     $$

IV. Simplificando a express√£o:
    $$
    |(Tq)(s, a) - (Tq')(s, a)| = \left| \sum_{s', r} p(s', r | s, a) \gamma \left[ \max_{a'} q(s', a') - \max_{a'} q'(s', a') \right] \right|
    $$

V. Usando a desigualdade do tri√¢ngulo e o fato de que $\sum_{s', r} p(s', r | s, a) = 1$:
$$
   |(Tq)(s, a) - (Tq')(s, a)| \leq \gamma \sum_{s', r} p(s', r | s, a) \left| \max_{a'} q(s', a') - \max_{a'} q'(s', a') \right| \leq \gamma \max_{s', a'} |q(s', a') - q'(s', a')|
$$

VI. Portanto:
    $$
    |(Tq)(s, a) - (Tq')(s, a)| \leq \gamma ||q - q'||_\infty
    $$

VII. Tomando o supremo sobre todos os estados e a√ß√µes:
     $$
     ||Tq - Tq'||_\infty = \max_{s, a} |(Tq)(s, a) - (Tq')(s, a)| \leq \gamma ||q - q'||_\infty
     $$

VIII. Como $\gamma \in [0, 1)$, $T$ √© uma contra√ß√£o. Pelo teorema do ponto fixo de Banach, existe um √∫nico ponto fixo $q_*$ tal que $Tq_* = q_*$. Portanto, existe uma √∫nica solu√ß√£o $q_*(s, a)$ para a *Bellman optimality equation*. ‚ñ†

### Conclus√£o

A rela√ß√£o recursiva $G_t = R_{t+1} + \gamma G_{t+1}$ √© uma pedra angular do *reinforcement learning*. Ela leva √† *Bellman equation*, que fornece uma base para calcular, aproximar e aprender *value functions* √≥timas.  A explora√ß√£o dessas equa√ß√µes e suas representa√ß√µes gr√°ficas, como os *backup diagrams*, √© essencial para a compreens√£o e implementa√ß√£o de algoritmos eficazes de *reinforcement learning*. $\blacksquare$

### Refer√™ncias

[^53]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, 2018, p. 53.
[^55]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, 2018, p. 55.
[^59]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, 2018, p. 59.
[^60]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, 2018, p. 60.
[^61]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, 2018, p. 61.
[^63]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, 2018, p. 63.
[^64]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd ed. Cambridge, MA: MIT Press, 2018, p. 64.
<!-- END -->