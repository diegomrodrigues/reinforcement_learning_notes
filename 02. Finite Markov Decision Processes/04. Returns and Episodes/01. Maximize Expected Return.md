## Retornos e Epis√≥dios em Processos de Decis√£o de Markov Finitos

### Introdu√ß√£o

Em um contexto de **aprendizado por refor√ßo**, o objetivo central √© formalizar a no√ß√£o de aprendizado em longo prazo. Assim, torna-se crucial definir precisamente o que o agente busca maximizar. A seguir, analisaremos formalmente como o objetivo de maximizar a recompensa cumulativa √© alcan√ßado e exploraremos os conceitos de retornos e epis√≥dios, cruciais na teoria de **Processos de Decis√£o de Markov Finitos (MDPs)** [^1].

### Conceitos Fundamentais

At√© agora, discutimos informalmente o objetivo do aprendizado. Mencionamos que o objetivo do agente √© maximizar a recompensa cumulativa que recebe a longo prazo. Se a sequ√™ncia de recompensas recebidas ap√≥s o passo de tempo *t* √© denotada por $R_{t+1}, R_{t+2}, R_{t+3},...$, ent√£o qual aspecto preciso dessa sequ√™ncia desejamos maximizar? Em geral, procuramos maximizar o **retorno esperado**, onde o retorno, denotado por $G_t$, √© definido como alguma fun√ß√£o espec√≠fica da sequ√™ncia de recompensas [^5].

No caso mais simples, o retorno √© a soma das recompensas:

$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \ldots + R_T,$$

onde *T* √© um passo de tempo final [^3]. Essa abordagem faz sentido em aplica√ß√µes onde existe uma no√ß√£o natural de passo de tempo final, ou seja, quando a intera√ß√£o agente-ambiente se divide naturalmente em sub-sequ√™ncias, que chamamos de **epis√≥dios**, como jogadas de um jogo, viagens por um labirinto ou qualquer tipo de intera√ß√£o repetida. Cada epis√≥dio termina em um estado especial chamado **estado terminal**, seguido por uma redefini√ß√£o para um estado inicial padr√£o ou para uma amostra de uma distribui√ß√£o padr√£o de estados iniciais [^7]. Mesmo se voc√™ pensar em epis√≥dios terminando de diferentes maneiras, como ganhar e perder um jogo, o pr√≥ximo epis√≥dio come√ßa independentemente de como o anterior terminou. Assim, os epis√≥dios podem todos ser considerados como terminando no mesmo estado terminal, com diferentes recompensas para os diferentes resultados. Tarefas com epis√≥dios desse tipo s√£o chamadas de **tarefas epis√≥dicas** [^7]. Em tarefas epis√≥dicas, √†s vezes precisamos distinguir o conjunto de todos os estados n√£o-terminais, denotados por $\mathcal{S}$, do conjunto de todos os estados mais o estado terminal, denotados por $\mathcal{S}^+$. O tempo de termina√ß√£o, *T*, √© uma vari√°vel aleat√≥ria que normalmente varia de epis√≥dio para epis√≥dio.

> üí° **Exemplo Num√©rico:** Considere um agente aprendendo a jogar um jogo simples, como Tic-Tac-Toe. Cada jogo √© um epis√≥dio. O agente recebe uma recompensa de +1 se ganhar, -1 se perder e 0 se empatar.  *T* seria o n√∫mero de movimentos at√© o final do jogo (vit√≥ria, derrota ou empate). Um poss√≠vel epis√≥dio poderia ter as recompensas: `[0, 0, 0, 0, 1]` se o agente ganhou no 5¬∫ movimento. Nesse caso, $G_0 = 0 + 0 + 0 + 0 + 1 = 1$.

Por outro lado, em muitos casos, a intera√ß√£o agente-ambiente n√£o se divide naturalmente em epis√≥dios identific√°veis, mas continua continuamente sem limite. Por exemplo, essa seria a maneira natural de formular uma tarefa cont√≠nua de controle de processo ou uma aplica√ß√£o para um rob√¥ com um longo tempo de vida. Chamamos essas de **tarefas cont√≠nuas**. A formula√ß√£o de retorno (3.7) √© problem√°tica para tarefas cont√≠nuas porque o passo de tempo final seria $T = \infty$, e o retorno, que √© o que estamos tentando maximizar, poderia facilmente ser infinito [^7].

Por exemplo, suponha que o agente receba uma recompensa de +1 a cada passo de tempo. Assim, neste livro, geralmente usamos uma defini√ß√£o de retorno que √© ligeiramente mais complexa conceitualmente, mas muito mais simples matematicamente [^5].

O conceito adicional que precisamos √© o de **desconto**. De acordo com essa abordagem, o agente tenta selecionar a√ß√µes para que a soma das recompensas descontadas que recebe ao longo do futuro seja maximizada. Em particular, ele escolhe $A$<sub>$t$</sub> para maximizar o **retorno descontado esperado**:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1},$$

onde $\gamma$ √© um par√¢metro, $0 \leq \gamma \leq 1$, chamado de **taxa de desconto** [^3].

> üí° **Exemplo Num√©rico:**  Suponha que um agente esteja navegando em um rob√¥ em um ambiente e receba recompensas para evitar obst√°culos. Seja a sequ√™ncia de recompensas `R = [1, 0, 1, 1, 0]`, onde 1 representa evitar um obst√°culo e 0 representa uma colis√£o. Se $\gamma = 0.9$, ent√£o:
>
> $G_0 = 1 + 0.9 * 0 + 0.9^2 * 1 + 0.9^3 * 1 + 0.9^4 * 0 = 1 + 0 + 0.81 + 0.729 + 0 = 2.539$
> $G_1 = 0 + 0.9 * 1 + 0.9^2 * 1 + 0.9^3 * 0 = 0 + 0.9 + 0.81 + 0 = 1.71$
> $G_2 = 1 + 0.9 * 1 + 0.9^2 * 0 = 1 + 0.9 + 0 = 1.9$
> $G_3 = 1 + 0.9 * 0 = 1 + 0 = 1$
> $G_4 = 0$

A taxa de desconto determina o valor presente de recompensas futuras: uma recompensa recebida *k* passos de tempo no futuro vale apenas $\gamma^{k-1}$ vezes o que valeria se fosse recebida imediatamente. Se $\gamma < 1$, o agente √© "m√≠ope" ao se preocupar apenas em maximizar recompensas imediatas: seu objetivo neste caso √© aprender como escolher A<sub>t</sub> de forma a maximizar apenas R<sub>t+1</sub> [^3]. Se cada uma das a√ß√µes do agente influenciasse apenas a recompensa imediata, n√£o as recompensas futuras tamb√©m, ent√£o um agente m√≠ope poderia maximizar (3.8) maximizando separadamente cada recompensa imediata. Mas, em geral, agir para maximizar a recompensa imediata pode reduzir o acesso a recompensas futuras, de modo que o retorno seja reduzido. Conforme $\gamma$ se aproxima de 1, o objetivo de retorno leva as recompensas futuras em conta com mais for√ßa; o agente torna-se mais previdente.

> üí° **Exemplo Num√©rico:** Considere um cen√°rio onde um agente pode escolher entre duas a√ß√µes:
>
> *   A√ß√£o A: Recompensa imediata de +1, mas recompensa futura de 0.
> *   A√ß√£o B: Recompensa imediata de 0, mas recompensa futura de +2.
>
> Se $\gamma = 0.5$, ent√£o:
>
> *   Retorno da A√ß√£o A: $1 + 0.5 * 0 = 1$
> *   Retorno da A√ß√£o B: $0 + 0.5 * 2 = 1$
>
> Nesse caso, as duas a√ß√µes s√£o igualmente boas. No entanto, se $\gamma = 0.9$, ent√£o:
>
> *   Retorno da A√ß√£o A: $1 + 0.9 * 0 = 1$
> *   Retorno da A√ß√£o B: $0 + 0.9 * 2 = 1.8$
>
> Aqui, a A√ß√£o B √© prefer√≠vel porque o agente valoriza mais as recompensas futuras.

Os retornos em passos de tempo sucessivos est√£o relacionados entre si de uma forma que √© importante para a teoria e os algoritmos de aprendizado por refor√ßo:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \ldots$$
$$= R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \ldots)$$
$$= R_{t+1} + \gamma G_{t+1}$$

Observe que isso funciona para todos os passos de tempo $t < T$, mesmo que a termina√ß√£o ocorra em $t + 1$, desde que definamos $G_T = 0$. Isso geralmente torna mais f√°cil computar retornos a partir de sequ√™ncias de recompensa [^9].

> üí° **Exemplo Num√©rico:**  Usando o exemplo anterior com `R = [1, 0, 1, 1, 0]` e $\gamma = 0.9`:
>
> $G_4 = 0$
> $G_3 = R_4 + \gamma G_4 = 1 + 0.9 * 0 = 1$
> $G_2 = R_3 + \gamma G_3 = 1 + 0.9 * 1 = 1.9$
> $G_1 = R_2 + \gamma G_2 = 0 + 0.9 * 1.9 = 1.71$
> $G_0 = R_1 + \gamma G_1 = 1 + 0.9 * 1.71 = 2.539$
>
> Isso demonstra a rela√ß√£o recursiva entre os retornos em passos de tempo sucessivos.

Observe que, embora o retorno (3.8) seja uma soma de um n√∫mero infinito de termos, ele ainda √© finito se a recompensa for n√£o-nula e constante ‚Äî se $\gamma < 1$. Por exemplo, se a recompensa for uma constante +1, ent√£o o retorno √©

$$G_t = \sum_{k=0}^{\infty} \gamma^k = \frac{1}{1 - \gamma}$$
*Prova.*
Provaremos que $\sum_{k=0}^{\infty} \gamma^k = \frac{1}{1 - \gamma}$ para $|\gamma| < 1$.

I. Seja $S = \sum_{k=0}^{\infty} \gamma^k = 1 + \gamma + \gamma^2 + \gamma^3 + ... $

II. Multiplique ambos os lados por $\gamma$:
$\gamma S = \gamma + \gamma^2 + \gamma^3 + \gamma^4 + ...$

III. Subtraia $\gamma S$ de $S$:
$S - \gamma S = (1 + \gamma + \gamma^2 + \gamma^3 + ...) - (\gamma + \gamma^2 + \gamma^3 + \gamma^4 + ...)$
$S(1 - \gamma) = 1$

IV. Divida ambos os lados por $(1 - \gamma)$:
$S = \frac{1}{1 - \gamma}$

V. Portanto, $\sum_{k=0}^{\infty} \gamma^k = \frac{1}{1 - \gamma}$ para $|\gamma| < 1$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $\gamma = 0.9$, e o agente recebe uma recompensa constante de +1 a cada passo de tempo, ent√£o o retorno √©:
>
> $G_t = \frac{1}{1 - 0.9} = \frac{1}{0.1} = 10$.
>
> Isso significa que, mesmo que o agente receba uma recompensa infinita de +1, o retorno total descontado √© limitado a 10 devido ao fator de desconto. Se $\gamma = 0.5$, ent√£o $G_t = \frac{1}{1 - 0.5} = 2$.

Para complementar a discuss√£o sobre retornos descontados, podemos considerar um caso em que a recompensa varia ao longo do tempo, mas ainda converge para zero.

**Proposi√ß√£o 1** Se $|R_{t}| \leq B \cdot \alpha^{t}$ para alguma constante $B > 0$ e $0 < \alpha < 1$, ent√£o $\sum_{t=0}^{\infty} \gamma^{t}R_{t+1}$ converge absolutamente para qualquer $0 \leq \gamma \leq 1$.

*Prova.*
Como $|R_{t}| \leq B \cdot \alpha^{t}$, ent√£o $|\gamma^{t}R_{t+1}| \leq |\gamma^{t} B \alpha^{t+1}| = B \alpha |\gamma \alpha|^{t}$. Se $\gamma \leq 1$, ent√£o $|\gamma \alpha| \leq \alpha < 1$, e a s√©rie $\sum_{t=0}^{\infty} |\gamma \alpha|^{t}$ converge (s√©rie geom√©trica). Portanto, $\sum_{t=0}^{\infty} \gamma^{t}R_{t+1}$ converge absolutamente pelo teste da compara√ß√£o. $\blacksquare$

> üí° **Exemplo Num√©rico:** Seja $B = 5$, $\alpha = 0.5$, e $R_t = 5 \cdot (0.5)^t$. Ent√£o, $|R_t| \leq 5 \cdot (0.5)^t$. Se $\gamma = 0.8$, ent√£o a s√©rie $\sum_{t=0}^{\infty} (0.8)^t R_{t+1} = \sum_{t=0}^{\infty} (0.8)^t \cdot 5 \cdot (0.5)^{t+1} = \sum_{t=0}^{\infty} 5 \cdot 0.5 \cdot (0.8 \cdot 0.5)^t = 2.5 \sum_{t=0}^{\infty} (0.4)^t$. Como $\sum_{t=0}^{\infty} (0.4)^t = \frac{1}{1-0.4} = \frac{1}{0.6} \approx 1.667$, ent√£o $\sum_{t=0}^{\infty} (0.8)^t R_{t+1} = 2.5 \cdot 1.667 \approx 4.167$. A s√©rie converge absolutamente.

Al√©m disso, podemos analisar o impacto da taxa de desconto no valor m√°ximo poss√≠vel do retorno descontado.

**Lema 2** Seja $R_{max}$ o valor m√°ximo poss√≠vel da recompensa. Ent√£o, o valor m√°ximo poss√≠vel do retorno descontado √© $\frac{R_{max}}{1-\gamma}$.

*Prova.* O retorno descontado √© dado por $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$. Para maximizar $G_t$, devemos maximizar cada $R_{t+k+1}$. Se $R_{t+k+1} = R_{max}$ para todo $k$, ent√£o $G_t = \sum_{k=0}^{\infty} \gamma^k R_{max} = R_{max} \sum_{k=0}^{\infty} \gamma^k$. Como $\sum_{k=0}^{\infty} \gamma^k = \frac{1}{1 - \gamma}$, segue que $G_t = \frac{R_{max}}{1-\gamma}$. $\blacksquare$

> üí° **Exemplo Num√©rico:** Se $R_{max} = 10$ e $\gamma = 0.9$, ent√£o o valor m√°ximo poss√≠vel do retorno descontado √© $\frac{10}{1 - 0.9} = \frac{10}{0.1} = 100$. Isso demonstra que, mesmo com recompensas limitadas, o retorno descontado pode ser significativamente maior dependendo da taxa de desconto.

A propriedade de Markov √© fundamental para os MDPs. Portanto, √© interessante analisar como os retornos se comportam sob essa propriedade.

**Teorema 3** Se o ambiente possui a propriedade de Markov, ent√£o o retorno esperado depende apenas do estado atual e da a√ß√£o tomada.

*Prova.* A propriedade de Markov implica que $P(S_{t+1}, R_{t+1} | S_t, A_t, ..., S_0, A_0) = P(S_{t+1}, R_{t+1} | S_t, A_t)$. O retorno esperado √© $E[G_t | S_t, A_t] = E[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t, A_t]$. Como o pr√≥ximo estado e recompensa dependem apenas do estado e a√ß√£o atuais, todas as recompensas futuras dependem indiretamente apenas do estado e a√ß√£o atuais. Portanto, $E[G_t | S_t, A_t] = E[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t, A_t]$ √© uma fun√ß√£o de $S_t$ e $A_t$ apenas. $\blacksquare$

> üí° **Exemplo Num√©rico:** Considere um rob√¥ em uma grade, onde o estado √© a localiza√ß√£o do rob√¥ e a a√ß√£o √© mover-se para cima, para baixo, para a esquerda ou para a direita. Se o ambiente obedece √† propriedade de Markov, a recompensa que o rob√¥ recebe ap√≥s se mover depende apenas de sua localiza√ß√£o atual e da a√ß√£o que tomou (e n√£o de seu hist√≥rico de movimentos anteriores).  Portanto, o retorno esperado do rob√¥ depende apenas de onde ele est√° e de qual movimento ele escolher fazer.

### Conclus√£o

Neste cap√≠tulo, definimos formalmente o objetivo de aprendizado em tarefas de **Processos de Decis√£o de Markov Finitos (MDPs)**. Introduzimos os conceitos de retornos e epis√≥dios, essenciais para a compreens√£o e resolu√ß√£o de problemas de aprendizado por refor√ßo. A formula√ß√£o do retorno como uma soma de recompensas descontadas nos permite lidar com tarefas cont√≠nuas, nas quais a intera√ß√£o agente-ambiente n√£o se divide naturalmente em epis√≥dios.

### Refer√™ncias

[^1]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. MIT press, 2018.
[^3]: *Ibid.*
[^5]: *Ibid.*
[^7]: *Ibid.*
[^9]: *Ibid.*
<!-- END -->