## Retornos e Epis√≥dios em Tarefas Epis√≥dicas e Cont√≠nuas

### Introdu√ß√£o
O conceito de **retorno** (*return*) em Reinforcement Learning (RL) formaliza o objetivo do agente de maximizar a recompensa cumulativa a longo prazo [^53]. No entanto, a natureza da tarefa (epis√≥dica ou cont√≠nua) e a introdu√ß√£o do desconto (*discounting*) influenciam a formula√ß√£o do retorno e, consequentemente, a implementa√ß√£o de algoritmos de RL [^54]. Este cap√≠tulo explora as nuances dessas considera√ß√µes e como elas impactam a defini√ß√£o do retorno e as equa√ß√µes relacionadas.

### Formula√ß√µes de Retorno para Diferentes Tipos de Tarefas
Como mencionado anteriormente, as tarefas de RL podem ser categorizadas como **epis√≥dicas** ou **cont√≠nuas** [^54]. Nas tarefas **epis√≥dicas**, a intera√ß√£o agente-ambiente se divide naturalmente em *epis√≥dios*, como jogos ou tentativas em um labirinto. Cada epis√≥dio termina em um **estado terminal** e pode ser seguido por um reset para um estado inicial padr√£o ou amostrado de uma distribui√ß√£o inicial [^54]. O retorno, nesse caso, √© definido como a soma das recompensas recebidas ao longo do epis√≥dio:
$$
G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T
$$
onde $T$ √© o tempo final do epis√≥dio [^54].

> üí° **Exemplo Num√©rico (Tarefa Epis√≥dica):**
> Imagine um agente tentando sair de um labirinto. O epis√≥dio termina quando o agente encontra a sa√≠da. As recompensas s√£o definidas como -1 por cada passo dado, e +10 quando encontra a sa√≠da. Se um epis√≥dio tem a seguinte sequ√™ncia de recompensas:
> $R_1 = -1, R_2 = -1, R_3 = -1, R_4 = 10$.
> Ent√£o, o retorno $G_0 = -1 + (-1) + (-1) + 10 = 7$.
> Isso significa que, no in√≠cio do epis√≥dio, o agente espera receber um total de 7 pontos considerando a sequ√™ncia de a√ß√µes tomadas.

Em contraste, as tarefas **cont√≠nuas** n√£o possuem uma no√ß√£o natural de um epis√≥dio final [^54]. A intera√ß√£o agente-ambiente prossegue indefinidamente, como em problemas de controle de processos ou rob√¥s com longos tempos de vida [^54]. A formula√ß√£o anterior do retorno, como uma soma simples de recompensas, torna-se problem√°tica, pois $T = \infty$, e o retorno pode ser infinito [^54].

Para lidar com tarefas cont√≠nuas, introduzimos o conceito de **desconto** (*discounting*) [^55]. O agente tenta selecionar a√ß√µes para maximizar a soma das recompensas *descontadas* que ele recebe no futuro. Em particular, ele escolhe a a√ß√£o $A_t$ para maximizar o retorno descontado esperado:

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

onde $\gamma$ √© o **fator de desconto** (*discount rate*), com $0 \leq \gamma \leq 1$ [^55]. O fator de desconto determina o valor presente de recompensas futuras: uma recompensa recebida $k$ passos de tempo no futuro vale apenas $\gamma^{k}$ vezes o que valeria se fosse recebida imediatamente [^55].  Se $\gamma = 0$, o agente √© "m√≠ope" e se preocupa apenas em maximizar as recompensas imediatas [^55]. √Ä medida que $\gamma$ se aproxima de 1, o objetivo de retorno leva em considera√ß√£o as recompensas futuras com mais intensidade; o agente se torna mais previdente [^55].

> üí° **Exemplo Num√©rico (Tarefa Cont√≠nua com Desconto):**
> Considere um rob√¥ que recebe recompensas por manter o equil√≠brio. As recompensas s√£o dadas a cada instante de tempo. Suponha a seguinte sequ√™ncia de recompensas:
> $R_1 = 1, R_2 = 1, R_3 = 1, R_4 = 1, ...$
>
> Se o fator de desconto $\gamma = 0.9$, ent√£o o retorno descontado $G_0$ √©:
> $G_0 = 1 + 0.9 * 1 + 0.9^2 * 1 + 0.9^3 * 1 + \ldots$
> $G_0 = \sum_{k=0}^{\infty} (0.9)^k = \frac{1}{1 - 0.9} = 10$
>
> Se o fator de desconto $\gamma = 0.5$, ent√£o o retorno descontado $G_0$ √©:
> $G_0 = 1 + 0.5 * 1 + 0.5^2 * 1 + 0.5^3 * 1 + \ldots$
> $G_0 = \sum_{k=0}^{\infty} (0.5)^k = \frac{1}{1 - 0.5} = 2$
>
> Isso demonstra como o fator de desconto influencia o valor do retorno total. Com um $\gamma$ menor, recompensas futuras t√™m um impacto menor no retorno atual.

Se $\gamma < 1$, a soma infinita em (3.8) tem um valor finito, desde que a sequ√™ncia de recompensas $\{R_k\}$ seja limitada [^55].  Se $\gamma = 0$, o agente √© "m√≠ope" e se preocupa apenas em maximizar as recompensas imediatas [^55]. √Ä medida que $\gamma$ se aproxima de 1, o objetivo de retorno leva em considera√ß√£o as recompensas futuras com mais intensidade; o agente se torna mais previdente [^55].

**Lema 1:**
O retorno descontado $G_t$ pode ser definido recursivamente como:
$$
G_t = R_{t+1} + \gamma G_{t+1}
$$

*Prova:*
Provaremos a identidade recursiva do retorno descontado.

I.  Come√ßamos com a defini√ß√£o do retorno descontado no tempo t:
    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$$

II. Fatoramos $\gamma$ de todos os termos ap√≥s $R_{t+1}$:
    $$G_t = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots)$$

III. Observamos que a express√£o entre par√™nteses √© o retorno descontado a partir do tempo $t+1$:
     $$G_{t+1} = R_{t+2} + \gamma R_{t+3} + \dots$$

IV. Substitu√≠mos essa express√£o na equa√ß√£o original:
    $$G_t = R_{t+1} + \gamma G_{t+1}$$

Portanto, o retorno descontado $G_t$ pode ser expresso recursivamente como $G_t = R_{t+1} + \gamma G_{t+1}$. $\blacksquare$

> üí° **Exemplo Num√©rico (Recorr√™ncia do Retorno):**
> Usando o exemplo do rob√¥ com $\gamma = 0.9$ e a sequ√™ncia de recompensas $R_1 = 1, R_2 = 1, R_3 = 1, ...$, podemos calcular $G_0$ recursivamente.
>
> Sabemos que $G_0 = R_1 + \gamma G_1$.  Assumindo que o estado do rob√¥ n√£o muda muito, podemos aproximar $G_1 \approx G_0$.
>
> Ent√£o, $G_0 = 1 + 0.9 * G_0$.
>
> Resolvendo para $G_0$, temos $0.1 * G_0 = 1$, ent√£o $G_0 = 10$.
>
> Isso coincide com o resultado anterior calculado pela soma infinita, demonstrando a validade da formula√ß√£o recursiva.  Essa recurs√£o √© crucial para algoritmos como Q-learning e SARSA.

Esta formula√ß√£o recursiva √© fundamental para o desenvolvimento de algoritmos de aprendizado por refor√ßo, pois permite o c√°lculo eficiente do retorno.

**Nota√ß√£o Unificada para Tarefas Epis√≥dicas e Cont√≠nuas**

Para expressar resultados e algoritmos de maneira concisa, √© √∫til ter uma nota√ß√£o unificada que cubra ambos os tipos de tarefas [^57]. Isso √© alcan√ßado considerando a termina√ß√£o do epis√≥dio como a entrada em um **estado absorvente especial** (*special absorbing state*) que transita apenas para si mesmo e gera apenas recompensas zero [^57].

![Diagrama de transi√ß√£o de estados ilustrando um MDP com um estado terminal absorvente.](./../images/image9.png)

A imagem apresenta um diagrama de transi√ß√£o de estados, com c√≠rculos rotulados de S0 a S2 representando estados, conectados por setas que indicam transi√ß√µes. Cada seta √© rotulada com um valor de recompensa (R1=+1, R2=+1, R3=+1), indicando o ganho ao transitar entre os estados. Ap√≥s o estado S2, o diagrama se ramifica em um estado absorvente representado por um quadrado cinza, com um loop de auto-transi√ß√£o e setas direcionadas para fora, indicando recompensas zero (R4=0, R5=0). Este diagrama, que pode ser encontrado na p√°gina 57, ilustra um processo de decis√£o de Markov (MDP) com um estado terminal absorvente, usado para unificar nota√ß√µes entre tarefas epis√≥dicas e cont√≠nuas.

Formalmente, em tarefas epis√≥dicas, denotamos por $\mathcal{S}$ o conjunto de todos os estados n√£o terminais e por $\mathcal{S}^+$ o conjunto de todos os estados, incluindo o estado terminal [^54]. O tempo de termina√ß√£o $T$ √© uma vari√°vel aleat√≥ria que normalmente varia de epis√≥dio para epis√≥dio [^54].

Podemos ent√£o definir o retorno de forma geral como [^57]:
$$
G_t = \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k
$$
incluindo a possibilidade de $T = \infty$ ou $\gamma = 1$ (mas n√£o ambos) [^57].

**Teorema 1:** (Decomposi√ß√£o do Retorno)
O retorno $G_t$ pode ser decomposto em uma recompensa imediata e o retorno subsequente, descontado pelo fator $\gamma$, independentemente se a tarefa √© epis√≥dica ou cont√≠nua.

*Prova:*
A prova demonstra a decomposi√ß√£o do retorno em uma recompensa imediata e o retorno subsequente descontado.

I. Come√ßamos com a defini√ß√£o geral do retorno $G_t$:
   $$G_t = \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k$$

II. Expandimos a soma separando o primeiro termo ($k = t+1$):
    $$G_t = \gamma^{(t+1)-t-1} R_{t+1} + \sum_{k=t+2}^{T} \gamma^{k-t-1} R_k$$
    $$G_t = R_{t+1} + \sum_{k=t+2}^{T} \gamma^{k-t-1} R_k$$

III. Fatoramos $\gamma$ da soma restante:
     $$G_t = R_{t+1} + \gamma \sum_{k=t+2}^{T} \gamma^{k-t-2} R_k$$

IV. Observamos que a soma restante √© equivalente a $G_{t+1}$:
    $$G_{t+1} = \sum_{k=t+2}^{T} \gamma^{k-(t+1)-1} R_k = \sum_{k=t+2}^{T} \gamma^{k-t-2} R_k$$

V. Substitu√≠mos $G_{t+1}$ na equa√ß√£o original:
   $$G_t = R_{t+1} + \gamma G_{t+1}$$

Portanto, demonstramos que o retorno $G_t$ pode ser decomposto como $G_t = R_{t+1} + \gamma G_{t+1}$. $\blacksquare$

Essa decomposi√ß√£o √© fundamental para o desenvolvimento de algoritmos de aprendizado por refor√ßo temporal-difference (TD).

### Impacto na Implementa√ß√£o do Algoritmo
A distin√ß√£o entre tarefas epis√≥dicas e cont√≠nuas, juntamente com a presen√ßa ou aus√™ncia de desconto, impacta a implementa√ß√£o de algoritmos de RL de v√°rias maneiras:

1.  **C√°lculo do Retorno:** Em tarefas epis√≥dicas sem desconto ($\gamma = 1$), o retorno √© simplesmente a soma das recompensas at√© o final do epis√≥dio. Em tarefas cont√≠nuas com desconto ($\gamma < 1$), o retorno √© uma soma ponderada das recompensas futuras, o que requer um c√°lculo diferente.

2.  **Atualiza√ß√£o de Valores:** Os algoritmos de RL frequentemente atualizam as estimativas das fun√ß√µes de valor com base nos retornos observados [^58]. A natureza do retorno (epis√≥dico ou descontado) influencia a forma como essas atualiza√ß√µes s√£o realizadas.

3.  **Converg√™ncia:** Em tarefas cont√≠nuas sem desconto ($\gamma = 1$), o retorno pode divergir, dificultando a converg√™ncia dos algoritmos de RL. O desconto garante que o retorno seja limitado, facilitando a converg√™ncia.

4. **Fun√ß√µes de valor**: As fun√ß√µes de valor em tarefas epis√≥dicas s√£o definidas como o retorno esperado dado um estado inicial [^58]. A presen√ßa do estado terminal altera a defini√ß√£o da fun√ß√£o de valor.

**Teorema 2:** (Converg√™ncia do Retorno Descontado)
Se a sequ√™ncia de recompensas $\{R_k\}$ √© limitada, ou seja, $|R_k| < M$ para todo $k$, onde $M$ √© uma constante positiva, e se $\gamma < 1$, ent√£o o retorno descontado $G_t$ √© finito e limitado.

*Prova:*
Esta prova demonstra que, sob certas condi√ß√µes, o retorno descontado √© finito e limitado.

I.  Come√ßamos com a defini√ß√£o do valor absoluto do retorno descontado:
    $$|G_t| = |\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|$$

II. Aplicamos a desigualdade triangular:
     $$|G_t| \leq \sum_{k=0}^{\infty} |\gamma^k R_{t+k+1}|$$

III. Reorganizamos os termos:
      $$|G_t| \leq \sum_{k=0}^{\infty} |\gamma^k| |R_{t+k+1}|$$

IV. Usamos a condi√ß√£o de que $|R_k| < M$ para todo $k$:
     $$|G_t| \leq \sum_{k=0}^{\infty} \gamma^k M$$

V. Fatoramos a constante $M$:
    $$|G_t| \leq M \sum_{k=0}^{\infty} \gamma^k$$

VI. Reconhecemos que $\sum_{k=0}^{\infty} \gamma^k$ √© uma s√©rie geom√©trica com raz√£o $\gamma$. Como $\gamma < 1$, a s√©rie converge para $\frac{1}{1 - \gamma}$:
    $$|G_t| \leq M \frac{1}{1 - \gamma}$$

VII. Conclu√≠mos que $|G_t|$ √© limitado superiormente por $\frac{M}{1 - \gamma}$. Portanto, $G_t$ √© finito e limitado. $\blacksquare$

> üí° **Exemplo Num√©rico (Converg√™ncia):**
> Suponha que a recompensa m√°xima que um agente pode receber em qualquer instante de tempo √© $M = 10$. Se usarmos um fator de desconto $\gamma = 0.9$, ent√£o o retorno descontado m√°ximo poss√≠vel √©:
>
> $|G_t| \leq \frac{10}{1 - 0.9} = \frac{10}{0.1} = 100$.
>
> Isso significa que, mesmo na pior das hip√≥teses (recebendo a recompensa m√°xima em todos os passos de tempo), o retorno total √© limitado a 100. Isso ajuda na estabilidade do aprendizado.  Se $\gamma$ fosse 1, o retorno poderia divergir para infinito.

Este resultado garante a estabilidade em muitos algoritmos de RL.

### Conclus√£o
A correta compreens√£o e formula√ß√£o do retorno s√£o cruciais para o sucesso da aplica√ß√£o de algoritmos de Reinforcement Learning. A natureza da tarefa (epis√≥dica ou cont√≠nua) e a escolha do fator de desconto afetam significativamente a defini√ß√£o do retorno e, consequentemente, a implementa√ß√£o e converg√™ncia dos algoritmos de RL. A nota√ß√£o unificada apresentada permite expressar os conceitos de forma mais clara e concisa, facilitando a an√°lise e o design de algoritmos para diferentes tipos de problemas [^57].

### Refer√™ncias
[^53]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT press.
[^54]: Cap√≠tulo 3: Finite Markov Decision Processes.
[^55]: Returns and Episodes -  Discounting Concept.
[^57]: Returns and Episodes - Unified Notation for Episodic and Continuing Tasks.
[^58]: Policies and Value Functions.
<!-- END -->