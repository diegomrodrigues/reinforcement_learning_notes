## Retornos com Desconto em Tarefas Cont√≠nuas

### Introdu√ß√£o
Em processos de decis√£o de Markov (MDPs), o objetivo do agente √© maximizar a recompensa cumulativa ao longo do tempo [^1]. No contexto de tarefas *epis√≥dicas*, onde a intera√ß√£o entre o agente e o ambiente se divide naturalmente em epis√≥dios, o retorno $G_t$ √© definido como a soma das recompensas recebidas at√© o final do epis√≥dio [^8]. No entanto, em tarefas *cont√≠nuas*, a intera√ß√£o n√£o possui um fim natural, e a soma infinita de recompensas pode divergir. Para lidar com essa situa√ß√£o, introduzimos o conceito de *retorno com desconto*, que pondera as recompensas futuras por um fator de desconto $\gamma$, onde $0 \leq \gamma \leq 1$ [^9]. Este cap√≠tulo explora em detalhes o conceito de retornos com desconto em tarefas cont√≠nuas, analisando o papel do fator de desconto e suas implica√ß√µes no comportamento do agente.

### Retornos com Desconto
Em tarefas cont√≠nuas, a formula√ß√£o original do retorno como a soma simples das recompensas (equa√ß√£o 3.7) torna-se problem√°tica, pois o horizonte temporal $T$ tende a infinito ($T = \infty$), e o retorno $G_t$ pode facilmente divergir, tornando a maximiza√ß√£o impratic√°vel [^8]. Para contornar essa dificuldade, introduzimos o conceito de *desconto*, que atribui um peso menor √†s recompensas recebidas no futuro [^9]. O *retorno com desconto* √© definido como:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

Onde $\gamma$ √© o *fator de desconto*, um par√¢metro que satisfaz $0 \leq \gamma \leq 1$ [^9].

> üí° **Exemplo Num√©rico:** Suponha que um agente recebe uma sequ√™ncia de recompensas: $R_{t+1} = 1$, $R_{t+2} = 2$, $R_{t+3} = 3$, $R_{t+4} = 4$, e assim por diante. Se $\gamma = 0.9$, o retorno com desconto no tempo $t$ seria:
>
> $G_t = 1 + 0.9 \cdot 2 + 0.9^2 \cdot 3 + 0.9^3 \cdot 4 + \dots$
> $G_t = 1 + 1.8 + 2.43 + 2.916 + \dots$
>
> Observe que cada recompensa futura √© ponderada por uma pot√™ncia decrescente de $\gamma$, reduzindo seu impacto no retorno total. Se $\gamma$ fosse 0, o retorno seria apenas $G_t = 1$, enfatizando apenas a recompensa imediata.
>
> Para os primeiros termos:
> *   $R_{t+1} = 1$
> *   $\gamma R_{t+2} = 0.9 \times 2 = 1.8$
> *   $\gamma^2 R_{t+3} = 0.9^2 \times 3 = 0.81 \times 3 = 2.43$
> *   $\gamma^3 R_{t+4} = 0.9^3 \times 4 = 0.729 \times 4 = 2.916$
>
> O retorno acumulado ap√≥s os primeiros quatro passos seria $1 + 1.8 + 2.43 + 2.916 = 8.146$. √Ä medida que adicionamos mais termos, o retorno continuar√° a aumentar, mas a taxa de aumento diminuir√° devido ao fator de desconto.

O fator de desconto $\gamma$ determina o valor presente das recompensas futuras. Uma recompensa recebida $k$ *time steps* no futuro tem um valor presente de $\gamma^{k-1}$ vezes seu valor imediato [^9]. Se $\gamma < 1$, a soma infinita na equa√ß√£o acima converge para um valor finito, desde que a sequ√™ncia de recompensas $\{R_k\}$ seja limitada [^9].

**Lema 1:** *Condi√ß√£o de converg√™ncia do retorno com desconto.*
Se $|R_k| \leq R_{max}$ para todo $k$ e $0 \leq \gamma < 1$, ent√£o o retorno com desconto $G_t$ √© limitado por:

$$|G_t| \leq \frac{R_{max}}{1 - \gamma}$$

*Prova.*
Como $|R_{t+k+1}| \leq R_{max}$ para todo $k$, temos:

$$|G_t| = \left|\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\right| \leq \sum_{k=0}^{\infty} \gamma^k |R_{t+k+1}| \leq \sum_{k=0}^{\infty} \gamma^k R_{max} = R_{max} \sum_{k=0}^{\infty} \gamma^k$$

Dado que $\sum_{k=0}^{\infty} \gamma^k$ √© uma s√©rie geom√©trica com raz√£o $\gamma$ e $|\gamma| < 1$, a s√©rie converge para $\frac{1}{1 - \gamma}$. Portanto:

$$|G_t| \leq R_{max} \frac{1}{1 - \gamma} = \frac{R_{max}}{1 - \gamma}$$

Essa condi√ß√£o garante que o retorno com desconto permanece finito e bem definido, mesmo em tarefas cont√≠nuas.

> üí° **Exemplo Num√©rico:**  Considere um cen√°rio onde a recompensa m√°xima que um agente pode receber √© $R_{max} = 10$. Se o fator de desconto for $\gamma = 0.9$, o limite superior para o retorno com desconto seria:
>
> $|G_t| \leq \frac{10}{1 - 0.9} = \frac{10}{0.1} = 100$
>
> Isso significa que, independentemente da sequ√™ncia de recompensas que o agente receber, o retorno com desconto nunca exceder√° 100. Se diminuirmos o fator de desconto para $\gamma = 0.5$, o limite superior para o retorno seria:
>
> $|G_t| \leq \frac{10}{1 - 0.5} = \frac{10}{0.5} = 20$
>
> Portanto, um menor fator de desconto resulta em um limite superior menor para o retorno, indicando que recompensas futuras t√™m menos impacto no retorno total.

**Caixa de Destaque:** O uso do desconto garante que o retorno permane√ßa finito em tarefas cont√≠nuas, permitindo que o agente compare diferentes sequ√™ncias de a√ß√µes com base no retorno esperado a longo prazo.

#### Interpreta√ß√µes do Fator de Desconto
O fator de desconto $\gamma$ possui diversas interpreta√ß√µes, influenciando diretamente o comportamento do agente:

*   **Prefer√™ncia Temporal:**  $\gamma$ representa a prefer√™ncia do agente por recompensas imediatas em rela√ß√£o √†s futuras. Um valor de $\gamma$ pr√≥ximo de 0 indica que o agente √© *m√≠ope*, priorizando apenas a recompensa imediata $R_{t+1}$ [^9].
*   **Probabilidade de Sobreviv√™ncia:** Em alguns cen√°rios, $\gamma$ pode representar a probabilidade de que o agente continue interagindo com o ambiente no pr√≥ximo *time step*. Se $\gamma$ for baixo, o agente pode priorizar recompensas imediatas, pois a probabilidade de receber recompensas futuras √© menor.
*   **Taxa de Juros:** Em problemas de otimiza√ß√£o financeira, $\gamma$ pode ser interpretado como uma taxa de juros, refletindo o custo de oportunidade de adiar o recebimento de recompensas.

> üí° **Exemplo Num√©rico:** Considere um agente que precisa escolher entre receber uma recompensa de 10 agora ou uma recompensa de 12 no pr√≥ximo *time step*. Se $\gamma = 0.8$, o valor presente da recompensa futura seria $0.8 \times 12 = 9.6$. Neste caso, o agente preferiria receber a recompensa imediata de 10, pois tem um valor presente maior. Se $\gamma = 0.9$, o valor presente da recompensa futura seria $0.9 \times 12 = 10.8$. Neste caso, o agente preferiria esperar pelo pr√≥ximo *time step* e receber a recompensa maior de 12. Isso demonstra como o fator de desconto influencia a tomada de decis√£o do agente.

**Caixa de Destaque:** A escolha do fator de desconto $\gamma$ √© crucial e depende da natureza da tarefa. Um valor muito baixo pode levar a um comportamento sub√≥timo, enquanto um valor muito alto pode dificultar a converg√™ncia do aprendizado.

#### Agente M√≠ope vs. Agente Farsighted
O valor do fator de desconto $\gamma$ influencia significativamente o comportamento do agente [^9].

*   **Agente M√≠ope ($\gamma \approx 0$):** Um agente com um fator de desconto pr√≥ximo de zero √© considerado *m√≠ope*, pois sua √∫nica preocupa√ß√£o √© maximizar a recompensa imediata [^9]. O objetivo do agente se resume a aprender como escolher a a√ß√£o $A_t$ que maximiza apenas $R_{t+1}$ [^9]. Se as a√ß√µes do agente afetassem apenas a recompensa imediata, um agente m√≠ope poderia maximizar o retorno separadamente, maximizando cada recompensa imediata. No entanto, em geral, agir para maximizar a recompensa imediata pode reduzir o acesso a recompensas futuras, de modo que o retorno √© reduzido [^9].

*   **Agente Farsighted ($\gamma \approx 1$):** Um agente com um fator de desconto pr√≥ximo de um √© considerado *farsighted*, pois o objetivo de retorno leva em conta as recompensas futuras de forma mais forte [^9].

> üí° **Exemplo Num√©rico:** Imagine um rob√¥ que precisa navegar por um labirinto.
>
> *   **Agente M√≠ope ($\gamma = 0.1$):** O rob√¥ est√° programado para priorizar a recompensa imediata, como encontrar uma pequena quantidade de energia. Ele pode ignorar o caminho mais longo que leva a uma fonte de energia maior no final do labirinto. Isso pode resultar em um comportamento sub√≥timo, pois o rob√¥ nunca aprende a alcan√ßar o objetivo principal (a fonte de energia maior).
> *   **Agente Farsighted ($\gamma = 0.9$):** O rob√¥ considera as recompensas futuras e est√° disposto a sacrificar uma pequena quantidade de energia imediata para encontrar a fonte de energia maior no final do labirinto. Ele explora o ambiente de forma mais eficaz e aprende a alcan√ßar o objetivo principal.
>
> Este exemplo ilustra como o fator de desconto afeta a capacidade do agente de aprender e alcan√ßar objetivos a longo prazo.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # Define os valores de gamma
> gammas = [0.1, 0.5, 0.9]
>
> # Define as recompensas para cada passo de tempo
> rewards = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
>
> # Calcula o retorno com desconto para cada valor de gamma
> returns = []
> for gamma in gammas:
>     discounted_return = [sum([gamma**k * rewards[k] for k in range(len(rewards))])]
>     returns.append(discounted_return[0])
>
> # Plota os retornos com desconto
> plt.figure(figsize=(8, 6))
> plt.bar(range(len(gammas)), returns, tick_label=[f"Gamma = {g}" for g in gammas])
> plt.xlabel("Fator de Desconto (Gamma)")
> plt.ylabel("Retorno com Desconto")
> plt.title("Compara√ß√£o dos Retornos com Desconto para Diferentes Valores de Gamma")
> plt.grid(axis='y', linestyle='--')
> plt.show()
> ```

Para complementar a discuss√£o sobre agentes m√≠opes e *farsighted*, considere o seguinte cen√°rio:

**Proposi√ß√£o 1:** *Sensibilidade do retorno a recompensas futuras.*
A sensibilidade do retorno $G_t$ a mudan√ßas na recompensa $R_{t+k+1}$ diminui exponencialmente com $k$.

*Justificativa.*
A equa√ß√£o do retorno com desconto $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ mostra que o peso de cada recompensa futura $R_{t+k+1}$ √© $\gamma^k$. Como $0 \leq \gamma \leq 1$, o peso $\gamma^k$ diminui exponencialmente com o aumento de $k$. Isso significa que mudan√ßas em recompensas pr√≥ximas a $t$ t√™m um impacto maior em $G_t$ do que mudan√ßas em recompensas distantes no futuro. Para agentes m√≠opes (Œ≥ pr√≥ximo de 0), o impacto de recompensas futuras √© quase nulo, enquanto para agentes *farsighted* (Œ≥ pr√≥ximo de 1), o impacto √© mais significativo, embora ainda decrescente com o tempo.

### Rela√ß√£o Recursiva do Retorno com Desconto
Uma propriedade importante do retorno com desconto √© sua rela√ß√£o recursiva, que simplifica o c√°lculo e a an√°lise:

$$G_t = R_{t+1} + \gamma G_{t+1}$$

Essa equa√ß√£o demonstra que o retorno no *time step* $t$ √© igual √† recompensa imediata $R_{t+1}$ mais o retorno com desconto no *time step* seguinte, $G_{t+1}$, multiplicado por $\gamma$. Essa rela√ß√£o recursiva √© fundamental para diversos algoritmos de *reinforcement learning*.

> üí° **Exemplo Num√©rico:** Suponha que $R_{t+1} = 5$ e $G_{t+1} = 10$. Se $\gamma = 0.9$, ent√£o:
>
> $G_t = 5 + 0.9 \cdot 10 = 5 + 9 = 14$
>
> Isso mostra como o retorno no *time step* $t$ √© calculado usando a recompensa imediata e o retorno no *time step* seguinte. Se $\gamma$ fosse menor, o impacto de $G_{t+1}$ seria reduzido. Por exemplo, se $\gamma = 0.5$:
>
> $G_t = 5 + 0.5 \cdot 10 = 5 + 5 = 10$
>
> A rela√ß√£o recursiva √© √∫til para algoritmos de *reinforcement learning*, como o Q-learning, onde o valor de uma a√ß√£o √© atualizado iterativamente usando essa f√≥rmula.
>
> Imagine um cen√°rio em que um agente est√° aprendendo a jogar um jogo. No *time step* $t$, o agente realiza uma a√ß√£o e recebe uma recompensa de $R_{t+1} = -1$ (penalidade). O agente j√° estimou que o retorno com desconto no *time step* seguinte, $G_{t+1}$, √© 20. Se o fator de desconto for $\gamma = 0.9$, o retorno com desconto no *time step* $t$ seria:
> $G_t = R_{t+1} + \gamma G_{t+1} = -1 + 0.9 \times 20 = -1 + 18 = 17$

**Teorema 1:** *Equival√™ncia entre a defini√ß√£o iterativa e a defini√ß√£o somat√≥ria do retorno com desconto.*
A defini√ß√£o recursiva $G_t = R_{t+1} + \gamma G_{t+1}$ √© equivalente √† defini√ß√£o somat√≥ria $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$.

*Prova.*
Podemos expandir a defini√ß√£o recursiva iterativamente:
$G_t = R_{t+1} + \gamma G_{t+1} = R_{t+1} + \gamma(R_{t+2} + \gamma G_{t+2}) = R_{t+1} + \gamma R_{t+2} + \gamma^2 G_{t+2}$.

Continuando a expans√£o, obtemos:
$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots + \gamma^n R_{t+n+1} + \gamma^{n+1}G_{t+n+1}$.

Tomando o limite quando $n \to \infty$, e assumindo que $\lim_{n \to \infty} \gamma^{n+1}G_{t+n+1} = 0$ (o que √© verdade se $|G_t|$ √© limitado, como demonstrado anteriormente) temos:
$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$.

I. Come√ßamos com a defini√ß√£o recursiva:
    $$G_t = R_{t+1} + \gamma G_{t+1}$$

II. Substitu√≠mos $G_{t+1}$ por sua defini√ß√£o recursiva:
    $$G_t = R_{t+1} + \gamma (R_{t+2} + \gamma G_{t+2}) = R_{t+1} + \gamma R_{t+2} + \gamma^2 G_{t+2}$$

III. Continuamos substituindo $G_{t+i}$ recursivamente:
    $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^n R_{t+n+1} + \gamma^{n+1} G_{t+n+1}$$

IV. Tomamos o limite quando $n$ tende ao infinito:
    $$G_t = \lim_{n \to \infty} \left( \sum_{k=0}^{n} \gamma^k R_{t+k+1} + \gamma^{n+1} G_{t+n+1} \right)$$

V. Assumindo que $\lim_{n \to \infty} \gamma^{n+1} G_{t+n+1} = 0$ (o que √© verdadeiro se $0 \leq \gamma < 1$ e as recompensas s√£o limitadas), temos:
    $$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

Portanto, demonstramos que a defini√ß√£o recursiva √© equivalente √† defini√ß√£o somat√≥ria: $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ ‚ñ†

Esta equival√™ncia √© crucial, pois a forma recursiva facilita a implementa√ß√£o de algoritmos de aprendizado por refor√ßo, enquanto a forma somat√≥ria fornece uma compreens√£o mais clara do significado do retorno com desconto.

### Conclus√£o
O conceito de retorno com desconto √© essencial para formular o problema de *reinforcement learning* em tarefas cont√≠nuas, onde a intera√ß√£o entre o agente e o ambiente n√£o se divide naturalmente em epis√≥dios [^9]. O fator de desconto $\gamma$ permite ponderar as recompensas futuras, evitando a diverg√™ncia do retorno e influenciando o comportamento do agente, que pode variar entre m√≠ope e farsighted [^9]. A rela√ß√£o recursiva do retorno com desconto simplifica o c√°lculo e a an√°lise, sendo fundamental para o desenvolvimento de algoritmos eficientes de *reinforcement learning* [^9].

### Refer√™ncias
[^1]: Chapter 3: Finite Markov Decision Processes
[^8]: Section 3.3: Returns and Episodes
[^9]: Section 3.3: Returns and Episodes
<!-- END -->