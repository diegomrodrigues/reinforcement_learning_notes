## Estimativa de Fun√ß√µes de Valor a partir da Experi√™ncia

### Introdu√ß√£o

Neste cap√≠tulo, exploramos os processos de decis√£o de Markov finitos (MDPs) e os conceitos fundamentais relacionados √†s pol√≠ticas e fun√ß√µes de valor. Em particular, focaremos na estima√ß√£o das fun√ß√µes de valor $v_\pi(s)$ e $q_\pi(s)$ a partir da experi√™ncia, abordando tanto o caso tabular, onde cada estado √© armazenado individualmente, quanto o caso em que as fun√ß√µes de valor s√£o aproximadas por fun√ß√µes parametrizadas para lidar com espa√ßos de estados extensos [^58].

### Estima√ß√£o de Fun√ß√µes de Valor a partir da Experi√™ncia

As fun√ß√µes de valor $v_\pi(s)$ e $q_\pi(s)$ s√£o cruciais para o aprendizado por refor√ßo, pois fornecem uma estimativa de "qu√£o bom" √© estar em um determinado estado ou executar uma determinada a√ß√£o em um estado espec√≠fico, seguindo uma pol√≠tica $\pi$ [^58]. Estas fun√ß√µes s√£o definidas em termos do retorno esperado, que √© a soma descontada das recompensas futuras [^58, 54].

Para estimar essas fun√ß√µes a partir da experi√™ncia, podemos seguir uma pol√≠tica $\pi$ e manter uma m√©dia dos retornos reais observados ap√≥s visitar cada estado $s$ ou par estado-a√ß√£o $(s, a)$. Este m√©todo √© conhecido como **m√©todo de Monte Carlo**, pois envolve a m√©dia de amostras aleat√≥rias de retornos reais [^59]. Formalmente, se denotarmos por $G_t$ o retorno obtido ap√≥s visitar o estado $s$ no tempo $t$, a estimativa da fun√ß√£o de valor do estado $s$ pode ser atualizada da seguinte forma:

$$
V(s) \leftarrow \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_i(s)
$$

Onde $N(s)$ √© o n√∫mero de vezes que o estado $s$ foi visitado e $G_i(s)$ √© o retorno observado na *$i$*-√©sima visita a $s$. Um processo similar pode ser realizado para a fun√ß√£o de valor da a√ß√£o $q_\pi(s,a)$.

> üí° **Exemplo Num√©rico:**
>
> Imagine um ambiente simples com tr√™s estados: $s_1$, $s_2$ e $s_3$. Seguimos uma pol√≠tica $\pi$ e obtemos os seguintes retornos ap√≥s visitar o estado $s_1$: 10, 12, 8, 11, 9. Ent√£o, $N(s_1) = 5$. A estimativa da fun√ß√£o de valor para o estado $s_1$ seria:
>
> $V(s_1) = \frac{1}{5} (10 + 12 + 8 + 11 + 9) = \frac{50}{5} = 10$
>
> Portanto, nossa estimativa de $v_\pi(s_1)$ √© 10.

√â crucial ressaltar que, para garantir a converg√™ncia para os valores corretos, √© preciso explorar suficientemente o espa√ßo de estados e a√ß√µes. No entanto, o texto n√£o aprofunda estrat√©gias para garantir uma explora√ß√£o adequada. Uma estrat√©gia comum para assegurar a explora√ß√£o √© o uso de pol√≠ticas $\epsilon$-greedy, que exploraremos a seguir.

**Teorema 1** (Converg√™ncia do M√©todo de Monte Carlo): Sob uma pol√≠tica $\pi$ que visita todos os estados e pares estado-a√ß√£o infinitamente, o m√©todo de Monte Carlo tabular converge para as fun√ß√µes de valor √≥timas $v_\pi(s)$ e $q_\pi(s, a)$.

*Prova*: A prova decorre da lei forte dos grandes n√∫meros, dado que cada $G_i(s)$ √© uma amostra independente e identicamente distribu√≠da do retorno esperado. Portanto, a m√©dia amostral converge para o valor esperado verdadeiro √† medida que $N(s) \rightarrow \infty$. $\blacksquare$

**Corol√°rio 1.1**: Se a pol√≠tica $\pi$ √© $\epsilon$-greedy, ent√£o, para $\epsilon > 0$, todos os estados e pares estado-a√ß√£o ser√£o visitados infinitamente com probabilidade 1, satisfazendo a condi√ß√£o de converg√™ncia do Teorema 1.

*Prova*: Por defini√ß√£o, uma pol√≠tica $\epsilon$-greedy seleciona uma a√ß√£o aleat√≥ria com probabilidade $\epsilon$, garantindo uma probabilidade n√£o nula de explorar qualquer a√ß√£o em qualquer estado. Portanto, a probabilidade de um estado ou par estado-a√ß√£o *n√£o* ser visitado tende a zero exponencialmente com o n√∫mero de passos. $\blacksquare$

Al√©m da m√©dia simples, podemos utilizar uma m√©dia ponderada exponencialmente, que atribui maior peso √†s amostras mais recentes. Isto √© especialmente √∫til em ambientes n√£o-estacion√°rios.

**Proposi√ß√£o 1**: O m√©todo de Monte Carlo com m√©dia ponderada exponencialmente pode ser expresso de forma incremental:

$V(s) \leftarrow V(s) + \alpha (G_t - V(s))$

onde $\alpha$ √© a taxa de aprendizado.

*Prova*: Seja $V_n(s)$ a estimativa da fun√ß√£o de valor ap√≥s $n$ visitas ao estado $s$. A atualiza√ß√£o da m√©dia ponderada exponencialmente √© dada por:

$V_{n+1}(s) = (1 - \alpha) V_n(s) + \alpha G_t$

Reorganizando, obtemos:

$V_{n+1}(s) = V_n(s) + \alpha (G_t - V_n(s))$

Esta forma incremental permite a atualiza√ß√£o da fun√ß√£o de valor a cada nova experi√™ncia, sem a necessidade de armazenar todos os retornos passados. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Suponha que $V(s)$ esteja inicialmente estimado como 5, $\alpha = 0.1$, e o retorno observado $G_t$ seja 15. A atualiza√ß√£o incremental seria:
>
> $V(s) \leftarrow 5 + 0.1 (15 - 5) = 5 + 0.1(10) = 5 + 1 = 6$
>
> A nova estimativa de $V(s)$ √© 6.

**Proposi√ß√£o 2**: A atualiza√ß√£o incremental da fun√ß√£o de valor $V(s) \leftarrow V(s) + \alpha (G_t - V(s))$ √© equivalente a calcular uma m√©dia ponderada dos retornos, com pesos que diminuem exponencialmente com o tempo.

*Prova*:
I.  Consideremos a sequ√™ncia de atualiza√ß√µes da fun√ß√£o de valor $V(s)$ ao longo do tempo. Seja $V_0(s)$ o valor inicial, e $G_1, G_2, \ldots, G_n$ os retornos observados nas primeiras $n$ visitas ao estado $s$.

II.  Ap√≥s a primeira atualiza√ß√£o:
    $V_1(s) = V_0(s) + \alpha(G_1 - V_0(s)) = (1-\alpha)V_0(s) + \alpha G_1$

III. Ap√≥s a segunda atualiza√ß√£o:
    $V_2(s) = V_1(s) + \alpha(G_2 - V_1(s)) = (1-\alpha)V_1(s) + \alpha G_2$
    Substituindo $V_1(s)$ da etapa II:
    $V_2(s) = (1-\alpha)[(1-\alpha)V_0(s) + \alpha G_1] + \alpha G_2 = (1-\alpha)^2 V_0(s) + (1-\alpha)\alpha G_1 + \alpha G_2$

IV. Generalizando para a $n$-√©sima atualiza√ß√£o:
    $V_n(s) = (1-\alpha)^n V_0(s) + \alpha \sum_{i=1}^{n} (1-\alpha)^{n-i} G_i$

V.  Reconhecemos que $V_n(s)$ √© uma m√©dia ponderada dos retornos $G_i$, onde o peso de cada retorno diminui exponencialmente com o tempo, com fator $(1-\alpha)$. O valor inicial $V_0(s)$ tamb√©m tem um peso que diminui exponencialmente. √Ä medida que $n$ aumenta, o peso de $V_0(s)$ tende a zero se $\alpha > 0$.

VI. Portanto, a atualiza√ß√£o incremental √© equivalente a calcular uma m√©dia ponderada exponencialmente dos retornos. ‚ñ†

### Fun√ß√µes de Valor Parametrizadas

No entanto, o m√©todo de Monte Carlo tabular torna-se impratic√°vel quando o n√∫mero de estados √© muito grande, pois seria necess√°rio armazenar e atualizar um valor para cada estado individualmente [^59]. Nesses casos, √© necess√°rio recorrer a fun√ß√µes de valor parametrizadas. Em vez de armazenar um valor para cada estado, aproximamos as fun√ß√µes de valor usando uma fun√ß√£o parametrizada $v_\pi(s; \mathbf{w})$ ou $q_\pi(s, a; \mathbf{w})$, onde $\mathbf{w}$ √© um vetor de par√¢metros com um n√∫mero de elementos muito menor que o n√∫mero de estados [^59].

> üí° **Exemplo Num√©rico:**
>
> Considere que queremos aproximar a fun√ß√£o de valor com uma fun√ß√£o linear: $v_\pi(s; \mathbf{w}) = w_1 * feature_1(s) + w_2 * feature_2(s)$.
>
> Se o estado $s$ for representado por dois atributos (features): $feature_1(s) = 2$ e $feature_2(s) = 3$, e os par√¢metros atuais forem $\mathbf{w} = [0.5, 1]$, ent√£o:
>
> $v_\pi(s; \mathbf{w}) = 0.5 * 2 + 1 * 3 = 1 + 3 = 4$
>
> Assim, a fun√ß√£o parametrizada estima o valor do estado $s$ como 4.

O objetivo, nesse caso, √© ajustar os par√¢metros $\mathbf{w}$ para que a fun√ß√£o aproximada $v_\pi(s; \mathbf{w})$ ou $q_\pi(s, a; \mathbf{w})$ se aproxime o m√°ximo poss√≠vel dos valores reais das fun√ß√µes de valor [^59]. Isso pode ser feito utilizando m√©todos de otimiza√ß√£o, como o gradiente descendente, para minimizar uma fun√ß√£o de perda que mede a diferen√ßa entre os valores previstos e os retornos observados [^58]. Uma fun√ß√£o de perda comum √© o erro quadr√°tico m√©dio:

$$
L(\mathbf{w}) = \mathbb{E}_\pi \left[ (G_t - v_\pi(S_t; \mathbf{w}))^2 \right]
$$

O gradiente descendente atualiza os par√¢metros na dire√ß√£o oposta do gradiente da fun√ß√£o de perda:

$$
\mathbf{w} \leftarrow \mathbf{w} - \alpha \nabla_{\mathbf{w}} L(\mathbf{w})
$$

onde $\alpha$ √© a taxa de aprendizado.

**Proposi√ß√£o 3:** A atualiza√ß√£o do gradiente descendente para o erro quadr√°tico m√©dio √© dada por:

$$\mathbf{w} \leftarrow \mathbf{w} + \alpha (G_t - v_\pi(S_t; \mathbf{w})) \nabla_{\mathbf{w}} v_\pi(S_t; \mathbf{w})$$

*Prova:*

I. Come√ßamos com a fun√ß√£o de perda de erro quadr√°tico m√©dio:
    $$L(\mathbf{w}) = \mathbb{E}_\pi \left[ (G_t - v_\pi(S_t; \mathbf{w}))^2 \right]$$

II. Calculamos o gradiente da fun√ß√£o de perda em rela√ß√£o aos par√¢metros $\mathbf{w}$:
    $$\nabla_{\mathbf{w}} L(\mathbf{w}) = \nabla_{\mathbf{w}} \mathbb{E}_\pi \left[ (G_t - v_\pi(S_t; \mathbf{w}))^2 \right]$$

III. Usando a regra da cadeia, diferenciamos o termo dentro da esperan√ßa:
     $$\nabla_{\mathbf{w}} (G_t - v_\pi(S_t; \mathbf{w}))^2 = 2 (G_t - v_\pi(S_t; \mathbf{w})) \nabla_{\mathbf{w}} (G_t - v_\pi(S_t; \mathbf{w}))$$

IV. Como $G_t$ n√£o depende de $\mathbf{w}$, temos:
    $$\nabla_{\mathbf{w}} (G_t - v_\pi(S_t; \mathbf{w})) = - \nabla_{\mathbf{w}} v_\pi(S_t; \mathbf{w})$$

V. Substituindo este resultado de volta na express√£o para o gradiente da fun√ß√£o de perda:
    $$\nabla_{\mathbf{w}} L(\mathbf{w}) = \mathbb{E}_\pi \left[ -2 (G_t - v_\pi(S_t; \mathbf{w})) \nabla_{\mathbf{w}} v_\pi(S_t; \mathbf{w}) \right]$$

VI. A atualiza√ß√£o do gradiente descendente √© dada por:
    $$\mathbf{w} \leftarrow \mathbf{w} - \alpha \nabla_{\mathbf{w}} L(\mathbf{w})$$

VII. Substituindo a express√£o para o gradiente de $L(\mathbf{w})$:
    $$\mathbf{w} \leftarrow \mathbf{w} - \alpha \mathbb{E}_\pi \left[ -2 (G_t - v_\pi(S_t; \mathbf{w})) \nabla_{\mathbf{w}} v_\pi(S_t; \mathbf{w}) \right]$$
    $$\mathbf{w} \leftarrow \mathbf{w} + 2\alpha \mathbb{E}_\pi \left[ (G_t - v_\pi(S_t; \mathbf{w})) \nabla_{\mathbf{w}} v_\pi(S_t; \mathbf{w}) \right]$$

VIII. Para uma √∫nica amostra (gradiente descendente estoc√°stico), a atualiza√ß√£o se torna:
     $$\mathbf{w} \leftarrow \mathbf{w} + \alpha (G_t - v_\pi(S_t; \mathbf{w})) \nabla_{\mathbf{w}} v_\pi(S_t; \mathbf{w})$$
     (Absorvendo o fator 2 na taxa de aprendizado $\alpha$)

IX. Portanto, a atualiza√ß√£o do gradiente descendente √© dada por:
    $$\mathbf{w} \leftarrow \mathbf{w} + \alpha (G_t - v_\pi(S_t; \mathbf{w})) \nabla_{\mathbf{w}} v_\pi(S_t; \mathbf{w})$$ ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo anterior, suponha que o retorno real $G_t$ para o estado $s$ seja 7. A fun√ß√£o de valor parametrizada estimou $v_\pi(s; \mathbf{w}) = 4$. Vamos usar $\alpha = 0.1$. Precisamos do gradiente da fun√ß√£o de valor em rela√ß√£o aos par√¢metros:
>
> $\nabla_{\mathbf{w}} v_\pi(S_t; \mathbf{w}) = [\frac{\partial v}{\partial w_1}, \frac{\partial v}{\partial w_2}] = [feature_1(s), feature_2(s)] = [2, 3]$
>
> A atualiza√ß√£o dos par√¢metros seria:
>
> $\mathbf{w} \leftarrow \mathbf{w} + \alpha (G_t - v_\pi(S_t; \mathbf{w})) \nabla_{\mathbf{w}} v_\pi(S_t; \mathbf{w})$
>
> $\mathbf{w} \leftarrow [0.5, 1] + 0.1 (7 - 4) [2, 3] = [0.5, 1] + 0.1 (3) [2, 3] = [0.5, 1] + [0.6, 0.9] = [1.1, 1.9]$
>
> Os novos par√¢metros s√£o $\mathbf{w} = [1.1, 1.9]$.

A escolha da fun√ß√£o de aproxima√ß√£o e do m√©todo de otimiza√ß√£o √© crucial para o sucesso do aprendizado por refor√ßo em espa√ßos de estados grandes. O texto menciona que estas possibilidades ser√£o discutidas na Parte II [^59].

### Conclus√£o

A estima√ß√£o das fun√ß√µes de valor a partir da experi√™ncia √© um passo fundamental para o aprendizado por refor√ßo [^58]. Os m√©todos de Monte Carlo fornecem uma maneira simples e intuitiva de estimar essas fun√ß√µes em espa√ßos de estados pequenos [^59]. No entanto, para lidar com espa√ßos de estados grandes, √© necess√°rio recorrer a fun√ß√µes de valor parametrizadas, que permitem aproximar as fun√ß√µes de valor com um n√∫mero muito menor de par√¢metros [^59].

### Refer√™ncias

[^58]: Cap√≠tulo 3: Finite Markov Decision Processes, p√°gina 58
[^54]: Cap√≠tulo 3: Finite Markov Decision Processes, p√°gina 54
[^59]: Cap√≠tulo 3: Finite Markov Decision Processes, p√°gina 59
<!-- END -->