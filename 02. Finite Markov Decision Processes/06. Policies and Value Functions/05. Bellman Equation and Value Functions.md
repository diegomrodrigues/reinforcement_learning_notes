## Rela√ß√µes Recursivas em Fun√ß√µes de Valor e a Equa√ß√£o de Bellman

### Introdu√ß√£o
Neste cap√≠tulo, exploramos em profundidade as **fun√ß√µes de valor** e suas propriedades dentro do contexto de **Processos de Decis√£o de Markov (MDPs) finitos** [^1]. Uma das caracter√≠sticas mais importantes dessas fun√ß√µes √© que elas satisfazem **rela√ß√µes recursivas**. Essas rela√ß√µes expressam uma consist√™ncia fundamental entre o valor de um estado e os valores de seus estados sucessores. A **Equa√ß√£o de Bellman** √© a representa√ß√£o matem√°tica chave dessa rela√ß√£o, e ela fornece a base para muitos algoritmos de reinforcement learning. Usaremos **diagramas de backup** para visualizar essas rela√ß√µes recursivas.

### Conceitos Fundamentais

A se√ß√£o 3.5 do texto [^58] introduz as **fun√ß√µes de valor** como estimativas de "qu√£o bom" √© para um agente estar em um determinado estado ou realizar uma determinada a√ß√£o em um determinado estado. Formalmente, a **fun√ß√£o de valor de estado** $v_{\pi}(s)$ define o retorno esperado ao come√ßar no estado $s$ e seguir a pol√≠tica $\pi$ dali em diante. A **fun√ß√£o de valor de a√ß√£o** $q_{\pi}(s, a)$ define o retorno esperado ao come√ßar no estado $s$, executar a a√ß√£o $a$ e seguir a pol√≠tica $\pi$ dali em diante [^58]. Essas fun√ß√µes de valor s√£o cruciais porque fornecem uma base para o agente tomar decis√µes informadas sobre quais a√ß√µes tomar em diferentes estados.

Uma propriedade fundamental das fun√ß√µes de valor, e que exploraremos agora, √© a de que elas satisfazem rela√ß√µes recursivas [^59]. Especificamente, o valor de um estado pode ser expresso em termos dos valores dos seus estados sucessores. Esta propriedade √© capturada pela **Equa√ß√£o de Bellman**. A **Equa√ß√£o de Bellman** √© uma rela√ß√£o de auto-consist√™ncia [^63]:

$$
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]
$$

> üí° **Exemplo Num√©rico:**
>
> Imagine um MDP com tr√™s estados: $S = \{s_1, s_2, s_3\}$. Considere uma pol√≠tica $\pi$ tal que, come√ßando em $s_1$, o agente recebe uma recompensa m√©dia de $R_{t+1} = 2$ e, com probabilidade 0.7, vai para $s_2$ e, com probabilidade 0.3, vai para $s_3$. Suponha que $v_{\pi}(s_2) = 5$ e $v_{\pi}(s_3) = 10$ e $\gamma = 0.9$. Podemos calcular $v_{\pi}(s_1)$ usando a Equa√ß√£o de Bellman:
>
> $v_{\pi}(s_1) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s_1] = 2 + 0.9 * (0.7 * 5 + 0.3 * 10) = 2 + 0.9 * (3.5 + 3) = 2 + 0.9 * 6.5 = 2 + 5.85 = 7.85$
>
> Isso significa que o valor de estar no estado $s_1$ seguindo a pol√≠tica $\pi$ √© 7.85.

Expandindo a Equa√ß√£o de Bellman, obtemos:
$$
v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_{\pi}(s')] \quad \forall s \in \mathcal{S}
$$

onde:
*   $\pi(a|s)$ √© a probabilidade de selecionar a a√ß√£o $a$ no estado $s$ sob a pol√≠tica $\pi$.
*   $p(s', r|s, a)$ √© a probabilidade de transi√ß√£o para o estado $s'$ e receber a recompensa $r$ ao tomar a a√ß√£o $a$ no estado $s$.
*   $\gamma$ √© o fator de desconto, que determina o valor das recompensas futuras.

A Equa√ß√£o de Bellman expressa que o valor de um estado $s$ sob uma pol√≠tica $\pi$ √© igual √† recompensa esperada imediata mais o valor descontado dos estados sucessores, ponderados pelas probabilidades de transi√ß√£o e pela pol√≠tica.

**Prova de que** $v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s] = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_{\pi}(s')]$:

I.  Come√ßamos com a defini√ß√£o da fun√ß√£o de valor de estado: $v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s]$.

II. Usamos a defini√ß√£o de retorno $G_t = R_{t+1} + \gamma G_{t+1}$, onde $R_{t+1}$ √© a recompensa recebida ap√≥s o estado $S_t$ e $\gamma$ √© o fator de desconto: $v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]$.

III. Expandimos a expectativa condicional sobre todas as a√ß√µes poss√≠veis $a$ no estado $s$ sob a pol√≠tica $\pi$: $v_{\pi}(s) = \sum_{a} \pi(a|s) \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$.

IV. Expandimos ainda mais a expectativa condicional sobre todos os estados sucessores poss√≠veis $s'$ e recompensas $r$ dados o estado $s$ e a a√ß√£o $a$: $v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s']]$.

V. Reconhecemos que $\mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s']$ √© a defini√ß√£o de $v_{\pi}(s')$: $v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_{\pi}(s')]$.

VI. Portanto, demonstramos a igualdade: $v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) [r + \gamma v_{\pi}(s')] \quad \forall s \in \mathcal{S}$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um estado $s$ onde o agente tem duas a√ß√µes poss√≠veis: $a_1$ e $a_2$. A pol√≠tica $\pi$ define $\pi(a_1|s) = 0.6$ e $\pi(a_2|s) = 0.4$. Ao tomar a a√ß√£o $a_1$, o agente transita para o estado $s'$ com recompensa $r = 5$ com probabilidade $p(s', r|s, a_1) = 0.8$ e para o estado $s''$ com recompensa $r = -3$ com probabilidade $p(s'', r|s, a_1) = 0.2$. Ao tomar a a√ß√£o $a_2$, o agente transita para o estado $s'$ com recompensa $r = 2$ com probabilidade $p(s', r|s, a_2) = 0.5$ e para o estado $s'''$ com recompensa $r = 10$ com probabilidade $p(s''', r|s, a_2) = 0.5$. Suponha que $v_{\pi}(s') = 10$, $v_{\pi}(s'') = 5$, e $v_{\pi}(s''') = 15$, e $\gamma = 0.9$.
>
> Ent√£o,
>
> $\sum_{s', r} p(s', r|s, a_1) [r + \gamma v_{\pi}(s')] = 0.8 * [5 + 0.9 * 10] + 0.2 * [-3 + 0.9 * 5] = 0.8 * 14 + 0.2 * 1.5 = 11.2 + 0.3 = 11.5$
>
> $\sum_{s', r} p(s', r|s, a_2) [r + \gamma v_{\pi}(s')] = 0.5 * [2 + 0.9 * 10] + 0.5 * [10 + 0.9 * 15] = 0.5 * 11 + 0.5 * 23.5 = 5.5 + 11.75 = 17.25$
>
> $v_{\pi}(s) = 0.6 * 11.5 + 0.4 * 17.25 = 6.9 + 6.9 = 13.8$

A **Equa√ß√£o de Bellman para $q_{\pi}(s,a)$** pode ser escrita como:

$$
q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] = \sum_{s', r} p(s', r|s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a')]
$$

Esta equa√ß√£o expressa que o valor de tomar a√ß√£o $a$ no estado $s$ sob uma pol√≠tica $\pi$ √© igual √† recompensa esperada imediata mais o valor descontado das fun√ß√µes de valor-a√ß√£o nos estados sucessores, ponderados pelas probabilidades de transi√ß√£o e pela pol√≠tica.

**Prova de que** $q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] = \sum_{s', r} p(s', r|s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a')]$:

I. Come√ßamos com a defini√ß√£o da fun√ß√£o de valor-a√ß√£o: $q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]$.

II. Usamos a defini√ß√£o de retorno $G_t = R_{t+1} + \gamma G_{t+1}$: $q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$.

III. Expandimos a expectativa condicional sobre todos os estados sucessores poss√≠veis $s'$ e recompensas $r$ dado o estado $s$ e a a√ß√£o $a$: $q_{\pi}(s, a) = \sum_{s', r} p(s', r|s, a) \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a, S_{t+1} = s', R_{t+1} = r]$.

IV. Simplificamos a expectativa: $q_{\pi}(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s']]$.

V. Expandimos a expectativa de $G_{t+1}$ dado $S_{t+1} = s'$ sobre todas as a√ß√µes poss√≠veis $a'$ sob a pol√≠tica $\pi$: $q_{\pi}(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma \sum_{a'} \pi(a'|s') \mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s', A_{t+1} = a']]$.

VI. Reconhecemos que $\mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s', A_{t+1} = a']$ √© a defini√ß√£o de $q_{\pi}(s', a')$: $q_{\pi}(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a')]$.

VII. Portanto, demonstramos a igualdade: $q_{\pi}(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a')]$. ‚ñ†

> üí° **Exemplo Num√©rico:**
>
> Considere um estado $s$ e uma a√ß√£o $a$. Ao tomar a a√ß√£o $a$ no estado $s$, o agente transita para o estado $s_1$ com recompensa $r_1 = 3$ e probabilidade $p(s_1, r_1|s, a) = 0.6$, e para o estado $s_2$ com recompensa $r_2 = -1$ e probabilidade $p(s_2, r_2|s, a) = 0.4$. No estado $s_1$, a pol√≠tica $\pi$ define $\pi(a'|s_1) = 0.7$ para a a√ß√£o $a'$ e $\pi(a''|s_1) = 0.3$ para a a√ß√£o $a''$. No estado $s_2$, a pol√≠tica $\pi$ define $\pi(a'|s_2) = 0.2$ para a a√ß√£o $a'$ e $\pi(a''|s_2) = 0.8$ para a a√ß√£o $a''$. Suponha que $q_{\pi}(s_1, a') = 5$, $q_{\pi}(s_1, a'') = 8$, $q_{\pi}(s_2, a') = 2$, e $q_{\pi}(s_2, a'') = 4$, e $\gamma = 0.9$.
>
> Ent√£o,
>
> $\sum_{a'} \pi(a'|s_1) q_{\pi}(s_1, a') = 0.7 * 5 + 0.3 * 8 = 3.5 + 2.4 = 5.9$
>
> $\sum_{a'} \pi(a'|s_2) q_{\pi}(s_2, a') = 0.2 * 2 + 0.8 * 4 = 0.4 + 3.2 = 3.6$
>
> $q_{\pi}(s, a) = 0.6 * [3 + 0.9 * 5.9] + 0.4 * [-1 + 0.9 * 3.6] = 0.6 * [3 + 5.31] + 0.4 * [-1 + 3.24] = 0.6 * 8.31 + 0.4 * 2.24 = 4.986 + 0.896 = 5.882$

**Teorema 1** A Equa√ß√£o de Bellman possui uma solu√ß√£o √∫nica para $v_{\pi}$.

*Prova:* A Equa√ß√£o de Bellman pode ser expressa na forma de uma equa√ß√£o linear $v = R + \gamma P v$, onde $v$ √© o vetor de valores de estado, $R$ √© o vetor de recompensas esperadas, e $P$ √© a matriz de transi√ß√£o.  Reescrevendo, temos $(I - \gamma P)v = R$.  Se a matriz $(I - \gamma P)$ for invert√≠vel, ent√£o existe uma √∫nica solu√ß√£o $v = (I - \gamma P)^{-1} R$.  Como $\gamma \in [0, 1)$, o raio espectral de $\gamma P$ √© menor que 1, garantindo que $(I - \gamma P)$ √© invert√≠vel. $\blacksquare$

Agora, vamos introduzir as **Equa√ß√µes de Bellman √ìtimas**. Estas equa√ß√µes definem $v_*(s)$ e $q_*(s, a)$, o valor √≥timo de um estado e o valor √≥timo de um par estado-a√ß√£o, respectivamente.

A **Equa√ß√£o de Bellman √ìtima para $v_*(s)$** √© dada por:

$$
v_*(s) = \max_a \sum_{s', r} p(s', r|s, a) [r + \gamma v_*(s')]
$$

> üí° **Exemplo Num√©rico:**
>
> Suponha que em um estado $s$, um agente tem duas a√ß√µes, $a_1$ e $a_2$. Tomar a a√ß√£o $a_1$ leva ao estado $s'$ com recompensa 5, com probabilidade 1. Tomar a a√ß√£o $a_2$ leva ao estado $s''$ com recompensa 10, com probabilidade 1. Assuma que $v_*(s') = 20$ e $v_*(s'') = 15$, e $\gamma = 0.9$.
>
> $\sum_{s', r} p(s', r|s, a_1) [r + \gamma v_*(s')] = 1 * [5 + 0.9 * 20] = 5 + 18 = 23$
>
> $\sum_{s', r} p(s', r|s, a_2) [r + \gamma v_*(s')] = 1 * [10 + 0.9 * 15] = 10 + 13.5 = 23.5$
>
> Portanto, $v_*(s) = \max(23, 23.5) = 23.5$. A a√ß√£o √≥tima √© $a_2$.

A **Equa√ß√£o de Bellman √ìtima para $q_*(s, a)$** √© dada por:

$$
q_*(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma \max_{a'} q_*(s', a')]
$$

> üí° **Exemplo Num√©rico:**
>
> Suponha que ao tomar a a√ß√£o $a$ no estado $s$, o agente transita para o estado $s_1$ com recompensa 2 (probabilidade 0.7) e para o estado $s_2$ com recompensa 4 (probabilidade 0.3). No estado $s_1$, as a√ß√µes poss√≠veis s√£o $a'$ e $a''$, com $q_*(s_1, a') = 10$ e $q_*(s_1, a'') = 5$. No estado $s_2$, as a√ß√µes poss√≠veis s√£o $a'$ e $a''$, com $q_*(s_2, a') = 8$ e $q_*(s_2, a'') = 12$. Assuma $\gamma = 0.9$.
>
> $\max_{a'} q_*(s_1, a') = \max(10, 5) = 10$
>
> $\max_{a'} q_*(s_2, a') = \max(8, 12) = 12$
>
> $q_*(s, a) = 0.7 * [2 + 0.9 * 10] + 0.3 * [4 + 0.9 * 12] = 0.7 * [2 + 9] + 0.3 * [4 + 10.8] = 0.7 * 11 + 0.3 * 14.8 = 7.7 + 4.44 = 12.14$

**Observa√ß√£o:** As equa√ß√µes de Bellman √≥timas n√£o s√£o lineares, devido ao operador $\max$. Isto torna a busca pela solu√ß√£o √≥tima mais complexa do que a solu√ß√£o para uma pol√≠tica fixa.

**Lema 1.1** A fun√ß√£o de valor √≥timo $v_*(s)$ √© o retorno esperado m√°ximo que pode ser alcan√ßado a partir do estado $s$, seguindo qualquer pol√≠tica.

*Prova:* Por defini√ß√£o, $v_*(s) = \max_{\pi} v_{\pi}(s)$. Portanto, $v_*(s)$ √© pelo menos t√£o bom quanto qualquer $v_{\pi}(s)$. Se existisse uma pol√≠tica que superasse $v_*(s)$, ent√£o $v_*(s)$ n√£o seria o valor √≥timo, contradizendo a defini√ß√£o. $\blacksquare$

**Corol√°rio 1.1** Similarmente, a fun√ß√£o de valor-a√ß√£o √≥timo $q_*(s, a)$ representa o retorno esperado m√°ximo ao iniciar no estado $s$, tomar a a√ß√£o $a$ e, posteriormente, seguir a pol√≠tica √≥tima.

**Diagramas de Backup**

Os **diagramas de backup** s√£o ferramentas visuais que ajudam a entender as rela√ß√µes de backup que formam a base para a atualiza√ß√£o ou opera√ß√µes de *backup* [^60]. Essas opera√ß√µes transferem informa√ß√µes de valor de volta para um estado (ou par de estado-a√ß√£o) a partir de seus estados sucessores (ou pares de estado-a√ß√£o).

Por exemplo, a Figura 3.3 [^59, 61] exibe um diagrama de backup para $v_{\pi}$. O diagrama demonstra como o valor de um estado √© determinado pelos valores ponderados de seus sucessores.

![Diagrama de backup para a fun√ß√£o de valor \(v_\pi\), ilustrando a rela√ß√£o entre um estado e seus sucessores sob uma pol√≠tica \(\pi\).](./../images/image3.png)

Similarmente, a Figura 3.4 [^64] ilustra diagramas de backup para $v_*$ e $q_*$, mostrando como os valores √≥timos est√£o relacionados.

![Backup diagrams illustrating the update rules for state-value \(v_*\) and action-value \(q_*\) functions in MDPs.](./../images/image1.png)

**Proposi√ß√£o 2** Uma pol√≠tica $\pi$ √© √≥tima se, e somente se, para todo estado $s$,

$$
\pi(a|s) > 0 \implies a \in \arg \max_{a'} \sum_{s', r} p(s', r|s, a') [r + \gamma v_*(s')]
$$

*Prova:* Se $\pi$ √© √≥tima, ent√£o $v_{\pi}(s) = v_*(s)$ para todo $s$.  Se $\pi(a|s) > 0$, ent√£o tomar a a√ß√£o $a$ em $s$ sob $\pi$ deve ser uma a√ß√£o √≥tima. Caso contr√°rio, poder√≠amos melhorar $\pi$ tomando uma a√ß√£o diferente em $s$, contradizendo a otimalidade de $\pi$.  A implica√ß√£o inversa segue de forma similar. $\blacksquare$

> üí° **Exemplo Num√©rico:**
>
> Considere um estado $s$ com duas a√ß√µes poss√≠veis, $a_1$ e $a_2$. Suponha que $v_*(s') = 10$ para todos os estados $s'$. Seja $\gamma = 0.9$.
>
> Se tomar a a√ß√£o $a_1$ leva a uma recompensa esperada de 5, ent√£o $\sum_{s', r} p(s', r|s, a_1) [r + \gamma v_*(s')] = 5 + 0.9 * 10 = 14$.
> Se tomar a a√ß√£o $a_2$ leva a uma recompensa esperada de 6, ent√£o $\sum_{s', r} p(s', r|s, a_2) [r + \gamma v_*(s')] = 6 + 0.9 * 10 = 15$.
>
> Para que a pol√≠tica $\pi$ seja √≥tima, se $\pi(a_1|s) > 0$, ent√£o $a_1$ deve ser a a√ß√£o que maximiza $\sum_{s', r} p(s', r|s, a') [r + \gamma v_*(s')]$. Neste caso, como a a√ß√£o $a_2$ leva a um valor maior (15 > 14), $\pi(a_1|s)$ s√≥ pode ser maior que zero se $a_1$ *tamb√©m* for uma das a√ß√µes que maximizam a express√£o (o que n√£o √© verdade neste caso). Assim, para que $\pi$ seja √≥tima, $\pi(a_1|s)$ deve ser 0.
<!-- END -->