## Value Functions in Finite Markov Decision Processes

### Introdu√ß√£o
O conceito de **value functions** √© fundamental em **Reinforcement Learning (RL)**, fornecendo uma estimativa de qu√£o bom √© para um agente estar em um determinado estado ou realizar uma determinada a√ß√£o em um estado espec√≠fico [^58]. Essas fun√ß√µes quantificam a no√ß√£o de "qu√£o bom" em termos de recompensa futura esperada, guiando o processo de tomada de decis√£o do agente. Este cap√≠tulo aprofunda a defini√ß√£o, propriedades e import√¢ncia das value functions em **Finite Markov Decision Processes (MDPs)**.

### Conceitos Fundamentais

As **value functions** s√£o estimativas de qu√£o bom √© para o agente estar em um determinado estado ou realizar uma determinada a√ß√£o [^58]. A "bondade" √© definida em termos de recompensas futuras esperadas. A recompensa que o agente espera receber no futuro depende das a√ß√µes que ele toma. Portanto, as **value functions** s√£o definidas com respeito a maneiras particulares de agir, chamadas *policies* [^58].

Formalmente, uma **policy** √© um mapeamento de estados para probabilidades de selecionar cada a√ß√£o poss√≠vel. Se o agente est√° seguindo a *policy* $\pi$ no tempo $t$, ent√£o $\pi(a|s)$ √© a probabilidade de que $A_t = a$ se $S_t = s$ [^58]. Assim como $p$, $\pi$ √© uma fun√ß√£o comum; o "|" no meio de $\pi(a|s)$ simplesmente nos lembra que ela define uma distribui√ß√£o de probabilidade sobre $a \in A(s)$ para cada $s \in S$ [^58]. Os m√©todos de **Reinforcement Learning** especificam como a *policy* do agente √© alterada como resultado de sua experi√™ncia [^58].

> üí° **Exemplo Num√©rico:**
> Imagine um agente em um ambiente simples com dois estados, $S = \{s_1, s_2\}$, e duas a√ß√µes, $A = \{a_1, a_2\}$. Seja $\pi(a_1|s_1) = 0.7$ e $\pi(a_2|s_1) = 0.3$. Isso significa que, no estado $s_1$, o agente escolhe a a√ß√£o $a_1$ com probabilidade 0.7 e a a√ß√£o $a_2$ com probabilidade 0.3. Da mesma forma, seja $\pi(a_1|s_2) = 0.1$ e $\pi(a_2|s_2) = 0.9$. No estado $s_2$, o agente escolhe $a_1$ com probabilidade 0.1 e $a_2$ com probabilidade 0.9. Esta √© uma *policy* $\pi$ que define como o agente se comporta em cada estado.

A **value function** de um estado $s$ sob uma *policy* $\pi$, denotada por $v_{\pi}(s)$, √© o retorno esperado quando se inicia em $s$ e se segue $\pi$ da√≠ em diante [^58]. Para MDPs, podemos definir $v_{\pi}$ formalmente por:

$$
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s \right], \text{ para todo } s \in S,
$$

onde $\mathbb{E}_{\pi}[\cdot]$ denota o valor esperado de uma vari√°vel aleat√≥ria, dado que o agente segue a *policy* $\pi$, e $t$ √© um passo de tempo qualquer [^58]. Note que o valor do estado terminal, se houver, √© sempre zero [^58]. Chamamos a fun√ß√£o $v_{\pi}$ de fun√ß√£o *state-value* para a *policy* $\pi$ [^58].

> üí° **Exemplo Num√©rico:**
> Suponha que temos um MDP com apenas um estado n√£o terminal $s_1$ e um estado terminal $s_T$. Ao iniciar em $s_1$, o agente sempre recebe uma recompensa de $R = 1$ e transita para o estado terminal $s_T$ ap√≥s cada a√ß√£o, onde o epis√≥dio termina. Se $\gamma = 0.9$ e a *policy* $\pi$ sempre escolhe a a√ß√£o $a_1$ em $s_1$, ent√£o:
>
> $v_\pi(s_1) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s_1 \right] = \sum_{k=0}^{\infty} (0.9)^k (1) = \frac{1}{1 - 0.9} = 10$.
>
> Isso significa que o valor de estar no estado $s_1$ sob a *policy* $\pi$ √© 10.

Similarmente, definimos o valor de tomar a a√ß√£o $a$ no estado $s$ sob uma *policy* $\pi$, denotado $q_{\pi}(s, a)$, como o retorno esperado come√ßando de $s$, tomando a a√ß√£o $a$, e da√≠ em diante seguindo a *policy* $\pi$ [^58]:

$$
q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a \right].
$$

Chamamos $q_{\pi}$ de fun√ß√£o *action-value* para a *policy* $\pi$ [^58].

> üí° **Exemplo Num√©rico:**
> Considere o mesmo MDP do exemplo anterior. Agora, queremos calcular $q_\pi(s_1, a_1)$. Como a *policy* $\pi$ sempre escolhe $a_1$ em $s_1$, e ap√≥s tomar $a_1$, o agente recebe $R=1$ e vai para o estado terminal, o c√°lculo √© o mesmo que antes:
>
> $q_\pi(s_1, a_1) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s_1, A_t = a_1 \right] = \sum_{k=0}^{\infty} (0.9)^k (1) = 10$.
>
> Portanto, o valor de tomar a a√ß√£o $a_1$ no estado $s_1$ sob a *policy* $\pi$ √© 10.

A fun√ß√£o $v_\pi(s)$ satisfaz uma importante propriedade recursiva, conhecida como **Bellman equation for $v_\pi$** [^59]:

$$
v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]
$$

$$
= \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) \left[ r + \gamma \mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s'] \right]
$$

$$
= \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) \left[ r + \gamma v_{\pi}(s') \right], \text{ para todo } s \in S.
$$

A equa√ß√£o de Bellman expressa uma rela√ß√£o entre o valor de um estado e os valores de seus estados sucessores [^59].

![Diagrama de backup para a fun√ß√£o de valor \(v_\pi\), ilustrando a rela√ß√£o entre um estado e seus sucessores sob uma pol√≠tica \(\pi\).](./../images/image3.png)

*Prova da Equa√ß√£o de Bellman para $v_\pi$*:
I. Come√ßamos com a defini√ß√£o de $v_\pi(s)$:
   $$v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s]$$

II. Expandimos o retorno $G_t$ em seus componentes:
    $$v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots | S_t = s]$$

III. Fatoramos $\gamma$ do segundo termo em diante:
     $$v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \ldots) | S_t = s]$$

IV. Reconhecemos que o termo entre par√™nteses √© $\mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s']$:
    $$v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]$$

V. Expressamos o valor esperado sobre todas as a√ß√µes e estados sucessores poss√≠veis, ponderados pelas suas probabilidades sob a *policy* $\pi$ e a din√¢mica do ambiente $p(s', r|s, a)$:
    $$v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) \left[ r + \gamma \mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s'] \right]$$

VI. Substitu√≠mos $\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s']$ por $v_{\pi}(s')$:
     $$v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r|s, a) \left[ r + \gamma v_{\pi}(s') \right]$$
     Portanto, a equa√ß√£o de Bellman para $v_\pi$ √© comprovada. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Considere um MDP com dois estados $S = \{s_1, s_2\}$ e duas a√ß√µes $A = \{a_1, a_2\}$. Seja $\gamma = 0.9$. Suponha que:
> - $\pi(a_1|s_1) = 0.6$ e $\pi(a_2|s_1) = 0.4$
> - $\pi(a_1|s_2) = 0.3$ e $\pi(a_2|s_2) = 0.7$
> - $p(s_1, 5 | s_1, a_1) = 1$ (tomar $a_1$ em $s_1$ leva a $s_1$ com recompensa 5)
> - $p(s_2, 10 | s_1, a_2) = 1$ (tomar $a_2$ em $s_1$ leva a $s_2$ com recompensa 10)
> - $p(s_1, -2 | s_2, a_1) = 1$ (tomar $a_1$ em $s_2$ leva a $s_1$ com recompensa -2)
> - $p(s_2, 3 | s_2, a_2) = 1$ (tomar $a_2$ em $s_2$ leva a $s_2$ com recompensa 3)
>
> Agora, vamos aplicar a equa√ß√£o de Bellman para $v_\pi(s_1)$:
>
> $v_\pi(s_1) = \sum_{a} \pi(a|s_1) \sum_{s', r} p(s', r|s_1, a) [r + \gamma v_\pi(s')]$
> $v_\pi(s_1) = \pi(a_1|s_1) * p(s_1, 5 | s_1, a_1) * [5 + 0.9 * v_\pi(s_1)] + \pi(a_2|s_1) * p(s_2, 10 | s_1, a_2) * [10 + 0.9 * v_\pi(s_2)]$
> $v_\pi(s_1) = 0.6 * 1 * [5 + 0.9 * v_\pi(s_1)] + 0.4 * 1 * [10 + 0.9 * v_\pi(s_2)]$
> $v_\pi(s_1) = 3 + 0.54 * v_\pi(s_1) + 4 + 0.36 * v_\pi(s_2)$
> $v_\pi(s_1) = 7 + 0.54 * v_\pi(s_1) + 0.36 * v_\pi(s_2)$
>
> Similarmente, para $v_\pi(s_2)$:
>
> $v_\pi(s_2) = \sum_{a} \pi(a|s_2) \sum_{s', r} p(s', r|s_2, a) [r + \gamma v_\pi(s')]$
> $v_\pi(s_2) = \pi(a_1|s_2) * p(s_1, -2 | s_2, a_1) * [-2 + 0.9 * v_\pi(s_1)] + \pi(a_2|s_2) * p(s_2, 3 | s_2, a_2) * [3 + 0.9 * v_\pi(s_2)]$
> $v_\pi(s_2) = -0.6 + 0.27 * v_\pi(s_1) + 2.1 + 0.63 * v_\pi(s_2)$
> $v_\pi(s_2) = 1.5 + 0.27 * v_\pi(s_1) + 0.63 * v_\pi(s_2)$
>
> Agora, temos um sistema de duas equa√ß√µes com duas inc√≥gnitas:
>
> 1.  $v_\pi(s_1) = 7 + 0.54 * v_\pi(s_1) + 0.36 * v_\pi(s_2)$
> 2.  $v_\pi(s_2) = 1.5 + 0.27 * v_\pi(s_1) + 0.63 * v_\pi(s_2)$
>
> Resolvendo este sistema, encontramos os valores de $v_\pi(s_1)$ e $v_\pi(s_2)$.

Analogamente, a fun√ß√£o $q_\pi(s, a)$ tamb√©m satisfaz uma equa√ß√£o de Bellman [^61]:

$$
q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] = \sum_{s', r} p(s', r|s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_\pi(s', a')]
$$

![Diagrama de backup qœÄ ilustrando as rela√ß√µes entre estados, a√ß√µes e recompensas em reinforcement learning.](./../images/image10.png)

*Prova da Equa√ß√£o de Bellman para $q_\pi$*:
I. Come√ßamos com a defini√ß√£o de $q_\pi(s, a)$:
   $$q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]$$

II. Expandimos o retorno $G_t$ em seus componentes:
    $$q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots | S_t = s, A_t = a]$$

III. Fatoramos $\gamma$ do segundo termo em diante:
     $$q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \ldots) | S_t = s, A_t = a]$$

IV. Reconhecemos que o termo entre par√™nteses est√° relacionado ao valor de $q_\pi$ no pr√≥ximo estado:
    $$q_{\pi}(s, a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$$

V. Expressamos o valor esperado sobre todos os estados sucessores e recompensas poss√≠veis, dados $s$ e $a$:
    $$q_{\pi}(s, a) = \sum_{s', r} p(s', r|s, a) \mathbb{E}_{\pi}[r + \gamma G_{t+1} | S_{t+1} = s']$$

VI. Expressamos $G_{t+1}$ em termos de $q_\pi(s', a')$:
     $$q_{\pi}(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_\pi(s', a')]$$
     Portanto, a equa√ß√£o de Bellman para $q_\pi$ √© comprovada. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Usando o mesmo cen√°rio do exemplo anterior com $S = \{s_1, s_2\}$, $A = \{a_1, a_2\}$, e $\gamma = 0.9$, e as mesmas probabilidades de transi√ß√£o e recompensas. Agora, vamos aplicar a equa√ß√£o de Bellman para $q_\pi(s_1, a_1)$:
>
> $q_\pi(s_1, a_1) = \sum_{s', r} p(s', r|s_1, a_1) [r + \gamma \sum_{a'} \pi(a'|s') q_\pi(s', a')]$
> $q_\pi(s_1, a_1) = p(s_1, 5 | s_1, a_1) * [5 + 0.9 * (\pi(a_1|s_1) * q_\pi(s_1, a_1) + \pi(a_2|s_1) * q_\pi(s_1, a_2))]$
> Assumindo que $\pi(a_1|s_1)=0.6$ e $\pi(a_2|s_1)=0.4$, $p(s_1, 5 | s_1, a_1) = 1$, ent√£o:
> $q_\pi(s_1, a_1) = 1 * [5 + 0.9 * (\pi(a_1|s_1) * q_\pi(s_1, a_1) + \pi(a_2|s_1) * q_\pi(s_1, a_2))]$
> $q_\pi(s_1, a_1) = 5 + 0.9 * (0.6 * q_\pi(s_1, a_1) + 0.4 * q_\pi(s_1, a_2))$
>
> Esta equa√ß√£o se relaciona a outros valores de $q_\pi$, requerendo a resolu√ß√£o de um sistema de equa√ß√µes para todos os pares estado-a√ß√£o.

Uma consequ√™ncia importante da equa√ß√£o de Bellman para $v_\pi$ √© a possibilidade de expressar $q_\pi$ em termos de $v_\pi$, e vice-versa. Isso fornece flexibilidade na escolha da fun√ß√£o de valor a ser utilizada em diferentes contextos.

**Proposi√ß√£o 1** A fun√ß√£o $q_\pi(s, a)$ pode ser expressa em termos de $v_\pi(s')$ da seguinte forma:

$$
q_{\pi}(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')]
$$

*Prova:*
I. Come√ßamos com a equa√ß√£o de Bellman para $q_\pi(s, a)$:
   $$q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$$

II. Expandimos $G_{t+1}$ usando a defini√ß√£o de $v_\pi(s')$:
    $$q_{\pi}(s, a) = \sum_{s', r} p(s', r|s, a) \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a]$$

III. Como $R_{t+1}$ e $S_{t+1}$ s√£o determinados por $s$, $a$ e a din√¢mica do ambiente, podemos escrever:
     $$q_{\pi}(s, a) = \sum_{s', r} p(s', r|s, a) [r + \gamma v_\pi(s')] $$

IV. Portanto, $q_\pi(s, a)$ est√° expressa em termos de $v_\pi(s')$. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Usando novamente o ambiente anterior, vamos calcular $q_\pi(s_1, a_1)$ usando a proposi√ß√£o 1. Assumindo que ao tomar $a_1$ em $s_1$ sempre transitamos para $s_1$ com uma recompensa de 5, ou seja $p(s_1, 5 | s_1, a_1) = 1$:
>
> $q_\pi(s_1, a_1) = \sum_{s', r} p(s', r|s_1, a_1) [r + \gamma v_\pi(s')] = 1 * [5 + 0.9 * v_\pi(s_1)]$
>
> Neste caso, $q_\pi(s_1, a_1)$ depende diretamente do valor de $v_\pi(s_1)$. Este exemplo ilustra como podemos utilizar a fun√ß√£o $v_\pi$ para calcular $q_\pi$ quando a din√¢mica do ambiente √© conhecida.

Al√©m disso, podemos definir a *optimal state-value function* $v_*(s)$ como o m√°ximo valor alcan√ß√°vel sobre todas as *policies*:

$$v_*(s) = \max_{\pi} v_{\pi}(s),$$

e similarmente, a *optimal action-value function* $q_*(s, a)$ como o m√°ximo valor alcan√ß√°vel sobre todas as *policies*:

$$q_*(s, a) = \max_{\pi} q_{\pi}(s, a).$$

![Backup diagrams illustrating the update rules for state-value \(v_*\) and action-value \(q_*\) functions in MDPs.](./../images/image1.png)

> üí° **Exemplo Num√©rico:**
> Considere um cen√°rio onde, no estado $s_1$, um agente pode escolher entre duas *policies*: $\pi_1$ e $\pi_2$. Ap√≥s algumas itera√ß√µes de avalia√ß√£o, estima-se que $v_{\pi_1}(s_1) = 15$ e $v_{\pi_2}(s_1) = 20$. Ent√£o, a optimal state-value function para o estado $s_1$ √©:
>
> $v_*(s_1) = \max(v_{\pi_1}(s_1), v_{\pi_2}(s_1)) = \max(15, 20) = 20$.
>
> Similarmente, se $q_{\pi_1}(s_1, a_1) = 25$ e $q_{\pi_2}(s_1, a_1) = 30$, ent√£o
>
> $q_*(s_1, a_1) = \max(q_{\pi_1}(s_1, a_1), q_{\pi_2}(s_1, a_1)) = \max(25, 30) = 30$.
>
> Este exemplo mostra como a fun√ß√£o optimal value representa o melhor valor poss√≠vel que um agente pode obter.

As value functions $v_{\pi}$ e $q_{\pi}$ podem ser estimadas a partir da experi√™ncia [^58]. Por exemplo, se um agente segue a *policy* $\pi$ e mant√©m uma m√©dia, para cada estado encontrado, dos retornos reais que se seguiram a esse estado, ent√£o a m√©dia convergir√° para o valor do estado, $v_{\pi}(s)$, conforme o n√∫mero de vezes que esse estado √© encontrado se aproxima do infinito [^58]. Se m√©dias separadas s√£o mantidas para cada a√ß√£o tomada em cada estado, ent√£o essas m√©dias convergir√£o similarmente para os valores de a√ß√£o, $q_{\pi}(s, a)$ [^59]. M√©todos de estima√ß√£o deste tipo s√£o chamados de **Monte Carlo methods** porque eles envolvem a m√©dia sobre muitas amostras aleat√≥rias de retornos reais [^59].

> üí° **Exemplo Num√©rico:**
> Suponha que um agente visita o estado $s_1$ cinco vezes seguindo a pol√≠tica $\pi$. Os retornos observados ap√≥s cada visita s√£o: 8, 12, 10, 14, 11. Usando um m√©todo de Monte Carlo, a estimativa do valor de $s_1$ seria a m√©dia desses retornos:
>
> $v_\pi(s_1) \approx \frac{8 + 12 + 10 + 14 + 11}{5} = \frac{55}{5} = 11$.
>
> Da mesma forma, se o agente tomou a a√ß√£o $a_1$ no estado $s_1$ tr√™s vezes com retornos de 15, 18 e 20, ent√£o a estimativa de $q_\pi(s_1, a_1)$ seria:
>
> $q_\pi(s_1, a_1) \approx \frac{15 + 18 + 20}{3} = \frac{53}{3} \approx 17.67$.

A converg√™ncia dos m√©todos de Monte Carlo depende crucialmente da explora√ß√£o do espa√ßo de estados e a√ß√µes. Para garantir que todos os estados e a√ß√µes sejam visitados um n√∫mero suficiente de vezes, frequentemente empregam-se *policies* explorat√≥rias, como a $\epsilon$-greedy policy.

**Defini√ß√£o (Pol√≠tica $\epsilon$-greedy):** Uma pol√≠tica $\epsilon$-greedy com respeito a uma fun√ß√£o de valor $q(s,a)$ seleciona a a√ß√£o com o maior valor estimado com probabilidade $1-\epsilon$, e seleciona uma a√ß√£o aleat√≥ria com probabilidade $\epsilon$. Formalmente:

$$
\pi(a|s) =
\begin{cases}
\frac{\epsilon}{|A(s)|} + (1 - \epsilon) & \text{se } a = \arg\max_{a' \in A(s)} q(s, a') \\
\frac{\epsilon}{|A(s)|} & \text{caso contr√°rio}
\end{cases}
$$

onde $A(s)$ √© o conjunto de a√ß√µes dispon√≠veis no estado $s$.

*Prova de que $\epsilon$-greedy √© uma pol√≠tica probabil√≠stica*:
I. Para mostrar que $\pi(a|s)$ define uma pol√≠tica probabil√≠stica v√°lida, precisamos demonstrar que $\sum_{a \in A(s)} \pi(a|s) = 1$ para cada estado $s$.

II. Seja $a^* = \arg\max_{a' \in A(s)} q(s, a')$. Ent√£o, $\pi(a^*|s) = \frac{\epsilon}{|A(s)|} + (1 - \epsilon)$.

III. Para todas as outras a√ß√µes $a \neq a'$, $\pi(a|s) = \frac{\epsilon}{|A(s)|}$.

IV. Somando sobre todas as a√ß√µes em $A(s)$:
    $$\sum_{a \in A(s)} \pi(a|s) = \pi(a^*|s) + \sum_{a \neq a^*} \pi(a|s) = \left( \frac{\epsilon}{|A(s)|} + (1 - \epsilon) \right) + \sum_{a \neq a^*} \frac{\epsilon}{|A(s)|}$$

V. Como existem $|A(s)| - 1$ a√ß√µes onde $a \neq a^*$:
   $$ \sum_{a \in A(s)} \pi(a|s) = \frac{\epsilon}{|A(s)|} + 1 - \epsilon + (|A(s)| - 1) \frac{\epsilon}{|A(s)|}$$

VI. Simplificando:
    $$ \sum_{a \in A(s)} \pi(a|s) = \frac{\epsilon}{|A(s)|} + 1 - \epsilon + \epsilon - \frac{\epsilon}{|A(s)|} = 1$$
    Portanto, $\epsilon$-greedy √© uma pol√≠tica probabil√≠stica v√°lida. ‚ñ†

> üí° **Exemplo Num√©rico:**
> Considere um estado $s_1$ com tr√™s a√ß√µes poss√≠veis: $A(s_1) = \{a_1, a_2, a_3\}$. Suponha que as estimativas atuais de action-value sejam $q(s_1, a_1) = 10$, $q(s_1, a_2) = 15$, e $q(s_1, a_3) = 12$.
>
> Usando uma pol√≠tica $\epsilon$-greedy com $\epsilon = 0.1$, a probabilidade de selecionar cada a√ß√£o √©:
>
> - $a^* = \arg\max_{a' \in A(s_1)} q(s_1, a') = a_2$ (porque $q(s_1, a_2)$ √© o maior).
> - $\pi(a_2|s_1) = \frac{0.1}{3} + (1 - 0.1) = \frac{0.1}{3} + 0.9 \approx 0.933$.
> - $\pi(a_1|s_1) = \frac{0.1}{3} \approx 0.033$.
> - $\pi(a_3|s_1) = \frac{0.1}{3} \approx 0.033$.
>
> Neste caso, a a√ß√£o $a_2$ (com o maior valor estimado) √© selecionada com probabilidade de aproximadamente 93.3%, enquanto as outras a√ß√µes s√£o selecionadas aleatoriamente com probabilidade de aproximadamente 3.3% cada.

### Conclus√£o
As value functions s√£o componentes essenciais de algoritmos de **Reinforcement Learning**. Elas fornecem uma maneira de avaliar a qualidade de um estado ou a√ß√£o, permitindo que os agentes tomem decis√µes informadas para maximizar a recompensa futura esperada. Compreender as propriedades e os relacionamentos entre state-value functions ($v_{\pi}$) e action-value functions ($q_{\pi}$), bem como a equa√ß√£o de Bellman, √© crucial para o desenvolvimento de algoritmos eficientes de **Reinforcement Learning**.

### Refer√™ncias
[^58]: Sutton, Richard S.; Barto, Andrew G. Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press, 2018.
[^59]: Sutton, Richard S.; Barto, Andrew G. Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press, 2018.
[^61]: Sutton, Richard S.; Barto, Andrew G. Reinforcement Learning: An Introduction. 2nd ed. Cambridge, MA: MIT Press, 2018.
<!-- END -->