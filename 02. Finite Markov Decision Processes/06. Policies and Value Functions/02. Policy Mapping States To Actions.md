## Pol√≠ticas em Processos de Decis√£o de Markov Finitos: Uma An√°lise Detalhada

### Introdu√ß√£o
No contexto de **Processos de Decis√£o de Markov Finitos (MDPs)**, uma **pol√≠tica** desempenha um papel central na defini√ß√£o do comportamento de um agente. Este cap√≠tulo se aprofunda no conceito de pol√≠ticas, explorando sua defini√ß√£o formal e sua rela√ß√£o com as **fun√ß√µes de valor**. Pol√≠ticas determinam como um agente seleciona a√ß√µes em diferentes estados, buscando maximizar a recompensa acumulada ao longo do tempo. Conforme introduzido no Cap√≠tulo 3 [^1], um MDP formaliza o problema de tomada de decis√£o sequencial, onde as a√ß√µes influenciam n√£o apenas recompensas imediatas, mas tamb√©m estados futuros e, por extens√£o, recompensas futuras. O entendimento de pol√≠ticas √©, portanto, crucial para a constru√ß√£o de agentes de *reinforcement learning* (RL) eficazes.

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

### Pol√≠ticas: Mapeamento de Estados para A√ß√µes
Formalmente, uma **pol√≠tica** √© definida como um mapeamento de estados para probabilidades de sele√ß√£o de cada a√ß√£o poss√≠vel [^58]. Em outras palavras, dada uma pol√≠tica $\pi$, para cada estado $s \in S$ e cada a√ß√£o $a \in A(s)$, $\pi(a|s)$ representa a probabilidade de selecionar a a√ß√£o $a$ quando o agente est√° no estado $s$. √â crucial notar que $\pi$ √© uma fun√ß√£o ordin√°ria; o s√≠mbolo "|" em $\pi(a|s)$ serve apenas para lembrar que ela define uma distribui√ß√£o de probabilidade sobre $a \in A(s)$ para cada $s \in S$ [^58].

$$
\sum_{a \in A(s)} \pi(a|s) = 1, \forall s \in S
$$

Essa equa√ß√£o garante que, para cada estado, a soma das probabilidades de selecionar cada a√ß√£o poss√≠vel seja igual a 1, o que caracteriza uma distribui√ß√£o de probabilidade v√°lida. A pol√≠tica, portanto, dita o comportamento do agente em cada estado, influenciando diretamente a trajet√≥ria do agente atrav√©s do ambiente e as recompensas que ele recebe.

**Prova:**

Para cada estado $s \in S$, $\pi(a|s)$ representa a probabilidade de selecionar a a√ß√£o $a \in A(s)$ dado o estado $s$. A soma das probabilidades de todas as a√ß√µes poss√≠veis em um dado estado deve ser igual a 1, j√° que o agente deve selecionar uma a√ß√£o com certeza.
I. Por defini√ß√£o, $\pi(a|s)$ √© a probabilidade de selecionar a a√ß√£o $a$ no estado $s$.
II. A soma das probabilidades de todas as a√ß√µes poss√≠veis $a \in A(s)$ no estado $s$ deve ser igual a 1, pois alguma a√ß√£o deve ser tomada.
III. Portanto, $\sum_{a \in A(s)} \pi(a|s) = 1$. ‚ñ†

**Exemplo:** Considere um agente em um ambiente de grade simples, como o *Gridworld* apresentado no exemplo 3.5 [^60]. Uma pol√≠tica poss√≠vel poderia ser:

*   Em estados na metade superior da grade, sempre mover para o sul (probabilidade 1).
*   Em estados na metade inferior da grade, mover para o norte com probabilidade 0.5 e para o leste com probabilidade 0.5.
*   Nos estados especiais A e B, escolher a√ß√µes aleatoriamente.

Esta √© uma pol√≠tica estoc√°stica, pois nem sempre determina uma √∫nica a√ß√£o. A escolha da pol√≠tica tem um impacto dram√°tico no desempenho do agente.

> üí° **Exemplo Num√©rico:**
>
> Vamos considerar um estado espec√≠fico, $s_1$, na metade inferior da grade. De acordo com a pol√≠tica descrita, $\pi(\text{norte}|s_1) = 0.5$ e $\pi(\text{leste}|s_1) = 0.5$. As probabilidades para as a√ß√µes "sul" e "oeste" neste estado seriam 0. Isso garante que a soma das probabilidades seja igual a 1:
>
> $\pi(\text{norte}|s_1) + \pi(\text{leste}|s_1) + \pi(\text{sul}|s_1) + \pi(\text{oeste}|s_1) = 0.5 + 0.5 + 0 + 0 = 1$
>
> Agora, considere um estado $s_2$ na metade superior da grade. Aqui, $\pi(\text{sul}|s_2) = 1$, e todas as outras a√ß√µes t√™m probabilidade 0.
>
> $\pi(\text{norte}|s_2) + \pi(\text{leste}|s_2) + \pi(\text{sul}|s_2) + \pi(\text{oeste}|s_2) = 0 + 0 + 1 + 0 = 1$
>
> Este exemplo num√©rico demonstra como as probabilidades das a√ß√µes s√£o definidas para diferentes estados de acordo com a pol√≠tica.

**Observa√ß√£o:** √â importante distinguir entre **pol√≠ticas determin√≠sticas** e **pol√≠ticas estoc√°sticas**. Uma pol√≠tica determin√≠stica sempre seleciona a mesma a√ß√£o em um dado estado, ou seja, existe uma a√ß√£o $a$ tal que $\pi(a|s) = 1$ para todo $s$. Em contraste, uma pol√≠tica estoc√°stica atribui probabilidades a diferentes a√ß√µes em um mesmo estado [^58].

Para clarificar ainda mais, podemos definir formalmente pol√≠ticas determin√≠sticas:

**Defini√ß√£o:** Uma pol√≠tica $\pi$ √© dita *determin√≠stica* se, para todo estado $s \in S$, existe uma a√ß√£o $a \in A(s)$ tal que $\pi(a|s) = 1$. Equivalentemente, para todo estado $s \in S$, existe uma √∫nica a√ß√£o $a \in A(s)$ tal que $\pi(a'|s) = 0$ para todo $a' \in A(s), a' \neq a$.

> üí° **Exemplo Num√©rico:**
>
> Considere uma pol√≠tica determin√≠stica no Gridworld onde, em todos os estados, o agente sempre se move para o norte. Neste caso, para qualquer estado $s$, $\pi(\text{norte}|s) = 1$ e $\pi(a|s) = 0$ para qualquer outra a√ß√£o $a$ diferente de "norte". Isso significa que a pol√≠tica sempre escolhe a a√ß√£o "norte" com probabilidade 1, independentemente do estado.

### Fun√ß√µes de Valor e Pol√≠ticas

As **fun√ß√µes de valor** s√£o estimativas da "bondade" de estar em um determinado estado ou de realizar uma determinada a√ß√£o em um determinado estado, no sentido de recompensas futuras que podem ser esperadas [^58]. No entanto, estas recompensas dependem das a√ß√µes que o agente tomar√°, que por sua vez s√£o ditadas pela pol√≠tica. As fun√ß√µes de valor s√£o, portanto, definidas em rela√ß√£o a pol√≠ticas espec√≠ficas.

**Valor de Estado:** A *fun√ß√£o de valor de estado* $v_\pi(s)$ sob uma pol√≠tica $\pi$ √© o retorno esperado ao iniciar no estado $s$ e seguir a pol√≠tica $\pi$ a partir de ent√£o [^58]. Formalmente:

$$
v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s \right], \forall s \in S
$$

onde $G_t$ √© o retorno no tempo $t$, $R_{t+k+1}$ √© a recompensa recebida $k$ passos ap√≥s o tempo $t$, e $\gamma$ √© o fator de desconto ( $0 \leq \gamma \leq 1$). O valor de estado representa, portanto, uma m√©dia ponderada das recompensas futuras, descontadas pelo fator $\gamma$, que o agente espera receber ao seguir a pol√≠tica $\pi$ a partir do estado $s$.

**Valor de A√ß√£o:** Similarmente, a *fun√ß√£o de valor de a√ß√£o* $q_\pi(s, a)$ sob uma pol√≠tica $\pi$ √© o retorno esperado ao iniciar no estado $s$, tomar a a√ß√£o $a$, e seguir a pol√≠tica $\pi$ a partir de ent√£o [^58]. Formalmente:

$$
q_\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a] = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a \right], \forall s \in S, a \in A(s)
$$

O valor de a√ß√£o representa o valor de realizar uma a√ß√£o espec√≠fica em um estado espec√≠fico, considerando o comportamento subsequente do agente ditado pela pol√≠tica $\pi$.

> üí° **Exemplo Num√©rico:**
>
> Imagine um estado $s_3$ no Gridworld onde o agente recebe uma recompensa de +10 se alcan√ßar o estado terminal e -1 para cada outro passo. Seja $\gamma = 0.9$. Se o agente est√° em $s_3$ e a pol√≠tica $\pi$ garante que ele chegar√° ao estado terminal em 2 passos, o valor de estado $v_\pi(s_3)$ pode ser calculado da seguinte forma:
>
> $v_\pi(s_3) = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s_3 \right] = (-1) + (0.9)(-1) + (0.9)^2(10) = -1 - 0.9 + 8.1 = 6.2$
>
> Isso significa que o valor esperado de estar no estado $s_3$ e seguir a pol√≠tica $\pi$ √© 6.2.
>
> Agora, suponha que no mesmo estado $s_3$, o agente tem a op√ß√£o de realizar a a√ß√£o $a_1$ que o leva diretamente ao estado terminal (recompensa +10) com probabilidade 0.8 e a um estado "ruim" (recompensa -5) com probabilidade 0.2. O valor de a√ß√£o $q_\pi(s_3, a_1)$ seria:
>
> $q_\pi(s_3, a_1) = (0.8)(10) + (0.2)(-5) = 8 - 1 = 7$

**Rela√ß√£o entre Valor de Estado e Valor de A√ß√£o:** O valor de um estado pode ser expresso em termos dos valores de suas a√ß√µes e da pol√≠tica [^58]:

$$
v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) q_\pi(s, a)
$$

Esta equa√ß√£o mostra que o valor de um estado √© a m√©dia ponderada dos valores de cada a√ß√£o poss√≠vel nesse estado, com os pesos sendo as probabilidades de selecionar cada a√ß√£o de acordo com a pol√≠tica $\pi$.

**Prova:**
Aqui, provaremos que $v_\pi(s)$ pode ser expresso em termos de $q_\pi(s, a)$ e $\pi(a|s)$.
I. Por defini√ß√£o, $v_\pi(s)$ √© o valor esperado do retorno $G_t$ dado que estamos no estado $s$ e seguindo a pol√≠tica $\pi$.
II. Podemos expressar o valor esperado $v_\pi(s)$ como uma m√©dia ponderada dos valores de a√ß√£o $q_\pi(s, a)$ para todas as a√ß√µes poss√≠veis $a \in A(s)$, ponderadas pelas probabilidades de tomar cada a√ß√£o $a$ no estado $s$ segundo a pol√≠tica $\pi(a|s)$.
III. Portanto, $v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \sum_{a \in A(s)} \pi(a|s) \mathbb{E}_\pi[G_t | S_t = s, A_t = a] = \sum_{a \in A(s)} \pi(a|s) q_\pi(s, a)$. ‚ñ†

Da mesma forma, o valor de uma a√ß√£o pode ser expresso em termos dos valores dos estados sucessores [^58]:

$$
q_\pi(s, a) = \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) \left[ r + \gamma v_\pi(s') \right]
$$

Esta equa√ß√£o mostra que o valor de uma a√ß√£o √© o retorno esperado ao realizar essa a√ß√£o e, em seguida, seguir a pol√≠tica $\pi$ nos estados sucessores. Ela considera a probabilidade de transi√ß√£o para cada estado sucessor, a recompensa recebida e o valor do estado sucessor, todos descontados pelo fator $\gamma$.

**Prova:**
Provaremos que $q_\pi(s, a)$ pode ser expresso em termos de $p(s', r | s, a)$, $r$ e $v_\pi(s')$.

I. Por defini√ß√£o, $q_\pi(s, a)$ √© o valor esperado do retorno $G_t$ dado que estamos no estado $s$, tomamos a a√ß√£o $a$ e seguimos a pol√≠tica $\pi$ a partir de ent√£o.

II. O retorno $G_t$ pode ser decomposto na recompensa imediata $R_{t+1} = r$ e o retorno descontado a partir do pr√≥ximo estado $S_{t+1} = s'$.

III. Portanto, $q_\pi(s, a) = \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a]$.

IV. Podemos expressar o valor esperado $q_\pi(s, a)$ como uma m√©dia ponderada das recompensas imediatas $r$ e dos valores de estado descontados $\gamma v_\pi(s')$ para todos os estados sucessores poss√≠veis $s'$, ponderados pelas probabilidades de transi√ß√£o $p(s', r | s, a)$.

V. Assim, $q_\pi(s, a) = \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) \left[ r + \gamma v_\pi(s') \right]$. ‚ñ†

Podemos combinar estas duas equa√ß√µes para expressar $v_\pi(s)$ diretamente em termos de $v_\pi(s')$:

**Teorema 1:**

$$
v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) \left[ r + \gamma v_\pi(s') \right]
$$

*Prova:* Substitu√≠mos a equa√ß√£o de $q_\pi(s, a)$ na equa√ß√£o de $v_\pi(s)$:

$v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) q_\pi(s, a) = \sum_{a \in A(s)} \pi(a|s) \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) \left[ r + \gamma v_\pi(s') \right]$.

> üí° **Exemplo Num√©rico:**
>
> Suponha que no estado $s_4$, o agente pode escolher entre duas a√ß√µes: $a_1$ e $a_2$. A pol√≠tica $\pi$ define $\pi(a_1|s_4) = 0.6$ e $\pi(a_2|s_4) = 0.4$. Se a a√ß√£o $a_1$ leva ao estado $s'$ com recompensa +5 e $v_\pi(s') = 10$, e a a√ß√£o $a_2$ leva ao estado $s''$ com recompensa -2 e $v_\pi(s'') = 2$, e $\gamma = 0.9$, ent√£o o valor de estado $v_\pi(s_4)$ pode ser calculado como:
>
> $v_\pi(s_4) = (0.6) \left[ 5 + (0.9)(10) \right] + (0.4) \left[ -2 + (0.9)(2) \right] = (0.6)[5 + 9] + (0.4)[-2 + 1.8] = (0.6)(14) + (0.4)(-0.2) = 8.4 - 0.08 = 8.32$
>
> Este c√°lculo demonstra como o valor de estado √© influenciado pelas probabilidades da pol√≠tica, as recompensas esperadas e os valores dos estados sucessores.

### A Equa√ß√£o de Bellman para $v_\pi$
A rela√ß√£o recursiva entre o valor de um estado e o valor de seus sucessores √© capturada pela **Equa√ß√£o de Bellman para $v_\pi$** [^59]:

$$
v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) [r + \gamma v_\pi(s')], \forall s \in S
$$

Esta equa√ß√£o expressa o valor de um estado em termos dos valores de seus estados sucessores, ponderados pelas probabilidades de transi√ß√£o e pelas probabilidades de sele√ß√£o de a√ß√£o ditadas pela pol√≠tica $\pi$. Ela estabelece uma restri√ß√£o de consist√™ncia que deve ser satisfeita pela fun√ß√£o de valor $v_\pi$. Especificamente, o valor do estado $s$ seguindo a pol√≠tica $\pi$ deve ser igual √† soma das probabilidades de tomar cada a√ß√£o $a$ em $s$ multiplicado pela recompensa imediata esperada $r$ mais o valor descontado $\gamma v_\pi(s')$ do pr√≥ximo estado $s'$, dado que a a√ß√£o $a$ foi tomada em $s$.

**Prova:**

I. Come√ßamos com a defini√ß√£o do valor de estado: $v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$.
II. Expandimos a expectativa em termos das a√ß√µes poss√≠veis e seus valores de a√ß√£o correspondentes: $v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) q_\pi(s, a)$.
III. Substitu√≠mos a defini√ß√£o do valor de a√ß√£o $q_\pi(s, a)$ em termos da recompensa esperada e do valor descontado do pr√≥ximo estado: $v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) [r + \gamma v_\pi(s')]$.
IV. Portanto, a Equa√ß√£o de Bellman para $v_\pi$ √©: $v_\pi(s) = \sum_{a \in A(s)} \pi(a|s) \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) [r + \gamma v_\pi(s')]$. ‚ñ†

**Backup Diagram:** O backup diagram para $v_\pi$ [^59] (Figura 3.2, p√°gina 60) visualiza essa rela√ß√£o recursiva, mostrando como o valor de um estado √© "backed up" a partir dos valores de seus sucessores, considerando as probabilidades de transi√ß√£o e as probabilidades de sele√ß√£o de a√ß√£o.

![Diagrama de backup para a fun√ß√£o de valor \(v_\pi\), ilustrando a rela√ß√£o entre um estado e seus sucessores sob uma pol√≠tica \(\pi\).](./../images/image3.png)

Da mesma forma, podemos escrever a Equa√ß√£o de Bellman para $q_\pi$:

**Teorema 2:** A Equa√ß√£o de Bellman para $q_\pi$ √© dada por:

$$
q_\pi(s, a) = \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) \left[ r + \gamma \sum_{a' \in A(s')} \pi(a'|s') q_\pi(s', a') \right], \forall s \in S, a \in A(s)
$$

*Prova:* Substitu√≠mos a equa√ß√£o de $v_\pi(s')$ na equa√ß√£o de $q_\pi(s, a)$:

$q_\pi(s, a) = \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) \left[ r + \gamma v_\pi(s') \right] = \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) \left[ r + \gamma \sum_{a' \in A(s')} \pi(a'|s') q_\pi(s', a') \right]$.

**Prova:**

I. Come√ßamos com a defini√ß√£o do valor de a√ß√£o: $q_\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]$.
II. Expandimos a expectativa em termos da recompensa imediata e do valor descontado do pr√≥ximo estado: $q_\pi(s, a) = \mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] = \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) [r + \gamma \mathbb{E}_\pi[G_{t+1} | S_{t+1} = s']]$.
III. Substitu√≠mos $\mathbb{E}_\pi[G_{t+1} | S_{t+1} = s']$ por $v_\pi(s')$: $q_\pi(s, a) = \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) [r + \gamma v_\pi(s')]$.
IV. Substitu√≠mos $v_\pi(s')$ pela sua defini√ß√£o em termos de $q_\pi(s', a')$: $q_\pi(s, a) = \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) \left[ r + \gamma \sum_{a' \in A(s')} \pi(a'|s') q_\pi(s', a') \right]$.
V. Portanto, a Equa√ß√£o de Bellman para $q_\pi$ √©: $q_\pi(s, a) = \sum_{s' \in S, r \in \mathbb{R}} p(s', r | s, a) \left[ r + \gamma \sum_{a' \in A(s')} \pi(a'|s') q_\pi(s', a') \right]$. ‚ñ†

Estas equa√ß√µes de Bellman s√£o cruciais para o desenvolvimento de algoritmos de RL.

> üí° **Exemplo Num√©rico:**
>
> Considere um estado $s_5$ e uma a√ß√£o $a_3$. Suponha que a a√ß√£o $a_3$ leva ao estado $s'$ com recompensa +3 com probabilidade 0.7 e ao estado $s''$ com recompensa -1 com probabilidade 0.3. Seja $\gamma = 0.9$. Assumindo que $\pi(a'|s') = 0.5$ para duas a√ß√µes $a'_1$ e $a'_2$ no estado $s'$, e que $q_\pi(s', a'_1) = 8$ e $q_\pi(s', a'_2) = 6$. Similarmente, no estado $s''$, assuma $\pi(a'|s'') = 0.5$ para duas a√ß√µes $a'_3$ e $a'_4$, e que $q_\pi(s'', a'_3) = 2$ e $q_\pi(s'', a'_4) = 0$.
>
> Ent√£o, $q_\pi(s', a') = (0.5)(8) + (0.5)(6) = 7$ e $q_\pi(s'', a') = (0.5)(2) + (0.5)(0) = 1$. Aplicando a Equa√ß√£o de Bellman para $q_\pi$:
>
> $q_\pi(s_5, a_3) = (0.7) \left[ 3 + (0.9)(7) \right] + (0.3) \left[ -1 + (0.9)(1) \right] = (0.7)[3 + 6.3] + (0.3)[-1 + 0.9] = (0.7)(9.3) + (0.3)(-0.1) = 6.51 - 0.03 = 6.48$.
>
> Este exemplo num√©rico ilustra o c√°lculo do valor da a√ß√£o, considerando as probabilidades de transi√ß√£o, as recompensas, o fator de desconto e os valores das a√ß√µes nos estados subsequentes.

![Diagrama de backup qœÄ ilustrando as rela√ß√µes entre estados, a√ß√µes e recompensas em reinforcement learning.](./../images/image10.png)

Um exemplo pr√°tico pode ser observado na Figura 3.2 [^60], que mostra um *Gridworld* e a fun√ß√£o de valor para uma pol√≠tica aleat√≥ria equiprov√°vel.

![Exemplo de Gridworld demonstrando din√¢micas de recompensa e fun√ß√£o de valor de estado para uma pol√≠tica equiprov√°vel.](./../images/image11.png)

Outros exemplos incluem o rob√¥ coletor de latas (Exemplo 3.3, p√°gina 52) [^60] e o jogo de golfe (Exemplos 3.6 e 3.7, p√°ginas 61 e 63) [^60], que ajudam a ilustrar a aplica√ß√£o pr√°tica destes conceitos.

![Representa√ß√£o do sistema de coleta de latas como um MDP finito, ilustrando as transi√ß√µes de estado e recompensas.](./../images/image4.png)

![State-value function for putting (upper) and optimal action-value function for using the driver (lower) in a golf scenario.](./../images/image8.png)
<!-- END -->