## Value Functions in Markov Decision Processes

### Introdu√ß√£o
Este cap√≠tulo aprofunda o conceito de **value functions** em **Finite Markov Decision Processes (MDPs)**, explorando como quantificam a "bondade" de estar em um determinado estado ou de executar uma a√ß√£o espec√≠fica sob uma determinada pol√≠tica [^1]. O objetivo central do Reinforcement Learning √© encontrar uma pol√≠tica que maximize a recompensa acumulada ao longo do tempo [^2]. As value functions fornecem uma ferramenta essencial para avaliar e comparar diferentes pol√≠ticas, guiando o processo de aprendizado. Ser√£o explorados em detalhe os conceitos de **state-value function** e **action-value function**, suas defini√ß√µes formais e suas inter-rela√ß√µes.

### State-Value Function

A **state-value function**, denotada por $v_\pi(s)$, representa o valor esperado do retorno acumulado ao iniciar no estado *s* e seguir a pol√≠tica $\pi$ da√≠ em diante [^12]. Formalmente, √© definida como:

$$v_\pi(s) = E_\pi[G_t | S_t = s] = E_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} \Big| S_t = s \right], \quad \forall s \in \mathcal{S}$$

Onde:

*   $E_\pi[\cdot]$ denota o valor esperado sob a pol√≠tica $\pi$.
*   $G_t$ √© o retorno no instante de tempo *t*.
*   $S_t$ √© o estado no instante de tempo *t*.
*   $\gamma$ √© o fator de desconto, com $0 \leq \gamma \leq 1$, que determina o peso dado √†s recompensas futuras [^8].
*   $R_{t+k+1}$ √© a recompensa recebida no instante de tempo $t+k+1$.
*   $\mathcal{S}$ √© o conjunto de todos os estados poss√≠veis no MDP [^2].

A state-value function $v_\pi(s)$ encapsula a "bondade" de um estado *s* sob uma pol√≠tica $\pi$. Um estado com um valor alto indica que, em m√©dia, o agente receber√° um alto retorno acumulado se come√ßar nesse estado e seguir a pol√≠tica $\pi$ [^12]. Por outro lado, um estado com um valor baixo indica que o agente provavelmente receber√° um retorno acumulado baixo [^12].

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP simples com tr√™s estados: $S = \{s_1, s_2, s_3\}$. Suponha que temos uma pol√≠tica $\pi$ e, ap√≥s algumas itera√ß√µes de avalia√ß√£o da pol√≠tica, estimamos os seguintes valores:
>
> *   $v_\pi(s_1) = 10$
> *   $v_\pi(s_2) = 5$
> *   $v_\pi(s_3) = -2$
>
> Esses valores indicam que, seguindo a pol√≠tica $\pi$, o agente espera obter um retorno acumulado de 10 se come√ßar no estado $s_1$, 5 se come√ßar no estado $s_2$, e -2 se come√ßar no estado $s_3$.  O estado $s_1$ √© o mais "valioso" sob esta pol√≠tica, enquanto $s_3$ √© o menos valioso. Se o agente tiver a escolha de come√ßar em $s_1$ ou $s_3$, ele preferir√° come√ßar em $s_1$ para maximizar seu retorno esperado.
>
> Agora, suponha que $\gamma = 0.9$ e que, ao iniciar em $s_1$ e seguir $\pi$, o agente receba as seguintes recompensas nos pr√≥ximos tr√™s passos: $R_{t+1}=2$, $R_{t+2}=3$, $R_{t+3}=4$.  O retorno acumulado nesses tr√™s passos seria:
>
> $G_t = 2 + 0.9 * 3 + 0.9^2 * 4 = 2 + 2.7 + 3.24 = 7.94$.
>
> Este √© um √∫nico exemplo de retorno.  A state-value function $v_\pi(s_1) = 10$ representa o *valor esperado* desses retornos ao longo de muitas simula√ß√µes, come√ßando em $s_1$ e seguindo a pol√≠tica $\pi$.

√â importante notar que, por conven√ß√£o, o valor de qualquer **estado terminal** √© sempre zero [^12]. Isto porque, uma vez que o agente atinge um estado terminal, a intera√ß√£o com o ambiente se encerra, e n√£o h√° recompensas futuras a serem consideradas [^8, 12].

**Proposi√ß√£o 1.** *Se $\gamma = 0$, ent√£o $v_\pi(s) = E_\pi[R_{t+1} | S_t = s]$.*

*Proof.* Quando $\gamma = 0$, todas as recompensas futuras s√£o ignoradas, e a state-value function se reduz ao valor esperado da recompensa imediata.
I. Come√ßamos com a defini√ß√£o geral da state-value function:
$$v_\pi(s) = E_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} \Big| S_t = s \right]$$

II. Assumimos que $\gamma = 0$:
$$v_\pi(s) = E_\pi \left[ \sum_{k=0}^\infty 0^k R_{t+k+1} \Big| S_t = s \right]$$

III.  Notamos que todo termo com $k > 0$ na soma ser√° zero, pois ser√° multiplicado por $0^k = 0$. O √∫nico termo que sobrevive √© quando $k = 0$:
$$v_\pi(s) = E_\pi \left[ 0^0 R_{t+0+1} \Big| S_t = s \right] = E_\pi \left[ R_{t+1} \Big| S_t = s \right]$$

IV. Portanto, se $\gamma = 0$, ent√£o $v_\pi(s) = E_\pi[R_{t+1} | S_t = s]$. ‚ñ†

> üí° **Exemplo Num√©rico (Proposi√ß√£o 1):**
>
> Suponha que um agente est√° no estado $s_1$ e, sob a pol√≠tica $\pi$, sempre recebe uma recompensa de 5 e transita para algum outro estado. Se $\gamma = 0$, ent√£o $v_\pi(s_1) = E_\pi[R_{t+1} | S_t = s_1] = 5$. O valor de estar em $s_1$ √© simplesmente a recompensa imediata, pois o agente n√£o se importa com recompensas futuras.

### Action-Value Function

A **action-value function**, tamb√©m conhecida como **Q-function**, denotada por $q_\pi(s, a)$, representa o valor esperado do retorno acumulado ao iniciar no estado *s*, executar a a√ß√£o *a*, e seguir a pol√≠tica $\pi$ da√≠ em diante [^12]. Formalmente, √© definida como:

$$q_\pi(s, a) = E_\pi[G_t | S_t = s, A_t = a] = E_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} \Big| S_t = s, A_t = a \right]$$

Onde:

*   $A_t$ √© a a√ß√£o tomada no instante de tempo *t*.
*   Os demais termos s√£o definidos como na state-value function.

A action-value function $q_\pi(s, a)$ encapsula a "bondade" de tomar uma a√ß√£o *a* em um estado *s* sob uma pol√≠tica $\pi$ [^12]. Ela permite que o agente avalie diretamente o impacto de suas a√ß√µes em diferentes estados [^12].

> üí° **Exemplo Num√©rico:**
>
> Imagine um agente em um labirinto (estado $s$) que pode escolher entre duas a√ß√µes: 'ir para a esquerda' ($a_1$) ou 'ir para a direita' ($a_2$). Ap√≥s aprender com a experi√™ncia, o agente estima as seguintes action-values:
>
> *   $q_\pi(s, a_1) = 2$
> *   $q_\pi(s, a_2) = 8$
>
> Isso significa que, se o agente estiver no estado $s$ e seguir a pol√≠tica $\pi$, espera-se que 'ir para a direita' resulte em um retorno acumulado maior (8) do que 'ir para a esquerda' (2).  Portanto, sob essa pol√≠tica, a a√ß√£o 'ir para a direita' √© prefer√≠vel no estado $s$.
>
> Se $\gamma = 0.9$, e ao tomar a a√ß√£o $a_2$ (ir para a direita) no estado $s$, o agente recebe uma recompensa imediata de 1 e transita para um novo estado $s'$. O valor esperado do retorno futuro a partir de $s'$ √© $v_\pi(s') = 7$. Ent√£o, o valor de $q_\pi(s, a_2)$ pode ser decomposto como:
>
> $q_\pi(s, a_2) = E[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a_2] = 1 + 0.9 * 7 = 1 + 6.3 = 7.3$.
>
> Este valor √© uma estimativa. Ap√≥s v√°rias experi√™ncias e atualiza√ß√µes, esperamos que $q_\pi(s, a_2)$ convirja para um valor pr√≥ximo a 8.

### Rela√ß√£o entre State-Value e Action-Value Functions

Existe uma rela√ß√£o fundamental entre a state-value function e a action-value function [^12]. A state-value function pode ser expressa em termos da action-value function, ponderada pela probabilidade de selecionar cada a√ß√£o no estado *s* sob a pol√≠tica $\pi$. Matematicamente, essa rela√ß√£o √© dada por [^13]:

$$v_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a|s) q_\pi(s, a)$$

Onde $\pi(a|s)$ √© a probabilidade de selecionar a a√ß√£o *a* no estado *s* sob a pol√≠tica $\pi$ e $\mathcal{A}(s)$ √© o conjunto de a√ß√µes dispon√≠veis no estado *s* [^2]. Essa equa√ß√£o captura a ideia de que o valor de um estado √© a m√©dia ponderada dos valores de todas as a√ß√µes poss√≠veis naquele estado, com os pesos sendo dados pela pol√≠tica [^12, 13].

> üí° **Exemplo Num√©rico:**
>
> Considere um estado $s$ onde um agente tem tr√™s a√ß√µes poss√≠veis: $a_1, a_2, a_3$. Suponha que sob a pol√≠tica $\pi$, as probabilidades de selecionar essas a√ß√µes s√£o: $\pi(a_1|s) = 0.2$, $\pi(a_2|s) = 0.5$, $\pi(a_3|s) = 0.3$. As action-values correspondentes s√£o: $q_\pi(s, a_1) = 10$, $q_\pi(s, a_2) = 5$, $q_\pi(s, a_3) = 2$.
>
> Usando a f√≥rmula, o state-value para o estado $s$ √©:
>
> $v_\pi(s) = (0.2 * 10) + (0.5 * 5) + (0.3 * 2) = 2 + 2.5 + 0.6 = 5.1$.
>
> O valor de estar no estado $s$ sob a pol√≠tica $\pi$ √© 5.1, que √© a m√©dia ponderada dos valores das a√ß√µes poss√≠veis.

Inversamente, a action-value function pode ser expressa em termos da state-value function do pr√≥ximo estado, ponderada pela din√¢mica do ambiente [^13]. Esta rela√ß√£o √© definida como:
$$q_\pi(s,a) = \sum_{s' \in \mathcal{S}, r \in \mathcal{R}} p(s', r | s, a) [r + \gamma v_\pi(s')] $$
Aqui, $p(s', r | s, a)$ √© a probabilidade de transi√ß√£o para o pr√≥ximo estado $s'$ e receber a recompensa *r* ao tomar a a√ß√£o *a* no estado *s* [^2]. $\mathcal{R}$ representa o conjunto de todas as poss√≠veis recompensas [^2]. Essa equa√ß√£o reflete que o valor de executar uma a√ß√£o em um estado √© a recompensa imediata esperada, mais o valor descontado do pr√≥ximo estado resultante [^12, 13].

> üí° **Exemplo Num√©rico:**
>
> Um agente est√° no estado $s$ e considera a a√ß√£o $a$. Existem dois poss√≠veis estados sucessores: $s_1'$ e $s_2'$. As probabilidades de transi√ß√£o e recompensas s√£o:
>
> *   $p(s_1', 2 | s, a) = 0.7$  (transi√ß√£o para $s_1'$ com recompensa 2)
> *   $p(s_2', -1 | s, a) = 0.3$ (transi√ß√£o para $s_2'$ com recompensa -1)
>
> Os state-values dos estados sucessores s√£o: $v_\pi(s_1') = 8$ e $v_\pi(s_2') = 4$.  Assumindo $\gamma = 0.9$, a action-value $q_\pi(s, a)$ √©:
>
> $q_\pi(s, a) = 0.7 * [2 + 0.9 * 8] + 0.3 * [-1 + 0.9 * 4] = 0.7 * [2 + 7.2] + 0.3 * [-1 + 3.6] = 0.7 * 9.2 + 0.3 * 2.6 = 6.44 + 0.78 = 7.22$.
>
> Esse valor indica o qu√£o "bom" √© tomar a a√ß√£o $a$ no estado $s$, levando em considera√ß√£o as poss√≠veis transi√ß√µes e recompensas futuras.

Para solidificar nossa compreens√£o sobre a rela√ß√£o entre as value functions, podemos derivar uma express√£o que relaciona diretamente $q_\pi(s, a)$ com outros valores $q_\pi(s', a')$:

**Teorema 2.**
$$q_\pi(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \sum_{a' \in \mathcal{A}(s')} \pi(a'|s') q_\pi(s', a') \right]$$

*Proof.*
Come√ßamos com a defini√ß√£o de $q_\pi(s, a)$:
$$q_\pi(s,a) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] $$
Substitu√≠mos $v_\pi(s')$ usando a rela√ß√£o $v_\pi(s') = \sum_{a' \in \mathcal{A}(s')} \pi(a'|s') q_\pi(s', a')$:
$$q_\pi(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \sum_{a' \in \mathcal{A}(s')} \pi(a'|s') q_\pi(s', a') \right]$$
Isso completa a demonstra√ß√£o.





![Diagrama de backup qœÄ ilustrando as rela√ß√µes entre estados, a√ß√µes e recompensas em reinforcement learning.](./../images/image10.png)

### Bellman Equation para Value Functions

As value functions satisfazem rela√ß√µes recursivas importantes, conhecidas como **Bellman equations** [^13]. Estas equa√ß√µes expressam a rela√ß√£o entre o valor de um estado (ou par estado-a√ß√£o) e os valores de seus sucessores [^13].

A **Bellman equation para state-value function** √© dada por:

$$v_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]$$

Esta equa√ß√£o afirma que o valor de um estado *s* √© a recompensa esperada para o pr√≥ximo estado, ponderada sobre todas as a√ß√µes poss√≠veis e transi√ß√µes do ambiente [^13].

> üí° **Exemplo Num√©rico:**
>
> Vamos usar o mesmo cen√°rio do exemplo anterior com tr√™s a√ß√µes ($a_1, a_2, a_3$), probabilidades de a√ß√£o ($\pi(a_1|s) = 0.2$, $\pi(a_2|s) = 0.5$, $\pi(a_3|s) = 0.3$), e agora adicionar informa√ß√µes sobre as recompensas e estados sucessores. Suponha que:
>
> *   Ao tomar $a_1$, o agente transita para $s_1'$ com recompensa 3 com probabilidade 1: $p(s_1', 3 | s, a_1) = 1$.
> *   Ao tomar $a_2$, o agente transita para $s_2'$ com recompensa 1 com probabilidade 1: $p(s_2', 1 | s, a_2) = 1$.
> *   Ao tomar $a_3$, o agente transita para $s_3'$ com recompensa -2 com probabilidade 1: $p(s_3', -2 | s, a_3) = 1$.
>
> E que $v_\pi(s_1') = 6$, $v_\pi(s_2') = 4$, $v_\pi(s_3') = 0$, e $\gamma = 0.9$.
>
> Ent√£o, usando a Bellman equation:
>
> $v_\pi(s) = 0.2 * [3 + 0.9 * 6] + 0.5 * [1 + 0.9 * 4] + 0.3 * [-2 + 0.9 * 0] = 0.2 * [3 + 5.4] + 0.5 * [1 + 3.6] + 0.3 * [-2 + 0] = 0.2 * 8.4 + 0.5 * 4.6 + 0.3 * -2 = 1.68 + 2.3 - 0.6 = 3.38$.
>
> Esse valor de 3.38 √© uma atualiza√ß√£o do valor anterior de $v_\pi(s)$ (que era 5.1 no exemplo anterior) com base em informa√ß√µes mais recentes sobre as recompensas e os valores dos estados sucessores.



![Diagrama de backup para a fun√ß√£o de valor \(v_\pi\), ilustrando a rela√ß√£o entre um estado e seus sucessores sob uma pol√≠tica \(\pi\).](./../images/image3.png)

Similarmente, a **Bellman equation para action-value function** √© dada por:
$$q_\pi(s, a) = \sum_{s', r} p(s', r | s, a) \left[ r + \gamma \sum_{a' \in \mathcal{A}(s')} \pi(a'|s') q_\pi(s', a') \right]$$

> üí° **Exemplo Num√©rico:**
>
> Considere um agente no estado $s$ tomando a a√ß√£o $a$. Ap√≥s tomar essa a√ß√£o, o agente pode transitar para dois estados poss√≠veis, $s_1'$ e $s_2'$, com as seguintes probabilidades e recompensas:
>
> *   $p(s_1', 5 | s, a) = 0.6$ (recompensa de 5 ao ir para $s_1'$)
> *   $p(s_2', -3 | s, a) = 0.4$ (recompensa de -3 ao ir para $s_2'$)
>
> No estado $s_1'$, o agente segue a pol√≠tica $\pi$, que escolhe a a√ß√£o $a_1'$ com probabilidade 0.7 e a a√ß√£o $a_2'$ com probabilidade 0.3. Os valores de a√ß√£o correspondentes s√£o $q_\pi(s_1', a_1') = 10$ e $q_\pi(s_1', a_2') = 5$.
>
> No estado $s_2'$, o agente segue a pol√≠tica $\pi$, que escolhe a a√ß√£o $a_3'$ com probabilidade 0.9 e a a√ß√£o $a_4'$ com probabilidade 0.1. Os valores de a√ß√£o correspondentes s√£o $q_\pi(s_2', a_3') = 2$ e $q_\pi(s_2', a_4') = -1$.
>
> Assumindo um fator de desconto $\gamma = 0.9$, podemos calcular $q_\pi(s, a)$ usando a Bellman equation:
>
> $q_\pi(s, a) = 0.6 * [5 + 0.9 * (0.7 * 10 + 0.3 * 5)] + 0.4 * [-3 + 0.9 * (0.9 * 2 + 0.1 * -1)] =$
> $0.6 * [5 + 0.9 * (7 + 1.5)] + 0.4 * [-3 + 0.9 * (1.8 - 0.1)] =$
> $0.6 * [5 + 0.9 * 8.5] + 0.4 * [-3 + 0.9 * 1.7] =$
> $0.6 * [5 + 7.65] + 0.4 * [-3 + 1.53] =$
> $0.6 * 12.65 + 0.4 * -1.47 = 7.59 - 0.588 = 7.002$
>
> Portanto, a action-value $q_\pi(s, a)$ √© aproximadamente 7.002.



![Decision tree illustrating the relationship between state-action pairs, rewards, and subsequent states in an MDP.](./../images/image5.png)

**Teorema 3.** *Se uma pol√≠tica $\pi$ √© determin√≠stica, ou seja, $\pi(a|s) = 1$ para alguma a√ß√£o $a$ e $0$ para todas as outras a√ß√µes, ent√£o a Bellman equation para a state-value function se simplifica para:*

$$v_\pi(s) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] \text{, onde } a = \pi(s)$$

*Proof.* Se $\pi$ √© determin√≠stica, ent√£o $\sum_{a \in \mathcal{A}(s)} \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]$ se reduz a um √∫nico termo onde $\pi(a|s) = 1$, e os outros termos s√£o zero.  Seja $a = \pi(s)$ a a√ß√£o selecionada pela pol√≠tica $\pi$ no estado $s$.  Ent√£o, $v_\pi(s) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')]$.

I. Come√ßamos com a Bellman equation geral para a state-value function:
$$v_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] $$

II. Assumimos que a pol√≠tica $\pi$ √© determin√≠stica, o que significa que para cada estado $s$, existe apenas uma a√ß√£o $a$ para a qual $\pi(a|s) = 1$, e para todas as outras a√ß√µes $a'$, $\pi(a'|s) = 0$.  Portanto, a soma sobre todas as a√ß√µes em $\mathcal{A}(s)$ se reduz a um √∫nico termo. Seja $a = \pi(s)$ a a√ß√£o selecionada pela pol√≠tica $\pi$ no estado $s$.

III. Substitu√≠mos na equa√ß√£o:
$$v_\pi(s) = \pi(a|s) \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] $$

IV. Como $\pi(a|s) = 1$:
$$v_\pi(s) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] $$

V. Portanto, demonstramos que se $\pi$ √© determin√≠stica, ent√£o $v_\pi(s) = \sum_{s', r} p(s', r | s, a) [r + \gamma v_\pi(s')] \text{, onde } a = \pi(s)$. ‚ñ†

> üí° **Exemplo Num√©rico (Teorema 3):**
>
> Imagine um rob√¥ em um estado $s$ que, sob uma pol√≠tica determin√≠stica $\pi$, sempre move para a frente (a√ß√£o $a$).  Quando move para a frente, ele tem 80% de chance de ir para o estado $s'$ com uma recompensa de 1, e 20% de chance de ir para o estado $s''$ com uma recompensa de -1.  Ent√£o, $p(s', 1 | s, a) = 0.8$ e $p(s'', -1 | s, a) = 0.2$. Suponha que $v_\pi(s') = 10$ e $v_\pi(s'') = 2$, e $\gamma = 0.9$.
>
> Usando a simplifica√ß√£o da Bellman equation:
>
> $v_\pi(s) = 0.8 * [1 + 0.9 * 10] + 0.2 * [-1 + 0.9 * 2] = 0.8 * [1 + 9] + 0.2 * [-1 + 1.8] = 0.8 * 10 + 0.2 * 0.8 = 8 + 0.16 = 8.16$.

### Conclus√£o

As **value functions** s√£o um conceito fundamental em **Reinforcement Learning**. Elas fornecem uma maneira de quantificar a "bondade" de um estado ou de executar uma a√ß√£o em um estado, levando em considera√ß√£o as recompensas futuras que podem ser obtidas [^12]. As Bellman equations fornecem uma rela√ß√£o recursiva entre os valores de estados sucessivos, essencial para algoritmos de otimiza√ß√£o [^13]. A compreens√£o profunda das value functions √© crucial para o desenvolvimento e an√°lise de algoritmos de Reinforcement Learning.
<!-- END -->