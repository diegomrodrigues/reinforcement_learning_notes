## O Interface Agente-Ambiente em Processos de DecisÃ£o de Markov Finitos

### IntroduÃ§Ã£o
Em processos de decisÃ£o de Markov (MDPs) finitos, a interaÃ§Ã£o entre o **agente** e o **ambiente** Ã© fundamental para o aprendizado e a tomada de decisÃµes. O agente, como o aprendiz e tomador de decisÃµes, busca atingir um objetivo atravÃ©s da interaÃ§Ã£o contÃ­nua com o ambiente. O ambiente, por sua vez, compreende tudo o que estÃ¡ fora do agente, respondendo Ã s aÃ§Ãµes do agente e fornecendo *recompensas* e *novos estados* [^47]. Este capÃ­tulo explora em detalhes a natureza dessa interaÃ§Ã£o, os papÃ©is do agente e do ambiente, e como essa estrutura permite a formalizaÃ§Ã£o do problema de aprendizado por reforÃ§o.

### Conceitos Fundamentais

A interaÃ§Ã£o entre o agente e o ambiente em um MDP finito ocorre em uma sequÃªncia de *passos de tempo discretos*, denotados por $t = 0, 1, 2, 3, \ldots$ [^48]. Em cada passo de tempo $t$, o agente recebe uma representaÃ§Ã£o do **estado** do ambiente, $S_t \in \mathcal{S}$, onde $\mathcal{S}$ Ã© o conjunto de todos os estados possÃ­veis. Com base neste estado, o agente seleciona uma **aÃ§Ã£o**, $A_t \in \mathcal{A}(s)$, onde $\mathcal{A}(s)$ Ã© o conjunto de aÃ§Ãµes disponÃ­veis no estado $s$. A escolha da aÃ§Ã£o influencia nÃ£o apenas a recompensa imediata, mas tambÃ©m os estados futuros [^47].

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine um agente (um robÃ´) em um ambiente simples com trÃªs estados: $\mathcal{S} = \{\text{Bateria Cheia, Bateria MÃ©dia, Bateria Fraca}\}$. O agente estÃ¡ no estado $S_t = \text{Bateria MÃ©dia}$. As aÃ§Ãµes possÃ­veis nesse estado sÃ£o $\mathcal{A}(\text{Bateria MÃ©dia}) = \{\text{Procurar Carregador, Continuar Tarefa}\}$. Se o agente escolher a aÃ§Ã£o $A_t = \text{Procurar Carregador}$, isso afetarÃ¡ a recompensa imediata (gasto de energia) e o prÃ³ximo estado (a bateria pode carregar ou nÃ£o).

Como consequÃªncia da aÃ§Ã£o do agente, o ambiente responde com uma **recompensa numÃ©rica**, $R_{t+1} \in \mathbb{R}$, e transita para um novo **estado**, $S_{t+1}$. Essa interaÃ§Ã£o contÃ­nua gera uma sequÃªncia ou trajetÃ³ria que se inicia da seguinte forma:
$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots$$ [^48]

> ðŸ’¡ **Exemplo NumÃ©rico:** Continuando o exemplo do robÃ´, se no estado $S_t = \text{Bateria MÃ©dia}$, o robÃ´ escolhe a aÃ§Ã£o $A_t = \text{Continuar Tarefa}$, o ambiente pode responder com uma recompensa $R_{t+1} = -1$ (indicando uma pequena perda de energia) e o prÃ³ximo estado pode ser $S_{t+1} = \text{Bateria Fraca}$. A sequÃªncia atÃ© agora seria:  $\text{Bateria MÃ©dia}, \text{Continuar Tarefa}, -1, \text{Bateria Fraca}, \ldots$

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

Em um MDP *finito*, os conjuntos de estados $\mathcal{S}$, aÃ§Ãµes $\mathcal{A}$ e recompensas $\mathcal{R}$ possuem um nÃºmero finito de elementos [^48]. As variÃ¡veis aleatÃ³rias $R_t$ e $S_t$ possuem *distribuiÃ§Ãµes de probabilidade discretas* bem definidas, que dependem apenas do estado e da aÃ§Ã£o precedentes. Em outras palavras, para valores especÃ­ficos $s' \in \mathcal{S}$ e $r \in \mathcal{R}$, existe uma probabilidade de que esses valores ocorram no tempo $t$, dado o estado $s$ e a aÃ§Ã£o $a$ no tempo anterior:

$$p(s', r | s, a) = Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\}$$ [^48]

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que $\mathcal{S} = \{s_1, s_2\}$, $\mathcal{A}(s_1) = \{a_1, a_2\}$ e $\mathcal{R} = \{0, 1\}$. Se o agente estiver no estado $s_1$ e tomar a aÃ§Ã£o $a_1$, a funÃ§Ã£o de dinÃ¢mica pode ser:
>
>  $p(s_1, 0 | s_1, a_1) = 0.6$ (probabilidade de permanecer em $s_1$ e receber recompensa 0)
>
>  $p(s_2, 0 | s_1, a_1) = 0.3$ (probabilidade de ir para $s_2$ e receber recompensa 0)
>
>  $p(s_1, 1 | s_1, a_1) = 0.1$ (probabilidade de permanecer em $s_1$ e receber recompensa 1)
>
>  $p(s_2, 1 | s_1, a_1) = 0.0$ (probabilidade de ir para $s_2$ e receber recompensa 1)

Essa funÃ§Ã£o $p$, denominada *funÃ§Ã£o de dinÃ¢mica* do MDP, define a dinÃ¢mica do ambiente. A funÃ§Ã£o $p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ Ã© uma funÃ§Ã£o determinÃ­stica de quatro argumentos [^48].

Ã‰ importante notar que $p$ especifica uma distribuiÃ§Ã£o de probabilidade para cada escolha de $s$ e $a$, ou seja:

$$\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r | s, a) = 1, \text{ para todo } s \in \mathcal{S}, a \in \mathcal{A}(s)$$ [^49]

**Prova:**

Provaremos que $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r | s, a) = 1$ para todo $s \in \mathcal{S}$ e $a \in \mathcal{A}(s)$.

I.  $p(s', r | s, a)$ representa a probabilidade condicional de transitar para o estado $s'$ e receber a recompensa $r$, dado o estado $s$ e a aÃ§Ã£o $a$.

II. A soma $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r | s, a)$ representa a soma das probabilidades de *todos* os possÃ­veis prÃ³ximos estados $s'$ e recompensas $r$, dado o estado $s$ e a aÃ§Ã£o $a$.

III. Uma vez que o agente estÃ¡ no estado $s$ e executa a aÃ§Ã£o $a$, o ambiente *deve* transitar para algum estado $s'$ e emitir alguma recompensa $r$.

IV.  Portanto, a soma das probabilidades de todos os resultados possÃ­veis deve ser igual a 1, pois representa a certeza de que algum resultado ocorrerÃ¡.

V.  Assim, $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r | s, a) = 1$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** No exemplo anterior, com $\mathcal{S} = \{s_1, s_2\}$, $\mathcal{A}(s_1) = \{a_1, a_2\}$ e $\mathcal{R} = \{0, 1\}$, e $p(s_1, 0 | s_1, a_1) = 0.6$, $p(s_2, 0 | s_1, a_1) = 0.3$, $p(s_1, 1 | s_1, a_1) = 0.1$, $p(s_2, 1 | s_1, a_1) = 0.0$, podemos verificar que a soma das probabilidades Ã© 1: $0.6 + 0.3 + 0.1 + 0.0 = 1.0$.

Em um processo de decisÃ£o de Markov, as probabilidades dadas por $p$ caracterizam completamente a dinÃ¢mica do ambiente [^49]. A probabilidade de cada valor possÃ­vel para $S_t$ e $R_t$ depende do estado e da aÃ§Ã£o imediatamente anteriores, $S_{t-1}$ e $A_{t-1}$, e nÃ£o depende de estados e aÃ§Ãµes anteriores, desde que $S_{t-1}$ contenha todas as informaÃ§Ãµes relevantes do histÃ³rico da interaÃ§Ã£o agente-ambiente [^49].

*A propriedade de Markov* impÃµe uma restriÃ§Ã£o nÃ£o sobre o processo de decisÃ£o, mas sobre o estado. O estado deve incluir informaÃ§Ãµes sobre todos os aspectos da interaÃ§Ã£o agente-ambiente passada que fazem diferenÃ§a para o futuro [^49]. Se o estado satisfaz a propriedade de Markov, diz-se que ele tem a *propriedade de Markov*. Assume-se que o estado sempre possui a propriedade de Markov. MÃ©todos de aproximaÃ§Ã£o que nÃ£o se baseiam nesta propriedade e construÃ§Ã£o de estados Markovianos a partir de observaÃ§Ãµes nÃ£o Markovianas sÃ£o tratados posteriormente [^49].

A partir da funÃ§Ã£o de dinÃ¢mica de quatro argumentos, $p$, Ã© possÃ­vel computar outras informaÃ§Ãµes sobre o ambiente, como as *probabilidades de transiÃ§Ã£o de estado* (denotadas como $p(s' | s, a)$), que representam a probabilidade de transitar para o estado $s'$ dado o estado $s$ e a aÃ§Ã£o $a$:
$$p(s' | s, a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum_{r \in \mathcal{R}} p(s', r | s, a)$$ [^49]

**Prova:**

Provaremos que $p(s' | s, a) = \sum_{r \in \mathcal{R}} p(s', r | s, a)$.

I. Queremos encontrar a probabilidade de transitar para o estado $s'$ dado o estado $s$ e a aÃ§Ã£o $a$, ou seja, $p(s' | s, a)$.

II. Sabemos que $p(s', r | s, a)$ Ã© a probabilidade conjunta de transitar para o estado $s'$ e receber a recompensa $r$, dado o estado $s$ e a aÃ§Ã£o $a$.

III. Para obter a probabilidade de transitar para o estado $s'$ independentemente da recompensa, precisamos somar a probabilidade conjunta $p(s', r | s, a)$ sobre todos os possÃ­veis valores de $r$.

IV. Portanto, $p(s' | s, a) = \sum_{r \in \mathcal{R}} p(s', r | s, a)$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Usando o exemplo anterior, para calcular $p(s_1 | s_1, a_1)$, somamos as probabilidades de ir para $s_1$ com cada possÃ­vel recompensa:
>
> $p(s_1 | s_1, a_1) = p(s_1, 0 | s_1, a_1) + p(s_1, 1 | s_1, a_1) = 0.6 + 0.1 = 0.7$.
>
> Similarmente, $p(s_2 | s_1, a_1) = p(s_2, 0 | s_1, a_1) + p(s_2, 1 | s_1, a_1) = 0.3 + 0.0 = 0.3$.

TambÃ©m Ã© possÃ­vel computar as *recompensas esperadas* para pares estado-aÃ§Ã£o $r(s, a)$:

$$r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r | s, a)$$ [^49]

**Prova:**

Provaremos que $r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r | s, a)$.

I.  A recompensa esperada $r(s, a)$ Ã© definida como o valor esperado da recompensa $R_t$ dado o estado anterior $S_{t-1} = s$ e a aÃ§Ã£o anterior $A_{t-1} = a$, ou seja, $\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a]$.

II. Pela definiÃ§Ã£o de valor esperado para variÃ¡veis discretas, o valor esperado de $R_t$ Ã© a soma de cada valor possÃ­vel de $R_t$ multiplicado pela sua probabilidade condicional. Neste caso, precisamos considerar todos os possÃ­veis prÃ³ximos estados $s'$ e recompensas $r$.

III. A probabilidade de receber a recompensa $r$ e transitar para o estado $s'$ dado $s$ e $a$ Ã© dada por $p(s', r | s, a)$.

IV. Para calcular a recompensa esperada, somamos o produto da recompensa $r$ pela probabilidade $p(s', r | s, a)$ sobre todos os possÃ­veis estados $s'$ e recompensas $r$:

V.  $\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \cdot p(s', r | s, a)$.

VI. Rearranjando a soma, obtemos $r(s, a) = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r | s, a)$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Usando os valores anteriores, a recompensa esperada para o estado $s_1$ e aÃ§Ã£o $a_1$ Ã©:
>
> $r(s_1, a_1) = \sum_{r \in \{0, 1\}} r \sum_{s' \in \{s_1, s_2\}} p(s', r | s_1, a_1) = 0 \cdot (p(s_1, 0 | s_1, a_1) + p(s_2, 0 | s_1, a_1)) + 1 \cdot (p(s_1, 1 | s_1, a_1) + p(s_2, 1 | s_1, a_1)) = 0 \cdot (0.6 + 0.3) + 1 \cdot (0.1 + 0.0) = 0 + 0.1 = 0.1$.

E as recompensas esperadas para as triplas estado-aÃ§Ã£o-prÃ³ximo estado $r(s, a, s')$:

$$r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r | s, a)}{p(s' | s, a)}$$ [^49]

**Prova:**

Provaremos que $r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r | s, a)}{p(s' | s, a)}$.

I.  A recompensa esperada $r(s, a, s')$ Ã© definida como o valor esperado da recompensa $R_t$ dado o estado anterior $S_{t-1} = s$, a aÃ§Ã£o anterior $A_{t-1} = a$ e o prÃ³ximo estado $S_t = s'$, ou seja, $\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s']$.

II. Pela definiÃ§Ã£o de valor esperado para variÃ¡veis discretas, o valor esperado de $R_t$ Ã© a soma de cada valor possÃ­vel de $R_t$ multiplicado pela sua probabilidade condicional. Neste caso, a probabilidade condicional Ã© $p(r | s, a, s')$.

III. Portanto, $\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \cdot p(r | s, a, s')$.

IV. Pela definiÃ§Ã£o de probabilidade condicional, $p(r | s, a, s') = \frac{p(s', r | s, a)}{p(s' | s, a)}$.

V.  Substituindo na equaÃ§Ã£o do valor esperado, obtemos $r(s, a, s') = \sum_{r \in \mathcal{R}} r \frac{p(s', r | s, a)}{p(s' | s, a)}$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**  Calcular $r(s_1, a_1, s_1)$. Primeiro, $p(s_1 | s_1, a_1) = 0.7$ (calculado anteriormente).
>
> $r(s_1, a_1, s_1) = \sum_{r \in \{0, 1\}} r \frac{p(s_1, r | s_1, a_1)}{p(s_1 | s_1, a_1)} = 0 \cdot \frac{p(s_1, 0 | s_1, a_1)}{0.7} + 1 \cdot \frac{p(s_1, 1 | s_1, a_1)}{0.7} = 0 \cdot \frac{0.6}{0.7} + 1 \cdot \frac{0.1}{0.7} = 0 + \frac{0.1}{0.7} \approx 0.143$.

O framework MDP Ã© abstrato e flexÃ­vel, podendo ser aplicado em diferentes problemas. Os passos de tempo podem se referir a estÃ¡gios sucessivos arbitrÃ¡rios de tomada de decisÃ£o e aÃ§Ã£o [^49]. As aÃ§Ãµes podem ser controles de baixo nÃ­vel ou decisÃµes de alto nÃ­vel [^49]. Os estados podem ser completamente determinados por sensaÃ§Ãµes de baixo nÃ­vel ou serem descriÃ§Ãµes simbÃ³licas mais abstratas [^49].

![RepresentaÃ§Ã£o do sistema de coleta de latas como um MDP finito, ilustrando as transiÃ§Ãµes de estado e recompensas.](./../images/image4.png)

Para complementar a descriÃ§Ã£o das recompensas esperadas, podemos definir a *funÃ§Ã£o de recompensa de transiÃ§Ã£o* $p(r | s, a, s')$, que representa a probabilidade de receber uma recompensa $r$ ao transitar do estado $s$ para o estado $s'$ sob a aÃ§Ã£o $a$:

$$p(r | s, a, s') = Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s'\} = \frac{p(s', r | s, a)}{p(s' | s, a)}$$

**Prova:**

Provaremos que $p(r | s, a, s') = Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s'\} = \frac{p(s', r | s, a)}{p(s' | s, a)}$.

I.  Pela definiÃ§Ã£o de probabilidade condicional, a probabilidade de um evento A dado o evento B Ã© definida como $P(A|B) = \frac{P(A \cap B)}{P(B)}$, onde $P(A \cap B)$ Ã© a probabilidade conjunta de A e B.

II. No nosso caso, queremos encontrar a probabilidade de receber a recompensa $r$ dado o estado anterior $s$, a aÃ§Ã£o $a$ e o prÃ³ximo estado $s'$. Portanto, queremos calcular $p(r | s, a, s') = Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s'\}$.

III. Usando a definiÃ§Ã£o de probabilidade condicional, podemos reescrever isso como:
$p(r | s, a, s') = \frac{Pr\{R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a\}}{Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\}}$.

IV. Usando a notaÃ§Ã£o da funÃ§Ã£o de dinÃ¢mica, isso se torna $p(r | s, a, s') = \frac{p(s', r | s, a)}{p(s' | s, a)}$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:** Calcular $p(1 | s_1, a_1, s_1)$. JÃ¡ sabemos que $p(s_1 | s_1, a_1) = 0.7$. EntÃ£o:
>
> $p(1 | s_1, a_1, s_1) = \frac{p(s_1, 1 | s_1, a_1)}{p(s_1 | s_1, a_1)} = \frac{0.1}{0.7} \approx 0.143$.
>
> Isso significa que, dado que o agente estava em $s_1$, tomou a aÃ§Ã£o $a_1$ e foi para o estado $s_1$, a probabilidade de ter recebido uma recompensa de 1 Ã© aproximadamente 0.143.

**Teorema 1** A funÃ§Ã£o de dinÃ¢mica $p(s', r | s, a)$ e a funÃ§Ã£o de recompensa de transiÃ§Ã£o $p(r | s, a, s')$ caracterizam completamente as recompensas e transiÃ§Ãµes de estado no MDP.

*Prova:* A funÃ§Ã£o de dinÃ¢mica descreve a probabilidade conjunta de transiÃ§Ã£o para um prÃ³ximo estado e receber uma recompensa. A funÃ§Ã£o de recompensa de transiÃ§Ã£o especifica a probabilidade de receber uma recompensa dado uma transiÃ§Ã£o de estado. Juntas, essas funÃ§Ãµes fornecem todas as informaÃ§Ãµes necessÃ¡rias para modelar as recompensas e transiÃ§Ãµes de estado.

Adicionalmente, Ã© Ãºtil definir a *funÃ§Ã£o de valor esperado do prÃ³ximo estado* dado o estado atual e a aÃ§Ã£o:

$$v(s, a) = \mathbb{E}[S_t | S_{t-1} = s, A_{t-1} = a] = \sum_{s' \in \mathcal{S}} s' p(s' | s, a)$$

Ã‰ importante notar que, em muitos casos, o estado nÃ£o Ã© numÃ©rico, e esta funÃ§Ã£o nÃ£o Ã© diretamente aplicÃ¡vel. No entanto, em situaÃ§Ãµes onde o estado possui uma representaÃ§Ã£o numÃ©rica, esta funÃ§Ã£o pode ser Ãºtil para anÃ¡lise.

**Prova:**

Provaremos que $v(s, a) = \mathbb{E}[S_t | S_{t-1} = s, A_{t-1} = a] = \sum_{s' \in \mathcal{S}} s' p(s' | s, a)$.

I. Definimos $v(s, a)$ como o valor esperado do prÃ³ximo estado $S_t$, dado o estado anterior $S_{t-1} = s$ e a aÃ§Ã£o anterior $A_{t-1} = a$. Ou seja, $v(s, a) = \mathbb{E}[S_t | S_{t-1} = s, A_{t-1} = a]$.

II. Pela definiÃ§Ã£o de valor esperado,  $\mathbb{E}[X] = \sum x P(X=x)$ para uma variÃ¡vel aleatÃ³ria discreta $X$.

III. Aplicando essa definiÃ§Ã£o ao nosso caso, onde $X = S_t$, temos que $\mathbb{E}[S_t | S_{t-1} = s, A_{t-1} = a] = \sum_{s' \in \mathcal{S}} s' \cdot Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\}$.

IV. Sabemos que $Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = p(s' | s, a)$, a probabilidade de transiÃ§Ã£o para o estado $s'$ dado $s$ e $a$.

V. Substituindo, obtemos $v(s, a) = \sum_{s' \in \mathcal{S}} s' p(s' | s, a)$. â– 

> ðŸ’¡ **Exemplo NumÃ©rico:**  Assumindo que os estados $s_1$ e $s_2$ sÃ£o representados numericamente como 1 e 2, respectivamente, podemos calcular o valor esperado do prÃ³ximo estado dado o estado $s_1$ e a aÃ§Ã£o $a_1$:
>
> $v(s_1, a_1) = \sum_{s' \in \{s_1, s_2\}} s' p(s' | s_1, a_1) = 1 \cdot p(s_1 | s_1, a_1) + 2 \cdot p(s_2 | s_1, a_1) = 1 \cdot 0.7 + 2 \cdot 0.3 = 0.7 + 0.6 = 1.3$.
>
> Isso significa que, em mÃ©dia, o prÃ³ximo estado serÃ¡ 1.3 (em uma escala onde $s_1$ Ã© 1 e $s_2$ Ã© 2).

### ConclusÃ£o

A interface agente-ambiente em MDPs finitos fornece uma estrutura poderosa para modelar problemas de tomada de decisÃ£o sequencial [^47]. Ao definir claramente os papÃ©is do agente e do ambiente, bem como a forma como eles interagem, essa estrutura permite que o problema de aprendizado por reforÃ§o seja formalizado matematicamente. A funÃ§Ã£o de dinÃ¢mica, $p(s', r | s, a)$, desempenha um papel central, pois captura a maneira como o ambiente responde Ã s aÃ§Ãµes do agente e fornece a base para a definiÃ§Ã£o de conceitos como polÃ­ticas, funÃ§Ãµes de valor e otimalidade [^48].

### ReferÃªncias
[^47]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. Cambridge, MA: MIT press, 2018.
[^48]: SeÃ§Ã£o 3.1, CapÃ­tulo 3, *Reinforcement learning: An introduction*, Sutton and Barto, 2018.
[^49]: SeÃ§Ã£o 3.1, CapÃ­tulo 3, *Reinforcement learning: An introduction*, Sutton and Barto, 2018.
<!-- END -->