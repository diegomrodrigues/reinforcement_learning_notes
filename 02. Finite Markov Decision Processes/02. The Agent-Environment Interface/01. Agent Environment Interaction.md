## O Interface Agente-Ambiente em Processos de Decis√£o de Markov Finitos

### Introdu√ß√£o
Em processos de decis√£o de Markov (MDPs) finitos, a intera√ß√£o entre o **agente** e o **ambiente** √© fundamental para o aprendizado e a tomada de decis√µes. O agente, como o aprendiz e tomador de decis√µes, busca atingir um objetivo atrav√©s da intera√ß√£o cont√≠nua com o ambiente. O ambiente, por sua vez, compreende tudo o que est√° fora do agente, respondendo √†s a√ß√µes do agente e fornecendo *recompensas* e *novos estados* [^47]. Este cap√≠tulo explora em detalhes a natureza dessa intera√ß√£o, os pap√©is do agente e do ambiente, e como essa estrutura permite a formaliza√ß√£o do problema de aprendizado por refor√ßo.

### Conceitos Fundamentais

A intera√ß√£o entre o agente e o ambiente em um MDP finito ocorre em uma sequ√™ncia de *passos de tempo discretos*, denotados por $t = 0, 1, 2, 3, \ldots$ [^48]. Em cada passo de tempo $t$, o agente recebe uma representa√ß√£o do **estado** do ambiente, $S_t \in \mathcal{S}$, onde $\mathcal{S}$ √© o conjunto de todos os estados poss√≠veis. Com base neste estado, o agente seleciona uma **a√ß√£o**, $A_t \in \mathcal{A}(s)$, onde $\mathcal{A}(s)$ √© o conjunto de a√ß√µes dispon√≠veis no estado $s$. A escolha da a√ß√£o influencia n√£o apenas a recompensa imediata, mas tamb√©m os estados futuros [^47].

> üí° **Exemplo Num√©rico:** Imagine um agente (um rob√¥) em um ambiente simples com tr√™s estados: $\mathcal{S} = \{\text{Bateria Cheia, Bateria M√©dia, Bateria Fraca}\}$. O agente est√° no estado $S_t = \text{Bateria M√©dia}$. As a√ß√µes poss√≠veis nesse estado s√£o $\mathcal{A}(\text{Bateria M√©dia}) = \{\text{Procurar Carregador, Continuar Tarefa}\}$. Se o agente escolher a a√ß√£o $A_t = \text{Procurar Carregador}$, isso afetar√° a recompensa imediata (gasto de energia) e o pr√≥ximo estado (a bateria pode carregar ou n√£o).

Como consequ√™ncia da a√ß√£o do agente, o ambiente responde com uma **recompensa num√©rica**, $R_{t+1} \in \mathbb{R}$, e transita para um novo **estado**, $S_{t+1}$. Essa intera√ß√£o cont√≠nua gera uma sequ√™ncia ou trajet√≥ria que se inicia da seguinte forma:
$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots$$ [^48]

> üí° **Exemplo Num√©rico:** Continuando o exemplo do rob√¥, se no estado $S_t = \text{Bateria M√©dia}$, o rob√¥ escolhe a a√ß√£o $A_t = \text{Continuar Tarefa}$, o ambiente pode responder com uma recompensa $R_{t+1} = -1$ (indicando uma pequena perda de energia) e o pr√≥ximo estado pode ser $S_{t+1} = \text{Bateria Fraca}$. A sequ√™ncia at√© agora seria:  $\text{Bateria M√©dia}, \text{Continuar Tarefa}, -1, \text{Bateria Fraca}, \ldots$

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

Em um MDP *finito*, os conjuntos de estados $\mathcal{S}$, a√ß√µes $\mathcal{A}$ e recompensas $\mathcal{R}$ possuem um n√∫mero finito de elementos [^48]. As vari√°veis aleat√≥rias $R_t$ e $S_t$ possuem *distribui√ß√µes de probabilidade discretas* bem definidas, que dependem apenas do estado e da a√ß√£o precedentes. Em outras palavras, para valores espec√≠ficos $s' \in \mathcal{S}$ e $r \in \mathcal{R}$, existe uma probabilidade de que esses valores ocorram no tempo $t$, dado o estado $s$ e a a√ß√£o $a$ no tempo anterior:

$$p(s', r | s, a) = Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\}$$ [^48]

> üí° **Exemplo Num√©rico:** Suponha que $\mathcal{S} = \{s_1, s_2\}$, $\mathcal{A}(s_1) = \{a_1, a_2\}$ e $\mathcal{R} = \{0, 1\}$. Se o agente estiver no estado $s_1$ e tomar a a√ß√£o $a_1$, a fun√ß√£o de din√¢mica pode ser:
>
>  $p(s_1, 0 | s_1, a_1) = 0.6$ (probabilidade de permanecer em $s_1$ e receber recompensa 0)
>
>  $p(s_2, 0 | s_1, a_1) = 0.3$ (probabilidade de ir para $s_2$ e receber recompensa 0)
>
>  $p(s_1, 1 | s_1, a_1) = 0.1$ (probabilidade de permanecer em $s_1$ e receber recompensa 1)
>
>  $p(s_2, 1 | s_1, a_1) = 0.0$ (probabilidade de ir para $s_2$ e receber recompensa 1)

Essa fun√ß√£o $p$, denominada *fun√ß√£o de din√¢mica* do MDP, define a din√¢mica do ambiente. A fun√ß√£o $p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ √© uma fun√ß√£o determin√≠stica de quatro argumentos [^48].

√â importante notar que $p$ especifica uma distribui√ß√£o de probabilidade para cada escolha de $s$ e $a$, ou seja:

$$\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r | s, a) = 1, \text{ para todo } s \in \mathcal{S}, a \in \mathcal{A}(s)$$ [^49]

**Prova:**

Provaremos que $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r | s, a) = 1$ para todo $s \in \mathcal{S}$ e $a \in \mathcal{A}(s)$.

I.  $p(s', r | s, a)$ representa a probabilidade condicional de transitar para o estado $s'$ e receber a recompensa $r$, dado o estado $s$ e a a√ß√£o $a$.

II. A soma $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r | s, a)$ representa a soma das probabilidades de *todos* os poss√≠veis pr√≥ximos estados $s'$ e recompensas $r$, dado o estado $s$ e a a√ß√£o $a$.

III. Uma vez que o agente est√° no estado $s$ e executa a a√ß√£o $a$, o ambiente *deve* transitar para algum estado $s'$ e emitir alguma recompensa $r$.

IV.  Portanto, a soma das probabilidades de todos os resultados poss√≠veis deve ser igual a 1, pois representa a certeza de que algum resultado ocorrer√°.

V.  Assim, $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r | s, a) = 1$. ‚ñ†

> üí° **Exemplo Num√©rico:** No exemplo anterior, com $\mathcal{S} = \{s_1, s_2\}$, $\mathcal{A}(s_1) = \{a_1, a_2\}$ e $\mathcal{R} = \{0, 1\}$, e $p(s_1, 0 | s_1, a_1) = 0.6$, $p(s_2, 0 | s_1, a_1) = 0.3$, $p(s_1, 1 | s_1, a_1) = 0.1$, $p(s_2, 1 | s_1, a_1) = 0.0$, podemos verificar que a soma das probabilidades √© 1: $0.6 + 0.3 + 0.1 + 0.0 = 1.0$.

Em um processo de decis√£o de Markov, as probabilidades dadas por $p$ caracterizam completamente a din√¢mica do ambiente [^49]. A probabilidade de cada valor poss√≠vel para $S_t$ e $R_t$ depende do estado e da a√ß√£o imediatamente anteriores, $S_{t-1}$ e $A_{t-1}$, e n√£o depende de estados e a√ß√µes anteriores, desde que $S_{t-1}$ contenha todas as informa√ß√µes relevantes do hist√≥rico da intera√ß√£o agente-ambiente [^49].

*A propriedade de Markov* imp√µe uma restri√ß√£o n√£o sobre o processo de decis√£o, mas sobre o estado. O estado deve incluir informa√ß√µes sobre todos os aspectos da intera√ß√£o agente-ambiente passada que fazem diferen√ßa para o futuro [^49]. Se o estado satisfaz a propriedade de Markov, diz-se que ele tem a *propriedade de Markov*. Assume-se que o estado sempre possui a propriedade de Markov. M√©todos de aproxima√ß√£o que n√£o se baseiam nesta propriedade e constru√ß√£o de estados Markovianos a partir de observa√ß√µes n√£o Markovianas s√£o tratados posteriormente [^49].

A partir da fun√ß√£o de din√¢mica de quatro argumentos, $p$, √© poss√≠vel computar outras informa√ß√µes sobre o ambiente, como as *probabilidades de transi√ß√£o de estado* (denotadas como $p(s' | s, a)$), que representam a probabilidade de transitar para o estado $s'$ dado o estado $s$ e a a√ß√£o $a$:
$$p(s' | s, a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum_{r \in \mathcal{R}} p(s', r | s, a)$$ [^49]

**Prova:**

Provaremos que $p(s' | s, a) = \sum_{r \in \mathcal{R}} p(s', r | s, a)$.

I. Queremos encontrar a probabilidade de transitar para o estado $s'$ dado o estado $s$ e a a√ß√£o $a$, ou seja, $p(s' | s, a)$.

II. Sabemos que $p(s', r | s, a)$ √© a probabilidade conjunta de transitar para o estado $s'$ e receber a recompensa $r$, dado o estado $s$ e a a√ß√£o $a$.

III. Para obter a probabilidade de transitar para o estado $s'$ independentemente da recompensa, precisamos somar a probabilidade conjunta $p(s', r | s, a)$ sobre todos os poss√≠veis valores de $r$.

IV. Portanto, $p(s' | s, a) = \sum_{r \in \mathcal{R}} p(s', r | s, a)$. ‚ñ†

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, para calcular $p(s_1 | s_1, a_1)$, somamos as probabilidades de ir para $s_1$ com cada poss√≠vel recompensa:
>
> $p(s_1 | s_1, a_1) = p(s_1, 0 | s_1, a_1) + p(s_1, 1 | s_1, a_1) = 0.6 + 0.1 = 0.7$.
>
> Similarmente, $p(s_2 | s_1, a_1) = p(s_2, 0 | s_1, a_1) + p(s_2, 1 | s_1, a_1) = 0.3 + 0.0 = 0.3$.

Tamb√©m √© poss√≠vel computar as *recompensas esperadas* para pares estado-a√ß√£o $r(s, a)$:

$$r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r | s, a)$$ [^49]

**Prova:**

Provaremos que $r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r | s, a)$.

I.  A recompensa esperada $r(s, a)$ √© definida como o valor esperado da recompensa $R_t$ dado o estado anterior $S_{t-1} = s$ e a a√ß√£o anterior $A_{t-1} = a$, ou seja, $\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a]$.

II. Pela defini√ß√£o de valor esperado para vari√°veis discretas, o valor esperado de $R_t$ √© a soma de cada valor poss√≠vel de $R_t$ multiplicado pela sua probabilidade condicional. Neste caso, precisamos considerar todos os poss√≠veis pr√≥ximos estados $s'$ e recompensas $r$.

III. A probabilidade de receber a recompensa $r$ e transitar para o estado $s'$ dado $s$ e $a$ √© dada por $p(s', r | s, a)$.

IV. Para calcular a recompensa esperada, somamos o produto da recompensa $r$ pela probabilidade $p(s', r | s, a)$ sobre todos os poss√≠veis estados $s'$ e recompensas $r$:

V.  $\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \cdot p(s', r | s, a)$.

VI. Rearranjando a soma, obtemos $r(s, a) = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}} p(s', r | s, a)$. ‚ñ†

> üí° **Exemplo Num√©rico:** Usando os valores anteriores, a recompensa esperada para o estado $s_1$ e a√ß√£o $a_1$ √©:
>
> $r(s_1, a_1) = \sum_{r \in \{0, 1\}} r \sum_{s' \in \{s_1, s_2\}} p(s', r | s_1, a_1) = 0 \cdot (p(s_1, 0 | s_1, a_1) + p(s_2, 0 | s_1, a_1)) + 1 \cdot (p(s_1, 1 | s_1, a_1) + p(s_2, 1 | s_1, a_1)) = 0 \cdot (0.6 + 0.3) + 1 \cdot (0.1 + 0.0) = 0 + 0.1 = 0.1$.

E as recompensas esperadas para as triplas estado-a√ß√£o-pr√≥ximo estado $r(s, a, s')$:

$$r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r | s, a)}{p(s' | s, a)}$$ [^49]

**Prova:**

Provaremos que $r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r | s, a)}{p(s' | s, a)}$.

I.  A recompensa esperada $r(s, a, s')$ √© definida como o valor esperado da recompensa $R_t$ dado o estado anterior $S_{t-1} = s$, a a√ß√£o anterior $A_{t-1} = a$ e o pr√≥ximo estado $S_t = s'$, ou seja, $\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s']$.

II. Pela defini√ß√£o de valor esperado para vari√°veis discretas, o valor esperado de $R_t$ √© a soma de cada valor poss√≠vel de $R_t$ multiplicado pela sua probabilidade condicional. Neste caso, a probabilidade condicional √© $p(r | s, a, s')$.

III. Portanto, $\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \cdot p(r | s, a, s')$.

IV. Pela defini√ß√£o de probabilidade condicional, $p(r | s, a, s') = \frac{p(s', r | s, a)}{p(s' | s, a)}$.

V.  Substituindo na equa√ß√£o do valor esperado, obtemos $r(s, a, s') = \sum_{r \in \mathcal{R}} r \frac{p(s', r | s, a)}{p(s' | s, a)}$. ‚ñ†

> üí° **Exemplo Num√©rico:**  Calcular $r(s_1, a_1, s_1)$. Primeiro, $p(s_1 | s_1, a_1) = 0.7$ (calculado anteriormente).
>
> $r(s_1, a_1, s_1) = \sum_{r \in \{0, 1\}} r \frac{p(s_1, r | s_1, a_1)}{p(s_1 | s_1, a_1)} = 0 \cdot \frac{p(s_1, 0 | s_1, a_1)}{0.7} + 1 \cdot \frac{p(s_1, 1 | s_1, a_1)}{0.7} = 0 \cdot \frac{0.6}{0.7} + 1 \cdot \frac{0.1}{0.7} = 0 + \frac{0.1}{0.7} \approx 0.143$.

O framework MDP √© abstrato e flex√≠vel, podendo ser aplicado em diferentes problemas. Os passos de tempo podem se referir a est√°gios sucessivos arbitr√°rios de tomada de decis√£o e a√ß√£o [^49]. As a√ß√µes podem ser controles de baixo n√≠vel ou decis√µes de alto n√≠vel [^49]. Os estados podem ser completamente determinados por sensa√ß√µes de baixo n√≠vel ou serem descri√ß√µes simb√≥licas mais abstratas [^49].

![Representa√ß√£o do sistema de coleta de latas como um MDP finito, ilustrando as transi√ß√µes de estado e recompensas.](./../images/image4.png)

Para complementar a descri√ß√£o das recompensas esperadas, podemos definir a *fun√ß√£o de recompensa de transi√ß√£o* $p(r | s, a, s')$, que representa a probabilidade de receber uma recompensa $r$ ao transitar do estado $s$ para o estado $s'$ sob a a√ß√£o $a$:

$$p(r | s, a, s') = Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s'\} = \frac{p(s', r | s, a)}{p(s' | s, a)}$$

**Prova:**

Provaremos que $p(r | s, a, s') = Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s'\} = \frac{p(s', r | s, a)}{p(s' | s, a)}$.

I.  Pela defini√ß√£o de probabilidade condicional, a probabilidade de um evento A dado o evento B √© definida como $P(A|B) = \frac{P(A \cap B)}{P(B)}$, onde $P(A \cap B)$ √© a probabilidade conjunta de A e B.

II. No nosso caso, queremos encontrar a probabilidade de receber a recompensa $r$ dado o estado anterior $s$, a a√ß√£o $a$ e o pr√≥ximo estado $s'$. Portanto, queremos calcular $p(r | s, a, s') = Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s'\}$.

III. Usando a defini√ß√£o de probabilidade condicional, podemos reescrever isso como:
$p(r | s, a, s') = \frac{Pr\{R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a\}}{Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\}}$.

IV. Usando a nota√ß√£o da fun√ß√£o de din√¢mica, isso se torna $p(r | s, a, s') = \frac{p(s', r | s, a)}{p(s' | s, a)}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Calcular $p(1 | s_1, a_1, s_1)$. J√° sabemos que $p(s_1 | s_1, a_1) = 0.7$. Ent√£o:
>
> $p(1 | s_1, a_1, s_1) = \frac{p(s_1, 1 | s_1, a_1)}{p(s_1 | s_1, a_1)} = \frac{0.1}{0.7} \approx 0.143$.
>
> Isso significa que, dado que o agente estava em $s_1$, tomou a a√ß√£o $a_1$ e foi para o estado $s_1$, a probabilidade de ter recebido uma recompensa de 1 √© aproximadamente 0.143.

**Teorema 1** A fun√ß√£o de din√¢mica $p(s', r | s, a)$ e a fun√ß√£o de recompensa de transi√ß√£o $p(r | s, a, s')$ caracterizam completamente as recompensas e transi√ß√µes de estado no MDP.

*Prova:* A fun√ß√£o de din√¢mica descreve a probabilidade conjunta de transi√ß√£o para um pr√≥ximo estado e receber uma recompensa. A fun√ß√£o de recompensa de transi√ß√£o especifica a probabilidade de receber uma recompensa dado uma transi√ß√£o de estado. Juntas, essas fun√ß√µes fornecem todas as informa√ß√µes necess√°rias para modelar as recompensas e transi√ß√µes de estado.

Adicionalmente, √© √∫til definir a *fun√ß√£o de valor esperado do pr√≥ximo estado* dado o estado atual e a a√ß√£o:

$$v(s, a) = \mathbb{E}[S_t | S_{t-1} = s, A_{t-1} = a] = \sum_{s' \in \mathcal{S}} s' p(s' | s, a)$$

√â importante notar que, em muitos casos, o estado n√£o √© num√©rico, e esta fun√ß√£o n√£o √© diretamente aplic√°vel. No entanto, em situa√ß√µes onde o estado possui uma representa√ß√£o num√©rica, esta fun√ß√£o pode ser √∫til para an√°lise.

**Prova:**

Provaremos que $v(s, a) = \mathbb{E}[S_t | S_{t-1} = s, A_{t-1} = a] = \sum_{s' \in \mathcal{S}} s' p(s' | s, a)$.

I. Definimos $v(s, a)$ como o valor esperado do pr√≥ximo estado $S_t$, dado o estado anterior $S_{t-1} = s$ e a a√ß√£o anterior $A_{t-1} = a$. Ou seja, $v(s, a) = \mathbb{E}[S_t | S_{t-1} = s, A_{t-1} = a]$.

II. Pela defini√ß√£o de valor esperado,  $\mathbb{E}[X] = \sum x P(X=x)$ para uma vari√°vel aleat√≥ria discreta $X$.

III. Aplicando essa defini√ß√£o ao nosso caso, onde $X = S_t$, temos que $\mathbb{E}[S_t | S_{t-1} = s, A_{t-1} = a] = \sum_{s' \in \mathcal{S}} s' \cdot Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\}$.

IV. Sabemos que $Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = p(s' | s, a)$, a probabilidade de transi√ß√£o para o estado $s'$ dado $s$ e $a$.

V. Substituindo, obtemos $v(s, a) = \sum_{s' \in \mathcal{S}} s' p(s' | s, a)$. ‚ñ†

> üí° **Exemplo Num√©rico:**  Assumindo que os estados $s_1$ e $s_2$ s√£o representados numericamente como 1 e 2, respectivamente, podemos calcular o valor esperado do pr√≥ximo estado dado o estado $s_1$ e a a√ß√£o $a_1$:
>
> $v(s_1, a_1) = \sum_{s' \in \{s_1, s_2\}} s' p(s' | s_1, a_1) = 1 \cdot p(s_1 | s_1, a_1) + 2 \cdot p(s_2 | s_1, a_1) = 1 \cdot 0.7 + 2 \cdot 0.3 = 0.7 + 0.6 = 1.3$.
>
> Isso significa que, em m√©dia, o pr√≥ximo estado ser√° 1.3 (em uma escala onde $s_1$ √© 1 e $s_2$ √© 2).

### Conclus√£o

A interface agente-ambiente em MDPs finitos fornece uma estrutura poderosa para modelar problemas de tomada de decis√£o sequencial [^47]. Ao definir claramente os pap√©is do agente e do ambiente, bem como a forma como eles interagem, essa estrutura permite que o problema de aprendizado por refor√ßo seja formalizado matematicamente. A fun√ß√£o de din√¢mica, $p(s', r | s, a)$, desempenha um papel central, pois captura a maneira como o ambiente responde √†s a√ß√µes do agente e fornece a base para a defini√ß√£o de conceitos como pol√≠ticas, fun√ß√µes de valor e otimalidade [^48].

### Refer√™ncias
[^47]: Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*. Cambridge, MA: MIT press, 2018.
[^48]: Se√ß√£o 3.1, Cap√≠tulo 3, *Reinforcement learning: An introduction*, Sutton and Barto, 2018.
[^49]: Se√ß√£o 3.1, Cap√≠tulo 3, *Reinforcement learning: An introduction*, Sutton and Barto, 2018.
<!-- END -->