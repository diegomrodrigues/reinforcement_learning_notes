## Modelagem da InteraÃ§Ã£o Agente-Ambiente em Processos de DecisÃ£o de Markov Finitos

### IntroduÃ§Ã£o
Em **Processos de DecisÃ£o de Markov Finitos (MDPs finitos)**, a interaÃ§Ã£o entre um agente e seu ambiente Ã© formalizada como uma sequÃªncia de etapas de tempo discretas. Este capÃ­tulo se aprofunda na modelagem dessa interaÃ§Ã£o, que Ã© fundamental para entender como os agentes aprendem a tomar decisÃµes Ã³timas ao longo do tempo. A interaÃ§Ã£o contÃ­nua entre o agente e o ambiente forma a base para o aprendizado por reforÃ§o [^1]. O agente percebe o estado do ambiente, seleciona uma aÃ§Ã£o, e o ambiente responde com uma recompensa e uma nova representaÃ§Ã£o de estado. Este ciclo contÃ­nuo Ã© essencial para o aprendizado e tomada de decisÃ£o do agente [^2].

### Conceitos Fundamentais

A interaÃ§Ã£o entre o agente e o ambiente Ã© modelada como uma sequÃªncia de passos de tempo discretos, representados como $t = 0, 1, 2, 3, \ldots$ [^2]. Em cada passo de tempo $t$, os seguintes eventos ocorrem:

1.  **PercepÃ§Ã£o do Estado:** O agente recebe uma representaÃ§Ã£o do estado do ambiente, denotado por $S_t \in S$, onde $S$ Ã© o conjunto de todos os estados possÃ­veis [^2]. O **estado** deve conter todas as informaÃ§Ãµes relevantes sobre o histÃ³rico da interaÃ§Ã£o agente-ambiente que influenciam as decisÃµes futuras [^3]. Se o estado satisfaz esta condiÃ§Ã£o, diz-se que possui a *propriedade de Markov* [^3].

2.  **SeleÃ§Ã£o da AÃ§Ã£o:** Com base na representaÃ§Ã£o do estado $S_t$, o agente seleciona uma aÃ§Ã£o $A_t \in A(s)$, onde $A(s)$ Ã© o conjunto de aÃ§Ãµes disponÃ­veis no estado $s$ [^2]. A seleÃ§Ã£o da aÃ§Ã£o Ã© guiada pela **polÃ­tica** do agente, denotada por $\pi$, que mapeia estados para probabilidades de seleÃ§Ã£o de cada aÃ§Ã£o possÃ­vel [^12]. Formalmente, $\pi(a|s)$ representa a probabilidade de selecionar a aÃ§Ã£o $a$ no estado $s$ [^12].

    > ğŸ’¡ **Exemplo NumÃ©rico:** Considere um agente em um ambiente simples com dois estados, $S = \{s_1, s_2\}$, e duas aÃ§Ãµes possÃ­veis em cada estado, $A(s) = \{a_1, a_2\}$ para todo $s \in S$. A polÃ­tica $\pi$ pode ser representada por uma tabela:
    >
    > | Estado | AÃ§Ã£o | Probabilidade |
    > |---|---|---|
    > | $s_1$ | $a_1$ | 0.7 |
    > | $s_1$ | $a_2$ | 0.3 |
    > | $s_2$ | $a_1$ | 0.1 |
    > | $s_2$ | $a_2$ | 0.9 |
    >
    > Isso significa que no estado $s_1$, o agente escolhe a aÃ§Ã£o $a_1$ com probabilidade 0.7 e a aÃ§Ã£o $a_2$ com probabilidade 0.3. No estado $s_2$, o agente escolhe a aÃ§Ã£o $a_1$ com probabilidade 0.1 e a aÃ§Ã£o $a_2$ com probabilidade 0.9.

3.  **Recebimento da Recompensa:** ApÃ³s a execuÃ§Ã£o da aÃ§Ã£o $A_t$, o agente recebe uma recompensa numÃ©rica, denotada por $R_{t+1} \in \mathbb{R}$ [^2]. A **recompensa** Ã© um sinal que indica o quÃ£o desejÃ¡vel foi a transiÃ§Ã£o do estado anterior para o estado atual, dada a aÃ§Ã£o tomada [^7]. O objetivo do agente Ã© maximizar a quantidade total de recompensa que recebe ao longo do tempo [^7].

    > ğŸ’¡ **Exemplo NumÃ©rico:** Um robÃ´ estÃ¡ aprendendo a navegar em um labirinto. Se ele dÃ¡ um passo em direÃ§Ã£o ao objetivo, recebe uma recompensa de +1. Se ele bate em uma parede, recebe uma recompensa de -1. Se ele nÃ£o faz nada de especial, recebe uma recompensa de 0. O objetivo do robÃ´ Ã© maximizar a soma das recompensas, ou seja, chegar ao objetivo sem bater nas paredes.

4.  **TransiÃ§Ã£o para o Novo Estado:** Como consequÃªncia da aÃ§Ã£o $A_t$, o ambiente transiciona para um novo estado, denotado por $S_{t+1} \in S$ [^2]. A transiÃ§Ã£o para o novo estado Ã© influenciada tanto pela aÃ§Ã£o do agente quanto pela dinÃ¢mica do ambiente. A dinÃ¢mica do ambiente Ã© descrita pela funÃ§Ã£o de probabilidade de transiÃ§Ã£o $p(s', r|s, a)$, que especifica a probabilidade de transicionar para o estado $s'$ e receber a recompensa $r$ apÃ³s tomar a aÃ§Ã£o $a$ no estado $s$ [^2]. Formalmente,
    $$
    p(s',r|s,a) = Pr\{S_t=s',R_{t+1}=r | S_{t-1}=s, A_{t-1}=a\} \quad \forall s', s \in S, r \in \mathbb{R}, a \in A(s)
    $$
    A funÃ§Ã£o $p$ define a dinÃ¢mica do MDP, onde $p : S \times R \times S \times A \rightarrow [0, 1]$ [^2]. A funÃ§Ã£o $p$ especifica uma distribuiÃ§Ã£o de probabilidade para cada escolha de $s$ e $a$, isto Ã©:
    $$
    \sum_{s' \in S} \sum_{r \in \mathbb{R}} p(s', r|s, a) = 1, \quad \forall s \in S, a \in A(s)
    $$
    Num **MDP finito**, os conjuntos de estados $S$, aÃ§Ãµes $A$ e recompensas $R$ sÃ£o finitos [^2]. Isto implica que as variÃ¡veis aleatÃ³rias $R_t$ e $S_t$ possuem distribuiÃ§Ãµes de probabilidade discretas bem definidas, dependentes apenas do estado e aÃ§Ã£o precedentes [^2].

    > ğŸ’¡ **Exemplo NumÃ©rico:** Considere um ambiente com dois estados ($S = \{s_1, s_2\}$) e duas aÃ§Ãµes ($A = \{a_1, a_2\}$). A funÃ§Ã£o de probabilidade de transiÃ§Ã£o $p(s', r|s, a)$ pode ser definida como:
    >
    > | $s$ | $a$ | $s'$ | $r$ | $p(s', r|s, a)$ |
    > |---|---|---|---|---|
    > | $s_1$ | $a_1$ | $s_1$ | 0 | 0.8 |
    > | $s_1$ | $a_1$ | $s_2$ | 1 | 0.2 |
    > | $s_1$ | $a_2$ | $s_1$ | -1 | 0.5 |
    > | $s_1$ | $a_2$ | $s_2$ | 0 | 0.5 |
    > | $s_2$ | $a_1$ | $s_1$ | 1 | 0.6 |
    > | $s_2$ | $a_1$ | $s_2$ | 0 | 0.4 |
    > | $s_2$ | $a_2$ | $s_1$ | 0 | 0.3 |
    > | $s_2$ | $a_2$ | $s_2$ | -1 | 0.7 |
    >
    > Por exemplo, se o agente estÃ¡ no estado $s_1$ e executa a aÃ§Ã£o $a_1$, hÃ¡ uma probabilidade de 0.8 de permanecer no estado $s_1$ e receber uma recompensa de 0, e uma probabilidade de 0.2 de transicionar para o estado $s_2$ e receber uma recompensa de 1.

A sequÃªncia completa de interaÃ§Ãµes, a partir do estado inicial $S_0$, gera uma *trajetÃ³ria* ou *histÃ³rico* da interaÃ§Ã£o agente-ambiente [^2]:
$$
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots
$$

Dado que o objetivo do agente Ã© maximizar a recompensa total ao longo do tempo, introduzimos o conceito de *retorno*. O retorno, denotado por $G_t$, Ã© a soma das recompensas futuras, podendo ser definida de diferentes formas dependendo do horizonte de tempo considerado.

**DefiniÃ§Ã£o (Retorno)** O retorno $G_t$ Ã© definido como uma funÃ§Ã£o das recompensas recebidas a partir do instante $t$.

**DefiniÃ§Ã£o (Retorno EpisÃ³dico)** Em tarefas *episÃ³dicas*, onde a interaÃ§Ã£o agente-ambiente se divide em episÃ³dios, o retorno $G_t$ Ã© definido como a soma das recompensas atÃ© o final do episÃ³dio:
$$
G_t = R_{t+1} + R_{t+2} + \ldots + R_T
$$
onde $T$ Ã© o passo de tempo final do episÃ³dio.

> ğŸ’¡ **Exemplo NumÃ©rico:** Um agente estÃ¡ aprendendo a completar um jogo. Cada jogo Ã© um episÃ³dio. Em um episÃ³dio, o agente recebe as seguintes recompensas: $R_1 = -1, R_2 = 0, R_3 = -1, R_4 = 10$. O retorno $G_0$ para este episÃ³dio Ã©:
> $G_0 = R_1 + R_2 + R_3 + R_4 = -1 + 0 + (-1) + 10 = 8$.

Em tarefas *contÃ­nuas*, onde a interaÃ§Ã£o agente-ambiente nÃ£o se divide em episÃ³dios, o retorno Ã© geralmente definido com um fator de desconto $\gamma \in [0, 1]$:
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
O fator de desconto $\gamma$ determina o quanto as recompensas futuras influenciam o retorno atual. Um valor de $\gamma$ prÃ³ximo de 0 faz com que o agente se preocupe apenas com as recompensas imediatas, enquanto um valor prÃ³ximo de 1 faz com que o agente se preocupe com as recompensas a longo prazo.

> ğŸ’¡ **Exemplo NumÃ©rico:** Um agente estÃ¡ em um ambiente contÃ­nuo e recebe as seguintes recompensas: $R_{t+1} = 1, R_{t+2} = 2, R_{t+3} = -1, R_{t+4} = 0, \ldots$. Se o fator de desconto $\gamma = 0.9$, o retorno $G_t$ Ã©:
> $G_t = 1 + 0.9 \cdot 2 + 0.9^2 \cdot (-1) + 0.9^3 \cdot 0 + \ldots = 1 + 1.8 - 0.81 + 0 + \ldots \approx 1.99$ (considerando os primeiros 4 termos).
> Se o fator de desconto $\gamma = 0.1$, o retorno $G_t$ Ã©:
> $G_t = 1 + 0.1 \cdot 2 + 0.1^2 \cdot (-1) + 0.1^3 \cdot 0 + \ldots = 1 + 0.2 - 0.01 + 0 + \ldots \approx 1.19$ (considerando os primeiros 4 termos).
> Observe como o valor de $\gamma$ influencia o retorno. Com $\gamma = 0.9$, as recompensas futuras tÃªm um peso maior, resultando em um retorno maior em comparaÃ§Ã£o com $\gamma = 0.1$, onde as recompensas futuras tÃªm um peso menor.

Para demonstrar que a definiÃ§Ã£o do retorno descontado Ã© bem definida para $\gamma \in [0, 1)$ e $|R_{t+k+1}| < R_{max} \; \forall k$, provaremos que a soma converge.

*Prova:*

Provaremos que a soma $\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ converge se $|\gamma| < 1$ e $|R_{t+k+1}| < R_{max}$ para todo $k$.

I. Considere a sÃ©rie $\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$.

II. Como $|R_{t+k+1}| < R_{max}$ para todo $k$, podemos escrever:
    $$|\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}| \leq \sum_{k=0}^{\infty} |\gamma^k R_{t+k+1}| \leq \sum_{k=0}^{\infty} |\gamma|^k R_{max} = R_{max} \sum_{k=0}^{\infty} |\gamma|^k$$

III. A sÃ©rie $\sum_{k=0}^{\infty} |\gamma|^k$ Ã© uma sÃ©rie geomÃ©trica com razÃ£o $|\gamma|$. Se $|\gamma| < 1$, entÃ£o esta sÃ©rie converge para $\frac{1}{1 - |\gamma|}$.

IV. Portanto:
    $$R_{max} \sum_{k=0}^{\infty} |\gamma|^k = R_{max} \cdot \frac{1}{1 - |\gamma|}$$

V. Isso mostra que a sÃ©rie $\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ Ã© absolutamente convergente quando $|\gamma| < 1$ e $|R_{t+k+1}| < R_{max}$.

VI. Consequentemente, o retorno descontado $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ Ã© bem definido para $\gamma \in [0, 1)$ e recompensas limitadas. $\blacksquare$

#### FunÃ§Ãµes Derivadas da DinÃ¢mica do Ambiente

A funÃ§Ã£o de dinÃ¢mica do ambiente $p(s', r|s, a)$ Ã© a base para o cÃ¡lculo de vÃ¡rias outras funÃ§Ãµes importantes [^3]:

*   **Probabilidade de TransiÃ§Ã£o de Estado:** $p(s'|s, a)$ representa a probabilidade de transicionar para o estado $s'$ apÃ³s tomar a aÃ§Ã£o $a$ no estado $s$, independentemente da recompensa recebida [^3]. Pode ser calculada como:
    $$
    p(s'|s, a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum_{r \in \mathbb{R}} p(s', r|s, a)
    $$
    *Prova:*

    Para demonstrar que  $p(s'|s, a) = \sum_{r \in \mathbb{R}} p(s', r|s, a)$:

    I. ComeÃ§amos com a definiÃ§Ã£o de probabilidade condicional:
    $$p(s'|s, a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\}$$

    II. Podemos expressar a probabilidade conjunta de $S_t = s'$ e $R_{t+1} = r$ dado $S_{t-1} = s$ e $A_{t-1} = a$ como $p(s', r|s, a)$.

    III. A probabilidade de $S_t = s'$ dado $S_{t-1} = s$ e $A_{t-1} = a$ Ã© a soma das probabilidades conjuntas sobre todos os valores possÃ­veis de $r$:
    $$p(s'|s, a) = \sum_{r \in \mathbb{R}} Pr\{S_t = s', R_{t+1} = r | S_{t-1} = s, A_{t-1} = a\}$$

    IV. Usando a definiÃ§Ã£o de $p(s', r|s, a)$, temos:
    $$p(s'|s, a) = \sum_{r \in \mathbb{R}} p(s', r|s, a)$$

    Portanto, demonstramos que a probabilidade de transiÃ§Ã£o de estado pode ser calculada somando sobre todas as recompensas possÃ­veis. $\blacksquare$

    > ğŸ’¡ **Exemplo NumÃ©rico:** Usando os dados do exemplo anterior (funÃ§Ã£o de probabilidade de transiÃ§Ã£o), podemos calcular $p(s'|s, a)$ para cada combinaÃ§Ã£o de $s$ e $a$. Por exemplo:
    >
    > $p(s_1|s_1, a_1) = p(s_1, r=0|s_1, a_1) + p(s_1, r=1|s_1, a_1) + p(s_1, r=-1|s_1, a_1) = 0.8 + 0 + 0 = 0.8$ (assumindo que sÃ³ existem essas recompensas possÃ­veis)
    > $p(s_2|s_1, a_1) = p(s_2, r=0|s_1, a_1) + p(s_2, r=1|s_1, a_1) + p(s_2, r=-1|s_1, a_1) = 0 + 0.2 + 0 = 0.2$ (assumindo que sÃ³ existem essas recompensas possÃ­veis)
    >
    > Similarmente, podemos calcular para outras combinaÃ§Ãµes de estados e aÃ§Ãµes.

*   **Recompensa Esperada para Pares Estado-AÃ§Ã£o:** $r(s, a)$ representa a recompensa esperada ao tomar a aÃ§Ã£o $a$ no estado $s$ [^3]. Ã‰ calculada como a mÃ©dia ponderada das recompensas possÃ­veis, ponderadas por suas probabilidades:
    $$
    r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s, a)
    $$

    *Prova:*

    Para demonstrar que $r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s, a)$:

    I. ComeÃ§amos com a definiÃ§Ã£o de valor esperado condicional:
       $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathbb{R}} r \cdot Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a\}$$

    II. Usamos a lei da probabilidade total para expandir a probabilidade condicional de $R_t = r$ dado $S_{t-1} = s$ e $A_{t-1} = a$, somando sobre todos os possÃ­veis estados $s'$:
       $$Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a\} = \sum_{s' \in S} Pr\{R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a\}$$

    III. SubstituÃ­mos a expressÃ£o acima na definiÃ§Ã£o de valor esperado:
       $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathbb{R}} r \cdot \sum_{s' \in S} Pr\{R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a\}$$

    IV. Reconhecemos que $Pr\{R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a\} = p(s', r|s, a)$, entÃ£o:
       $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s, a)$$

    Portanto, a recompensa esperada para pares estado-aÃ§Ã£o Ã© demonstrada. $\blacksquare$

    > ğŸ’¡ **Exemplo NumÃ©rico:** Usando novamente os dados do exemplo anterior:
    >
    > $r(s_1, a_1) = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s_1, a_1) = 0 \cdot p(s_1, 0|s_1, a_1) + 1 \cdot p(s_2, 1|s_1, a_1) + (-1) \cdot p(s_1, -1|s_1, a_1) + 0 \cdot p(s_2, 0|s_1, a_1) + 1 \cdot p(s_1, 1| s_1, a_1) + (-1) \cdot p(s_2, -1| s_1, a_1) =  0 * 0.8 + 1 * 0.2 + (-1) * 0 + 0* 0 + 1 * 0 + (-1) *0= 0.2$
    >
    > $r(s_1, a_2) = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s_1, a_2) = 0 \cdot p(s_1, 0|s_1, a_2) + 1 \cdot p(s_2, 1|s_1, a_2) + (-1) \cdot p(s_1, -1|s_1, a_2) + 0 \cdot p(s_2, 0|s_1, a_2) + 1 \cdot p(s_1, 1| s_1, a_2) + (-1) \cdot p(s_2, -1| s_1, a_2) = 0*0.5 + 0 + (-1)*0.5 + 0 *0.5 +0 + 0= -0.5$

*   **Recompensa Esperada para Triplas Estado-AÃ§Ã£o-PrÃ³ximo Estado:** $r(s, a, s')$ representa a recompensa esperada ao transicionar do estado $s$ para o estado $s'$ apÃ³s tomar a aÃ§Ã£o $a$ [^3]. Ã‰ calculada como:

    $$
    r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathbb{R}} r \cdot \frac{p(s', r|s, a)}{p(s'|s, a)}
    $$

*Prova:*
Para provar que $r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathbb{R}} r \cdot \frac{p(s', r|s, a)}{p(s'|s, a)}$:

I. Iniciamos com a definiÃ§Ã£o de esperanÃ§a condicional:
$$
\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathbb{R}} r \cdot P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s')
$$

II. Pela definiÃ§Ã£o de probabilidade condicional:
$$
P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s') = \frac{P(R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a)}{P(S_t = s' | S_{t-1} = s, A_{t-1} = a)}
$$

III. Usando a notaÃ§Ã£o compacta, temos:
$$
P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s') = \frac{p(s', r|s, a)}{p(s'|s, a)}
$$

IV. Substituindo este resultado na equaÃ§Ã£o da esperanÃ§a condicional:
$$
\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathbb{R}} r \cdot \frac{p(s', r|s, a)}{p(s'|s, a)}
$$

Portanto, provamos que a recompensa esperada para triplas estado-aÃ§Ã£o-prÃ³ximo estado Ã© dada pela fÃ³rmula acima. $\blacksquare$

> ğŸ’¡ **Exemplo NumÃ©rico:** Usando os dados do exemplo anterior, e os calculos de $p(s'|s,a)$ feitos anteriormente:
>
> $r(s_1, a_1, s_1) = \sum_{r \in \mathbb{R}} r \cdot \frac{p(s_1, r|s_1, a_1)}{p(s_1|s_1, a_1)} = 0 * (0.8/0.8) + 1*(0/0.8) + (-1)*(0/0.8)=0$
> $r(s_1, a_1, s_2) = \sum_{r \in \mathbb{R}} r \cdot \frac{p(s_2, r|s_1, a_1)}{p(s_2|s_1, a_1)} = 0*(0/0.2) + 1*(0.2/0.2) + (-1)*(0/0.2) = 1$
>

**Teorema 1** (EquaÃ§Ã£o de Bellman para $r(s, a)$): A recompensa esperada $r(s, a)$ pode ser reescrita utilizando a probabilidade de transiÃ§Ã£o de estado $p(s'|s, a)$ e a recompensa esperada para triplas $r(s, a, s')$ como:

$$
r(s, a) = \sum_{s' \in S} p(s'|s, a) r(s, a, s')
$$

*Prova:*

Pela definiÃ§Ã£o de $r(s, a)$:

$$
r(s, a) = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s, a)
$$

Reagrupando a soma:

$$
r(s, a) = \sum_{s' \in S} \sum_{r \in \mathbb{R}} r \cdot p(s', r|s, a)
$$

Multiplicando e dividindo por $p(s'|s, a)$ (assumindo $p(s'|s, a) > 0$):

$$
r(s, a) = \sum_{s' \in S} p(s'|s, a) \sum_{r \in \mathbb{R}} r \cdot \frac{p(s', r|s, a)}{p(s'|s, a)}
$$

Reconhecendo a definiÃ§Ã£o de $r(s, a, s')$:

$$
r(s, a) = \sum_{s' \in S} p(s'|s, a) r(s, a, s')
$$

Esta equaÃ§Ã£o Ã© Ãºtil para decompor o cÃ¡lculo da recompensa esperada em termos da probabilidade de transiÃ§Ã£o para um novo estado e da recompensa esperada associada a essa transiÃ§Ã£o especÃ­fica.

> ğŸ’¡ **Exemplo NumÃ©rico:** Podemos verificar o Teorema 1 usando os valores calculados nos exemplos anteriores para $r(s_1, a_1)$, $p(s'|s, a)$ e $r(s, a, s')$:
>
> $r(s_1, a_1) = \sum_{s' \in S} p(s'|s_1, a_1) r(s_1, a_1, s') = p(s_1|s_1, a_1) \cdot r(s_1, a_1, s_1) + p(s_2|s_1, a_1) \cdot r(s_1, a_1, s_2) = 0.8 * 0 + 0.2 * 1 = 0.2$
>
> Este resultado corresponde ao valor de $r(s_1, a_1) = 0.2$ calculado anteriormente, confirmando o Teorema 1 para este exemplo.





![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

### ConclusÃ£o

A modelagem da interaÃ§Ã£o agente-ambiente como uma sequÃªncia de etapas de tempo discretas em um MDP finito fornece uma estrutura formal para o aprendizado por reforÃ§o. A percepÃ§Ã£o do estado, a seleÃ§Ã£o da aÃ§Ã£o, o recebimento da recompensa e a transiÃ§Ã£o para um novo estado definem um ciclo iterativo que permite ao agente aprender a tomar decisÃµes Ã³timas ao longo do tempo [^2]. AtravÃ©s da exploraÃ§Ã£o e da explotaÃ§Ã£o, o agente ajusta sua polÃ­tica para maximizar a recompensa cumulativa, tornando-se mais apto a atingir seus objetivos no ambiente.

### ReferÃªncias

[^1]: Chapter 3: Finite Markov Decision Processes
[^2]: 3.1 The Agent-Environment Interface
[^3]: 3.1. The Agent-Environment Interface
[^7]: 3.2 Goals and Rewards
[^12]: 3.5 Policies and Value Functions
<!-- END -->