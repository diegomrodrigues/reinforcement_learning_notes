## Modelagem da Intera√ß√£o Agente-Ambiente em Processos de Decis√£o de Markov Finitos

### Introdu√ß√£o
Em **Processos de Decis√£o de Markov Finitos (MDPs finitos)**, a intera√ß√£o entre um agente e seu ambiente √© formalizada como uma sequ√™ncia de etapas de tempo discretas. Este cap√≠tulo se aprofunda na modelagem dessa intera√ß√£o, que √© fundamental para entender como os agentes aprendem a tomar decis√µes √≥timas ao longo do tempo. A intera√ß√£o cont√≠nua entre o agente e o ambiente forma a base para o aprendizado por refor√ßo [^1]. O agente percebe o estado do ambiente, seleciona uma a√ß√£o, e o ambiente responde com uma recompensa e uma nova representa√ß√£o de estado. Este ciclo cont√≠nuo √© essencial para o aprendizado e tomada de decis√£o do agente [^2].

### Conceitos Fundamentais

A intera√ß√£o entre o agente e o ambiente √© modelada como uma sequ√™ncia de passos de tempo discretos, representados como $t = 0, 1, 2, 3, \ldots$ [^2]. Em cada passo de tempo $t$, os seguintes eventos ocorrem:

1.  **Percep√ß√£o do Estado:** O agente recebe uma representa√ß√£o do estado do ambiente, denotado por $S_t \in S$, onde $S$ √© o conjunto de todos os estados poss√≠veis [^2]. O **estado** deve conter todas as informa√ß√µes relevantes sobre o hist√≥rico da intera√ß√£o agente-ambiente que influenciam as decis√µes futuras [^3]. Se o estado satisfaz esta condi√ß√£o, diz-se que possui a *propriedade de Markov* [^3].

2.  **Sele√ß√£o da A√ß√£o:** Com base na representa√ß√£o do estado $S_t$, o agente seleciona uma a√ß√£o $A_t \in A(s)$, onde $A(s)$ √© o conjunto de a√ß√µes dispon√≠veis no estado $s$ [^2]. A sele√ß√£o da a√ß√£o √© guiada pela **pol√≠tica** do agente, denotada por $\pi$, que mapeia estados para probabilidades de sele√ß√£o de cada a√ß√£o poss√≠vel [^12]. Formalmente, $\pi(a|s)$ representa a probabilidade de selecionar a a√ß√£o $a$ no estado $s$ [^12].

    > üí° **Exemplo Num√©rico:** Considere um agente em um ambiente simples com dois estados, $S = \{s_1, s_2\}$, e duas a√ß√µes poss√≠veis em cada estado, $A(s) = \{a_1, a_2\}$ para todo $s \in S$. A pol√≠tica $\pi$ pode ser representada por uma tabela:
    >
    > | Estado | A√ß√£o | Probabilidade |
    > |---|---|---|
    > | $s_1$ | $a_1$ | 0.7 |
    > | $s_1$ | $a_2$ | 0.3 |
    > | $s_2$ | $a_1$ | 0.1 |
    > | $s_2$ | $a_2$ | 0.9 |
    >
    > Isso significa que no estado $s_1$, o agente escolhe a a√ß√£o $a_1$ com probabilidade 0.7 e a a√ß√£o $a_2$ com probabilidade 0.3. No estado $s_2$, o agente escolhe a a√ß√£o $a_1$ com probabilidade 0.1 e a a√ß√£o $a_2$ com probabilidade 0.9.

3.  **Recebimento da Recompensa:** Ap√≥s a execu√ß√£o da a√ß√£o $A_t$, o agente recebe uma recompensa num√©rica, denotada por $R_{t+1} \in \mathbb{R}$ [^2]. A **recompensa** √© um sinal que indica o qu√£o desej√°vel foi a transi√ß√£o do estado anterior para o estado atual, dada a a√ß√£o tomada [^7]. O objetivo do agente √© maximizar a quantidade total de recompensa que recebe ao longo do tempo [^7].

    > üí° **Exemplo Num√©rico:** Um rob√¥ est√° aprendendo a navegar em um labirinto. Se ele d√° um passo em dire√ß√£o ao objetivo, recebe uma recompensa de +1. Se ele bate em uma parede, recebe uma recompensa de -1. Se ele n√£o faz nada de especial, recebe uma recompensa de 0. O objetivo do rob√¥ √© maximizar a soma das recompensas, ou seja, chegar ao objetivo sem bater nas paredes.

4.  **Transi√ß√£o para o Novo Estado:** Como consequ√™ncia da a√ß√£o $A_t$, o ambiente transiciona para um novo estado, denotado por $S_{t+1} \in S$ [^2]. A transi√ß√£o para o novo estado √© influenciada tanto pela a√ß√£o do agente quanto pela din√¢mica do ambiente. A din√¢mica do ambiente √© descrita pela fun√ß√£o de probabilidade de transi√ß√£o $p(s', r|s, a)$, que especifica a probabilidade de transicionar para o estado $s'$ e receber a recompensa $r$ ap√≥s tomar a a√ß√£o $a$ no estado $s$ [^2]. Formalmente,
    $$
    p(s',r|s,a) = Pr\{S_t=s',R_{t+1}=r | S_{t-1}=s, A_{t-1}=a\} \quad \forall s', s \in S, r \in \mathbb{R}, a \in A(s)
    $$
    A fun√ß√£o $p$ define a din√¢mica do MDP, onde $p : S \times R \times S \times A \rightarrow [0, 1]$ [^2]. A fun√ß√£o $p$ especifica uma distribui√ß√£o de probabilidade para cada escolha de $s$ e $a$, isto √©:
    $$
    \sum_{s' \in S} \sum_{r \in \mathbb{R}} p(s', r|s, a) = 1, \quad \forall s \in S, a \in A(s)
    $$
    Num **MDP finito**, os conjuntos de estados $S$, a√ß√µes $A$ e recompensas $R$ s√£o finitos [^2]. Isto implica que as vari√°veis aleat√≥rias $R_t$ e $S_t$ possuem distribui√ß√µes de probabilidade discretas bem definidas, dependentes apenas do estado e a√ß√£o precedentes [^2].

    > üí° **Exemplo Num√©rico:** Considere um ambiente com dois estados ($S = \{s_1, s_2\}$) e duas a√ß√µes ($A = \{a_1, a_2\}$). A fun√ß√£o de probabilidade de transi√ß√£o $p(s', r|s, a)$ pode ser definida como:
    >
    > | $s$ | $a$ | $s'$ | $r$ | $p(s', r|s, a)$ |
    > |---|---|---|---|---|
    > | $s_1$ | $a_1$ | $s_1$ | 0 | 0.8 |
    > | $s_1$ | $a_1$ | $s_2$ | 1 | 0.2 |
    > | $s_1$ | $a_2$ | $s_1$ | -1 | 0.5 |
    > | $s_1$ | $a_2$ | $s_2$ | 0 | 0.5 |
    > | $s_2$ | $a_1$ | $s_1$ | 1 | 0.6 |
    > | $s_2$ | $a_1$ | $s_2$ | 0 | 0.4 |
    > | $s_2$ | $a_2$ | $s_1$ | 0 | 0.3 |
    > | $s_2$ | $a_2$ | $s_2$ | -1 | 0.7 |
    >
    > Por exemplo, se o agente est√° no estado $s_1$ e executa a a√ß√£o $a_1$, h√° uma probabilidade de 0.8 de permanecer no estado $s_1$ e receber uma recompensa de 0, e uma probabilidade de 0.2 de transicionar para o estado $s_2$ e receber uma recompensa de 1.

A sequ√™ncia completa de intera√ß√µes, a partir do estado inicial $S_0$, gera uma *trajet√≥ria* ou *hist√≥rico* da intera√ß√£o agente-ambiente [^2]:
$$
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots
$$

Dado que o objetivo do agente √© maximizar a recompensa total ao longo do tempo, introduzimos o conceito de *retorno*. O retorno, denotado por $G_t$, √© a soma das recompensas futuras, podendo ser definida de diferentes formas dependendo do horizonte de tempo considerado.

**Defini√ß√£o (Retorno)** O retorno $G_t$ √© definido como uma fun√ß√£o das recompensas recebidas a partir do instante $t$.

**Defini√ß√£o (Retorno Epis√≥dico)** Em tarefas *epis√≥dicas*, onde a intera√ß√£o agente-ambiente se divide em epis√≥dios, o retorno $G_t$ √© definido como a soma das recompensas at√© o final do epis√≥dio:
$$
G_t = R_{t+1} + R_{t+2} + \ldots + R_T
$$
onde $T$ √© o passo de tempo final do epis√≥dio.

> üí° **Exemplo Num√©rico:** Um agente est√° aprendendo a completar um jogo. Cada jogo √© um epis√≥dio. Em um epis√≥dio, o agente recebe as seguintes recompensas: $R_1 = -1, R_2 = 0, R_3 = -1, R_4 = 10$. O retorno $G_0$ para este epis√≥dio √©:
> $G_0 = R_1 + R_2 + R_3 + R_4 = -1 + 0 + (-1) + 10 = 8$.

Em tarefas *cont√≠nuas*, onde a intera√ß√£o agente-ambiente n√£o se divide em epis√≥dios, o retorno √© geralmente definido com um fator de desconto $\gamma \in [0, 1]$:
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
O fator de desconto $\gamma$ determina o quanto as recompensas futuras influenciam o retorno atual. Um valor de $\gamma$ pr√≥ximo de 0 faz com que o agente se preocupe apenas com as recompensas imediatas, enquanto um valor pr√≥ximo de 1 faz com que o agente se preocupe com as recompensas a longo prazo.

> üí° **Exemplo Num√©rico:** Um agente est√° em um ambiente cont√≠nuo e recebe as seguintes recompensas: $R_{t+1} = 1, R_{t+2} = 2, R_{t+3} = -1, R_{t+4} = 0, \ldots$. Se o fator de desconto $\gamma = 0.9$, o retorno $G_t$ √©:
> $G_t = 1 + 0.9 \cdot 2 + 0.9^2 \cdot (-1) + 0.9^3 \cdot 0 + \ldots = 1 + 1.8 - 0.81 + 0 + \ldots \approx 1.99$ (considerando os primeiros 4 termos).
> Se o fator de desconto $\gamma = 0.1$, o retorno $G_t$ √©:
> $G_t = 1 + 0.1 \cdot 2 + 0.1^2 \cdot (-1) + 0.1^3 \cdot 0 + \ldots = 1 + 0.2 - 0.01 + 0 + \ldots \approx 1.19$ (considerando os primeiros 4 termos).
> Observe como o valor de $\gamma$ influencia o retorno. Com $\gamma = 0.9$, as recompensas futuras t√™m um peso maior, resultando em um retorno maior em compara√ß√£o com $\gamma = 0.1$, onde as recompensas futuras t√™m um peso menor.

Para demonstrar que a defini√ß√£o do retorno descontado √© bem definida para $\gamma \in [0, 1)$ e $|R_{t+k+1}| < R_{max} \; \forall k$, provaremos que a soma converge.

*Prova:*

Provaremos que a soma $\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ converge se $|\gamma| < 1$ e $|R_{t+k+1}| < R_{max}$ para todo $k$.

I. Considere a s√©rie $\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$.

II. Como $|R_{t+k+1}| < R_{max}$ para todo $k$, podemos escrever:
    $$|\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}| \leq \sum_{k=0}^{\infty} |\gamma^k R_{t+k+1}| \leq \sum_{k=0}^{\infty} |\gamma|^k R_{max} = R_{max} \sum_{k=0}^{\infty} |\gamma|^k$$

III. A s√©rie $\sum_{k=0}^{\infty} |\gamma|^k$ √© uma s√©rie geom√©trica com raz√£o $|\gamma|$. Se $|\gamma| < 1$, ent√£o esta s√©rie converge para $\frac{1}{1 - |\gamma|}$.

IV. Portanto:
    $$R_{max} \sum_{k=0}^{\infty} |\gamma|^k = R_{max} \cdot \frac{1}{1 - |\gamma|}$$

V. Isso mostra que a s√©rie $\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ √© absolutamente convergente quando $|\gamma| < 1$ e $|R_{t+k+1}| < R_{max}$.

VI. Consequentemente, o retorno descontado $G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$ √© bem definido para $\gamma \in [0, 1)$ e recompensas limitadas. $\blacksquare$

#### Fun√ß√µes Derivadas da Din√¢mica do Ambiente

A fun√ß√£o de din√¢mica do ambiente $p(s', r|s, a)$ √© a base para o c√°lculo de v√°rias outras fun√ß√µes importantes [^3]:

*   **Probabilidade de Transi√ß√£o de Estado:** $p(s'|s, a)$ representa a probabilidade de transicionar para o estado $s'$ ap√≥s tomar a a√ß√£o $a$ no estado $s$, independentemente da recompensa recebida [^3]. Pode ser calculada como:
    $$
    p(s'|s, a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum_{r \in \mathbb{R}} p(s', r|s, a)
    $$
    *Prova:*

    Para demonstrar que  $p(s'|s, a) = \sum_{r \in \mathbb{R}} p(s', r|s, a)$:

    I. Come√ßamos com a defini√ß√£o de probabilidade condicional:
    $$p(s'|s, a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\}$$

    II. Podemos expressar a probabilidade conjunta de $S_t = s'$ e $R_{t+1} = r$ dado $S_{t-1} = s$ e $A_{t-1} = a$ como $p(s', r|s, a)$.

    III. A probabilidade de $S_t = s'$ dado $S_{t-1} = s$ e $A_{t-1} = a$ √© a soma das probabilidades conjuntas sobre todos os valores poss√≠veis de $r$:
    $$p(s'|s, a) = \sum_{r \in \mathbb{R}} Pr\{S_t = s', R_{t+1} = r | S_{t-1} = s, A_{t-1} = a\}$$

    IV. Usando a defini√ß√£o de $p(s', r|s, a)$, temos:
    $$p(s'|s, a) = \sum_{r \in \mathbb{R}} p(s', r|s, a)$$

    Portanto, demonstramos que a probabilidade de transi√ß√£o de estado pode ser calculada somando sobre todas as recompensas poss√≠veis. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Usando os dados do exemplo anterior (fun√ß√£o de probabilidade de transi√ß√£o), podemos calcular $p(s'|s, a)$ para cada combina√ß√£o de $s$ e $a$. Por exemplo:
    >
    > $p(s_1|s_1, a_1) = p(s_1, r=0|s_1, a_1) + p(s_1, r=1|s_1, a_1) + p(s_1, r=-1|s_1, a_1) = 0.8 + 0 + 0 = 0.8$ (assumindo que s√≥ existem essas recompensas poss√≠veis)
    > $p(s_2|s_1, a_1) = p(s_2, r=0|s_1, a_1) + p(s_2, r=1|s_1, a_1) + p(s_2, r=-1|s_1, a_1) = 0 + 0.2 + 0 = 0.2$ (assumindo que s√≥ existem essas recompensas poss√≠veis)
    >
    > Similarmente, podemos calcular para outras combina√ß√µes de estados e a√ß√µes.

*   **Recompensa Esperada para Pares Estado-A√ß√£o:** $r(s, a)$ representa a recompensa esperada ao tomar a a√ß√£o $a$ no estado $s$ [^3]. √â calculada como a m√©dia ponderada das recompensas poss√≠veis, ponderadas por suas probabilidades:
    $$
    r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s, a)
    $$

    *Prova:*

    Para demonstrar que $r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s, a)$:

    I. Come√ßamos com a defini√ß√£o de valor esperado condicional:
       $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathbb{R}} r \cdot Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a\}$$

    II. Usamos a lei da probabilidade total para expandir a probabilidade condicional de $R_t = r$ dado $S_{t-1} = s$ e $A_{t-1} = a$, somando sobre todos os poss√≠veis estados $s'$:
       $$Pr\{R_t = r | S_{t-1} = s, A_{t-1} = a\} = \sum_{s' \in S} Pr\{R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a\}$$

    III. Substitu√≠mos a express√£o acima na defini√ß√£o de valor esperado:
       $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathbb{R}} r \cdot \sum_{s' \in S} Pr\{R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a\}$$

    IV. Reconhecemos que $Pr\{R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a\} = p(s', r|s, a)$, ent√£o:
       $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s, a)$$

    Portanto, a recompensa esperada para pares estado-a√ß√£o √© demonstrada. $\blacksquare$

    > üí° **Exemplo Num√©rico:** Usando novamente os dados do exemplo anterior:
    >
    > $r(s_1, a_1) = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s_1, a_1) = 0 \cdot p(s_1, 0|s_1, a_1) + 1 \cdot p(s_2, 1|s_1, a_1) + (-1) \cdot p(s_1, -1|s_1, a_1) + 0 \cdot p(s_2, 0|s_1, a_1) + 1 \cdot p(s_1, 1| s_1, a_1) + (-1) \cdot p(s_2, -1| s_1, a_1) =  0 * 0.8 + 1 * 0.2 + (-1) * 0 + 0* 0 + 1 * 0 + (-1) *0= 0.2$
    >
    > $r(s_1, a_2) = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s_1, a_2) = 0 \cdot p(s_1, 0|s_1, a_2) + 1 \cdot p(s_2, 1|s_1, a_2) + (-1) \cdot p(s_1, -1|s_1, a_2) + 0 \cdot p(s_2, 0|s_1, a_2) + 1 \cdot p(s_1, 1| s_1, a_2) + (-1) \cdot p(s_2, -1| s_1, a_2) = 0*0.5 + 0 + (-1)*0.5 + 0 *0.5 +0 + 0= -0.5$

*   **Recompensa Esperada para Triplas Estado-A√ß√£o-Pr√≥ximo Estado:** $r(s, a, s')$ representa a recompensa esperada ao transicionar do estado $s$ para o estado $s'$ ap√≥s tomar a a√ß√£o $a$ [^3]. √â calculada como:

    $$
    r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathbb{R}} r \cdot \frac{p(s', r|s, a)}{p(s'|s, a)}
    $$

*Prova:*
Para provar que $r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathbb{R}} r \cdot \frac{p(s', r|s, a)}{p(s'|s, a)}$:

I. Iniciamos com a defini√ß√£o de esperan√ßa condicional:
$$
\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathbb{R}} r \cdot P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s')
$$

II. Pela defini√ß√£o de probabilidade condicional:
$$
P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s') = \frac{P(R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a)}{P(S_t = s' | S_{t-1} = s, A_{t-1} = a)}
$$

III. Usando a nota√ß√£o compacta, temos:
$$
P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s') = \frac{p(s', r|s, a)}{p(s'|s, a)}
$$

IV. Substituindo este resultado na equa√ß√£o da esperan√ßa condicional:
$$
\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathbb{R}} r \cdot \frac{p(s', r|s, a)}{p(s'|s, a)}
$$

Portanto, provamos que a recompensa esperada para triplas estado-a√ß√£o-pr√≥ximo estado √© dada pela f√≥rmula acima. $\blacksquare$

> üí° **Exemplo Num√©rico:** Usando os dados do exemplo anterior, e os calculos de $p(s'|s,a)$ feitos anteriormente:
>
> $r(s_1, a_1, s_1) = \sum_{r \in \mathbb{R}} r \cdot \frac{p(s_1, r|s_1, a_1)}{p(s_1|s_1, a_1)} = 0 * (0.8/0.8) + 1*(0/0.8) + (-1)*(0/0.8)=0$
> $r(s_1, a_1, s_2) = \sum_{r \in \mathbb{R}} r \cdot \frac{p(s_2, r|s_1, a_1)}{p(s_2|s_1, a_1)} = 0*(0/0.2) + 1*(0.2/0.2) + (-1)*(0/0.2) = 1$
>

**Teorema 1** (Equa√ß√£o de Bellman para $r(s, a)$): A recompensa esperada $r(s, a)$ pode ser reescrita utilizando a probabilidade de transi√ß√£o de estado $p(s'|s, a)$ e a recompensa esperada para triplas $r(s, a, s')$ como:

$$
r(s, a) = \sum_{s' \in S} p(s'|s, a) r(s, a, s')
$$

*Prova:*

Pela defini√ß√£o de $r(s, a)$:

$$
r(s, a) = \sum_{r \in \mathbb{R}} \sum_{s' \in S} r \cdot p(s', r|s, a)
$$

Reagrupando a soma:

$$
r(s, a) = \sum_{s' \in S} \sum_{r \in \mathbb{R}} r \cdot p(s', r|s, a)
$$

Multiplicando e dividindo por $p(s'|s, a)$ (assumindo $p(s'|s, a) > 0$):

$$
r(s, a) = \sum_{s' \in S} p(s'|s, a) \sum_{r \in \mathbb{R}} r \cdot \frac{p(s', r|s, a)}{p(s'|s, a)}
$$

Reconhecendo a defini√ß√£o de $r(s, a, s')$:

$$
r(s, a) = \sum_{s' \in S} p(s'|s, a) r(s, a, s')
$$

Esta equa√ß√£o √© √∫til para decompor o c√°lculo da recompensa esperada em termos da probabilidade de transi√ß√£o para um novo estado e da recompensa esperada associada a essa transi√ß√£o espec√≠fica.

> üí° **Exemplo Num√©rico:** Podemos verificar o Teorema 1 usando os valores calculados nos exemplos anteriores para $r(s_1, a_1)$, $p(s'|s, a)$ e $r(s, a, s')$:
>
> $r(s_1, a_1) = \sum_{s' \in S} p(s'|s_1, a_1) r(s_1, a_1, s') = p(s_1|s_1, a_1) \cdot r(s_1, a_1, s_1) + p(s_2|s_1, a_1) \cdot r(s_1, a_1, s_2) = 0.8 * 0 + 0.2 * 1 = 0.2$
>
> Este resultado corresponde ao valor de $r(s_1, a_1) = 0.2$ calculado anteriormente, confirmando o Teorema 1 para este exemplo.





![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

### Conclus√£o

A modelagem da intera√ß√£o agente-ambiente como uma sequ√™ncia de etapas de tempo discretas em um MDP finito fornece uma estrutura formal para o aprendizado por refor√ßo. A percep√ß√£o do estado, a sele√ß√£o da a√ß√£o, o recebimento da recompensa e a transi√ß√£o para um novo estado definem um ciclo iterativo que permite ao agente aprender a tomar decis√µes √≥timas ao longo do tempo [^2]. Atrav√©s da explora√ß√£o e da explota√ß√£o, o agente ajusta sua pol√≠tica para maximizar a recompensa cumulativa, tornando-se mais apto a atingir seus objetivos no ambiente.

### Refer√™ncias

[^1]: Chapter 3: Finite Markov Decision Processes
[^2]: 3.1 The Agent-Environment Interface
[^3]: 3.1. The Agent-Environment Interface
[^7]: 3.2 Goals and Rewards
[^12]: 3.5 Policies and Value Functions
<!-- END -->