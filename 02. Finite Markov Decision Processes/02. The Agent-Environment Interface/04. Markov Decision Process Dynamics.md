## Din√¢mica em Processos de Decis√£o de Markov Finitos

### Introdu√ß√£o
Neste cap√≠tulo, exploramos os Processos de Decis√£o de Markov Finitos (MDPs), que fornecem uma estrutura formal para modelar problemas de tomada de decis√£o sequencial sob incerteza. Um componente fundamental dos MDPs √© a fun√ß√£o de din√¢mica, que descreve como o ambiente evolui em resposta √†s a√ß√µes do agente. Em particular, focaremos na fun√ß√£o de din√¢mica $p(s', r | s, a)$, que √© a probabilidade de observar o pr√≥ximo estado $s'$ e recompensa $r$, dado o estado anterior $s$ e a√ß√£o $a$ [^48]. Esta fun√ß√£o √© crucial para planejar e aprender pol√≠ticas √≥timas.

### Conceitos Fundamentais

A intera√ß√£o entre um **agente** e seu **ambiente** √© uma caracter√≠stica central do aprendizado por refor√ßo [^47]. O agente observa o estado do ambiente e seleciona uma a√ß√£o, e o ambiente responde com uma recompensa e um novo estado. Essa intera√ß√£o sequencial √© modelada formalmente pelos MDPs [^47].

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

**Defini√ß√£o de MDP Finito:** Um MDP finito √© definido por um conjunto finito de estados $S$, um conjunto finito de a√ß√µes $A(s)$ para cada estado $s \in S$, um conjunto de recompensas $R$ e a fun√ß√£o de din√¢mica $p(s', r | s, a)$ [^48]. A fun√ß√£o de din√¢mica define a probabilidade de transi√ß√£o do ambiente:

$$p(s', r | s, a) = Pr\{S_t = s', R_{t+1} = r | S_{t-1} = s, A_{t-1} = a\}$$

Esta equa√ß√£o [^48] expressa a probabilidade de, no instante de tempo $t$, o ambiente transitar para o estado $s'$ e gerar uma recompensa $r$, dado que no instante anterior $t-1$, o agente estava no estado $s$ e executou a a√ß√£o $a$.

> üí° **Exemplo Num√©rico:**
>
> Considere um MDP simples com dois estados $S = \{s_1, s_2\}$, duas a√ß√µes $A = \{a_1, a_2\}$, e recompensas $R = \{0, 1\}$.  A fun√ß√£o de din√¢mica pode ser definida da seguinte forma:
>
> *   $p(s_1, 0 | s_1, a_1) = 0.7$
> *   $p(s_2, 1 | s_1, a_1) = 0.3$
> *   $p(s_1, 1 | s_1, a_2) = 0.6$
> *   $p(s_2, 0 | s_1, a_2) = 0.4$
> *   $p(s_1, 0 | s_2, a_1) = 0.2$
> *   $p(s_2, 1 | s_2, a_1) = 0.8$
> *   $p(s_1, 1 | s_2, a_2) = 0.9$
> *   $p(s_2, 0 | s_2, a_2) = 0.1$
>
> Isso significa que, por exemplo, se o agente est√° no estado $s_1$ e executa a a√ß√£o $a_1$, h√° uma probabilidade de 0.7 de permanecer no estado $s_1$ e receber uma recompensa de 0, e uma probabilidade de 0.3 de transitar para o estado $s_2$ e receber uma recompensa de 1.
>
> Podemos verificar se a fun√ß√£o de din√¢mica satisfaz a propriedade de que a soma das probabilidades para cada estado e a√ß√£o √© igual a 1:
>
> Para $s_1, a_1$: $0.7 + 0.3 = 1$
> Para $s_1, a_2$: $0.6 + 0.4 = 1$
> Para $s_2, a_1$: $0.2 + 0.8 = 1$
> Para $s_2, a_2$: $0.9 + 0.1 = 1$

A fun√ß√£o de din√¢mica $p$ mapeia qu√°druplas $(s, a, s', r)$ para probabilidades no intervalo $[0, 1]$ [^48]. Ela encapsula o conhecimento completo sobre como o ambiente se comporta. A fun√ß√£o de din√¢mica satisfaz a seguinte propriedade [^49]:

$$\sum_{s' \in S} \sum_{r \in R} p(s', r | s, a) = 1, \text{ para todo } s \in S, a \in A(s)$$

Esta equa√ß√£o garante que $p$ especifica uma distribui√ß√£o de probabilidade para cada escolha de $s$ e $a$ [^49].

**Prova:**
Provaremos que $\sum_{s' \in S} \sum_{r \in R} p(s', r | s, a) = 1$ para todo $s \in S, a \in A(s)$.

I. A fun√ß√£o $p(s', r | s, a)$ representa a probabilidade condicional de transitar para o estado $s'$ e receber a recompensa $r$, dado que estamos no estado $s$ e executamos a a√ß√£o $a$.

II. Para um dado estado $s$ e a√ß√£o $a$, o agente *deve* transitar para *algum* estado $s'$ e receber *alguma* recompensa $r$. Em outras palavras, o conjunto de todos os poss√≠veis estados $s'$ e recompensas $r$ forma o espa√ßo amostral completo para o pr√≥ximo estado e recompensa, dado $s$ e $a$.

III. A soma das probabilidades de todos os resultados poss√≠veis no espa√ßo amostral deve ser igual a 1. Portanto, somando sobre todos os poss√≠veis estados $s'$ em $S$ e todas as poss√≠veis recompensas $r$ em $R$, obtemos a probabilidade total, que deve ser igual a 1.

IV. Formalmente:
    $$\sum_{s' \in S} \sum_{r \in R} p(s', r | s, a) = \sum_{s' \in S} \sum_{r \in R} Pr\{S_t = s', R_{t+1} = r | S_{t-1} = s, A_{t-1} = a\} = 1$$

V. Conclu√≠mos que $\sum_{s' \in S} \sum_{r \in R} p(s', r | s, a) = 1$ para todo $s \in S, a \in A(s)$. $\blacksquare$

**A Propriedade de Markov:** Uma caracter√≠stica essencial dos MDPs √© a propriedade de Markov, que afirma que o estado atual cont√©m todas as informa√ß√µes relevantes sobre a hist√≥ria passada para a tomada de decis√£o [^49]. Formalmente, a probabilidade de transi√ß√£o para o pr√≥ximo estado e receber uma recompensa depende apenas do estado e a√ß√£o imediatamente anteriores, e n√£o de estados ou a√ß√µes anteriores:

$$Pr\{S_t = s', R_{t+1} = r | S_{t-1} = s, A_{t-1} = a\} = Pr\{S_t = s', R_{t+1} = r | S_0, A_0, R_1, \ldots, S_{t-2}, A_{t-2}, R_{t-1}, S_{t-1} = s, A_{t-1} = a\}$$

Se o estado satisfaz a propriedade de Markov, ele √© considerado um **estado de Markov** [^49]. Em outras palavras, o estado deve incluir todas as informa√ß√µes sobre a intera√ß√£o passada entre o agente e o ambiente que podem influenciar o futuro [^49].

A fun√ß√£o de din√¢mica pode ser usada para calcular outras quantidades √∫teis, como as probabilidades de transi√ß√£o de estado [^49]:

$$p(s' | s, a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum_{r \in R} p(s', r | s, a)$$

Esta equa√ß√£o representa a probabilidade de transitar do estado $s$ para o estado $s'$ ap√≥s executar a a√ß√£o $a$, independentemente da recompensa recebida [^49].

> üí° **Exemplo Num√©rico:**
>
> Usando o exemplo num√©rico anterior, podemos calcular a probabilidade de transi√ß√£o de estado $p(s' | s, a)$. Por exemplo, a probabilidade de transitar do estado $s_1$ para o estado $s_2$ ao executar a a√ß√£o $a_1$ √©:
>
> $p(s_2 | s_1, a_1) = \sum_{r \in R} p(s_2, r | s_1, a_1) = p(s_2, 0 | s_1, a_1) + p(s_2, 1 | s_1, a_1) = 0 + 0.3 = 0.3$
>
> Similarmente:
>
> $p(s_1 | s_1, a_1) = 0.7$
> $p(s_2 | s_1, a_2) = 0.4$
> $p(s_1 | s_1, a_2) = 0.6$
> $p(s_2 | s_2, a_1) = 0.8$
> $p(s_1 | s_2, a_1) = 0.2$
> $p(s_2 | s_2, a_2) = 0.1$
> $p(s_1 | s_2, a_2) = 0.9$

**Prova:**
Provaremos que $p(s' | s, a) = \sum_{r \in R} p(s', r | s, a)$.

I. Queremos encontrar a probabilidade de transitar para o estado $s'$ dado o estado $s$ e a a√ß√£o $a$, independentemente da recompensa.

II. Sabemos que $p(s', r | s, a)$ √© a probabilidade conjunta de transitar para o estado $s'$ e receber a recompensa $r$, dado $s$ e $a$.

III. Para encontrar a probabilidade marginal de transitar para o estado $s'$, precisamos somar a probabilidade conjunta $p(s', r | s, a)$ sobre todos os poss√≠veis valores de $r$ em $R$.

IV. Formalmente, usando a lei da probabilidade total:
   $$p(s' | s, a) = \sum_{r \in R} p(s', r | s, a)$$
   Isso ocorre porque estamos somando sobre todos os poss√≠veis valores de $r$ para obter a probabilidade de $s'$ ocorrer, dado $s$ e $a$.

V. Portanto, provamos que $p(s' | s, a) = \sum_{r \in R} p(s', r | s, a)$. $\blacksquare$

Outra quantidade √∫til √© a recompensa esperada para pares estado-a√ß√£o [^49]:

$$r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in R} \sum_{s' \in S} r \cdot p(s', r | s, a)$$

Esta equa√ß√£o define a recompensa m√©dia esperada ao executar a a√ß√£o $a$ no estado $s$ [^49]. Tamb√©m podemos definir as recompensas esperadas para triplas estado-a√ß√£o-pr√≥ximo estado [^49]:

$$r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in R} r \frac{p(s', r | s, a)}{p(s' | s, a)}$$

> üí° **Exemplo Num√©rico:**
>
> Continuando com o exemplo anterior, podemos calcular a recompensa esperada $r(s, a)$ para o estado $s_1$ e a√ß√£o $a_1$:
>
> $r(s_1, a_1) = \sum_{r \in R} \sum_{s' \in S} r \cdot p(s', r | s_1, a_1) = (0 \cdot p(s_1, 0 | s_1, a_1)) + (1 \cdot p(s_1, 1 | s_1, a_1)) + (0 \cdot p(s_2, 0 | s_1, a_1)) + (1 \cdot p(s_2, 1 | s_1, a_1)) = (0 \cdot 0.7) + (0 \cdot 0) + (0 \cdot 0) + (1 \cdot 0.3) = 0.3$
>
> Para $r(s_1, a_2)$:
> $r(s_1, a_2) = (1 \cdot 0.6) + (0 \cdot 0.4) = 0.6$
>
> Para $r(s_2, a_1)$:
> $r(s_2, a_1) = (0 \cdot 0.2) + (1 \cdot 0.8) = 0.8$
>
> Para $r(s_2, a_2)$:
> $r(s_2, a_2) = (1 \cdot 0.9) + (0 \cdot 0.1) = 0.9$
>
> Agora, vamos calcular $r(s, a, s')$ para $s_1, a_1, s_2$:
>
> $r(s_1, a_1, s_2) = \sum_{r \in R} r \frac{p(s_2, r | s_1, a_1)}{p(s_2 | s_1, a_1)} = (0 \cdot \frac{0}{0.3}) + (1 \cdot \frac{0.3}{0.3}) = 1$
>
> E para $s_1, a_1, s_1$:
>
> $r(s_1, a_1, s_1) = \sum_{r \in R} r \frac{p(s_1, r | s_1, a_1)}{p(s_1 | s_1, a_1)} = (0 \cdot \frac{0.7}{0.7}) + (0 \cdot \frac{0}{0.7}) = 0$

**Prova:**
Provaremos que $r(s, a, s') = \sum_{r \in R} r \frac{p(s', r | s, a)}{p(s' | s, a)}$.

I. Queremos calcular a recompensa esperada dado o estado atual $s$, a a√ß√£o tomada $a$ e o pr√≥ximo estado $s'$.

II. Pela defini√ß√£o de valor esperado condicional:
   $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in R} r \cdot P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s')$$

III. Usando a defini√ß√£o de probabilidade condicional:
    $$P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s') = \frac{P(S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a)}{P(S_t = s' | S_{t-1} = s, A_{t-1} = a)}$$

IV. Substituindo as nota√ß√µes compactas:
    $$P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s') = \frac{p(s', r | s, a)}{p(s' | s, a)}$$

V. Substituindo na equa√ß√£o do valor esperado:
    $$r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in R} r \cdot \frac{p(s', r | s, a)}{p(s' | s, a)}$$

VI. Portanto, demonstramos que $r(s, a, s') = \sum_{r \in R} r \frac{p(s', r | s, a)}{p(s' | s, a)}$. $\blacksquare$

√â crucial notar que, embora geralmente usemos a fun√ß√£o $p$ de quatro argumentos, as outras nota√ß√µes s√£o ocasionalmente convenientes [^49].

Dado que temos a defini√ß√£o de $r(s, a)$, podemos tamb√©m definir a vari√¢ncia da recompensa esperada para um par estado-a√ß√£o, que nos d√° uma medida da incerteza da recompensa obtida ao tomar a a√ß√£o $a$ no estado $s$.

**Defini√ß√£o da Vari√¢ncia da Recompensa Esperada:** A vari√¢ncia da recompensa esperada para um par estado-a√ß√£o $(s, a)$ √© definida como:

$$Var[R_t | S_{t-1} = s, A_{t-1} = a] = \mathbb{E}[(R_t - r(s, a))^2 | S_{t-1} = s, A_{t-1} = a]$$

Expandindo a defini√ß√£o, podemos reescrever a vari√¢ncia como:

$$Var[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in R} \sum_{s' \in S} (r - r(s, a))^2 \cdot p(s', r | s, a)$$

> üí° **Exemplo Num√©rico:**
>
> Usando os valores j√° calculados, vamos calcular a vari√¢ncia da recompensa esperada para o par estado-a√ß√£o $(s_1, a_1)$. Primeiro, lembramos que $r(s_1, a_1) = 0.3$.
>
> $Var[R_t | S_{t-1} = s_1, A_{t-1} = a_1] = \sum_{r \in R} \sum_{s' \in S} (r - r(s_1, a_1))^2 \cdot p(s', r | s_1, a_1)$
>
> $= (0 - 0.3)^2 \cdot p(s_1, 0 | s_1, a_1) + (1 - 0.3)^2 \cdot p(s_1, 1 | s_1, a_1) + (0 - 0.3)^2 \cdot p(s_2, 0 | s_1, a_1) + (1 - 0.3)^2 \cdot p(s_2, 1 | s_1, a_1)$
>
> $= (0.09 \cdot 0.7) + (0.49 \cdot 0) + (0.09 \cdot 0) + (0.49 \cdot 0.3)$
>
> $= 0.063 + 0 + 0 + 0.147 = 0.21$
>
> Portanto, a vari√¢ncia da recompensa esperada para o par estado-a√ß√£o $(s_1, a_1)$ √© 0.21.
>
> Da mesma forma, podemos calcular a vari√¢ncia para $(s_1, a_2)$, sabendo que $r(s_1, a_2) = 0.6$:
>
> $Var[R_t | S_{t-1} = s_1, A_{t-1} = a_2] = (0 - 0.6)^2 \cdot p(s_1, 0 | s_1, a_2) + (1 - 0.6)^2 \cdot p(s_1, 1 | s_1, a_2) + (0 - 0.6)^2 \cdot p(s_2, 0 | s_1, a_2) + (1 - 0.6)^2 \cdot p(s_2, 1 | s_1, a_2)$
> $= (0.36 \cdot 0) + (0.16 \cdot 0.6) + (0.36 \cdot 0.4) + (0.16 \cdot 0)$
> $= 0 + 0.096 + 0.144 + 0 = 0.24$

Essa medida pode ser √∫til para agentes que precisam considerar o risco associado a diferentes a√ß√µes.

Al√©m disso, a fun√ß√£o de din√¢mica nos permite definir o conceito de *atingibilidade* entre estados.

**Defini√ß√£o de Atingibilidade:** Um estado $s'$ √© ating√≠vel a partir de um estado $s$ se existe uma pol√≠tica $\pi$ e uma sequ√™ncia de a√ß√µes tal que a probabilidade de transitar de $s$ para $s'$ seguindo $\pi$ √© maior que zero. Formalmente, existe uma sequ√™ncia de a√ß√µes $a_0, a_1, \ldots, a_{n-1}$ e estados intermedi√°rios $s_1, s_2, \ldots, s_{n-1}$ tal que:

$$p(s_1 | s, a_0) > 0, p(s_2 | s_1, a_1) > 0, \ldots, p(s' | s_{n-1}, a_{n-1}) > 0$$

Para alguma pol√≠tica $\pi$ que seleciona essas a√ß√µes com probabilidade n√£o nula.

> üí° **Exemplo Num√©rico:**
>
> Ainda utilizando o exemplo num√©rico, podemos analisar a atingibilidade dos estados. √â o estado $s_2$ ating√≠vel a partir de $s_1$?
>
> Sim, porque $p(s_2 | s_1, a_1) = 0.3 > 0$ e $p(s_2 | s_1, a_2) = 0.4 > 0$. Portanto, independentemente da a√ß√£o escolhida em $s_1$, existe uma probabilidade positiva de transitar para $s_2$.
>
> √â o estado $s_1$ ating√≠vel a partir de $s_2$?
>
> Sim, porque $p(s_1 | s_2, a_1) = 0.2 > 0$ e $p(s_1 | s_2, a_2) = 0.9 > 0$.
>
> Portanto, neste MDP simples, todos os estados s√£o ating√≠veis a partir de qualquer outro estado. Isso √© importante para garantir que o agente possa explorar todo o espa√ßo de estados e encontrar a pol√≠tica √≥tima.

### Conclus√£o
A fun√ß√£o de din√¢mica $p(s', r | s, a)$ √© um componente central dos MDPs finitos [^48]. Ela fornece uma descri√ß√£o completa de como o ambiente se comporta em resposta √†s a√ß√µes do agente. Ao especificar a probabilidade de observar o pr√≥ximo estado e recompensa, dado o estado e a√ß√£o atuais, a fun√ß√£o de din√¢mica permite que o agente planeje e aprenda pol√≠ticas √≥timas [^48]. A propriedade de Markov simplifica o problema de tomada de decis√£o sequencial, garantindo que apenas o estado atual seja relevante para decis√µes futuras [^49]. A capacidade de calcular outras quantidades √∫teis, como probabilidades de transi√ß√£o de estado e recompensas esperadas, a partir da fun√ß√£o de din√¢mica torna-a uma ferramenta essencial na an√°lise e solu√ß√£o de MDPs [^49].

### Refer√™ncias
[^47]: Chapter 3: Finite Markov Decision Processes
[^48]: Chapter 3: Finite Markov Decision Processes, pag 48
[^49]: Chapter 3: Finite Markov Decision Processes, pag 49
<!-- END -->