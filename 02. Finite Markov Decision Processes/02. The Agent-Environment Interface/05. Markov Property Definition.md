## 3.1.5 A Propriedade de Markov e a Interface Agente-Ambiente

### Introdu√ß√£o
Expandindo sobre a **interface agente-ambiente** em **Processos de Decis√£o de Markov (MDPs) finitos**, exploraremos a crucial **propriedade de Markov** e sua influ√™ncia na estrutura do estado. A propriedade de Markov simplifica o processo de tomada de decis√£o, garantindo que o estado atual contenha todas as informa√ß√µes relevantes do hist√≥rico de intera√ß√µes passadas, permitindo que o agente tome decis√µes ideais sem precisar reter uma mem√≥ria completa da trajet√≥ria [^4]. Al√©m disso, examinaremos a flexibilidade do *framework* MDP e sua aplicabilidade em diversos problemas, considerando diferentes interpreta√ß√µes de *time steps*, estados e a√ß√µes [^3].

### Conceitos Fundamentais
A **propriedade de Markov** √© um conceito central em MDPs, influenciando diretamente a defini√ß√£o do **estado**. A propriedade de Markov postula que o estado $S_t$ deve encapsular toda a informa√ß√£o relevante do hist√≥rico de intera√ß√µes agente-ambiente at√© o tempo *t* [^3]. Formalmente, isso implica que a probabilidade de transi√ß√£o para um pr√≥ximo estado $S_{t+1}$ e receber uma recompensa $R_{t+1}$ depende apenas do estado e da a√ß√£o atuais, $S_t$ e $A_t$, respectivamente, e n√£o dos estados e a√ß√µes anteriores [^2]. Em outras palavras:

$$
p(s',r|s,a) = Pr\{S_{t+1}=s', R_{t+1}=r | S_t=s, A_t=a\}
$$
para todo $s', s \in \mathcal{S}$, $r \in \mathcal{R}$ e $a \in \mathcal{A}(s)$ [^2].

> üí° **Exemplo Num√©rico:** Imagine um rob√¥ que pode se mover para frente ou para tr√°s em uma linha reta. O estado $S_t$ √© sua posi√ß√£o. A a√ß√£o $A_t$ pode ser "avan√ßar" ou "retroceder". Se a probabilidade de avan√ßar para a pr√≥xima posi√ß√£o (ex: $S_{t+1} = S_t + 1$) depende apenas da posi√ß√£o atual e da a√ß√£o "avan√ßar", e n√£o de como o rob√¥ chegou √† posi√ß√£o atual, ent√£o a propriedade de Markov se mant√©m. Se o rob√¥ tiver um bug que faz com que avance com menos precis√£o ap√≥s ter retrocedido 3 vezes seguidas, ent√£o o estado precisaria incluir essa informa√ß√£o (ex: n√∫mero de retrocessos consecutivos).

**Teorema 1** (Equival√™ncia da Propriedade de Markov) A propriedade de Markov, expressa na forma de probabilidade de transi√ß√£o e recompensa, √© equivalente √† propriedade de que o estado atual condensa toda a informa√ß√£o relevante do passado para a predi√ß√£o do futuro. Formalmente, para qualquer sequ√™ncia de estados e a√ß√µes $S_1, A_1, S_2, A_2, ..., S_t, A_t$, temos:
$$
Pr\{S_{t+1}=s' | S_t=s, A_t=a\} = Pr\{S_{t+1}=s' | S_1=s_1, A_1=a_1, \ldots, S_t=s, A_t=a\}
$$
e
$$
Pr\{R_{t+1}=r | S_t=s, A_t=a\} = Pr\{R_{t+1}=r | S_1=s_1, A_1=a_1, \ldots, S_t=s, A_t=a\}
$$
para todo $s', s \in \mathcal{S}$, $r \in \mathcal{R}$ e $a \in \mathcal{A}(s)$.

*Prova:* A prova segue diretamente da defini√ß√£o da propriedade de Markov. A primeira equa√ß√£o afirma que a probabilidade de transi√ß√£o para o pr√≥ximo estado depende apenas do estado e a√ß√£o atuais. A segunda equa√ß√£o afirma que a recompensa esperada depende apenas do estado e a√ß√£o atuais. Juntas, essas duas equa√ß√µes garantem que o estado atual √© suficiente para prever o futuro, dado a a√ß√£o.

*Observa√ß√£o:* O Teorema 1 oferece uma perspectiva alternativa sobre a propriedade de Markov, enfatizando a ideia de que o estado atual √© uma estat√≠stica suficiente do hist√≥rico.

*A propriedade de Markov implica uma restri√ß√£o n√£o sobre o processo de decis√£o em si, mas sobre a defini√ß√£o do estado* [^3]. O estado deve incorporar toda a informa√ß√£o relevante do hist√≥rico agente-ambiente que possa impactar o futuro. Se o estado satisfaz a propriedade de Markov, ele √© considerado um **estado de Markov** [^3]. Para ilustrar, considere um agente em um labirinto. Se o estado inclui a localiza√ß√£o atual do agente e se o labirinto n√£o muda, ent√£o ele tem a propriedade de Markov. No entanto, se o labirinto se transforma dependendo das a√ß√µes anteriores do agente, ent√£o o estado deve incluir informa√ß√µes adicionais, como as a√ß√µes recentes do agente, para satisfazer a propriedade de Markov.

> üí° **Exemplo Num√©rico:** Suponha um labirinto 3x3 onde o agente inicia em (1,1). As a√ß√µes s√£o "cima", "baixo", "esquerda", "direita". Se o estado for apenas as coordenadas (x,y) atuais, e cada a√ß√£o tem 80% de chance de mover o agente na dire√ß√£o desejada e 20% de chance de n√£o mover, ent√£o a propriedade de Markov se mant√©m. No entanto, se ap√≥s o agente executar "cima" tr√™s vezes seguidas, uma parede aparece entre (1,3) e (2,3), ent√£o o estado precisa incluir o n√∫mero de vezes que "cima" foi executado consecutivamente.

**Lema 1** Se o ambiente √© determin√≠stico, ou seja, a transi√ß√£o para o pr√≥ximo estado e a recompensa s√£o unicamente determinadas pelo estado e a√ß√£o atuais, ent√£o a propriedade de Markov se mant√©m trivialmente, *dado que o estado capture toda a informa√ß√£o relevante*.

*Prova:* Se o ambiente √© determin√≠stico, ent√£o $S_{t+1} = f(S_t, A_t)$ e $R_{t+1} = g(S_t, A_t)$ para algumas fun√ß√µes $f$ e $g$. Portanto, $Pr\{S_{t+1}=s' | S_t=s, A_t=a\}$ √© 1 se $s' = f(s, a)$ e 0 caso contr√°rio. Similarmente, $Pr\{R_{t+1}=r | S_t=s, A_t=a\}$ √© 1 se $r = g(s, a)$ e 0 caso contr√°rio. Claramente, essas probabilidades dependem apenas do estado e a√ß√£o atuais, e n√£o do hist√≥rico.

**A Propriedade de Markov simplifica drasticamente o problema de tomada de decis√£o**. Com a propriedade de Markov, o agente s√≥ precisa considerar o estado atual ao selecionar uma a√ß√£o, em vez de analisar todo o hist√≥rico de intera√ß√µes passadas. Isso reduz a complexidade computacional do problema e torna poss√≠vel o desenvolvimento de algoritmos eficientes de *reinforcement learning*.

Al√©m da propriedade de Markov, o *framework* MDP oferece **flexibilidade substancial** na modelagem de diferentes tipos de problemas [^3].

*   **Time Steps:** Os *time steps* em um MDP n√£o precisam corresponder a intervalos fixos de tempo real. Eles podem representar est√°gios arbitr√°rios sucessivos de tomada de decis√£o e atua√ß√£o [^3]. Por exemplo, em um jogo de xadrez, cada *time step* poderia representar o movimento de um jogador, que pode variar em dura√ß√£o no tempo real.
*   **Estados:** Os estados em um MDP podem assumir uma ampla gama de formas. Eles podem ser completamente determinados por sensa√ß√µes de baixo n√≠vel, como leituras diretas de sensores, ou podem ser descri√ß√µes simb√≥licas mais abstratas de objetos em um ambiente [^3]. Por exemplo, em um rob√¥ aspirador, o estado pode incluir leituras de sensores de proximidade, n√≠veis de bateria e representa√ß√µes simb√≥licas de √°reas limpas e sujas.
*   **A√ß√µes:** Similarmente, as a√ß√µes em um MDP podem ser controles de baixo n√≠vel, como voltagens aplicadas aos motores de um bra√ßo rob√≥tico, ou decis√µes de alto n√≠vel, como escolher ter almo√ßo ou ir para a p√≥s-gradua√ß√£o [^3]. Em um ve√≠culo aut√¥nomo, as a√ß√µes podem incluir controlar o acelerador, o freio e a dire√ß√£o, ou tamb√©m decis√µes de n√≠vel superior como mudar de faixa ou fazer uma curva.

> üí° **Exemplo Num√©rico:** Em um jogo de videogame, um *time step* pode ser cada frame exibido na tela. O estado pode ser uma representa√ß√£o dos objetos na tela (posi√ß√£o do personagem, inimigos, proj√©teis), a vida do personagem (ex: um n√∫mero entre 0 e 100), e a muni√ß√£o dispon√≠vel. As a√ß√µes podem ser "pular", "atirar", "mover para a esquerda", "mover para a direita". A recompensa pode ser +1 por cada inimigo derrotado e -1 por cada vez que o personagem √© atingido. A modelagem de um jogo como um MDP permite que um agente aprenda a jogar o jogo automaticamente.



![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

Essa flexibilidade torna o *framework* MDP aplic√°vel a uma gama impressionante de problemas, do controle rob√≥tico √† aloca√ß√£o de recursos, passando pelo planejamento estrat√©gico [^1]. A abstra√ß√£o fornecida pelo *framework* permite aos pesquisadores focar nos aspectos essenciais da tomada de decis√£o, sem serem atolados por detalhes de implementa√ß√£o espec√≠ficos do dom√≠nio.

**Proposi√ß√£o 1** (Consequ√™ncia da flexibilidade dos MDPs). Dado um problema de tomada de decis√£o sequencial, geralmente existe mais de uma maneira de formular o problema como um MDP. Escolher uma representa√ß√£o de estado que satisfa√ßa a propriedade de Markov pode exigir a inclus√£o de informa√ß√£o adicional, potencialmente aumentando a dimensionalidade do estado. No entanto, a simplifica√ß√£o resultante no processo de tomada de decis√£o muitas vezes compensa essa complexidade adicional.

*Justificativa:* Essa proposi√ß√£o reflete a natureza da modelagem com MDPs. √â comum encontrar diferentes representa√ß√µes de estado para o mesmo problema. Uma representa√ß√£o mais compacta pode n√£o satisfazer a propriedade de Markov, enquanto uma representa√ß√£o mais complexa pode. A escolha depende de um *trade-off* entre a complexidade do estado e a complexidade do aprendizado ou planejamento.

Para ilustrar essa proposi√ß√£o, considere o seguinte exemplo e prova formal:

**Exemplo:** Um agente est√° navegando em uma grade onde o vento √†s vezes o empurra em uma dire√ß√£o aleat√≥ria. Se o estado for apenas a localiza√ß√£o atual do agente, ele n√£o ter√° a propriedade de Markov, pois o movimento do agente depende do vento, que n√£o √© capturado pela localiza√ß√£o. Para satisfazer a propriedade de Markov, o estado pode ser expandido para incluir a localiza√ß√£o atual e uma estimativa da for√ßa e dire√ß√£o do vento.

**Teorema 2:** (Impacto da Informa√ß√£o no Estado) Seja um ambiente modelado como um Processo de Decis√£o de Markov (MDP). Expandir a representa√ß√£o do estado para incluir informa√ß√µes adicionais do hist√≥rico sempre preservar√° ou melhorar√° a propriedade de Markov.

*Prova:*

I.  Seja $S_t$ o estado no tempo *t* e $H_t = \{S_1, A_1, R_2, \ldots, S_t\}$ o hist√≥rico completo at√© o tempo *t*.

II. Defina uma nova representa√ß√£o de estado expandida $S'_t = f(S_t, h_t)$, onde $h_t$ √© alguma fun√ß√£o do hist√≥rico $H_t$, e $f$ √© alguma fun√ß√£o que combina o estado anterior com a informa√ß√£o do hist√≥rico.

III. Devemos mostrar que se $S_t$ n√£o satisfaz a propriedade de Markov, ent√£o existe uma fun√ß√£o $f$ e $h_t$ tal que $S'_t$ satisfaz a propriedade de Markov, ou que se $S_t$ satisfaz a propriedade de Markov, ent√£o $S'_t$ tamb√©m satisfaz.

IV. Caso 1: $S_t$ n√£o satisfaz a propriedade de Markov. Ent√£o, por defini√ß√£o, $P(S_{t+1} | S_t, A_t) \neq P(S_{t+1} | H_t, A_t)$. Isso significa que o hist√≥rico cont√©m informa√ß√µes relevantes para prever $S_{t+1}$ que n√£o est√£o presentes em $S_t$.

V.  Podemos definir $S'_t = (S_t, h_t)$, onde $h_t$ √© uma fun√ß√£o do hist√≥rico que captura a informa√ß√£o relevante para a transi√ß√£o de estado. Por exemplo, se a transi√ß√£o depende do √∫ltimo estado, ent√£o $h_t = S_{t-1}$.

VI. Com essa nova representa√ß√£o, $P(S_{t+1} | S'_t, A_t) = P(S_{t+1} | S_t, h_t, A_t) = P(S_{t+1} | H_t, A_t)$, pois $S'_t$ inclui toda a informa√ß√£o relevante do hist√≥rico. Portanto, $S'_t$ satisfaz a propriedade de Markov.

VII. Caso 2: $S_t$ satisfaz a propriedade de Markov. Ent√£o, por defini√ß√£o, $P(S_{t+1} | S_t, A_t) = P(S_{t+1} | H_t, A_t)$.

VIII. Se expandirmos o estado para $S'_t = (S_t, h_t)$, onde $h_t$ √© qualquer fun√ß√£o do hist√≥rico, ent√£o $P(S_{t+1} | S'_t, A_t) = P(S_{t+1} | S_t, h_t, A_t)$. Uma vez que $S_t$ j√° cont√©m toda a informa√ß√£o relevante, $h_t$ √© redundante, e $P(S_{t+1} | S_t, h_t, A_t) = P(S_{t+1} | S_t, A_t)$. Portanto, $S'_t$ tamb√©m satisfaz a propriedade de Markov.

IX. Em ambos os casos, expandir o estado preserva ou melhora a propriedade de Markov.  A expans√£o garante que qualquer depend√™ncia hist√≥rica relevante seja incorporada no estado, tornando-o uma estat√≠stica suficiente para prever o futuro. ‚ñ†

### Conclus√£o
A **propriedade de Markov** √© essencial em MDPs, simplificando a tomada de decis√£o ao garantir que o estado atual contenha todas as informa√ß√µes relevantes para previs√µes futuras [^3]. A flexibilidade do *framework* MDP, no que diz respeito √†s defini√ß√µes de *time steps*, estados e a√ß√µes, permite que ele seja aplicado a uma ampla gama de problemas, tornando-o uma ferramenta poderosa para modelar e resolver problemas de tomada de decis√£o sequencial [^3]. A abstra√ß√£o fornecida pelo *framework* permite aos pesquisadores focar nos aspectos essenciais da tomada de decis√£o, sem serem atolados por detalhes de implementa√ß√£o espec√≠ficos do dom√≠nio.

### Refer√™ncias
[^2]: Cap√≠tulo 3: Finite Markov Decision Processes, p√°gina 48
[^3]: Cap√≠tulo 3: Finite Markov Decision Processes, p√°gina 49
[^4]: Cap√≠tulo 3: Finite Markov Decision Processes, p√°gina 48
[^1]: Cap√≠tulo 3: Finite Markov Decision Processes, p√°gina 47
<!-- END -->