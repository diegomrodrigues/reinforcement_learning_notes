## Finite Markov Decision Processes com Estados, AÃ§Ãµes e Recompensas Finitas

### IntroduÃ§Ã£o
Em um **Processo de DecisÃ£o de Markov** (MDP), o agente interage com o ambiente atravÃ©s de uma sequÃªncia de passos de tempo discretos. A cada passo, o agente observa o estado do ambiente, seleciona uma aÃ§Ã£o e recebe uma recompensa. O objetivo do agente Ã© aprender uma *polÃ­tica* que maximize a quantidade total de recompensa que recebe ao longo do tempo. Neste capÃ­tulo, exploramos o caso especial de **MDPs finitos**, onde o nÃºmero de estados, aÃ§Ãµes e recompensas Ã© finito. Essa restriÃ§Ã£o permite a definiÃ§Ã£o de distribuiÃ§Ãµes de probabilidade discretas bem definidas, simplificando a anÃ¡lise e o projeto de algoritmos de *reinforcement learning* [^4].

### Conceitos Fundamentais
Um **MDP finito** Ã© caracterizado por conjuntos finitos de estados $\mathcal{S}$, aÃ§Ãµes $\mathcal{A}$, e recompensas $\mathcal{R}$ [^2]. A interaÃ§Ã£o entre o agente e o ambiente Ã© modelada como uma sequÃªncia de passos de tempo discretos $t = 0, 1, 2, 3, \ldots$ [^2]. No tempo $t$, o agente observa o estado do ambiente $S_t \in \mathcal{S}$ e seleciona uma aÃ§Ã£o $A_t \in \mathcal{A}(s)$, onde $\mathcal{A}(s)$ representa o conjunto de aÃ§Ãµes disponÃ­veis no estado $s$ [^2]. Como consequÃªncia dessa aÃ§Ã£o, o agente recebe uma recompensa numÃ©rica $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$ e o ambiente transita para um novo estado $S_{t+1}$ [^2]. Essa dinÃ¢mica gera uma *trajetÃ³ria* de interaÃ§Ãµes que se inicia como:
$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots$$[^2]

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

A **dinÃ¢mica** do MDP Ã© definida pela funÃ§Ã£o de probabilidade $p(s', r | s, a)$, que especifica a probabilidade de transitar para o estado $s' \in \mathcal{S}$ e receber a recompensa $r \in \mathcal{R}$ dado que o agente estava no estado $s \in \mathcal{S}$ e executou a aÃ§Ã£o $a \in \mathcal{A}(s)$ [^2]:
$$p(s',r|s,a) = Pr\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\}$$[^2]
Essa funÃ§Ã£o $p: \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ Ã© uma funÃ§Ã£o determinÃ­stica ordinÃ¡ria de quatro argumentos [^2].
Ã‰ crucial observar que $p$ especifica uma distribuiÃ§Ã£o de probabilidade para cada escolha de $s$ e $a$, ou seja [^3]:
$$\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r|s, a) = 1, \quad \text{para todo} \quad s \in \mathcal{S}, a \in \mathcal{A}(s)$$[^3]
A equaÃ§Ã£o acima garante que a soma das probabilidades de todas as transiÃ§Ãµes possÃ­veis a partir de um estado e aÃ§Ã£o Ã© igual a 1.

**Prova:**
Vamos provar que $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r|s, a) = 1$ para todo $s \in \mathcal{S}$ e $a \in \mathcal{A}(s)$.

I. $p(s', r|s, a)$ representa a probabilidade condicional de transitar para o estado $s'$ e receber a recompensa $r$, dado que o agente estava no estado $s$ e executou a aÃ§Ã£o $a$.

II. A soma $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r|s, a)$ representa a soma das probabilidades de *todos* os possÃ­veis estados $s'$ e recompensas $r$ que podem ocorrer quando o agente estÃ¡ no estado $s$ e executa a aÃ§Ã£o $a$.

III. Uma vez que o agente deve transitar para *algum* estado $s'$ e receber *alguma* recompensa $r$, a soma das probabilidades de todos os resultados possÃ­veis deve ser igual a 1.

IV. Portanto, $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r|s, a) = 1$ para todo $s \in \mathcal{S}$ e $a \in \mathcal{A}(s)$. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um MDP com dois estados, $\mathcal{S} = \{s_1, s_2\}$, duas aÃ§Ãµes em cada estado, $\mathcal{A}(s_1) = \{a_1, a_2\}$ e $\mathcal{A}(s_2) = \{a_1, a_2\}$, e duas recompensas possÃ­veis, $\mathcal{R} = \{0, 1\}$. Suponha que, no estado $s_1$, ao realizar a aÃ§Ã£o $a_1$, o agente transite para o estado $s_2$ com recompensa 1 com probabilidade 0.7 e permaneÃ§a no estado $s_1$ com recompensa 0 com probabilidade 0.3. Formalmente:
>
> $p(s_2, 1 | s_1, a_1) = 0.7$
> $p(s_1, 0 | s_1, a_1) = 0.3$
>
> EntÃ£o, $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r|s_1, a_1) = p(s_1, 0 | s_1, a_1) + p(s_1, 1 | s_1, a_1) + p(s_2, 0 | s_1, a_1) + p(s_2, 1 | s_1, a_1) = 0.3 + 0 + 0 + 0.7 = 1$.  A soma das probabilidades deve ser igual a 1.

Em um **processo de decisÃ£o de Markov**, as probabilidades dadas por $p$ caracterizam completamente a dinÃ¢mica do ambiente [^3]. A probabilidade de cada valor possÃ­vel para $S_t$ e $R_t$ depende apenas do estado e da aÃ§Ã£o imediatamente anteriores, $S_{t-1}$ e $A_{t-1}$, e nÃ£o de estados e aÃ§Ãµes anteriores [^3]. Essa propriedade Ã© conhecida como a **propriedade de Markov** [^3]. Formalmente:
$$Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a, S_{t-2}, A_{t-2}, \ldots , S_0\}$$

A propriedade de Markov Ã© uma restriÃ§Ã£o sobre o estado, nÃ£o sobre o processo de decisÃ£o em si [^3]. O estado deve incluir todas as informaÃ§Ãµes sobre a interaÃ§Ã£o passada entre o agente e o ambiente que podem influenciar o futuro [^3]. Se o fizer, diz-se que o estado tem a propriedade de Markov [^3]. Assumiremos a propriedade de Markov ao longo deste livro [^3].

A partir da funÃ§Ã£o de dinÃ¢mica de quatro argumentos, $p$, pode-se computar outras informaÃ§Ãµes sobre o ambiente. Por exemplo, a probabilidade de transiÃ§Ã£o de estado, denotada por $p(s'|s, a)$, pode ser calculada como a soma de $p(s', r|s, a)$ sobre todas as recompensas possÃ­veis [^3]:
$$p(s'|s, a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum_{r \in \mathcal{R}} p(s', r|s, a)$$[^3]
Esta notaÃ§Ã£o Ã© um leve abuso de notaÃ§Ã£o, pois usamos $p$ para representar tanto a probabilidade de transiÃ§Ã£o estado-recompensa quanto a probabilidade de transiÃ§Ã£o de estado [^3].

**ObservaÃ§Ã£o:** Uma outra forma de definir a probabilidade de transiÃ§Ã£o de estado Ã© marginalizando a funÃ§Ã£o de probabilidade conjunta sobre as recompensas.

> ğŸ’¡ **Exemplo NumÃ©rico:** Usando o exemplo anterior, a probabilidade de transiÃ§Ã£o do estado $s_1$ para o estado $s_2$ ao tomar a aÃ§Ã£o $a_1$ Ã©:
>
> $p(s_2 | s_1, a_1) = \sum_{r \in \mathcal{R}} p(s_2, r | s_1, a_1) = p(s_2, 0 | s_1, a_1) + p(s_2, 1 | s_1, a_1) = 0 + 0.7 = 0.7$.

Da mesma forma, a recompensa esperada para um par estado-aÃ§Ã£o $r(s, a)$ pode ser calculada como a mÃ©dia ponderada das recompensas, usando $p(s', r|s, a)$ como os pesos [^3]:
$$r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \, p(s', r|s, a)$$[^3]

**Prova:**
Vamos provar que $r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \, p(s', r|s, a)$.

I. Pela definiÃ§Ã£o de valor esperado de uma variÃ¡vel aleatÃ³ria discreta, temos:
   $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \cdot P(R_t = r | S_{t-1} = s, A_{t-1} = a)$$

II. Podemos expressar a probabilidade condicional $P(R_t = r | S_{t-1} = s, A_{t-1} = a)$ como a soma das probabilidades conjuntas $p(s', r | s, a)$ sobre todos os possÃ­veis estados futuros $s'$:
   $$P(R_t = r | S_{t-1} = s, A_{t-1} = a) = \sum_{s' \in \mathcal{S}} p(s', r | s, a)$$

III. Substituindo esta expressÃ£o na equaÃ§Ã£o do valor esperado, obtemos:
    $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \cdot \sum_{s' \in \mathcal{S}} p(s', r | s, a)$$

IV. Reorganizando a soma, temos:
    $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \, p(s', r|s, a)$$

V. Portanto, $r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \, p(s', r|s, a)$. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:**  Continuando com o exemplo anterior, a recompensa esperada ao realizar a aÃ§Ã£o $a_1$ no estado $s_1$ Ã©:
>
> $r(s_1, a_1) = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \, p(s', r|s_1, a_1) = (0 \cdot p(s_1, 0 | s_1, a_1)) + (1 \cdot p(s_1, 1 | s_1, a_1)) + (0 \cdot p(s_2, 0 | s_1, a_1)) + (1 \cdot p(s_2, 1 | s_1, a_1)) = (0 \cdot 0.3) + (0 \cdot 0) + (0 \cdot 0) + (1 \cdot 0.7) = 0 + 0 + 0 + 0.7 = 0.7$.

A recompensa esperada para uma transiÃ§Ã£o estado-aÃ§Ã£o-prÃ³ximo estado $r(s, a, s')$ Ã© dada por [^3]:
$$r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r|s, a)}{p(s'|s, a)}$$[^3]

**Prova:**
Vamos provar que $r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r|s, a)}{p(s'|s, a)}$.

I. Pela definiÃ§Ã£o de valor esperado condicional de uma variÃ¡vel aleatÃ³ria discreta, temos:
   $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \cdot P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s')$$

II. Usando a definiÃ§Ã£o de probabilidade condicional, podemos escrever:
   $$P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s') = \frac{P(R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a)}{P(S_t = s' | S_{t-1} = s, A_{t-1} = a)}$$

III. Pela definiÃ§Ã£o de $p(s', r | s, a)$ e $p(s' | s, a)$, temos:
   $$P(R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a) = p(s', r | s, a)$$
   $$P(S_t = s' | S_{t-1} = s, A_{t-1} = a) = p(s' | s, a)$$

IV. Substituindo estas expressÃµes na equaÃ§Ã£o da probabilidade condicional, obtemos:
   $$P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s') = \frac{p(s', r | s, a)}{p(s' | s, a)}$$

V. Substituindo esta expressÃ£o na equaÃ§Ã£o do valor esperado, obtemos:
   $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \cdot \frac{p(s', r | s, a)}{p(s' | s, a)}$$

VI. Portanto, $r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r|s, a)}{p(s'|s, a)}$. â– 

> ğŸ’¡ **Exemplo NumÃ©rico:** Usando o mesmo exemplo, a recompensa esperada ao transitar do estado $s_1$ para o estado $s_2$ ao tomar a aÃ§Ã£o $a_1$ Ã©:
>
> $r(s_1, a_1, s_2) = \sum_{r \in \mathcal{R}} r \frac{p(s_2, r|s_1, a_1)}{p(s_2|s_1, a_1)} = 0 \cdot \frac{p(s_2, 0|s_1, a_1)}{p(s_2|s_1, a_1)} + 1 \cdot \frac{p(s_2, 1|s_1, a_1)}{p(s_2|s_1, a_1)} = 0 \cdot \frac{0}{0.7} + 1 \cdot \frac{0.7}{0.7} = 0 + 1 = 1$.

Na prÃ¡tica, normalmente usamos a funÃ§Ã£o $p$ de quatro argumentos, mas as outras notaÃ§Ãµes podem ser convenientes [^3].

Para completar a caracterizaÃ§Ã£o do MDP, precisamos definir o conceito de polÃ­tica. Uma **polÃ­tica**, denotada por $\pi$, especifica como o agente se comporta em um dado estado [^2]. Formalmente, $\pi(a|s)$ Ã© a probabilidade de selecionar a aÃ§Ã£o $a$ no estado $s$.

$$\pi(a|s) = Pr\{A_t = a | S_t = s\}$$

Uma polÃ­tica $\pi$ Ã© *estacionÃ¡ria* se nÃ£o depende do tempo. Ou seja, a probabilidade de selecionar uma aÃ§Ã£o $a$ no estado $s$ Ã© sempre a mesma, independentemente do passo de tempo $t$.

**DefiniÃ§Ã£o:** Uma polÃ­tica $\pi$ Ã© dita *determinÃ­stica* se, para cada estado $s$, existe uma Ãºnica aÃ§Ã£o $a$ tal que $\pi(a|s) = 1$. Caso contrÃ¡rio, a polÃ­tica Ã© dita *estocÃ¡stica*.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere um MDP com estados $\mathcal{S} = \{s_1, s_2\}$ e aÃ§Ãµes $\mathcal{A}(s_1) = \{a_1, a_2\}$ e $\mathcal{A}(s_2) = \{a_1, a_2\}$. Uma polÃ­tica determinÃ­stica $\pi_1$ pode ser definida como $\pi_1(a_1 | s_1) = 1$ e $\pi_1(a_2 | s_2) = 1$. Uma polÃ­tica estocÃ¡stica $\pi_2$ pode ser definida como $\pi_2(a_1 | s_1) = 0.6$, $\pi_2(a_2 | s_1) = 0.4$, $\pi_2(a_1 | s_2) = 0.2$ e $\pi_2(a_2 | s_2) = 0.8$. Note que, para cada estado, a soma das probabilidades das aÃ§Ãµes Ã© igual a 1.

O conceito de valor esperado do retorno Ã© fundamental para avaliar a qualidade de uma polÃ­tica.

**DefiniÃ§Ã£o:** O **retorno** $G_t$ Ã© a soma das recompensas recebidas a partir do tempo $t$:

$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \ldots + R_T$$

onde $T$ Ã© o passo de tempo final se o processo terminar, ou $T = \infty$ se o processo continuar indefinidamente.

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que um agente execute uma trajetÃ³ria com as seguintes recompensas: $R_1 = 1, R_2 = 0, R_3 = -1, R_4 = 1, R_5 = 0$ e o processo termina em $T=5$. EntÃ£o, o retorno a partir do tempo $t=1$ Ã© $G_1 = 0 + (-1) + 1 + 0 = 0$. O retorno a partir do tempo $t=3$ Ã© $G_3 = 1 + 0 = 1$.

Para evitar que o retorno se torne infinito em processos contÃ­nuos, introduzimos um fator de desconto $\gamma \in [0, 1]$. O **retorno com desconto** Ã© definido como:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

O fator de desconto $\gamma$ determina o quanto o agente valoriza as recompensas futuras. Se $\gamma = 0$, o agente se preocupa apenas com a recompensa imediata. Se $\gamma = 1$, o agente valoriza todas as recompensas futuras da mesma forma que a recompensa imediata.

> ğŸ’¡ **Exemplo NumÃ©rico:** Considere a mesma sequÃªncia de recompensas do exemplo anterior, $R_1 = 1, R_2 = 0, R_3 = -1, R_4 = 1, R_5 = 0$, e um fator de desconto $\gamma = 0.9$. EntÃ£o, o retorno com desconto a partir do tempo $t=1$ Ã© $G_1 = 0 + 0.9 \cdot (-1) + 0.9^2 \cdot 1 + 0.9^3 \cdot 0 = 0 - 0.9 + 0.81 + 0 = -0.09$. Se $\gamma = 0$, entÃ£o $G_1 = 0$. Se $\gamma = 1$, entÃ£o $G_1 = 0$.

**DefiniÃ§Ã£o:** A **funÃ§Ã£o valor de estado** $v_{\pi}(s)$ de uma polÃ­tica $\pi$ Ã© o valor esperado do retorno com desconto quando o agente comeÃ§a no estado $s$ e segue a polÃ­tica $\pi$:

$$v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Big| S_t = s\right]$$

**DefiniÃ§Ã£o:** A **funÃ§Ã£o valor de aÃ§Ã£o** $q_{\pi}(s, a)$ de uma polÃ­tica $\pi$ Ã© o valor esperado do retorno com desconto quando o agente comeÃ§a no estado $s$, executa a aÃ§Ã£o $a$ e, em seguida, segue a polÃ­tica $\pi$:

$$q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Big| S_t = s, A_t = a\right]$$

> ğŸ’¡ **Exemplo NumÃ©rico:** Suponha que, seguindo a polÃ­tica $\pi$ a partir do estado $s_1$, o agente receba as seguintes recompensas (com $\gamma = 0.9$): $R_1 = 1, R_2 = 0, R_3 = 1, R_4 = 0, R_5 = 1, \ldots$ (alternando entre 1 e 0). EntÃ£o, $v_{\pi}(s_1) = 1 + 0.9 \cdot 0 + 0.9^2 \cdot 1 + 0.9^3 \cdot 0 + 0.9^4 \cdot 1 + \ldots = 1 + 0.81 + 0.6561 + \ldots = \sum_{k=0}^{\infty} (0.9^2)^k = \frac{1}{1 - 0.9^2} = \frac{1}{1 - 0.81} = \frac{1}{0.19} \approx 5.26$.
> Agora, suponha que no estado $s_1$, o agente execute a aÃ§Ã£o $a_1$ e depois siga a polÃ­tica $\pi$ acima. As recompensas obtidas sÃ£o as mesmas que antes. EntÃ£o, $q_{\pi}(s_1, a_1) = v_{\pi}(s_1) \approx 5.26$.

As funÃ§Ãµes valor de estado e aÃ§Ã£o sÃ£o fundamentais para determinar a qualidade de uma polÃ­tica. Uma polÃ­tica $\pi$ Ã© considerada melhor que outra polÃ­tica $\pi'$ se, para todos os estados $s$, o valor de estado de $\pi$ for maior ou igual ao valor de estado de $\pi'$:

$$\pi \geq \pi' \iff v_{\pi}(s) \geq v_{\pi'}(s), \quad \text{para todo} \quad s \in \mathcal{S}$$

Em um MDP finito, sempre existe pelo menos uma polÃ­tica que Ã© melhor ou igual a todas as outras polÃ­ticas. Essa polÃ­tica Ã© chamada de **polÃ­tica Ã³tima**, denotada por $\pi_*$.

**DefiniÃ§Ã£o:** A **funÃ§Ã£o valor de estado Ã³tima** $v_*(s)$ Ã© o valor esperado do retorno com desconto quando o agente comeÃ§a no estado $s$ e segue a polÃ­tica Ã³tima $\pi_*$:

$$v_*(s) = \max_{\pi} v_{\pi}(s), \quad \text{para todo} \quad s \in \mathcal{S}$$

**DefiniÃ§Ã£o:** A **funÃ§Ã£o valor de aÃ§Ã£o Ã³tima** $q_*(s, a)$ Ã© o valor esperado do retorno com desconto quando o agente comeÃ§a no estado $s$, executa a aÃ§Ã£o $a$ e, em seguida, segue a polÃ­tica Ã³tima $\pi_*$:

$$q_*(s, a) = \max_{\pi} q_{\pi}(s, a), \quad \text{para todo} \quad s \in \mathcal{S}, a \in \mathcal{A}(s)$$

As funÃ§Ãµes valor Ã³timas satisfazem as equaÃ§Ãµes de otimalidade de Bellman, que fornecem uma forma de calcular as polÃ­ticas Ã³timas.



![Backup diagrams illustrating the update rules for state-value \(v_*\) and action-value \(q_*\) functions in MDPs.](./../images/image1.png)

![Diagrama de backup para a funÃ§Ã£o de valor \(v_\pi\), ilustrando a relaÃ§Ã£o entre um estado e seus sucessores sob uma polÃ­tica \(\pi\).](./../images/image3.png)

![Diagrama de backup qÏ€ ilustrando as relaÃ§Ãµes entre estados, aÃ§Ãµes e recompensas em reinforcement learning.](./../images/image10.png)

![Decision tree illustrating the relationship between state-action pairs, rewards, and subsequent states in an MDP.](./../images/image5.png)

### ConclusÃ£o
Em um MDP finito, a finitude dos conjuntos de estados, aÃ§Ãµes e recompensas permite uma representaÃ§Ã£o e anÃ¡lise mais direta do problema de *reinforcement learning*. A funÃ§Ã£o de dinÃ¢mica $p(s', r|s, a)$ Ã© uma ferramenta fundamental para modelar o comportamento do ambiente e permite o cÃ¡lculo de probabilidades de transiÃ§Ã£o de estado e recompensas esperadas. A propriedade de Markov simplifica ainda mais a anÃ¡lise, garantindo que o estado atual contenha todas as informaÃ§Ãµes relevantes para a tomada de decisÃ£o futura. Essa estrutura fornece uma base sÃ³lida para o desenvolvimento de algoritmos que visam encontrar polÃ­ticas Ã³timas em ambientes complexos.
### ReferÃªncias
[^2]: Sutton, Richard S.; Barto, Andrew G. Reinforcement Learning: An Introduction. 2nd Edition. The MIT Press, 2018.
[^3]: Sutton, Richard S.; Barto, Andrew G. Reinforcement Learning: An Introduction. 2nd Edition. The MIT Press, 2018.
[^4]: Sutton, Richard S.; Barto, Andrew G. Reinforcement Learning: An Introduction. 2nd Edition. The MIT Press, 2018.
<!-- END -->