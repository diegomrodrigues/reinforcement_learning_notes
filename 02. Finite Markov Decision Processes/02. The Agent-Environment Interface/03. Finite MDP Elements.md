## Finite Markov Decision Processes com Estados, A√ß√µes e Recompensas Finitas

### Introdu√ß√£o
Em um **Processo de Decis√£o de Markov** (MDP), o agente interage com o ambiente atrav√©s de uma sequ√™ncia de passos de tempo discretos. A cada passo, o agente observa o estado do ambiente, seleciona uma a√ß√£o e recebe uma recompensa. O objetivo do agente √© aprender uma *pol√≠tica* que maximize a quantidade total de recompensa que recebe ao longo do tempo. Neste cap√≠tulo, exploramos o caso especial de **MDPs finitos**, onde o n√∫mero de estados, a√ß√µes e recompensas √© finito. Essa restri√ß√£o permite a defini√ß√£o de distribui√ß√µes de probabilidade discretas bem definidas, simplificando a an√°lise e o projeto de algoritmos de *reinforcement learning* [^4].

### Conceitos Fundamentais
Um **MDP finito** √© caracterizado por conjuntos finitos de estados $\mathcal{S}$, a√ß√µes $\mathcal{A}$, e recompensas $\mathcal{R}$ [^2]. A intera√ß√£o entre o agente e o ambiente √© modelada como uma sequ√™ncia de passos de tempo discretos $t = 0, 1, 2, 3, \ldots$ [^2]. No tempo $t$, o agente observa o estado do ambiente $S_t \in \mathcal{S}$ e seleciona uma a√ß√£o $A_t \in \mathcal{A}(s)$, onde $\mathcal{A}(s)$ representa o conjunto de a√ß√µes dispon√≠veis no estado $s$ [^2]. Como consequ√™ncia dessa a√ß√£o, o agente recebe uma recompensa num√©rica $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$ e o ambiente transita para um novo estado $S_{t+1}$ [^2]. Essa din√¢mica gera uma *trajet√≥ria* de intera√ß√µes que se inicia como:
$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \ldots$$[^2]

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

A **din√¢mica** do MDP √© definida pela fun√ß√£o de probabilidade $p(s', r | s, a)$, que especifica a probabilidade de transitar para o estado $s' \in \mathcal{S}$ e receber a recompensa $r \in \mathcal{R}$ dado que o agente estava no estado $s \in \mathcal{S}$ e executou a a√ß√£o $a \in \mathcal{A}(s)$ [^2]:
$$p(s',r|s,a) = Pr\{S_t=s', R_t=r | S_{t-1}=s, A_{t-1}=a\}$$[^2]
Essa fun√ß√£o $p: \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ √© uma fun√ß√£o determin√≠stica ordin√°ria de quatro argumentos [^2].
√â crucial observar que $p$ especifica uma distribui√ß√£o de probabilidade para cada escolha de $s$ e $a$, ou seja [^3]:
$$\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r|s, a) = 1, \quad \text{para todo} \quad s \in \mathcal{S}, a \in \mathcal{A}(s)$$[^3]
A equa√ß√£o acima garante que a soma das probabilidades de todas as transi√ß√µes poss√≠veis a partir de um estado e a√ß√£o √© igual a 1.

**Prova:**
Vamos provar que $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r|s, a) = 1$ para todo $s \in \mathcal{S}$ e $a \in \mathcal{A}(s)$.

I. $p(s', r|s, a)$ representa a probabilidade condicional de transitar para o estado $s'$ e receber a recompensa $r$, dado que o agente estava no estado $s$ e executou a a√ß√£o $a$.

II. A soma $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r|s, a)$ representa a soma das probabilidades de *todos* os poss√≠veis estados $s'$ e recompensas $r$ que podem ocorrer quando o agente est√° no estado $s$ e executa a a√ß√£o $a$.

III. Uma vez que o agente deve transitar para *algum* estado $s'$ e receber *alguma* recompensa $r$, a soma das probabilidades de todos os resultados poss√≠veis deve ser igual a 1.

IV. Portanto, $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r|s, a) = 1$ para todo $s \in \mathcal{S}$ e $a \in \mathcal{A}(s)$. ‚ñ†

> üí° **Exemplo Num√©rico:** Considere um MDP com dois estados, $\mathcal{S} = \{s_1, s_2\}$, duas a√ß√µes em cada estado, $\mathcal{A}(s_1) = \{a_1, a_2\}$ e $\mathcal{A}(s_2) = \{a_1, a_2\}$, e duas recompensas poss√≠veis, $\mathcal{R} = \{0, 1\}$. Suponha que, no estado $s_1$, ao realizar a a√ß√£o $a_1$, o agente transite para o estado $s_2$ com recompensa 1 com probabilidade 0.7 e permane√ßa no estado $s_1$ com recompensa 0 com probabilidade 0.3. Formalmente:
>
> $p(s_2, 1 | s_1, a_1) = 0.7$
> $p(s_1, 0 | s_1, a_1) = 0.3$
>
> Ent√£o, $\sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r|s_1, a_1) = p(s_1, 0 | s_1, a_1) + p(s_1, 1 | s_1, a_1) + p(s_2, 0 | s_1, a_1) + p(s_2, 1 | s_1, a_1) = 0.3 + 0 + 0 + 0.7 = 1$.  A soma das probabilidades deve ser igual a 1.

Em um **processo de decis√£o de Markov**, as probabilidades dadas por $p$ caracterizam completamente a din√¢mica do ambiente [^3]. A probabilidade de cada valor poss√≠vel para $S_t$ e $R_t$ depende apenas do estado e da a√ß√£o imediatamente anteriores, $S_{t-1}$ e $A_{t-1}$, e n√£o de estados e a√ß√µes anteriores [^3]. Essa propriedade √© conhecida como a **propriedade de Markov** [^3]. Formalmente:
$$Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a, S_{t-2}, A_{t-2}, \ldots , S_0\}$$

A propriedade de Markov √© uma restri√ß√£o sobre o estado, n√£o sobre o processo de decis√£o em si [^3]. O estado deve incluir todas as informa√ß√µes sobre a intera√ß√£o passada entre o agente e o ambiente que podem influenciar o futuro [^3]. Se o fizer, diz-se que o estado tem a propriedade de Markov [^3]. Assumiremos a propriedade de Markov ao longo deste livro [^3].

A partir da fun√ß√£o de din√¢mica de quatro argumentos, $p$, pode-se computar outras informa√ß√µes sobre o ambiente. Por exemplo, a probabilidade de transi√ß√£o de estado, denotada por $p(s'|s, a)$, pode ser calculada como a soma de $p(s', r|s, a)$ sobre todas as recompensas poss√≠veis [^3]:
$$p(s'|s, a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum_{r \in \mathcal{R}} p(s', r|s, a)$$[^3]
Esta nota√ß√£o √© um leve abuso de nota√ß√£o, pois usamos $p$ para representar tanto a probabilidade de transi√ß√£o estado-recompensa quanto a probabilidade de transi√ß√£o de estado [^3].

**Observa√ß√£o:** Uma outra forma de definir a probabilidade de transi√ß√£o de estado √© marginalizando a fun√ß√£o de probabilidade conjunta sobre as recompensas.

> üí° **Exemplo Num√©rico:** Usando o exemplo anterior, a probabilidade de transi√ß√£o do estado $s_1$ para o estado $s_2$ ao tomar a a√ß√£o $a_1$ √©:
>
> $p(s_2 | s_1, a_1) = \sum_{r \in \mathcal{R}} p(s_2, r | s_1, a_1) = p(s_2, 0 | s_1, a_1) + p(s_2, 1 | s_1, a_1) = 0 + 0.7 = 0.7$.

Da mesma forma, a recompensa esperada para um par estado-a√ß√£o $r(s, a)$ pode ser calculada como a m√©dia ponderada das recompensas, usando $p(s', r|s, a)$ como os pesos [^3]:
$$r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \, p(s', r|s, a)$$[^3]

**Prova:**
Vamos provar que $r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \, p(s', r|s, a)$.

I. Pela defini√ß√£o de valor esperado de uma vari√°vel aleat√≥ria discreta, temos:
   $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \cdot P(R_t = r | S_{t-1} = s, A_{t-1} = a)$$

II. Podemos expressar a probabilidade condicional $P(R_t = r | S_{t-1} = s, A_{t-1} = a)$ como a soma das probabilidades conjuntas $p(s', r | s, a)$ sobre todos os poss√≠veis estados futuros $s'$:
   $$P(R_t = r | S_{t-1} = s, A_{t-1} = a) = \sum_{s' \in \mathcal{S}} p(s', r | s, a)$$

III. Substituindo esta express√£o na equa√ß√£o do valor esperado, obtemos:
    $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \cdot \sum_{s' \in \mathcal{S}} p(s', r | s, a)$$

IV. Reorganizando a soma, temos:
    $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \, p(s', r|s, a)$$

V. Portanto, $r(s, a) = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \, p(s', r|s, a)$. ‚ñ†

> üí° **Exemplo Num√©rico:**  Continuando com o exemplo anterior, a recompensa esperada ao realizar a a√ß√£o $a_1$ no estado $s_1$ √©:
>
> $r(s_1, a_1) = \sum_{r \in \mathcal{R}} \sum_{s' \in \mathcal{S}} r \, p(s', r|s_1, a_1) = (0 \cdot p(s_1, 0 | s_1, a_1)) + (1 \cdot p(s_1, 1 | s_1, a_1)) + (0 \cdot p(s_2, 0 | s_1, a_1)) + (1 \cdot p(s_2, 1 | s_1, a_1)) = (0 \cdot 0.3) + (0 \cdot 0) + (0 \cdot 0) + (1 \cdot 0.7) = 0 + 0 + 0 + 0.7 = 0.7$.

A recompensa esperada para uma transi√ß√£o estado-a√ß√£o-pr√≥ximo estado $r(s, a, s')$ √© dada por [^3]:
$$r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r|s, a)}{p(s'|s, a)}$$[^3]

**Prova:**
Vamos provar que $r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r|s, a)}{p(s'|s, a)}$.

I. Pela defini√ß√£o de valor esperado condicional de uma vari√°vel aleat√≥ria discreta, temos:
   $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \cdot P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s')$$

II. Usando a defini√ß√£o de probabilidade condicional, podemos escrever:
   $$P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s') = \frac{P(R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a)}{P(S_t = s' | S_{t-1} = s, A_{t-1} = a)}$$

III. Pela defini√ß√£o de $p(s', r | s, a)$ e $p(s' | s, a)$, temos:
   $$P(R_t = r, S_t = s' | S_{t-1} = s, A_{t-1} = a) = p(s', r | s, a)$$
   $$P(S_t = s' | S_{t-1} = s, A_{t-1} = a) = p(s' | s, a)$$

IV. Substituindo estas express√µes na equa√ß√£o da probabilidade condicional, obtemos:
   $$P(R_t = r | S_{t-1} = s, A_{t-1} = a, S_t = s') = \frac{p(s', r | s, a)}{p(s' | s, a)}$$

V. Substituindo esta express√£o na equa√ß√£o do valor esperado, obtemos:
   $$\mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \cdot \frac{p(s', r | s, a)}{p(s' | s, a)}$$

VI. Portanto, $r(s, a, s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s', r|s, a)}{p(s'|s, a)}$. ‚ñ†

> üí° **Exemplo Num√©rico:** Usando o mesmo exemplo, a recompensa esperada ao transitar do estado $s_1$ para o estado $s_2$ ao tomar a a√ß√£o $a_1$ √©:
>
> $r(s_1, a_1, s_2) = \sum_{r \in \mathcal{R}} r \frac{p(s_2, r|s_1, a_1)}{p(s_2|s_1, a_1)} = 0 \cdot \frac{p(s_2, 0|s_1, a_1)}{p(s_2|s_1, a_1)} + 1 \cdot \frac{p(s_2, 1|s_1, a_1)}{p(s_2|s_1, a_1)} = 0 \cdot \frac{0}{0.7} + 1 \cdot \frac{0.7}{0.7} = 0 + 1 = 1$.

Na pr√°tica, normalmente usamos a fun√ß√£o $p$ de quatro argumentos, mas as outras nota√ß√µes podem ser convenientes [^3].

Para completar a caracteriza√ß√£o do MDP, precisamos definir o conceito de pol√≠tica. Uma **pol√≠tica**, denotada por $\pi$, especifica como o agente se comporta em um dado estado [^2]. Formalmente, $\pi(a|s)$ √© a probabilidade de selecionar a a√ß√£o $a$ no estado $s$.

$$\pi(a|s) = Pr\{A_t = a | S_t = s\}$$

Uma pol√≠tica $\pi$ √© *estacion√°ria* se n√£o depende do tempo. Ou seja, a probabilidade de selecionar uma a√ß√£o $a$ no estado $s$ √© sempre a mesma, independentemente do passo de tempo $t$.

**Defini√ß√£o:** Uma pol√≠tica $\pi$ √© dita *determin√≠stica* se, para cada estado $s$, existe uma √∫nica a√ß√£o $a$ tal que $\pi(a|s) = 1$. Caso contr√°rio, a pol√≠tica √© dita *estoc√°stica*.

> üí° **Exemplo Num√©rico:** Considere um MDP com estados $\mathcal{S} = \{s_1, s_2\}$ e a√ß√µes $\mathcal{A}(s_1) = \{a_1, a_2\}$ e $\mathcal{A}(s_2) = \{a_1, a_2\}$. Uma pol√≠tica determin√≠stica $\pi_1$ pode ser definida como $\pi_1(a_1 | s_1) = 1$ e $\pi_1(a_2 | s_2) = 1$. Uma pol√≠tica estoc√°stica $\pi_2$ pode ser definida como $\pi_2(a_1 | s_1) = 0.6$, $\pi_2(a_2 | s_1) = 0.4$, $\pi_2(a_1 | s_2) = 0.2$ e $\pi_2(a_2 | s_2) = 0.8$. Note que, para cada estado, a soma das probabilidades das a√ß√µes √© igual a 1.

O conceito de valor esperado do retorno √© fundamental para avaliar a qualidade de uma pol√≠tica.

**Defini√ß√£o:** O **retorno** $G_t$ √© a soma das recompensas recebidas a partir do tempo $t$:

$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \ldots + R_T$$

onde $T$ √© o passo de tempo final se o processo terminar, ou $T = \infty$ se o processo continuar indefinidamente.

> üí° **Exemplo Num√©rico:** Suponha que um agente execute uma trajet√≥ria com as seguintes recompensas: $R_1 = 1, R_2 = 0, R_3 = -1, R_4 = 1, R_5 = 0$ e o processo termina em $T=5$. Ent√£o, o retorno a partir do tempo $t=1$ √© $G_1 = 0 + (-1) + 1 + 0 = 0$. O retorno a partir do tempo $t=3$ √© $G_3 = 1 + 0 = 1$.

Para evitar que o retorno se torne infinito em processos cont√≠nuos, introduzimos um fator de desconto $\gamma \in [0, 1]$. O **retorno com desconto** √© definido como:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

O fator de desconto $\gamma$ determina o quanto o agente valoriza as recompensas futuras. Se $\gamma = 0$, o agente se preocupa apenas com a recompensa imediata. Se $\gamma = 1$, o agente valoriza todas as recompensas futuras da mesma forma que a recompensa imediata.

> üí° **Exemplo Num√©rico:** Considere a mesma sequ√™ncia de recompensas do exemplo anterior, $R_1 = 1, R_2 = 0, R_3 = -1, R_4 = 1, R_5 = 0$, e um fator de desconto $\gamma = 0.9$. Ent√£o, o retorno com desconto a partir do tempo $t=1$ √© $G_1 = 0 + 0.9 \cdot (-1) + 0.9^2 \cdot 1 + 0.9^3 \cdot 0 = 0 - 0.9 + 0.81 + 0 = -0.09$. Se $\gamma = 0$, ent√£o $G_1 = 0$. Se $\gamma = 1$, ent√£o $G_1 = 0$.

**Defini√ß√£o:** A **fun√ß√£o valor de estado** $v_{\pi}(s)$ de uma pol√≠tica $\pi$ √© o valor esperado do retorno com desconto quando o agente come√ßa no estado $s$ e segue a pol√≠tica $\pi$:

$$v_{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Big| S_t = s\right]$$

**Defini√ß√£o:** A **fun√ß√£o valor de a√ß√£o** $q_{\pi}(s, a)$ de uma pol√≠tica $\pi$ √© o valor esperado do retorno com desconto quando o agente come√ßa no estado $s$, executa a a√ß√£o $a$ e, em seguida, segue a pol√≠tica $\pi$:

$$q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Big| S_t = s, A_t = a\right]$$

> üí° **Exemplo Num√©rico:** Suponha que, seguindo a pol√≠tica $\pi$ a partir do estado $s_1$, o agente receba as seguintes recompensas (com $\gamma = 0.9$): $R_1 = 1, R_2 = 0, R_3 = 1, R_4 = 0, R_5 = 1, \ldots$ (alternando entre 1 e 0). Ent√£o, $v_{\pi}(s_1) = 1 + 0.9 \cdot 0 + 0.9^2 \cdot 1 + 0.9^3 \cdot 0 + 0.9^4 \cdot 1 + \ldots = 1 + 0.81 + 0.6561 + \ldots = \sum_{k=0}^{\infty} (0.9^2)^k = \frac{1}{1 - 0.9^2} = \frac{1}{1 - 0.81} = \frac{1}{0.19} \approx 5.26$.
> Agora, suponha que no estado $s_1$, o agente execute a a√ß√£o $a_1$ e depois siga a pol√≠tica $\pi$ acima. As recompensas obtidas s√£o as mesmas que antes. Ent√£o, $q_{\pi}(s_1, a_1) = v_{\pi}(s_1) \approx 5.26$.

As fun√ß√µes valor de estado e a√ß√£o s√£o fundamentais para determinar a qualidade de uma pol√≠tica. Uma pol√≠tica $\pi$ √© considerada melhor que outra pol√≠tica $\pi'$ se, para todos os estados $s$, o valor de estado de $\pi$ for maior ou igual ao valor de estado de $\pi'$:

$$\pi \geq \pi' \iff v_{\pi}(s) \geq v_{\pi'}(s), \quad \text{para todo} \quad s \in \mathcal{S}$$

Em um MDP finito, sempre existe pelo menos uma pol√≠tica que √© melhor ou igual a todas as outras pol√≠ticas. Essa pol√≠tica √© chamada de **pol√≠tica √≥tima**, denotada por $\pi_*$.

**Defini√ß√£o:** A **fun√ß√£o valor de estado √≥tima** $v_*(s)$ √© o valor esperado do retorno com desconto quando o agente come√ßa no estado $s$ e segue a pol√≠tica √≥tima $\pi_*$:

$$v_*(s) = \max_{\pi} v_{\pi}(s), \quad \text{para todo} \quad s \in \mathcal{S}$$

**Defini√ß√£o:** A **fun√ß√£o valor de a√ß√£o √≥tima** $q_*(s, a)$ √© o valor esperado do retorno com desconto quando o agente come√ßa no estado $s$, executa a a√ß√£o $a$ e, em seguida, segue a pol√≠tica √≥tima $\pi_*$:

$$q_*(s, a) = \max_{\pi} q_{\pi}(s, a), \quad \text{para todo} \quad s \in \mathcal{S}, a \in \mathcal{A}(s)$$

As fun√ß√µes valor √≥timas satisfazem as equa√ß√µes de otimalidade de Bellman, que fornecem uma forma de calcular as pol√≠ticas √≥timas.



![Backup diagrams illustrating the update rules for state-value \(v_*\) and action-value \(q_*\) functions in MDPs.](./../images/image1.png)

![Diagrama de backup para a fun√ß√£o de valor \(v_\pi\), ilustrando a rela√ß√£o entre um estado e seus sucessores sob uma pol√≠tica \(\pi\).](./../images/image3.png)

![Diagrama de backup qœÄ ilustrando as rela√ß√µes entre estados, a√ß√µes e recompensas em reinforcement learning.](./../images/image10.png)

![Decision tree illustrating the relationship between state-action pairs, rewards, and subsequent states in an MDP.](./../images/image5.png)

### Conclus√£o
Em um MDP finito, a finitude dos conjuntos de estados, a√ß√µes e recompensas permite uma representa√ß√£o e an√°lise mais direta do problema de *reinforcement learning*. A fun√ß√£o de din√¢mica $p(s', r|s, a)$ √© uma ferramenta fundamental para modelar o comportamento do ambiente e permite o c√°lculo de probabilidades de transi√ß√£o de estado e recompensas esperadas. A propriedade de Markov simplifica ainda mais a an√°lise, garantindo que o estado atual contenha todas as informa√ß√µes relevantes para a tomada de decis√£o futura. Essa estrutura fornece uma base s√≥lida para o desenvolvimento de algoritmos que visam encontrar pol√≠ticas √≥timas em ambientes complexos.
### Refer√™ncias
[^2]: Sutton, Richard S.; Barto, Andrew G. Reinforcement Learning: An Introduction. 2nd Edition. The MIT Press, 2018.
[^3]: Sutton, Richard S.; Barto, Andrew G. Reinforcement Learning: An Introduction. 2nd Edition. The MIT Press, 2018.
[^4]: Sutton, Richard S.; Barto, Andrew G. Reinforcement Learning: An Introduction. 2nd Edition. The MIT Press, 2018.
<!-- END -->