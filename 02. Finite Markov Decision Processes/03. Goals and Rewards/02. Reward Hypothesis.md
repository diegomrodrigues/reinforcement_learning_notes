## Goals and Rewards in Reinforcement Learning: Formalizing the Objective

### Introdu√ß√£o
Em reinforcement learning, a formaliza√ß√£o do objetivo do agente √© crucial para direcionar o aprendizado e o comportamento. Este cap√≠tulo explora como os **goals** e **rewards** s√£o definidos dentro do framework de **Markov Decision Processes (MDPs)**, focando na **reward hypothesis** como um elemento distintivo do reinforcement learning [^1]. A hip√≥tese prop√µe que *todos os objetivos e prop√≥sitos podem ser bem representados como a maximiza√ß√£o do valor esperado da soma cumulativa de um sinal escalar recebido (a recompensa)* [^1]. Este cap√≠tulo aprofunda essa hip√≥tese, explorando suas implica√ß√µes e exemplos pr√°ticos.

### Conceitos Fundamentais

A **reward hypothesis** √© um dos pilares do reinforcement learning, fornecendo uma forma clara e concisa de definir o que o agente deve alcan√ßar. Ao contr√°rio de outras abordagens de aprendizado de m√°quina, o reinforcement learning depende fortemente de um sinal de recompensa externo para guiar o aprendizado.
No reinforcement learning, o prop√≥sito ou objetivo do agente √© formalizado em termos de um sinal especial, chamado **recompensa**, que passa do ambiente para o agente [^7]. A cada passo de tempo, a recompensa √© um n√∫mero simples, $R_t \in \mathbb{R}$ [^7]. Informalmente, o objetivo do agente √© maximizar a quantidade total de recompensa que ele recebe [^7]. Isso significa maximizar n√£o a recompensa imediata, mas a recompensa cumulativa a longo prazo [^7]. Podemos declarar claramente essa ideia informal como a **hip√≥tese da recompensa** [^7]:

>Que tudo o que queremos dizer por objetivos e prop√≥sitos pode ser bem pensado como a maximiza√ß√£o do valor esperado da soma cumulativa de um sinal escalar recebido (chamado de recompensa) [^7].

A utiliza√ß√£o de um sinal de recompensa para formalizar a ideia de um objetivo √© uma das caracter√≠sticas mais distintas do reinforcement learning [^7]. Embora formular objetivos em termos de sinais de recompensa possa parecer limitante √† primeira vista, na pr√°tica se mostrou flex√≠vel e amplamente aplic√°vel [^7].



![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

**Exemplos Pr√°ticos:**

1.  **Robot Learning to Walk:** Para fazer um rob√¥ aprender a andar, pesquisadores forneceram recompensa em cada passo de tempo proporcional ao movimento para frente do rob√¥ [^7].
2.  **Robot Escaping a Maze:** Para fazer um rob√¥ aprender a escapar de um labirinto, a recompensa √© frequentemente -1 para cada passo de tempo que passa antes da fuga; isso incentiva o agente a escapar o mais r√°pido poss√≠vel [^7].
3.  **Robot Recycling Cans:** Para fazer um rob√¥ aprender a encontrar e coletar latas de refrigerante vazias para reciclagem, pode-se dar uma recompensa de zero na maior parte do tempo e, em seguida, uma recompensa de +1 para cada lata coletada [^7]. Tamb√©m se pode querer dar ao rob√¥ recompensas negativas quando ele bate em coisas ou quando algu√©m grita com ele [^7].

![Representa√ß√£o do sistema de coleta de latas como um MDP finito, ilustrando as transi√ß√µes de estado e recompensas.](./../images/image4.png)

4.  **Game Playing:** Para um agente aprender a jogar damas ou xadrez, as recompensas naturais s√£o +1 para ganhar, -1 para perder e 0 para empatar e para todas as posi√ß√µes n√£o terminais [^7].

> üí° **Exemplo Num√©rico:** Considere um jogo simples onde o agente recebe +1 por ganhar, -1 por perder e 0 em outros momentos. Se o agente joga 3 partidas e obt√©m as seguintes recompensas: [0, -1, +1], o retorno cumulativo sem desconto ($\gamma = 1$) √© 0 - 1 + 1 = 0. Com um desconto de $\gamma = 0.9$, o retorno seria $0 + (-1 * 0.9) + (1 * 0.9^2) = -0.9 + 0.81 = -0.09$. Este exemplo ilustra como o fator de desconto influencia a avalia√ß√£o das recompensas futuras.

Em todos esses exemplos, o agente sempre aprende a maximizar sua recompensa [^7]. Se queremos que ele fa√ßa algo por n√≥s, devemos fornecer recompensas para ele de tal forma que, ao maximiz√°-las, o agente tamb√©m alcance nossos objetivos [^7]. √â, portanto, crucial que as recompensas que configuramos indiquem verdadeiramente o que queremos que seja realizado [^8]. Em particular, o sinal de recompensa n√£o √© o lugar para transmitir ao agente conhecimento pr√©vio sobre *como* alcan√ßar o que queremos que ele fa√ßa [^8]. Por exemplo, um agente de jogo de xadrez deve ser recompensado apenas por realmente vencer, n√£o por atingir sub-objetivos, como tomar as pe√ßas do oponente ou controlar o centro do tabuleiro [^8]. Se alcan√ßar esses tipos de sub-objetivos fosse recompensado, ent√£o o agente poderia encontrar uma maneira de alcan√ß√°-los sem atingir o objetivo real [^8]. Por exemplo, ele pode encontrar uma maneira de tomar as pe√ßas do oponente, mesmo ao custo de perder o jogo [^8]. *O sinal de recompensa √© sua maneira de comunicar ao agente o que voc√™ quer que seja alcan√ßado, n√£o como voc√™ quer que seja alcan√ßado* [^8].

Para complementar essa discuss√£o sobre como formular recompensas, podemos introduzir o conceito de *recompensas esparsas* e suas implica√ß√µes.

**Recompensas Esparsas:**

Em muitos problemas do mundo real, obter um sinal de recompensa significativo √© raro. Nesses casos, o agente recebe recompensas zero ou muito pequenas na maioria dos passos de tempo e uma recompensa significativa apenas quando atinge um objetivo espec√≠fico. Esses ambientes s√£o caracterizados por *recompensas esparsas*.

> üí° **Exemplo Num√©rico:** Imagine um rob√¥ tentando pegar um objeto em uma sala. Ele recebe uma recompensa de +1 apenas quando consegue pegar o objeto. Se ele tenta 100 vezes e s√≥ pega o objeto uma vez, ele recebe 99 recompensas de 0 e uma recompensa de +1. Este √© um exemplo de recompensa esparsa.

**Desafios das Recompensas Esparsas:**

O principal desafio em ambientes com recompensas esparsas √© a dificuldade de aprendizado. Como o agente raramente recebe feedback positivo, pode levar um tempo excessivamente longo para descobrir a√ß√µes que levam a recompensas. Isso resulta em uma explora√ß√£o ineficiente do espa√ßo de estados-a√ß√µes.

**Estrat√©gias para Lidar com Recompensas Esparsas:**

V√°rias t√©cnicas podem ser empregadas para mitigar os problemas associados a recompensas esparsas:

*   **Shaping de Recompensa:** Introduzir recompensas intermedi√°rias para guiar o agente em dire√ß√£o ao objetivo. No entanto, √© crucial projetar o shaping de recompensa cuidadosamente para evitar consequ√™ncias indesejadas, conforme mencionado anteriormente.
*   **Curriculum Learning:** Treinar o agente em uma sequ√™ncia de tarefas progressivamente mais dif√≠ceis. Isso permite que o agente aprenda habilidades b√°sicas em ambientes mais simples antes de enfrentar o problema complexo com recompensas esparsas.
*   **Explora√ß√£o Intr√≠nseca:** Incentivar a explora√ß√£o do agente, recompensando-o por visitar estados novos ou executar a√ß√µes inesperadas. Isso pode ajudar o agente a descobrir caminhos para recompensas mesmo em ambientes esparsos. Exemplos incluem *curiosity-driven exploration* e *novelty search*.
*   **Aprendizado por Imita√ß√£o:** Utilizar demonstra√ß√µes de um especialista para inicializar o aprendizado do agente. Ao imitar as a√ß√µes do especialista, o agente pode aprender a atingir o objetivo mais rapidamente, mesmo com recompensas esparsas.

> üí° **Exemplo Num√©rico:** Em um ambiente com recompensas esparsas, um agente pode inicialmente explorar aleatoriamente. Sem explora√ß√£o intr√≠nseca, ele pode receber [0, 0, 0, 0, 0, 1] como recompensas ap√≥s 6 passos, onde 1 √© a recompensa rara. Ao adicionar um b√¥nus de curiosidade (e.g., +0.1 para visitar um novo estado), as recompensas podem se tornar [0.1, 0.1, 0.1, 0.1, 0.1, 1.1]. Isso incentiva o agente a continuar explorando e aprender mais r√°pido.

### Conclus√£o

A **reward hypothesis** n√£o √© isenta de cr√≠ticas, mas sua efic√°cia em uma ampla gama de problemas de reinforcement learning √© ineg√°vel [^7]. Ela fornece uma base s√≥lida para projetar agentes inteligentes capazes de aprender e se adaptar a ambientes complexos.
Ao formular o problema em termos de maximiza√ß√£o de recompensa, o framework de MDP permite a aplica√ß√£o de t√©cnicas matem√°ticas e computacionais para encontrar **pol√≠ticas √≥timas** [^18]. A escolha cuidadosa de sinais de recompensa √© essencial para garantir que o agente aprenda o comportamento desejado [^8].
Em ess√™ncia, a reward hypothesis oferece um meio poderoso e flex√≠vel de formalizar o conceito de objetivo em reinforcement learning [^7]. Atrav√©s da manipula√ß√£o cuidadosa dos sinais de recompensa, √© poss√≠vel direcionar agentes de reinforcement learning para aprender e executar uma ampla variedade de tarefas complexas [^7].

Para formalizar a ideia de maximiza√ß√£o da recompensa cumulativa, podemos definir o conceito de *retorno*.

**Defini√ß√£o de Retorno:**

O *retorno* $G_t$ √© definido como a soma das recompensas futuras, descontadas por um fator $\gamma \in [0, 1]$:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

O fator de desconto $\gamma$ determina a import√¢ncia das recompensas futuras. Se $\gamma = 0$, o agente se preocupa apenas com a recompensa imediata. Se $\gamma = 1$, todas as recompensas futuras s√£o consideradas igualmente importantes.

Com essa defini√ß√£o, a reward hypothesis pode ser reformulada como: o objetivo do agente √© maximizar o valor esperado do retorno $G_t$. Essa formula√ß√£o √© fundamental para o desenvolvimento de algoritmos de reinforcement learning que visam encontrar pol√≠ticas √≥timas.

> üí° **Exemplo Num√©rico:** Suponha que um agente receba as seguintes recompensas ao longo de 5 passos: [1, 0, -1, 2, 1]. Se o fator de desconto $\gamma = 0.9$, o retorno no tempo t=0 seria:
> $G_0 = 1 + 0.9 * 0 + 0.9^2 * (-1) + 0.9^3 * 2 + 0.9^4 * 1 = 1 + 0 - 0.81 + 1.458 + 0.6561 = 2.3041$.
> Se $\gamma = 0$, o retorno no tempo t=0 seria simplesmente $G_0 = 1$.

**Prova da soma do retorno:**

A equa√ß√£o para o retorno $G_t$ pode ser escrita de forma recursiva, o que √© √∫til para muitas deriva√ß√µes e algoritmos em reinforcement learning. Vamos provar a forma recursiva do retorno.

I. Come√ßamos com a defini√ß√£o de retorno:
   $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots$$

II. Podemos fatorar $\gamma$ do segundo termo em diante:
    $$G_t = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \ldots)$$

III. Note que o termo entre par√™nteses √© exatamente a defini√ß√£o de $G_{t+1}$:
     $$G_{t+1} = R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \ldots$$

IV. Substituindo $G_{t+1}$ na equa√ß√£o de $G_t$:
    $$G_t = R_{t+1} + \gamma G_{t+1}$$

V. Portanto, provamos que o retorno $G_t$ pode ser expresso recursivamente como $G_t = R_{t+1} + \gamma G_{t+1}$. ‚ñ†

### Refer√™ncias
[^1]: Chapter 3: Finite Markov Decision Processes
[^7]: Section 3.2 Goals and Rewards
[^8]: Section 3.2 Goals and Rewards
[^18]: Chapter 4: Dynamic Programming
<!-- END -->