## Goals and Rewards in Reinforcement Learning: Formalizing the Objective

### IntroduÃ§Ã£o
Em reinforcement learning, a formalizaÃ§Ã£o do objetivo do agente Ã© crucial para direcionar o aprendizado e o comportamento. Este capÃ­tulo explora como os **goals** e **rewards** sÃ£o definidos dentro do framework de **Markov Decision Processes (MDPs)**, focando na **reward hypothesis** como um elemento distintivo do reinforcement learning [^1]. A hipÃ³tese propÃµe que *todos os objetivos e propÃ³sitos podem ser bem representados como a maximizaÃ§Ã£o do valor esperado da soma cumulativa de um sinal escalar recebido (a recompensa)* [^1]. Este capÃ­tulo aprofunda essa hipÃ³tese, explorando suas implicaÃ§Ãµes e exemplos prÃ¡ticos.

### Conceitos Fundamentais

A **reward hypothesis** Ã© um dos pilares do reinforcement learning, fornecendo uma forma clara e concisa de definir o que o agente deve alcanÃ§ar. Ao contrÃ¡rio de outras abordagens de aprendizado de mÃ¡quina, o reinforcement learning depende fortemente de um sinal de recompensa externo para guiar o aprendizado.
No reinforcement learning, o propÃ³sito ou objetivo do agente Ã© formalizado em termos de um sinal especial, chamado **recompensa**, que passa do ambiente para o agente [^7]. A cada passo de tempo, a recompensa Ã© um nÃºmero simples, $R_t \in \mathbb{R}$ [^7]. Informalmente, o objetivo do agente Ã© maximizar a quantidade total de recompensa que ele recebe [^7]. Isso significa maximizar nÃ£o a recompensa imediata, mas a recompensa cumulativa a longo prazo [^7]. Podemos declarar claramente essa ideia informal como a **hipÃ³tese da recompensa** [^7]:

>Que tudo o que queremos dizer por objetivos e propÃ³sitos pode ser bem pensado como a maximizaÃ§Ã£o do valor esperado da soma cumulativa de um sinal escalar recebido (chamado de recompensa) [^7].

A utilizaÃ§Ã£o de um sinal de recompensa para formalizar a ideia de um objetivo Ã© uma das caracterÃ­sticas mais distintas do reinforcement learning [^7]. Embora formular objetivos em termos de sinais de recompensa possa parecer limitante Ã  primeira vista, na prÃ¡tica se mostrou flexÃ­vel e amplamente aplicÃ¡vel [^7].



![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

**Exemplos PrÃ¡ticos:**

1.  **Robot Learning to Walk:** Para fazer um robÃ´ aprender a andar, pesquisadores forneceram recompensa em cada passo de tempo proporcional ao movimento para frente do robÃ´ [^7].
2.  **Robot Escaping a Maze:** Para fazer um robÃ´ aprender a escapar de um labirinto, a recompensa Ã© frequentemente -1 para cada passo de tempo que passa antes da fuga; isso incentiva o agente a escapar o mais rÃ¡pido possÃ­vel [^7].
3.  **Robot Recycling Cans:** Para fazer um robÃ´ aprender a encontrar e coletar latas de refrigerante vazias para reciclagem, pode-se dar uma recompensa de zero na maior parte do tempo e, em seguida, uma recompensa de +1 para cada lata coletada [^7]. TambÃ©m se pode querer dar ao robÃ´ recompensas negativas quando ele bate em coisas ou quando alguÃ©m grita com ele [^7].

![RepresentaÃ§Ã£o do sistema de coleta de latas como um MDP finito, ilustrando as transiÃ§Ãµes de estado e recompensas.](./../images/image4.png)

4.  **Game Playing:** Para um agente aprender a jogar damas ou xadrez, as recompensas naturais sÃ£o +1 para ganhar, -1 para perder e 0 para empatar e para todas as posiÃ§Ãµes nÃ£o terminais [^7].

> ðŸ’¡ **Exemplo NumÃ©rico:** Considere um jogo simples onde o agente recebe +1 por ganhar, -1 por perder e 0 em outros momentos. Se o agente joga 3 partidas e obtÃ©m as seguintes recompensas: [0, -1, +1], o retorno cumulativo sem desconto ($\gamma = 1$) Ã© 0 - 1 + 1 = 0. Com um desconto de $\gamma = 0.9$, o retorno seria $0 + (-1 * 0.9) + (1 * 0.9^2) = -0.9 + 0.81 = -0.09$. Este exemplo ilustra como o fator de desconto influencia a avaliaÃ§Ã£o das recompensas futuras.

Em todos esses exemplos, o agente sempre aprende a maximizar sua recompensa [^7]. Se queremos que ele faÃ§a algo por nÃ³s, devemos fornecer recompensas para ele de tal forma que, ao maximizÃ¡-las, o agente tambÃ©m alcance nossos objetivos [^7]. Ã‰, portanto, crucial que as recompensas que configuramos indiquem verdadeiramente o que queremos que seja realizado [^8]. Em particular, o sinal de recompensa nÃ£o Ã© o lugar para transmitir ao agente conhecimento prÃ©vio sobre *como* alcanÃ§ar o que queremos que ele faÃ§a [^8]. Por exemplo, um agente de jogo de xadrez deve ser recompensado apenas por realmente vencer, nÃ£o por atingir sub-objetivos, como tomar as peÃ§as do oponente ou controlar o centro do tabuleiro [^8]. Se alcanÃ§ar esses tipos de sub-objetivos fosse recompensado, entÃ£o o agente poderia encontrar uma maneira de alcanÃ§Ã¡-los sem atingir o objetivo real [^8]. Por exemplo, ele pode encontrar uma maneira de tomar as peÃ§as do oponente, mesmo ao custo de perder o jogo [^8]. *O sinal de recompensa Ã© sua maneira de comunicar ao agente o que vocÃª quer que seja alcanÃ§ado, nÃ£o como vocÃª quer que seja alcanÃ§ado* [^8].

Para complementar essa discussÃ£o sobre como formular recompensas, podemos introduzir o conceito de *recompensas esparsas* e suas implicaÃ§Ãµes.

**Recompensas Esparsas:**

Em muitos problemas do mundo real, obter um sinal de recompensa significativo Ã© raro. Nesses casos, o agente recebe recompensas zero ou muito pequenas na maioria dos passos de tempo e uma recompensa significativa apenas quando atinge um objetivo especÃ­fico. Esses ambientes sÃ£o caracterizados por *recompensas esparsas*.

> ðŸ’¡ **Exemplo NumÃ©rico:** Imagine um robÃ´ tentando pegar um objeto em uma sala. Ele recebe uma recompensa de +1 apenas quando consegue pegar o objeto. Se ele tenta 100 vezes e sÃ³ pega o objeto uma vez, ele recebe 99 recompensas de 0 e uma recompensa de +1. Este Ã© um exemplo de recompensa esparsa.

**Desafios das Recompensas Esparsas:**

O principal desafio em ambientes com recompensas esparsas Ã© a dificuldade de aprendizado. Como o agente raramente recebe feedback positivo, pode levar um tempo excessivamente longo para descobrir aÃ§Ãµes que levam a recompensas. Isso resulta em uma exploraÃ§Ã£o ineficiente do espaÃ§o de estados-aÃ§Ãµes.

**EstratÃ©gias para Lidar com Recompensas Esparsas:**

VÃ¡rias tÃ©cnicas podem ser empregadas para mitigar os problemas associados a recompensas esparsas:

*   **Shaping de Recompensa:** Introduzir recompensas intermediÃ¡rias para guiar o agente em direÃ§Ã£o ao objetivo. No entanto, Ã© crucial projetar o shaping de recompensa cuidadosamente para evitar consequÃªncias indesejadas, conforme mencionado anteriormente.
*   **Curriculum Learning:** Treinar o agente em uma sequÃªncia de tarefas progressivamente mais difÃ­ceis. Isso permite que o agente aprenda habilidades bÃ¡sicas em ambientes mais simples antes de enfrentar o problema complexo com recompensas esparsas.
*   **ExploraÃ§Ã£o IntrÃ­nseca:** Incentivar a exploraÃ§Ã£o do agente, recompensando-o por visitar estados novos ou executar aÃ§Ãµes inesperadas. Isso pode ajudar o agente a descobrir caminhos para recompensas mesmo em ambientes esparsos. Exemplos incluem *curiosity-driven exploration* e *novelty search*.
*   **Aprendizado por ImitaÃ§Ã£o:** Utilizar demonstraÃ§Ãµes de um especialista para inicializar o aprendizado do agente. Ao imitar as aÃ§Ãµes do especialista, o agente pode aprender a atingir o objetivo mais rapidamente, mesmo com recompensas esparsas.

> ðŸ’¡ **Exemplo NumÃ©rico:** Em um ambiente com recompensas esparsas, um agente pode inicialmente explorar aleatoriamente. Sem exploraÃ§Ã£o intrÃ­nseca, ele pode receber [0, 0, 0, 0, 0, 1] como recompensas apÃ³s 6 passos, onde 1 Ã© a recompensa rara. Ao adicionar um bÃ´nus de curiosidade (e.g., +0.1 para visitar um novo estado), as recompensas podem se tornar [0.1, 0.1, 0.1, 0.1, 0.1, 1.1]. Isso incentiva o agente a continuar explorando e aprender mais rÃ¡pido.

### ConclusÃ£o

A **reward hypothesis** nÃ£o Ã© isenta de crÃ­ticas, mas sua eficÃ¡cia em uma ampla gama de problemas de reinforcement learning Ã© inegÃ¡vel [^7]. Ela fornece uma base sÃ³lida para projetar agentes inteligentes capazes de aprender e se adaptar a ambientes complexos.
Ao formular o problema em termos de maximizaÃ§Ã£o de recompensa, o framework de MDP permite a aplicaÃ§Ã£o de tÃ©cnicas matemÃ¡ticas e computacionais para encontrar **polÃ­ticas Ã³timas** [^18]. A escolha cuidadosa de sinais de recompensa Ã© essencial para garantir que o agente aprenda o comportamento desejado [^8].
Em essÃªncia, a reward hypothesis oferece um meio poderoso e flexÃ­vel de formalizar o conceito de objetivo em reinforcement learning [^7]. AtravÃ©s da manipulaÃ§Ã£o cuidadosa dos sinais de recompensa, Ã© possÃ­vel direcionar agentes de reinforcement learning para aprender e executar uma ampla variedade de tarefas complexas [^7].

Para formalizar a ideia de maximizaÃ§Ã£o da recompensa cumulativa, podemos definir o conceito de *retorno*.

**DefiniÃ§Ã£o de Retorno:**

O *retorno* $G_t$ Ã© definido como a soma das recompensas futuras, descontadas por um fator $\gamma \in [0, 1]$:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

O fator de desconto $\gamma$ determina a importÃ¢ncia das recompensas futuras. Se $\gamma = 0$, o agente se preocupa apenas com a recompensa imediata. Se $\gamma = 1$, todas as recompensas futuras sÃ£o consideradas igualmente importantes.

Com essa definiÃ§Ã£o, a reward hypothesis pode ser reformulada como: o objetivo do agente Ã© maximizar o valor esperado do retorno $G_t$. Essa formulaÃ§Ã£o Ã© fundamental para o desenvolvimento de algoritmos de reinforcement learning que visam encontrar polÃ­ticas Ã³timas.

> ðŸ’¡ **Exemplo NumÃ©rico:** Suponha que um agente receba as seguintes recompensas ao longo de 5 passos: [1, 0, -1, 2, 1]. Se o fator de desconto $\gamma = 0.9$, o retorno no tempo t=0 seria:
> $G_0 = 1 + 0.9 * 0 + 0.9^2 * (-1) + 0.9^3 * 2 + 0.9^4 * 1 = 1 + 0 - 0.81 + 1.458 + 0.6561 = 2.3041$.
> Se $\gamma = 0$, o retorno no tempo t=0 seria simplesmente $G_0 = 1$.

**Prova da soma do retorno:**

A equaÃ§Ã£o para o retorno $G_t$ pode ser escrita de forma recursiva, o que Ã© Ãºtil para muitas derivaÃ§Ãµes e algoritmos em reinforcement learning. Vamos provar a forma recursiva do retorno.

I. ComeÃ§amos com a definiÃ§Ã£o de retorno:
   $$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots$$

II. Podemos fatorar $\gamma$ do segundo termo em diante:
    $$G_t = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \ldots)$$

III. Note que o termo entre parÃªnteses Ã© exatamente a definiÃ§Ã£o de $G_{t+1}$:
     $$G_{t+1} = R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + \ldots$$

IV. Substituindo $G_{t+1}$ na equaÃ§Ã£o de $G_t$:
    $$G_t = R_{t+1} + \gamma G_{t+1}$$

V. Portanto, provamos que o retorno $G_t$ pode ser expresso recursivamente como $G_t = R_{t+1} + \gamma G_{t+1}$. â– 

### ReferÃªncias
[^1]: Chapter 3: Finite Markov Decision Processes
[^7]: Section 3.2 Goals and Rewards
[^8]: Section 3.2 Goals and Rewards
[^18]: Chapter 4: Dynamic Programming
<!-- END -->