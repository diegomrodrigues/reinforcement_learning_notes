## O Papel Crucial do Sinal de Recompensa na Defini√ß√£o de Objetivos em MDPs Finitos

### Introdu√ß√£o
Em processos de decis√£o de Markov finitos (MDPs finitos), o **sinal de recompensa** desempenha um papel fundamental na defini√ß√£o dos objetivos do agente. Conforme discutido no Cap√≠tulo 3, Se√ß√£o 3.2, o objetivo do agente √© maximizar a quantidade total de recompensa que recebe [^7]. No entanto, a maneira como esse sinal de recompensa √© estruturado √© crucial para garantir que o agente aprenda a atingir os resultados desejados. Especificamente, o sinal de recompensa deve indicar *o que* o agente deve alcan√ßar, e n√£o *como* deve alcan√ß√°-lo.

### Conceitos Fundamentais
A import√¢ncia de fornecer um sinal de recompensa bem definido reside no fato de que o agente aprende a maximizar esse sinal por meio da intera√ß√£o com o ambiente. Se o sinal de recompensa for mal definido ou direcionado para sub-objetivos, o agente pode encontrar maneiras de maximizar a recompensa sem realmente atingir o objetivo final desejado [^54].

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

Para ilustrar este ponto, considere o exemplo de um agente de xadrez [^54]. Se o agente for recompensado por capturar pe√ßas do oponente, ele pode aprender a se concentrar na captura de pe√ßas, mesmo que isso leve √† perda do jogo. Em vez disso, o agente deve ser recompensado apenas por vencer o jogo. Dessa forma, o agente √© incentivado a desenvolver estrat√©gias que levem √† vit√≥ria, e n√£o a simplesmente capturar pe√ßas sem um objetivo maior.

> üí° **Exemplo Num√©rico:** Imagine um agente de xadrez.
>
> *   **Recompensa por Captura:** +1 por pe√ßa capturada, -100 por perder o jogo.
> *   **Recompensa por Vit√≥ria:** 0 por a√ß√£o, +100 por vencer o jogo, -100 por perder.
>
> Sob a recompensa por captura, o agente pode maximizar capturando muitas pe√ßas, mas entrando em situa√ß√µes desfavor√°veis que levam √† derrota.  Sob a recompensa por vit√≥ria, ele √© diretamente incentivado a vencer o jogo.  Digamos que em um jogo, o agente captura 5 pe√ßas, mas perde. A recompensa total seria 5 - 100 = -95.  Se ele vencesse o jogo, mesmo sem capturar pe√ßas, a recompensa seria +100. Isso incentiva o objetivo final de vencer, e n√£o apenas de capturar pe√ßas.

*√â fundamental que as recompensas que configuramos indiquem verdadeiramente o que queremos realizar* [^54]. O sinal de recompensa n√£o √© o lugar para transmitir ao agente o conhecimento pr√©vio sobre *como* atingir o que queremos que ele fa√ßa [^54].

**O sinal de recompensa √© sua forma de comunicar ao agente o que voc√™ deseja alcan√ßar, n√£o como voc√™ deseja que seja alcan√ßado** [^54].

Considere outro exemplo, o de um rob√¥ que deve aprender a escapar de um labirinto [^53]. Se o rob√¥ receber uma recompensa negativa para cada passo que d√° dentro do labirinto, ele ser√° incentivado a escapar o mais r√°pido poss√≠vel. No entanto, se o rob√¥ receber uma recompensa positiva por explorar diferentes √°reas do labirinto, ele poder√° aprender a vagar sem rumo, em vez de se concentrar em encontrar a sa√≠da.

> üí° **Exemplo Num√©rico:** Um rob√¥ em um labirinto de 10x10.
>
> *   **Recompensa Negativa por Passo:** -1 por passo.
> *   **Recompensa Positiva por Explora√ß√£o:** +0.1 por visitar uma c√©lula nova.
>
> Com a recompensa negativa por passo, o rob√¥ aprende a encontrar o caminho mais curto para a sa√≠da. Com a recompensa positiva por explora√ß√£o, ele pode "vagar" pelo labirinto visitando o m√°ximo de c√©lulas poss√≠vel antes de sair, mesmo que isso n√£o seja o que se deseja.

A defini√ß√£o correta do sinal de recompensa tamb√©m est√° intimamente ligada ao **vi√©s** que √© introduzido no processo de aprendizado. Por exemplo, se o agente de xadrez √© recompensado por controlar o centro do tabuleiro, ele pode aprender uma estrat√©gia que priorize o controle central, mesmo que existam outras estrat√©gias mais eficazes. Este tipo de vi√©s pode ser ben√©fico em alguns casos, auxiliando a explora√ß√£o, mas pode tamb√©m impedir que o agente descubra solu√ß√µes √≥timas.

Para formalizar a no√ß√£o de um sinal de recompensa "bem definido", podemos introduzir o conceito de *recompensa esparsa* versus *recompensa densa*. Um sinal de recompensa esparso fornece recompensa apenas quando o objetivo final √© alcan√ßado (e.g., vencer o jogo de xadrez), enquanto um sinal de recompensa denso fornece recompensa por sub-objetivos ou a√ß√µes que se aproximam do objetivo final (e.g., capturar pe√ßas no xadrez). Embora recompensas densas possam acelerar o aprendizado em alguns casos, elas tamb√©m podem levar a pol√≠ticas sub√≥timas, como discutido anteriormente.

> üí° **Exemplo Num√©rico:** Compara√ß√£o de recompensa esparsa vs. densa em um problema de navega√ß√£o.
>
> *   **Cen√°rio:** Um agente deve navegar de um ponto A para um ponto B.
> *   **Recompensa Esparsa:** +1 no ponto B, 0 caso contr√°rio.
> *   **Recompensa Densa:** -0.1 por passo, +1 no ponto B.
>
> A recompensa esparsa pode levar mais tempo para convergir, mas garante que o agente encontre o caminho mais eficiente uma vez que aprende. A recompensa densa fornece um sinal mais imediato, o que pode acelerar o aprendizado inicial, mas pode levar o agente a adotar caminhos mais longos para evitar a penalidade de -0.1 por passo.

**Defini√ß√£o:** Um sinal de recompensa √© considerado **esparso** se a recompensa √© significativamente diferente de zero apenas em um subconjunto pequeno do espa√ßo de estados. Caso contr√°rio, o sinal de recompensa √© considerado **denso**.

√â importante notar que a escolha entre um sinal de recompensa esparso ou denso depende do problema em quest√£o. Problemas com espa√ßos de estados grandes e objetivos complexos podem se beneficiar de recompensas densas no in√≠cio do aprendizado, para guiar o agente em dire√ß√£o ao objetivo. No entanto, √† medida que o agente aprende, pode ser ben√©fico mudar para um sinal de recompensa mais esparso para evitar que o agente se fixe em estrat√©gias sub√≥timas.

**Teorema 1:** *Em um MDP finito, um agente que maximiza um sinal de recompensa denso n√£o necessariamente maximizar√° a recompensa obtida em um sinal de recompensa esparso definido sobre o mesmo objetivo final.*

*Prova (Esbo√ßo):* Considere um MDP onde o agente pode atingir o objetivo final *G* por meio de dois caminhos distintos, *A* e *B*. O caminho *A* leva a *G* em poucos passos, mas envolve a√ß√µes que n√£o geram recompensa intermedi√°ria. O caminho *B* leva a *G* em mais passos, mas envolve a√ß√µes que geram recompensas intermedi√°rias. Um sinal de recompensa denso pode levar o agente a preferir o caminho *B*, mesmo que o caminho *A* seja mais eficiente em termos de custo total (assumindo que atingir *G* √© o objetivo principal). Portanto, maximizar a recompensa densa n√£o garante a maximiza√ß√£o da recompensa esparsa. $\blacksquare$

Podemos tornar esta prova mais formal da seguinte forma:

**Prova formal do Teorema 1:**

I. Seja um MDP definido por $(S, A, P, R, \gamma)$, onde:
    *  $S$ √© o conjunto de estados.
    *  $A$ √© o conjunto de a√ß√µes.
    *  $P(s'|s, a)$ √© a probabilidade de transi√ß√£o para o estado $s'$ ao realizar a a√ß√£o $a$ no estado $s$.
    *  $R(s, a)$ √© a fun√ß√£o de recompensa.
    *  $\gamma \in [0, 1]$ √© o fator de desconto.

II. Suponha que o objetivo final seja atingir o estado $G \in S$. Existem dois caminhos para chegar a $G$: $A$ e $B$.

III. O caminho $A$ tem comprimento $n_A$ e a sequ√™ncia de estados e a√ß√µes √©: $s_0, a_0, s_1, a_1, \ldots, s_{n_A-1}, a_{n_A-1}, G$.  O caminho $B$ tem comprimento $n_B$ e a sequ√™ncia de estados e a√ß√µes √©: $s'_0, a'_0, s'_1, a'_1, \ldots, s'_{n_B-1}, a'_{n_B-1}, G$.  Assumimos que $n_A < n_B$.

IV. Defina duas fun√ß√µes de recompensa: uma esparsa $R_s(s, a)$ e uma densa $R_d(s, a)$.
    *  $R_s(s, a) = \begin{cases} 1, & \text{se } s' = G \\ 0, & \text{caso contr√°rio} \end{cases}$
    *  $R_d(s, a) = \begin{cases} \epsilon, & \text{se } (s, a) \text{ pertence ao caminho } B \text{ e } s' \neq G \\ 1, & \text{se } s' = G \\ 0, & \text{caso contr√°rio} \end{cases}$, onde $\epsilon > 0$ √© uma pequena recompensa.

V. Seja $V^*(s)$ o valor √≥timo de um estado $s$. Sob a recompensa esparsa $R_s$, o valor √≥timo de $s_0$ √© $V_s^*(s_0) = \gamma^{n_A}$. Sob a recompensa densa $R_d$, o valor de seguir o caminho $B$ a partir de $s'_0$ √© $V_d(s'_0) = \epsilon \sum_{t=0}^{n_B-1} \gamma^t + \gamma^{n_B}$.

VI. Para mostrar que maximizar a recompensa densa n√£o implica maximizar a recompensa esparsa, precisamos encontrar condi√ß√µes sob as quais $V_d(s'_0) > V_s^*(s_0)$. Ou seja:
    $$\epsilon \sum_{t=0}^{n_B-1} \gamma^t + \gamma^{n_B} > \gamma^{n_A}$$
    $$\epsilon \frac{1 - \gamma^{n_B}}{1 - \gamma} > \gamma^{n_A} - \gamma^{n_B}$$
    $$\epsilon > \frac{(\gamma^{n_A} - \gamma^{n_B})(1 - \gamma)}{1 - \gamma^{n_B}}$$
    Como $n_A < n_B$, $\gamma^{n_A} > \gamma^{n_B}$, ent√£o o lado direito √© positivo. Portanto, existe um $\epsilon > 0$ que satisfaz essa desigualdade.

VII. Isso mostra que, para algum $\epsilon$, o agente pode preferir o caminho $B$ (que maximiza a recompensa densa) ao caminho $A$ (que levaria a atingir o objetivo *G* mais rapidamente, e, portanto, maximizar a recompensa esparsa).

VIII. Portanto, um agente que maximiza um sinal de recompensa denso n√£o necessariamente maximizar√° a recompensa obtida em um sinal de recompensa esparso definido sobre o mesmo objetivo final. $\blacksquare$

A escolha de um sinal de recompensa apropriado √©, portanto, uma tarefa crucial e desafiadora no design de agentes de aprendizado por refor√ßo.

> üí° **Exemplo Num√©rico:** Consideremos $\gamma = 0.9$, $n_A = 5$ e $n_B = 10$.  Ent√£o, $V_s^*(s_0) = 0.9^5 \approx 0.5905$ e para que $V_d(s'_0) > V_s^*(s_0)$, devemos ter:
>
> $\epsilon > \frac{(0.9^5 - 0.9^{10})(1 - 0.9)}{1 - 0.9^{10}} \approx \frac{(0.5905 - 0.3487)(0.1)}{1 - 0.3487} \approx 0.0371$.
>
> Portanto, se $\epsilon > 0.0371$, o agente ir√° preferir o caminho $B$ (mais longo com recompensa densa) ao caminho $A$ (mais curto com recompensa esparsa).

Al√©m disso, √© importante considerar a **escala** das recompensas. Recompensas muito pequenas podem tornar o aprendizado lento ou inst√°vel, enquanto recompensas muito grandes podem levar a comportamentos de risco ou explora√ß√£o excessiva. T√©cnicas como a normaliza√ß√£o de recompensas podem ser usadas para garantir que as recompensas estejam em uma escala apropriada.

> üí° **Exemplo Num√©rico:** Um agente que aprende a dirigir um carro.
>
> *   **Recompensa n√£o normalizada:** +1000 por chegar ao destino, -1 por segundo de atraso, -100 por colis√£o.
> *   **Recompensa normalizada:** +1 por chegar ao destino, -0.001 por segundo de atraso, -0.1 por colis√£o.
>
> Na recompensa n√£o normalizada, o agente pode priorizar fortemente chegar ao destino, mesmo que isso signifique dirigir de forma imprudente e causar v√°rias colis√µes pequenas. A recompensa normalizada equilibra os diferentes objetivos, incentivando o agente a chegar ao destino de forma r√°pida e segura.

A seguir, alguns exemplos de representa√ß√µes visuais de MDPs, que ajudam a ilustrar como os sinais de recompensa interagem com o ambiente:

![Diagrama de transi√ß√£o de estados para um MDP simples com a√ß√µes e recompensas associadas.](./../images/image6.png)

![Representa√ß√£o do sistema de coleta de latas como um MDP finito, ilustrando as transi√ß√µes de estado e recompensas.](./../images/image4.png)

![Diagrama de transi√ß√£o de estados ilustrando um MDP com um estado terminal absorvente.](./../images/image9.png)

E a seguir, diagramas de backup que auxiliam no entendimento do c√°lculo dos valores das fun√ß√µes:

![Backup diagrams illustrating the update rules for state-value \(v_*\) and action-value \(q_*\) functions in MDPs.](./../images/image1.png)

![Diagrama de backup para a fun√ß√£o de valor \(v_\pi\), ilustrando a rela√ß√£o entre um estado e seus sucessores sob uma pol√≠tica \(\pi\).](./../images/image3.png)

![Diagrama de backup qœÄ ilustrando as rela√ß√µes entre estados, a√ß√µes e recompensas em reinforcement learning.](./../images/image10.png)

Tamb√©m podemos ilustrar a √°rvore de decis√£o:

![Decision tree illustrating the relationship between state-action pairs, rewards, and subsequent states in an MDP.](./../images/image5.png)

Finalmente, exemplos de gridworld e o exemplo do golfe que mostram a fun√ß√£o valor:

![Exemplo de Gridworld demonstrando din√¢micas de recompensa e fun√ß√£o de valor de estado para uma pol√≠tica equiprov√°vel.](./../images/image11.png)

![Optimal solutions to the gridworld example, illustrating the optimal policy and value function.](./../images/image2.png)

![State-value function for putting (upper) and optimal action-value function for using the driver (lower) in a golf scenario.](./../images/image8.png)

### Conclus√£o
Em resumo, o sinal de recompensa √© um componente essencial em MDPs finitos. Ele define o objetivo do agente e influencia diretamente o comportamento aprendido. Ao projetar um sinal de recompensa, √© crucial focar em *o que* se deseja que o agente alcance, em vez de *como* ele deve faz√™-lo. Isso evita que o agente aprenda estrat√©gias sub√≥timas que maximizam a recompensa sem atingir o objetivo final. Um sinal de recompensa bem definido garante que o agente aprenda a atingir os resultados desejados de forma eficiente e eficaz. A complexidade de definir recompensas apropriadas √© algo que as op√ß√µes de design de recompensa podem ser mais arte do que ci√™ncia [^51].

### Refer√™ncias
[^7]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^51]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^53]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
[^54]: Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*. MIT press.
<!-- END -->