## Metas e Recompensas em Processos de Decis√£o de Markov Finitos

### Introdu√ß√£o
Em Reinforcement Learning (RL), o conceito de **recompensa** √© fundamental para definir o objetivo que o agente deve perseguir. Ao contr√°rio do aprendizado supervisionado, onde o agente recebe feedback expl√≠cito sobre a a√ß√£o correta, em RL o agente recebe apenas um sinal de recompensa, que indica a qualidade da a√ß√£o tomada em rela√ß√£o ao objetivo geral [^53]. Este cap√≠tulo aprofunda a formaliza√ß√£o desse conceito, explorando como as recompensas s√£o usadas para guiar o agente em dire√ß√£o ao comportamento desejado.

### O Sinal de Recompensa
Em cada passo de tempo $t$, o agente recebe uma recompensa $R_t \in \mathbb{R}$ do ambiente [^53]. Esta recompensa √© um valor escalar que quantifica o qu√£o bem o agente est√° performando em rela√ß√£o ao objetivo definido. A escolha de recompensas apropriadas √© crucial, pois elas moldam o comportamento do agente.

![The agent-environment interaction in a Markov decision process.](./../images/image7.png)

> üí° **Exemplo Num√©rico:** Imagine um agente treinando para dirigir um carro aut√¥nomo. Uma poss√≠vel fun√ß√£o de recompensa poderia ser definida como:
> - +1 por cada segundo em que o carro se mant√©m na faixa correta.
> - -10 por sair da faixa.
> - -100 por colidir com outro objeto.
> Essa fun√ß√£o de recompensa incentiva o carro a permanecer na faixa e evitar colis√µes.

A *meta informal* do agente √© maximizar a quantidade total de recompensa que ele recebe ao longo do tempo. Isso significa que o agente n√£o deve se concentrar apenas em recompensas imediatas, mas deve considerar as consequ√™ncias de longo prazo de suas a√ß√µes [^53]. Essa considera√ß√£o √© capturada pela **hip√≥tese da recompensa**:

> Tudo o que queremos dizer por metas e prop√≥sitos pode ser bem pensado como a maximiza√ß√£o do valor esperado da soma cumulativa de um sinal escalar recebido (chamado recompensa) [^53].

Essa hip√≥tese implica que qualquer objetivo que o agente deva perseguir pode ser traduzido em um sinal de recompensa. Por exemplo, se quisermos que um rob√¥ aprenda a andar, podemos dar uma recompensa proporcional ao seu movimento para frente em cada passo de tempo [^53]. Se quisermos que um rob√¥ aprenda a escapar de um labirinto, podemos dar uma recompensa de -1 para cada passo de tempo que passa antes da fuga [^53].

Para refor√ßar essa ideia, podemos considerar um cen√°rio onde o objetivo √© que um agente aprenda a jogar um jogo de tabuleiro. A recompensa poderia ser +1 quando o agente ganha o jogo, -1 quando perde e 0 para todos os outros movimentos.

> üí° **Exemplo Num√©rico:** No jogo de xadrez, poder√≠amos definir a recompensa da seguinte forma:
> - +10: Ganhar o jogo
> - -10: Perder o jogo
> - 0: Empate ou qualquer outro movimento
> Esta fun√ß√£o de recompensa simples incentiva o agente a buscar a vit√≥ria e evitar a derrota. A dificuldade reside em que a recompensa √© esparsa, ocorrendo apenas no final do jogo.

### Formalizando o Objetivo
A *formaliza√ß√£o do objetivo* do agente envolve definir matematicamente o que significa maximizar a recompensa total. A forma mais simples de fazer isso √© definir o **retorno** $G_t$ como a soma das recompensas recebidas a partir do tempo $t+1$ at√© o final do epis√≥dio [^54]:

$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_T$$

onde $T$ √© o passo de tempo final. Essa abordagem √© apropriada para tarefas **epis√≥dicas**, onde a intera√ß√£o entre o agente e o ambiente se divide naturalmente em subsequ√™ncias, ou epis√≥dios, como jogos, viagens por um labirinto ou qualquer tipo de intera√ß√£o repetida [^54]. Cada epis√≥dio termina em um estado especial chamado **estado terminal**, seguido por um reset para um estado inicial padr√£o ou para uma amostra de uma distribui√ß√£o padr√£o de estados iniciais [^54].

![Diagrama de transi√ß√£o de estados ilustrando um MDP com um estado terminal absorvente.](./../images/image9.png)

> üí° **Exemplo Num√©rico:** Considere um agente aprendendo a navegar em um labirinto. Um epis√≥dio termina quando o agente encontra a sa√≠da. Suponha que o agente recebe uma recompensa de -1 por cada passo que d√° at√© encontrar a sa√≠da e +10 quando encontra a sa√≠da. Se o agente encontra a sa√≠da no passo 10, o retorno $G_0$ seria:
> $G_0 = -1 + (-1) + \ldots + (-1) + 10 = -9 + 10 = 1$ (9 passos com recompensa -1, seguido de um passo com recompensa +10)
> Este retorno representa a recompensa total acumulada ao longo do epis√≥dio.

Para tarefas **cont√≠nuas**, onde a intera√ß√£o n√£o se divide naturalmente em epis√≥dios, a formula√ß√£o do retorno como uma soma simples pode ser problem√°tica, pois $T = \infty$ e o retorno pode se tornar infinito [^54]. Para lidar com essa situa√ß√£o, introduzimos o conceito de **desconto**.

O desconto √© um fator $\gamma \in [0, 1]$ que determina o quanto as recompensas futuras s√£o valorizadas em rela√ß√£o √†s recompensas imediatas. O retorno com desconto √© definido como:

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

Se $\gamma = 0$, o agente se preocupa apenas com a recompensa imediata. Se $\gamma = 1$, o agente se preocupa igualmente com todas as recompensas futuras. Um valor de $\gamma$ entre 0 e 1 indica que o agente valoriza mais as recompensas imediatas do que as recompensas futuras. A escolha de $\gamma$ afeta significativamente o comportamento do agente. Um $\gamma$ pr√≥ximo de 1 incentiva o agente a buscar recompensas de longo prazo, enquanto um $\gamma$ pr√≥ximo de 0 incentiva o agente a buscar recompensas imediatas, mesmo que isso signifique perder recompensas maiores no futuro.

> üí° **Exemplo Num√©rico:**  Suponha que um agente tem a op√ß√£o de receber uma recompensa de +1 imediatamente ou uma recompensa de +10 ap√≥s 5 passos. Vamos calcular o retorno com desconto para diferentes valores de $\gamma$:
> - Se $\gamma = 0$, $G_0 = 1$ (a recompensa imediata √© prefer√≠vel).
> - Se $\gamma = 0.5$, $G_0 = 1 + 0.5^5 * 10 = 1 + 0.3125 = 1.3125$ (a recompensa futura come√ßa a ser mais valorizada, mas a recompensa imediata ainda √© ligeiramente melhor).
> - Se $\gamma = 0.9$, $G_0 = 1 + 0.9^5 * 10 = 1 + 5.9049 = 6.9049$ (a recompensa futura √© muito mais valorizada, e o agente preferir√° esperar pelos 5 passos).
> Este exemplo demonstra como o valor de $\gamma$ influencia a prefer√™ncia do agente por recompensas imediatas ou futuras.

**Observa√ß√£o:** √â importante notar que mesmo em tarefas epis√≥dicas, o uso de desconto pode ser ben√©fico. Ele permite ao agente diferenciar entre epis√≥dios de diferentes dura√ß√µes e priorizar a obten√ß√£o de recompensas mais cedo no epis√≥dio.

**Teorema 1:** Para $| \gamma | < 1$ e recompensas $R_t$ limitadas, o retorno com desconto $G_t$ √© finito.

*Prova:* Seja $|R_t| \leq R_{max}$ para todo $t$. Ent√£o:

$$|G_t| = |\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}| \leq \sum_{k=0}^{\infty} |\gamma|^k |R_{t+k+1}| \leq R_{max} \sum_{k=0}^{\infty} |\gamma|^k = \frac{R_{max}}{1 - |\gamma|}$$

Portanto, $G_t$ √© limitado e, consequentemente, finito.

I. Assumimos que as recompensas s√£o limitadas, ou seja, existe um valor $R_{max}$ tal que $|R_t| \leq R_{max}$ para todo $t$. Tamb√©m assumimos que $|\gamma| < 1$.

II. Come√ßamos com a defini√ß√£o do valor absoluto do retorno com desconto:
    $$|G_t| = |\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}|$$

III. Usamos a propriedade de que o valor absoluto de uma soma √© menor ou igual √† soma dos valores absolutos:
     $$|G_t| \leq \sum_{k=0}^{\infty} |\gamma^k R_{t+k+1}|$$

IV. Usamos a propriedade de que o valor absoluto de um produto √© o produto dos valores absolutos:
    $$|G_t| \leq \sum_{k=0}^{\infty} |\gamma|^k |R_{t+k+1}|$$

V.  Como $|R_{t+k+1}| \leq R_{max}$ para todo $k$, podemos substituir $|R_{t+k+1}|$ por $R_{max}$:
    $$|G_t| \leq \sum_{k=0}^{\infty} |\gamma|^k R_{max}$$

VI. Fatoramos $R_{max}$ para fora da soma, pois √© uma constante:
    $$|G_t| \leq R_{max} \sum_{k=0}^{\infty} |\gamma|^k$$

VII. Reconhecemos que $\sum_{k=0}^{\infty} |\gamma|^k$ √© uma s√©rie geom√©trica com raz√£o $|\gamma|$. Como $|\gamma| < 1$, a s√©rie converge para $\frac{1}{1 - |\gamma|}$:
     $$|G_t| \leq R_{max} \cdot \frac{1}{1 - |\gamma|}$$

VIII. Portanto, $|G_t| \leq \frac{R_{max}}{1 - |\gamma|}$, o que significa que $G_t$ √© limitado. Como $G_t$ √© limitado, ele √© finito. ‚ñ†

> üí° **Exemplo Num√©rico:** Seja $R_{max} = 1$ e $\gamma = 0.9$. Ent√£o, o retorno com desconto m√°ximo poss√≠vel √©:
>  $G_t \leq \frac{1}{1 - 0.9} = \frac{1}{0.1} = 10$
> Isso significa que, mesmo com recompensas m√°ximas em cada passo, o retorno total com desconto √© limitado a 10, devido ao fator de desconto.

### Conclus√£o
A recompensa √© o mecanismo pelo qual comunicamos ao agente o que queremos que ele realize. O agente aprende a maximizar a recompensa que recebe e, portanto, aprende a alcan√ßar nossos objetivos. A sele√ß√£o de recompensas apropriadas √©, portanto, uma parte essencial do projeto de um sistema de aprendizado por refor√ßo. O conceito de recompensa est√° intrinsecamente ligado ao conceito de retorno, que define formalmente o objetivo do agente, seja em tarefas epis√≥dicas ou cont√≠nuas. As escolhas de design sobre como definir recompensas e retornos s√£o, portanto, cruciais para o sucesso de qualquer sistema de aprendizado por refor√ßo.

Al√©m disso, a escolha do fator de desconto $\gamma$ √© fundamental para equilibrar a import√¢ncia das recompensas imediatas versus as recompensas futuras, impactando diretamente o comportamento do agente e sua capacidade de aprender estrat√©gias √≥timas em diferentes ambientes.

### Refer√™ncias
[^53]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd Edition, MIT Press, 2018.
[^54]: Sutton, Richard S.; Barto, Andrew G. *Reinforcement Learning: An Introduction*. 2nd Edition, MIT Press, 2018.
<!-- END -->